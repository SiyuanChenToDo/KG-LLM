{
  "text_to_index": {
    "Depth estimation from Light Field (LF) images is a crucial basis for LF related applications. Since multiple views with abundant information are available, how to effectively fuse features of these views is a key point for accurate LF depth estimation. In this paper, we propose a novel attention-based multi-level fusion network. Combined with the four-branch structure, we design intra-branch fusion strategy and inter-branch fusion strategy to hierarchically fuse effective features from different views. By introducing the attention mechanism, features of views with less occlusions and richer textures are selected inside and between these branches to provide more effective information for depth estimation. The depth maps are finally estimated after further aggregation. Experimental results shows the proposed method achieves state-of-the-art performance in both quantitative and qualitative evaluation, which also ranks first in the commonly used HCI 4D Light Field Benchmark.": 0,
    "How can the accuracy and reliability of depth estimation from light field images be improved, particularly in challenging areas such as occlusions and texture-less regions?": 1,
    "Domain generalization (DG) offers a preferable real-world setting for Person Re-Identification (Re-ID), which trains a model using multiple source domain datasets and expects it to perform well in an unseen target domain without any model updating. Unfortunately, most DG approaches are designed explicitly for classification tasks, which fundamentally differs from the retrieval task Re-ID. Moreover, existing applications of DG in Re-ID cannot correctly handle the massive variation among Re-ID datasets. In this paper, we identify two fundamental challenges in DG for Person Re-ID: domain-wise variations and identity-wise similarities. To this end, we propose an end-to-end Dual Distribution Alignment Network (DDAN) to learn domain-invariant features with dual-level constraints: the domain-wise adversarial feature learning and the identity-wise similarity enhancement. These constraints effectively reduce the domain-shift among multiple source domains further while agreeing to real-world scenarios. We evaluate our method in a large-scale DG Re-ID benchmark and compare it with various cutting-edge DG approaches. Quantitative results show that DDAN achieves state-of-the-art performance.": 2,
    "How can a generalizable Person Re-Identification (Re-ID) model be developed that performs well on unseen target domains without requiring model updates, effectively addressing both domain-wise variations and identity-wise similarities inherent in diverse Re-ID datasets?": 3,
    "When can we expect a text-video retrieval system to work effectively on datasets that differ from its training domain? In this work, we investigate this question through the lens of unsupervised domain adaptation in which the objective is to match natural language queries and video content in the presence of domain shift at query-time. Such systems have significant practical applications since they are capable generalising to new data sources without requiring corresponding text annotations. We make the following contributions: (1) We propose the UDAVR (Unsupervised Domain Adaptation for Video Retrieval) benchmark and employ it to study the performance of text-video retrieval in the presence of domain shift. (2) We propose Concept-Aware-Pseudo-Query (CAPQ), a method for learning discriminative and transferable features that bridge these cross-domain discrepancies to enable effective target domain retrieval using source domain supervision. (3) We show that CAPQ outperforms alternative domain adaptation strategies on UDAVR.": 4,
    "How can a text-video retrieval system effectively generalize to new datasets that exhibit domain shift at query-time, without requiring corresponding text annotations in the target domain?": 5,
    "This paper focuses on the unsupervised domain adaptation problem for video-based crowd counting, in which we use labeled data as source domain and unlabelled video data as target domain. It is challenging as there is a huge gap between the source and the target domain and no annotations of samples are available in the target domain. The key issue is how to utilize unlabelled videos in the target domain for knowledge learning and transferring from the source domain. To tackle this problem, we propose a novel Error-aware Density Isomorphism REConstruction Network (EDIREC-Net) for cross-domain crowd counting. EDIREC-Net jointly transfers a pre-trained counting model to target domains using a density isomorphism reconstruction objective and models the reconstruction erroneousness by error reasoning. Specifically, as crowd flows in videos are consecutive, the density maps in adjacent frames turn out to be isomorphic. On this basis, we regard the density isomorphism reconstruction error as a self-supervised signal to transfer the pre-trained counting models to different target domains. Moreover, we leverage an estimation-reconstruction consistency to monitor the density reconstruction erroneousness and suppress unreliable density reconstructions during training. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method and ablation studies investigate the efficiency and robustness. The source code is available at https://github.com/GehenHe/EDIREC-Net.": 6,
    "How can pre-trained crowd counting models be effectively adapted to new, unlabeled video domains despite significant data distribution differences and the absence of target domain annotations?": 7,
    "Existing techniques to adapt semantic segmentation networks across source and target domains within deep convolutional neural networks (CNNs) deal with all the samples from the two domains in a global or category-aware manner. They do not consider an inter-class variation within the target domain itself or estimated category, providing the limitation to encode the domains having a multi-modal data distribution. To overcome this limitation, we introduce a learnable clustering module, and a novel domain adaptation framework, called cross-domain grouping and alignment. To cluster the samples across domains with an aim to maximize the domain alignment without forgetting precise segmentation ability on the source domain, we present two loss functions, in particular, for encouraging semantic consistency and orthogonality among the clusters. We also present a loss so as to solve a class imbalance problem, which is the other limitation of the previous methods. Our experiments show that our method consistently boosts the adaptation performance in semantic segmentation, outperforming the state-of-the-arts on various domain adaptation settings.": 8,
    "How can semantic segmentation models be effectively adapted from a labeled source domain to an unlabeled target domain, especially when the target domain exhibits multi-modal data distributions, significant inter-class variations, and class imbalance?": 9,
    "Unsupervised domain adaption (UDA) is a promising solution to enhance the generalization ability of a model from a source domain to a target domain without manually annotating labels for target data. Recent works in cross-domain object detection mostly resort to adversarial feature adaptation to match the marginal distributions of two domains. However, perfect feature alignment is hard to achieve and is likely to cause negative transfer due to the high complexity of object detection. In this paper, we propose a category dictionary guided (CDG) UDA model for cross-domain object detection, which learns category-specific dictionaries from the source domain to represent the candidate boxes in target domain. The representation residual can be used for not only pseudo label assignment but also quality (e.g., IoU) estimation of the candidate box. A residual weighted self-training paradigm is then developed to implicitly align source and target domains for detection model training. Compared with decision boundary based classifiers such as softmax, the proposed CDG scheme can select more informative and reliable pseudo-boxes. Experimental results on benchmark datasets show that the proposed CDG significantly exceeds the state-of-the-arts in cross-domain object detection.": 10,
    "How can unsupervised domain adaptation improve the reliability and informativeness of pseudo-labels for object detection in a target domain without labeled data?": 11,
    "We develop an algorithm for adapting a semantic segmentation model that is trained using a labeled source domain to generalize well in an unlabeled target domain. A similar problem has been studied extensively in the unsupervised domain adaptation (UDA) literature, but existing UDA algorithms require access to both the source domain labeled data and the target domain unlabeled data for training a domain agnostic semantic segmentation model. Relaxing this constraint enables a user to adapt pretrained models to generalize in a target domain, without requiring access to source data. To this end, we learn a prototypical distribution for the source domain in an intermediate embedding space. This distribution encodes the abstract knowledge that is learned from the source domain. We then use this distribution for aligning the target domain distribution with the source domain distribution in the embedding space. We provide theoretical analysis and explain conditions under which our algorithm is effective. Experiments on benchmark adaptation task demonstrate our method achieves competitive performance even compared with joint UDA approaches.": 12,
    "How can a semantic segmentation model, initially trained on a labeled source domain, be effectively adapted to generalize well in an unlabeled target domain without requiring concurrent access to the original source data?": 13,
    "Human beings can quickly adapt to environmental changes by leveraging learning experience. However, the poor ability of adapting to dynamic environments remains a major challenge for AI models. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labeled source domain and a sequence of unlabeled target domains. There are two major obstacles in this problem: domain shifts and catastrophic forgetting. In this work, we propose Gradient Regularized Contrastive Learning to solve the above obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforces the gradient of contrastive loss not to increase the supervised training loss on the source domain, which maintains the discriminative power of learned features; (2) regularizes the gradient update on the new domain not to increase the classification loss on the old target domains, which enables the model to adapt to an in-coming target domain while preserving the performance of previously observed domains. Hence our method can jointly learn both semantically discriminative and domain-invariant features with labeled source domain and unlabeled target domains. The experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art.": 14,
    "How can deep neural networks incrementally adapt to new target domains in a sequential manner without losing generalization ability on previously encountered domains, given the challenges of domain shift and catastrophic forgetting?": 15,
    "Although current face anti-spoofing methods achieve promising results under intra-dataset testing, they suffer from poor generalization to unseen attacks. Most existing works adopt domain adaptation (DA) or domain generalization (DG) techniques to address this problem. However, the target domain is often unknown during training which limits the utilization of DA methods. DG methods can conquer this by learning domain invariant features without seeing any target data. However, they fail in utilizing the information of target data. In this paper, we propose a self-domain adaptation framework to leverage the unlabeled test domain data at inference. Specifically, a domain adaptor is designed to adapt the model for test domain. In order to learn a better adaptor, a meta-learning based adaptor learning algorithm is proposed using the data of multiple source domains at the training step. At test time, the adaptor is updated using only the test domain data according to the proposed unsupervised adaptor loss to further improve the performance. Extensive experiments on four public datasets validate the effectiveness of the proposed method.": 16,
    "How can face anti-spoofing methods achieve robust generalization to unseen attacks and domains, especially when target domain data is not available during the training phase?": 17,
    "Many practical applications require the solution of numerically challenging linear programs (LPs) and mixed integer programs (MIPs). Scaling is a widely used preconditioning technique that aims at reducing the error propagation of the involved linear systems, thereby improving the numerical behavior of the dual simplex algorithm and, consequently, LP-based branch-and-bound. A reliable scaling method often makes the difference whether these problems can be solved correctly or not. In this paper, we investigate the use of machine learning to choose at the beginning of the solution process between two common scaling methods: Standard scaling and Curtis-Reid scaling. The latter often, but not always, leads to a more robust solution process, but may suffer from longer solution times.  Rather than training for overall solution time, we propose to use the attention level of a MIP solution process as a learning label. We evaluate the predictive power of a random forest approach and a linear regressor that learns the (square-root of the) difference in attention level. It turns out that the resulting classification not only reduces various types of numerical errors by large margins, but it also improves the performance of the dual simplex algorithm.  The learned model has been implemented within the FICO Xpress MIP solver and it is used by default since release 8.9, May 2020, to determine the scaling algorithm Xpress applies before solving an LP or a MIP.": 18,
    "How can machine learning be effectively utilized to automatically select the most numerically robust scaling method for mixed-integer programs (MIPs) at the beginning of the solution process, thereby mitigating numerical errors and improving solver behavior?": 19,
    "Adapting semantic segmentation models to new domains is an important but challenging problem. Recently enlightening progress has been made, but the performance of existing methods is unsatisfactory on real datasets where the new target domain comprises of heterogeneous sub-domains (e.g. diverse weather characteristics). We point out that carefully reasoning about the multiple modalities in the target domain can improve the robustness of adaptation models. To this end, we propose a condition-guided adaptation framework that is empowered by a special attentive progressive adversarial training (APAT) mechanism and a novel self-training policy. The APAT strategy progressively performs condition-specific alignment and attentive global feature matching. The new self-training scheme exploits the adversarial ambivalences of easy and hard adaptation regions and the correlations among target sub-domains effectively. We evaluate our method (DCAA) on various adaptation scenarios where the target images vary in weather conditions. The comparisons against baselines and the state-of-the-art approaches demonstrate the superiority of DCAA over the competitors.": 20,
    "How can semantic segmentation models be robustly adapted to new target domains that comprise heterogeneous sub-domains with diverse characteristics, such as varying weather conditions?": 21,
    "Domain adaptation on time series data is an important but challenging task. Most of the existing works in this area are based on the learning of the domain-invariant representation of the data with the help of restrictions like MMD. However, such extraction of the domain-invariant representation is a non-trivial task for time series data, due to the complex dependence among the timestamps. In detail, in the fully dependent time series, a small change of the time lags or the offsets may lead to difficulty in the domain invariant extraction. Fortunately, the stability of the causality inspired us to explore the domain invariant structure of the data. To reduce the difficulty in the discovery of causal structure, we relax it to the sparse associative structure and propose a novel sparse associative structure alignment model for domain adaptation. First, we generate the segment set to exclude the obstacle of offsets. Second, the intra-variables and inter-variables sparse attention mechanisms are devised to extract associative structure time-series data with considering time lags. Finally, the associative structure alignment is used to guide the transfer of knowledge from the source domain to the target one. Experimental studies not only verify the good performance of our methods on three real-world datasets but also provide some insightful discoveries on the transferred knowledge.": 22,
    "How can domain-invariant representations be extracted from time series data for domain adaptation, given the complex dependencies among timestamps, including varying time lags and offsets, which hinder the discovery of stable underlying structures?": 23,
    "Despite pre-trained language models such as BERT have achieved appealing performance in a wide range of Natural Language Processing (NLP) tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment data for BERT Knowledge Distillation in target domains with scarce labeled data, by learning a cross-domain manipulation scheme that automatically augments the target domain with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced controller to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on different NLP tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ~13.3%) when only a few labeled examples available.": 24,
    "How can knowledge distillation be effectively applied to compress large pre-trained language models like BERT for deployment in real-time applications, especially when the target domain has scarce training data, which typically leads to performance degradation for the smaller student models?": 25,
    "The problem of unsupervised domain adaptation in semantic segmentation is a major challenge for numerous computer vision tasks because acquiring pixel-level labels is time-consuming with expensive human labor. A large gap exists among data distributions in different domains, which will cause severe performance loss when a model trained with synthetic data is generalized to real data. Hence, we propose a novel domain adaptation approach, called Content Invariant Representation Network, to narrow the domain gap between the source (S) and target (T) domains. The previous works developed a network to directly transfer the knowledge from the S to T. On the contrary, the proposed method aims to progressively reduce the gap between S and T on the basis of a Content Invariant Representation (CIR). CIR is an intermediate domain (I) sharing invariant content with S and having similar data distribution to T. Then, an Ancillary Classifier Module (ACM) is designed to focus on pixel-level details and generate attention-aware results. ACM adaptively assigns different weights to pixels according to their domain offsets, thereby reducing local domain gaps. The global domain gap between CIR and T is also narrowed by enforcing local alignments. Last, we perform self-supervised training in the pseudo-labeled target domain to further fit the distribution of the real data. Comprehensive experiments on two domain adaptation tasks, that is, GTAV → Cityscapes and SYNTHIA → Cityscapes, clearly demonstrate the superiority of our method compared with state-of-the-art methods.": 26,
    "How can the domain gap between source and target domains in semantic segmentation be effectively reduced to improve performance on real-world data without requiring extensive manual labeling?": 27,
    "We propose a hypothesis disparity regularized mutual information maximization~(HDMI) approach to tackle unsupervised hypothesis transfer -- as an effort towards unifying hypothesis transfer learning (HTL) and unsupervised domain adaptation (UDA) -- where the knowledge from a source domain is transferred solely through hypotheses and adapted to the target domain in an unsupervised manner. In contrast to the prevalent HTL and UDA approaches that typically use a single hypothesis, HDMI employs multiple hypotheses to leverage the underlying distributions of the source and target hypotheses. To better utilize the crucial relationship among different hypotheses -- as opposed to unconstrained optimization of each hypothesis independently -- while adapting to the unlabeled target domain through mutual information maximization, HDMI incorporates a hypothesis disparity regularization that coordinates the target hypotheses jointly learn better target representations while preserving more transferable source knowledge with better-calibrated prediction uncertainty. HDMI achieves state-of-the-art adaptation performance on benchmark datasets for UDA in the context of HTL, without the need to access the source data during the adaptation.": 28,
    "How can knowledge be effectively transferred from a source domain to a target domain solely through hypotheses, without requiring access to source data or target labels during adaptation, while addressing the limitations of single-hypothesis approaches and the instability of mutual information maximization in unsupervised settings?": 29,
    "Deep learning based subspace clustering methods have attracted increasing attention in recent years, where a basic theme is to non-linearly map data into a latent space, and then uncover subspace structures based upon the data self-expressiveness property. However, almost all existing deep subspace clustering methods only rely on target domain data, and always resort to shallow neural networks for modeling data, leaving huge room to design more effective representation learning mechanisms tailored for subspace clustering. In this paper, we propose a novel subspace clustering framework through learning precise sample representations. In contrast to previous approaches, the proposed method aims to leverage external data through constructing lots of relevant tasks to guide the training of the encoder, motivated by the idea of meta-learning. Considering limited layer structures of current deep subspace clustering models, we intend to distill knowledge from a deeper network trained on the external data, and transfer it into the shallower model. To reach the above two goals, we propose a new loss function to realize them in a joint framework. Moreover, we propose to construct a new pretext task for self-supervised training of the model, such that the representation ability of the model can be further improved. Extensive experiments are performed on four publicly available datasets, and experimental results clearly demonstrate the efficacy of our method, compared to state-of-the-art methods.": 30,
    "How can more effective representation learning mechanisms be designed for subspace clustering, particularly by leveraging external data and overcoming the limitations of shallow neural networks in existing deep subspace clustering methods?": 31,
    "Unsupervised domain adaptation challenges the problem of transferring knowledge from a well-labelled source domain to an unlabelled target domain. Recently, adversarial learning with bi-classifier has been proven effective in pushing cross-domain distributions close. Prior approaches typically leverage the disagreement between bi-classifier to learn transferable representations, however, they often neglect the classifier determinacy in the target domain, which could result in a lack of feature discriminability. In this paper, we present a simple yet effective method, namely Bi-Classifier Determinacy Maximization (BCDM), to tackle this problem. Motivated by the observation that target samples cannot always be separated distinctly by the decision boundary, here in the proposed BCDM, we design a novel classifier determinacy disparity (CDD) metric, which formulates classifier discrepancy as the class relevance of distinct target predictions and implicitly introduces constraint on the target feature discriminability. To this end, the BCDM can generate discriminative representations by encouraging target predictive outputs to be consistent and determined, meanwhile, preserve the diversity of predictions in an adversarial manner. Furthermore, the properties of CDD as well as the theoretical guarantees of BCDM's generalization bound are both elaborated. Extensive experiments show that BCDM compares favorably against the existing state-of-the-art domain adaptation methods.": 32,
    "How can knowledge be effectively transferred from a labeled source domain to an unlabeled target domain in unsupervised domain adaptation, while ensuring both feature transferability and discriminability, particularly by addressing the lack of classifier determinacy in the target domain?": 33,
    "Unsupervised domain adaptation (UDA) assumes that source and target domain data are freely available and usually trained together to reduce the domain gap. However, considering the data privacy and the inefficiency of data transmission, it is impractical in real scenarios. Hence, it draws our eyes to optimize the network in the target domain without accessing labeled source data. To explore this direction in object detection, for the first time, we propose a source data-free domain adaptive object detection (SFOD) framework via modeling it into a problem of learning with noisy labels. Generally, a straightforward method is to leverage the pre-trained network from the source domain to generate the pseudo labels for target domain optimization. However, it is difficult to evaluate the quality of pseudo labels since no labels are available in target domain. In this paper, self-entropy descent (SED) is a metric proposed to search an appropriate confidence threshold for reliable pseudo label generation without using any handcrafted labels. Nonetheless, completely clean labels are still unattainable. After a thorough experimental analysis, false negatives are found to dominate in the generated noisy labels. Undoubtedly, false negatives mining is helpful for performance improvement, and we ease it to false negatives simulation through data augmentation like Mosaic. Extensive experiments conducted in four representative adaptation tasks have demonstrated that the proposed framework can easily achieve state-of-the-art performance. From another view, it also reminds the UDA community that the labeled source data are not fully exploited in the existing methods.": 34,
    "How can unsupervised domain adaptive object detection be effectively performed in real-world scenarios where access to source domain data is restricted due to privacy concerns or practical limitations?": 35,
    "This paper studies the problem of unsupervised domain adaption in the universal scenario, in which only some of the classes are shared between the source and target domains. We present a scoring scheme that is effective in identifying the samples of the shared classes. The score is used to select samples in the target domain for which to apply specific losses during training; pseudo-labels for high scoring samples and confidence regularization for low scoring samples. Taken together, our method is shown to outperform, by a sizeable margin, the current state of the art on the literature benchmarks.": 36,
    "How can a classification model effectively adapt to a target domain with a different and unknown class distribution, specifically when some classes are shared and others are unique to either the source or target domain, without experiencing negative transfer?": 37,
    "We present *-CFQ (\"star-CFQ\"): a suite of large-scale datasets of varying scope based on the CFQ semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases.": 38,
    "How can the scalability of machine learning systems, particularly Transformers, be systematically investigated in realistic compositional natural language understanding tasks under conditions of fixed computational cost?": 39,
    "Unsupervised domain adaptation for semantic segmentation has been intensively studied due to the low cost of the pixel-level annotation for synthetic data. The most common approaches try to generate images or features mimicking the distribution in the target domain while preserving the semantic contents in the source domain so that a model can be trained with annotations from the latter. However, such methods highly rely on an image translator or feature extractor trained in an elaborated mechanism including adversarial training, which brings in extra complexity and instability in the adaptation process. Furthermore, these methods mainly focus on taking advantage of the labeled source dataset, leaving the unlabeled target dataset not fully utilized. In this paper, we propose a bidirectional style-induced domain adaptation method, called BiSIDA, that employs consistency regularization to efficiently exploit information from the unlabeled target domain dataset, requiring only a simple neural style transfer model. BiSIDA aligns domains by not only transferring source images into the style of target images but also transferring target images into the style of source images to perform high-dimensional perturbation on the unlabeled target images, which is crucial to the success in applying consistency regularization in segmentation tasks. Extensive experiments show that our BiSIDA achieves new state-of-the-art on two commonly-used synthetic-to-real domain adaptation benchmarks: GTA5-to-CityScapes and SYNTHIA-to-CityScapes. Code and pretrained style transfer model are available at: https://github.com/wangkaihong/BiSIDA.": 40,
    "How can the performance of deep learning models for semantic segmentation be optimized on real-world target domains when trained primarily on synthetic source data, given the significant domain gap and the high cost of pixel-level annotations for real images?": 41,
    "Transfer learning, which is to improve the learning performance in the target domain by leveraging useful knowledge from the source domain, often requires that those two domains are very close, which limits its application scope. Recently, distant transfer learning has been studied to transfer knowledge between two distant or even totally unrelated domains via auxiliary domains that are usually unlabeled as a bridge in the spirit of human transitive inference that it is possible to connect two completely unrelated concepts together through gradual knowledge transfer. In this paper, we study distant transfer learning by proposing a DeEp Random Walk basEd distaNt Transfer (DERWENT) method. Different from existing distant transfer learning models that implicitly identify the path of knowledge transfer between the source and target instances through auxiliary instances, the proposed DERWENT model can explicitly learn such paths via the deep random walk technique. Specifically, based on sequences identified by the random walk technique on a data graph where source and target data have no direct edges, the proposed DERWENT model enforces adjacent data points in a squence to be similar, makes the ending data point be represented by other data points in the same sequence, and considers weighted training losses of source data. Empirical studies on several benchmark datasets demonstrate that the proposed DERWENT algorithm yields the state-of-the-art performance.": 42,
    "How can knowledge be effectively transferred between two distant or unrelated domains when direct transfer methods fail, by leveraging auxiliary domains as a bridge?": 43,
    "This paper studies the unsupervised cross-domain translation problem by proposing a generative framework, in which the probability distribution of each domain is represented by a generative cooperative network that consists of an energy-based model and a latent variable model. The use of generative cooperative network enables maximum likelihood learning of the domain model by MCMC teaching, where the energy-based model seeks to fit the data distribution of domain and distills its knowledge to the latent variable model via MCMC. Specifically, in the MCMC teaching process, the latent variable model parameterized by an encoder-decoder maps examples from the source domain to the target domain, while the energy-based model further refines the mapped results by Langevin revision such that the revised results match to the examples in the target domain in terms of the statistical properties, which are defined by the learned energy function. For the purpose of building up a correspondence between two unpaired domains, the proposed framework simultaneously learns a pair of cooperative networks with cycle consistency, accounting for a two-way translation between two domains, by alternating MCMC teaching. Experiments show that the proposed framework is useful for unsupervised image-to-image translation and unpaired image sequence translation.": 44,
    "How can unsupervised cross-domain translation be achieved without paired training examples, by effectively learning the underlying probability distributions and correspondences between two distinct domains?": 45,
    "Despite the recent success of deep reinforcement learning (RL), domain adaptation remains an open problem. Although the generalization ability of RL agents is critical for the real-world applicability of Deep RL, zero-shot policy transfer is still a challenging problem since even minor visual changes could make the trained agent completely fail in the new task. To address this issue, we propose a two-stage RL agent that first learns a latent unified state representation (LUSR) which is consistent across multiple domains in the first stage, and then do RL training in one source domain based on LUSR in the second stage. The cross-domain consistency of LUSR allows the policy acquired from the source domain to generalize to other target domains without extra training. We first demonstrate our approach in variants of CarRacing games with customized manipulations, and then verify it in CARLA, an autonomous driving simulator with more complex and realistic visual observations. Our results show that this approach can achieve state-of-the-art domain adaptation performance in related RL tasks and outperforms prior approaches based on latent-representation based RL and image-to-image translation.": 46,
    "How can reinforcement learning agents achieve zero-shot policy transfer across visually diverse domains without requiring additional training?": 47,
    "In wearable-sensor-based activity recognition, it is often assumed that the training and test samples follow the same data distribution. This assumption neglects practical scenarios where the activity patterns inevitably vary from person to person. To solve this problem, transfer learning and domain adaptation approaches are often leveraged to reduce the gaps between different participants. Nevertheless, these approaches require additional information (i.e., labeled or unlabeled data, meta-information) from the target domain during the training stage. In this paper, we introduce a novel method named Generalizable Independent Latent Excitation (GILE) for human activity recognition, which greatly enhances the cross-person generalization capability of the model. Our proposed method is superior to existing methods in the sense that it does not require any access to the target domain information. Besides, this novel model can be directly applied to various target domains without re-training or fine-tuning. Specifically, the proposed model learns to automatically disentangle domain-agnostic and domain-specific features, the former of which are expected to be invariant across various persons. To further remove correlations between the two types of features, a novel Independent Excitation mechanism is incorporated in the latent feature space. Comprehensive experimental evaluations are conducted on three benchmark datasets to demonstrate the superiority of the proposed method over the state-of-the-art solutions.": 48,
    "How can a model be constructed for wearable-sensor-based human activity recognition that maintains high accuracy across different, previously unseen persons without requiring additional information or retraining for each new user?": 49,
    "Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets -- achieving a new state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that our model can capture interpretable, humor-inducing patterns from all modalities.": 50,
    "How can an algorithm effectively recognize humor from video utterances by integrating verbal and non-verbal components, appropriate context, and external knowledge?": 51,
    "Neural machine translation often adopts the fine-tuning approach to adapt to specific domains. However, nonrestricted fine-tuning can easily degrade on the general domain and over-fit to the target domain. To mitigate the issue, we propose Prune-Tune, a novel domain adaptation method via gradual pruning. It learns tiny domain-specific sub-networks during fine-tuning on new domains. Prune-Tune alleviates the over-fitting and the degradation problem without model modification. Furthermore, Prune-Tune is able to sequentially learn a single network with multiple disjoint domain-specific sub-networks for multiple domains. Empirical experiment results show that Prune-Tune outperforms several strong competitors in the target domain test set without sacrificing the quality on the general domain in both single and multi-domain settings. The source code and data are available at https://github.com/ohlionel/Prune-Tune.": 52,
    "How can Neural Machine Translation models be effectively adapted to specific domains while mitigating issues of overfitting to target domains and catastrophic forgetting of general domain knowledge, especially in low-resource settings?": 53,
    "Multi-task learning (MTL) has been widely applied in Natural Language Processing. A major task and its associated auxiliary tasks share the same encoder; hence, an MTL encoder can learn the sharing abstract information between the major and auxiliary tasks. Task-specific towers are then employed upon the sharing encoder to learn task-specific information. Previous works demonstrated that exchanging information between task-specific towers yielded extra gains. This is known as soft-parameter sharing MTL. In this paper, we propose a novel gating mechanism for the bridging of MTL towers. Our method is evaluated based on aspect-based sentiment analysis and sequential metaphor identification tasks. The experiments demonstrate that our method can yield better performance than the baselines on both tasks. Based on the same Transformer backbone, we compare our gating mechanism with other information transformation mechanisms, e.g., cross-stitch, attention and vanilla gating. The experiments show that our method also surpasses these baselines.": 54,
    "How can information exchange between task-specific towers in multi-task learning be optimized to selectively utilize useful information from auxiliary tasks while filtering out irrelevant or noisy information, thereby enhancing performance on major tasks?": 55,
    "Despite the near-human performances already achieved on formal texts such as news articles, neural machine translation still has difficulty in dealing with \"user-generated\" texts that have diverse linguistic phenomena but lack large-scale high-quality parallel corpora. To address this problem, we propose a counterfactual domain adaptation method to better leverage both large-scale source-domain data (formal texts)  and small-scale target-domain data (informal texts). Specifically, by considering effective counterfactual conditions (the concatenations of source-domain texts and the target-domain tag), we construct the counterfactual representations to fill the sparse latent space of the target domain caused by a small amount of data, that is, bridging the gap between the source-domain data and the target-domain data. Experiments on English-to-Chinese and Chinese-to-English translation tasks show that our method outperforms the base model that is trained only on the informal corpus by a large margin, and consistently surpasses different baseline methods by +1.12 ~ 4.34 BLEU points on different datasets. Furthermore, we also show that our method achieves competitive performances on cross-domain language translation on four language pairs.": 56,
    "How can neural machine translation models effectively handle user-generated informal texts that exhibit diverse linguistic phenomena but lack large-scale, high-quality parallel corpora, especially when trained on formal texts?": 57,
    "A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. ``Reading papers can be dull sometimes,like watching grass grow\". Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. However, none of existing work has explored neural simile interpolation, including both locating and generation. In this paper, we propose a new task of Writing Polishment with Simile (WPS) to investigate whether machines are able to polish texts with similes as we human do. Accordingly, we design a two-staged Locate&Gen model based on transformer architecture. Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. We also release a large-scale Chinese Simile (CS) dataset containing 5 million similes with context. The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment.": 58,
    "Is it possible to develop a system that can automatically enhance plain text by inserting appropriate similes, thereby improving the expressiveness and appeal of the writing?": 59,
    "Cross-domain aspect-based sentiment analysis aims to utilize the useful knowledge in a source domain to extract aspect terms and predict their sentiment polarities in a target domain. Recently, methods based on adversarial training have been applied to this task and achieved promising results. In such methods, both the source and target data are utilized to learn domain-invariant features through deceiving a domain discriminator. However, the task classifier is only trained on the source data, which causes the aspect and sentiment information lying in the target data can not be exploited by the task classifier. In this paper, we propose an Adaptive Hybrid Framework (AHF) for cross-domain aspect-based sentiment analysis. We integrate pseudo-label based semi-supervised learning and adversarial training in a unified network. Thus the target data can be used not only to align the features via the training of domain discriminator, but also to refine the task classifier. Furthermore, we design an adaptive mean teacher as the semi-supervised part of our network, which can mitigate the effects of noisy pseudo labels generated on the target data. We conduct experiments on four public datasets and the experimental results show that our framework significantly outperforms the state-of-the-art methods.": 60,
    "How can the framework effectively leverage unlabeled target data for refining the task classifier and learning domain-invariant features in cross-domain aspect-based sentiment analysis?": 61,
    "Existing methods for named entity recognition (NER) are critically relied on the amount of labeled data. However, these methods suffer from performance decline in a new domain which is fully-unlabeled. To handle the situation, we propose an entity-aware adversarial domain adaptation network, which utilizes the labeled data from source domain and then adapts to unlabeled target domain. We first apply adversarial training to reduce the distribution gap between different domains. Furthermore, we introduce an entity-aware attention to guide adversarial to achieve the alignment of entity features. The experimental results show that our model outperforms the state-of-the-art approaches.": 62,
    "How can named entity recognition models adapt to fully-unlabeled target domains when critically relying on labeled data from a source domain, especially when direct adversarial training fails to achieve fine-grained entity-level alignment?": 63,
    "Recently, various image-to-image translation (I2I) methods have improved mode diversity and visual quality in terms of neural networks or regularization terms. However, conventional I2I methods relies on a static decision boundary and the encoded representations in those methods are entangled with each other, so they often face with ‘mode collapse’ phenomenon. To mitigate mode collapse, 1) we design a so-called style-guided discriminator that guides an input image to the target image style based on the strategy of flexible decision boundary. 2) Also, we make the encoded representations include independent domain attributes. Based on two ideas, this paper proposes Style-Guided and Disentangled Representation for Robust Image-to-Image Translation (SRIT). SRIT showed outstanding FID by 8%, 22.8%, and 10.1% for CelebA-HQ, AFHQ, and Yosemite datasets, respectively. The translated images of SRIT reflect the styles of target domain successfully. This indicates that SRIT shows better mode diversity than previous works.": 64,
    "How can the phenomenon of 'mode collapse' be mitigated in image-to-image translation tasks to improve mode diversity and visual quality?": 65,
    "Source-Free Unsupervised Domain Adaptation(SFUDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to the original labeled source domain samples. Many existing SFUDA approaches apply the self-training strategy, which involves iteratively selecting confidently predicted target samples as pseudo-labeled samples used to train the model to fit the target domain. However, the self-training strategy may also suffer from  sample selection bias and be impacted by the label noise of the pseudo-labeled samples. In this work, we provide a rigorous theoretical analysis on how these two issues affect the model generalization ability when applying the self-training strategy for the SFUDA problem. Based on this theoretical analysis, we then propose a new Denoised Maximum Classifier Discrepancy (D-MCD) method for SFUDA to effectively address these two issues. In particular, we first minimize the distribution mismatch between the selected pseudo-labeled samples and the remaining target domain samples to alleviate the sample selection bias. Moreover, we design a strong-weak self-training paradigm to denoise the selected pseudo-labeled samples, where the strong network is used to select pseudo-labeled samples while the weak network helps the strong network to filter out hard samples to avoid incorrect labels. In this way, we are able to ensure both the quality of the pseudo-labels and the generalization ability of the trained model on the target domain. We achieve state-of-the-art results on three domain adaptation benchmark datasets, which clearly validates the effectiveness of our proposed approach. Full code is available at https://github.com/kkkkkkon/D-MCD.": 66,
    "How can a pre-trained source model be effectively adapted to an unlabeled target domain without access to the original labeled source domain data, while mitigating the inherent challenges of sample selection bias and label noise in pseudo-labeled samples?": 67,
    "Clustering-based unsupervised domain adaptive (UDA) person re-identification (ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature embedding and imperfect clustering, pseudo labels for target domain data inherently contain an unknown proportion of wrong ones, which would mislead feature learning. In this paper, we propose an approach named probabilistic uncertainty guided progressive label refinery (P2LR) for domain adaptive person re-identification. First, we propose to model the labeling uncertainty with the probabilistic distance along with ideal single-peak distributions. A quantitative criterion is established to measure the uncertainty of pseudo labels and facilitate the network training. Second, we explore a progressive strategy for refining pseudo labels. With the uncertainty-guided alternative optimization, we balance between the exploration of target domain data and the negative effects of noisy labeling. On top of a strong baseline, we obtain significant improvements and achieve the state-of-the-art performance on four UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5% mAP on the Duke2Market task, while surpassing the state-of-the-art method by 2.5% mAP on the Market2MSMT task. Code is available at: https://github.com/JeyesHan/P2LR.": 68,
    "How can the inherent uncertainty and noise in pseudo labels generated for target domain data in unsupervised domain adaptive person re-identification be effectively identified and mitigated to prevent misleading feature learning and improve model performance?": 69,
    "Clustering is important for domain adaptive person re-identification(re-ID). A majority of unsupervised domain adaptation (UDA) methods conduct clustering on the target domain and then use the generated pseudo labels for adaptive training. Albeit important, the clustering pipeline adopted by current literature is quite standard and lacks consideration for two characteristics of re-ID, i.e., 1) a single person has various feature distribution in multiple cameras. 2) a person’s occurrence in the same camera are usually temporally continuous. We argue that the multi-camera distribution hinders clustering because it enlarges the intra-class distances. In contrast, the temporal continuity prior is beneficial, because it offers clue for distinguishing some look-alike person (who are temporally far away from each other). These two insight motivate us to propose a novel Divide-And-Regroup Clustering (DARC) pipeline for re-ID UDA. Specifically, DARC divides the unlabeled data into multiple camera-specific groups and conducts local clustering within each camera. Afterwards, it regroups those local clusters potentially belonging to the same person into a unity. Through this divide-and-regroup pipeline, DARC avoids directly clustering across multiple cameras and focuses on the feature distribution within each individual camera. Moreover, during the local clustering, DARC uses the temporal continuity prior to distinguish some look-alike person and thus reduces false positive pseudo labels. Consequentially, DARC effectively reduces clustering errors and improves UDA. Importantly, we show that DARC is compatible to many pseudo label-based UDA methods and brings general improvement. Based on a recent UDA method, DARC advances the state of the art (e.g, 85.1% mAP on MSMT-to-Market and 83.1% mAP on PersonX-to-Market).": 70,
    "How can clustering be improved in domain adaptive person re-identification (re-ID) to reduce ID switch and ID split errors while leveraging the temporal continuity and multi-camera distribution characteristics?": 71,
    "Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains. We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA. Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output. However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST). During adaptation, we employ the AST auto-encoder for two purposes. First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent. Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype. Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment. We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks.": 72,
    "How can knowledge be effectively transferred from a labeled source domain to an unlabeled, multi-modal target domain, ensuring robust generalization to novel, unseen domains, without relying on complex, deployment-unfriendly mechanisms such as adversarial alignment, sub-target clustering, or online model updates?": 73,
    "The domain gap severely limits the transferability and scalability of object detectors trained in a specific domain when applied to a novel one. Most existing works bridge the domain gap by minimizing the domain discrepancy in the category space and aligning category-agnostic global features. Though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level distribution. Besides, the category-agnostic alignment leads to the disagreement of class-specific distributions in the two domains, further causing inevitable classification errors. To overcome these two challenges, we propose a novel Semantic Conditioned AdaptatioN (SCAN) framework such that well-modeled unbiased semantics can support semantic conditioned adaptation for precise domain adaptive object detection. Specifically, class-specific semantics crossing different images in the source domain are graphically aggregated as the input to learn an unbiased semantic paradigm incrementally. The paradigm is then sent to a lightweight manifestation module to obtain conditional kernels to serve as the role of extracting semantics from the target domain for better adaptation. Subsequently, conditional kernels are integrated into global alignment to support the class-specific adaptation in a well-designed Conditional Kernel guided Alignment (CKA) module. Meanwhile, rich knowledge of the unbiased paradigm is transferred to the target domain with a novel Graph-based Semantic Transfer (GST) mechanism, yielding the adaptation in the category-based feature space. Comprehensive experiments conducted on three adaptation benchmarks demonstrate that SCAN outperforms existing works by a large margin.": 74,
    "How can object detectors trained in a specific domain effectively transfer their capabilities to a novel, unlabeled domain despite significant domain discrepancies, particularly by addressing biased semantic estimations and the lack of class-specific alignment in global features?": 75,
    "Person re-identifcation (Re-ID) based on unsupervised domain adaptation (UDA) aims to transfer the pre-trained model from one labeled source domain to an unlabeled target domain. Existing methods tackle this problem by using clustering methods to generate pseudo labels. However, pseudo labels produced by these techniques may be unstable and noisy, substantially deteriorating models’ performance. In this paper, we propose a Reliability Exploration with Self-ensemble Learning (RESL) framework for domain adaptive person ReID. First, to increase the feature diversity, multiple branches are presented to extract features from different data augmentations. Taking the temporally average model as a mean teacher model, online label refning is conducted by using its dynamic ensemble predictions from different branches as soft labels. Second, to combat the adverse effects of unreliable samples in clusters, sample reliability is estimated by evaluating the consistency of different clusters’ results, followed by selecting reliable instances for training and re-weighting sample contribution within Re-ID losses. A contrastive loss is also utilized with cluster-level memory features which are updated by the mean feature. The experiments demonstrate that our method can signifcantly surpass the state-of-the-art performance on the unsupervised domain adaptive person ReID.": 76,
    "How can the performance of unsupervised domain adaptive person re-identification models be improved by effectively addressing the instability and noise inherent in pseudo-labels generated through clustering methods?": 77,
    "Domain generalization typically requires data from multiple source domains for model learning. However, such strong assumption may not always hold in practice, especially in medical field where the data sharing is highly concerned and sometimes prohibitive due to privacy issue. This paper studies the important yet challenging single domain generalization problem, in which a model is learned under the worst-case scenario with only one source domain to directly generalize to different unseen target domains. We present a novel approach to address this problem in medical image segmentation, which extracts and integrates the semantic shape prior information of segmentation that are invariant across domains and can be well-captured even from single domain data to facilitate segmentation under distribution shifts. Besides, a test-time adaptation strategy with dual-consistency regularization is further devised to promote dynamic incorporation of these shape priors under each unseen domain to improve model generalizability. Extensive experiments on two medical image segmentation tasks demonstrate the consistent improvements of our method across various unseen domains, as well as its superiority over state-of-the-art approaches in addressing domain generalization under the worst-case scenario.": 78,
    "How can a deep learning model, trained with data from only a single source domain, effectively generalize to diverse and unseen target domains, particularly in medical image segmentation where data sharing is restricted and domain discrepancies are significant?": 79,
    "In this paper, we tackle the problem of one-shot unsupervised domain adaptation (OSUDA) for semantic segmentation where the segmentors only see one unlabeled target image during training. In this case, traditional unsupervised domain adaptation models usually fail since they cannot adapt to the target domain with over-fitting to one (or few) target samples. To address this problem, existing OSUDA methods usually integrate a style-transfer module to perform domain randomization based on the unlabeled target sample, with which multiple domains around the target sample can be explored during training. However, such a style-transfer module relies on an additional set of images as style reference for pre-training and also increases the memory demand for domain adaptation. Here we propose a new OSUDA method that can effectively relieve such computational burden. Specifically, we integrate several style-mixing layers into the segmentor which play the role of style-transfer module to stylize the source images without introducing any learned parameters. Moreover, we propose a patchwise prototypical matching (PPM) method to weighted consider the importance of source pixels during the supervised training to relieve the negative adaptation. Experimental results show that our method achieves new state-of-the-art performance on two commonly used benchmarks for domain adaptive semantic segmentation under the one-shot setting and is more efficient than all comparison approaches.": 80,
    "How can semantic segmentation models effectively adapt to a new target domain when only a single unlabeled image from that domain is available for adaptation, without incurring high computational costs or requiring additional pre-training data?": 81,
    "Unsupervised domain adaptation (UDA) has been highly successful in transferring knowledge acquired from a label-rich source domain to a label-scarce target domain. Open-set domain adaptation (open-set DA) and universal domain adaptation (UniDA) have been proposed as solutions to the problem concerning the presence of additional novel categories in the target domain. Existing open-set DA and UniDA approaches treat all novel categories as one unified unknown class and attempt to detect this unknown class during the training process. However, the features of the novel categories learned by these methods are not discriminative. This limits the applicability of UDA in the further classification of these novel categories into their original categories, rather than assigning them to a single unified class. In this paper, we propose a self-labeling framework to cluster all target samples, including those in the ''unknown'' categories. We train the network to learn the representations of target samples via self-supervised learning (SSL) and to identify the seen and unseen (novel) target-sample categories simultaneously by maximizing the mutual information between labels and input data. We evaluated our approach under different DA settings and concluded that our method generally outperformed existing ones by a wide margin.": 82,
    "How can novel categories in a target domain be accurately discovered and classified into their distinct semantic classes, rather than being grouped into a single \"\"unknown\"\" category, within a domain adaptation setting?": 83,
    "In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ``small\" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently.": 84,
    "How can backdoors to Mixed Integer Linear Programs (MIPs) be effectively and efficiently found, particularly addressing the exploration-exploitation imbalance of existing methods?": 85,
    "Anomaly detection attempts to find examples in a dataset that do not conform to the expected behavior. Algorithms for this task assign an anomaly score to each example representing its degree of anomalousness. Setting a threshold on the anomaly scores enables converting these scores into a discrete prediction for each example. Setting an appropriate threshold is challenging in practice since anomaly detection is often treated as an unsupervised problem. A common approach is to set the threshold based on the dataset's contamination factor, i.e., the proportion of anomalous examples in the data. While the contamination factor may be known based on domain knowledge, it is often necessary to estimate it by labeling data. However, many anomaly detection problems involve monitoring multiple related, yet slightly different entities (e.g., a fleet of machines). Then, estimating the contamination factor for each dataset separately by labeling data would be extremely time-consuming. Therefore, this paper introduces a method for transferring the known contamination factor from one dataset (the source domain) to a related dataset where it is unknown (the target domain). Our approach does not require labeled target data and is based on modeling the shape of the distribution of the anomaly scores in both domains. We theoretically analyze how our method behaves when the (biased) target domain anomaly score distribution converges to its true one. Empirically, our method outperforms several baselines on real-world datasets.": 86,
    "How can the known contamination factor from one anomaly detection domain be effectively transferred to a related target domain where it is unknown, without requiring labeled target data?": 87,
    "Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.": 88,
    "How can a pre-trained language model like BERT be efficiently applied to unsupervised cross-domain sentiment analysis, given its inherent task-agnostic nature and lack of domain awareness, especially when transferring knowledge between source and target domains with limited labeled data?": 89,
    "Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.": 90,
    "How can multi-domain task-oriented dialog systems effectively transfer knowledge from data-rich domains to data-poor domains while maintaining high performance?": 91,
    "Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.": 92,
    "How can the performance degradation of Chinese Word Segmentation models be alleviated when applied to out-of-domain data, considering both distribution gaps and out-of-vocabulary issues?": 93,
    "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.": 94,
    "How can computational systems effectively perceive human humor by accurately detecting and locating puns within sentences, especially given their reliance on implicit semantic and phonological tricks?": 95,
    "Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.": 96,
    "How can a computational model generate creative and grammatically correct homophonic puns that effectively exploit the sound similarity between two words while expressing their distinct semantic meanings?": 97,
    "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality \\textit{and} reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.": 98,
    "How can the performance of iterative back-translation in neural machine translation be further improved by dynamically selecting and weighting monolingual data to ensure high quality and target domain relevance?": 99,
    "Literary tropes, from poetry to stories, are at the crux of human imagination\r\nand communication. Figurative language such as a simile go beyond plain\r\nexpressions to give readers new insights and inspirations. In this paper, we\r\ntackle the problem of simile generation. Generating a simile requires proper\r\nunderstanding for effective mapping of properties between two concepts. To this\r\nend, we first propose a method to automatically construct a parallel corpus by\r\ntransforming a large number of similes collected from Reddit to their literal\r\ncounterpart using structured common sense knowledge. We then propose to\r\nfine-tune a pretrained sequence to sequence model, BART~\\cite{lewis2019bart},\r\non the literal-simile pairs to gain generalizability, so that we can generate\r\nnovel similes given a literal sentence. Experiments show that our approach\r\ngenerates $88\\%$ novel similes that do not share properties with the training\r\ndata. Human evaluation on an independent set of literal statements shows that\r\nour model generates similes better than two literary experts\r\n\\textit{37\\%}\\footnote{We average 32.6\\% and 41.3\\% for 2 humans.} of the\r\ntimes, and three baseline systems including a recent metaphor generation model\r\n\\textit{71\\%}\\footnote{We average 82\\% ,63\\% and 68\\% for three baselines.} of\r\nthe times when compared pairwise.\\footnote{The simile in the title is generated\r\nby our best model. Input: Generating similes effortlessly, output: Generating\r\nsimiles \\textit{like a Pro}.} We also show how replacing literal sentences with\r\nsimiles from our best model in machine generated stories improves evocativeness\r\nand leads to better acceptance by human judges.": 100,
    "How can a computational model generate novel and high-quality similes from literal sentences that effectively map properties between concepts?": 101,
    "The new version of the International Classification of Diseases (ICD‐11) mentions the existence of four different profiles in the verbal part of the Autism Spectrum Disorder (ASD), describing them as combinations of either spared or impaired functional language and intellectual abilities. The aim of the present study was to put ASD heterogeneity to the forefront by exploring whether clear profiles related to language and intellectual abilities emerge when investigation is extended to the entire spectrum, focusing on verbal children. Our study proposed a systematic investigation of both language (specifically, structural language abilities) and intellectual abilities (specifically, nonverbal cognitive abilities) in 51 6‐ to 12‐year‐old verbal children with ASD based on explicitly motivated measures. For structural language abilities, sentence repetition and nonword repetition tasks were selected; for nonverbal cognitive abilities, we chose Raven's Progressive Matrices, as well as Matrix Reasoning and Block Design from the Wechsler Scales. An integrative approach based on cluster analyses revealed five distinct profiles. Among these five profiles, all four logically possible combinations of structural language and nonverbal abilities mentioned in the ICD‐11 were detected. Three profiles emerged among children with normal language abilities and two emerged among language‐impaired children. Crucially, the existence of discrepant profiles of abilities suggests that children with ASD can display impaired language in presence of spared nonverbal intelligence or spared language in the presence of impaired nonverbal intelligence, reinforcing the hypothesis of the existence of a separate language module in the brain. Autism Res 2020, 13: 1155‐1167. © 2020 International Society for Autism Research, Wiley Periodicals, Inc.": 102,
    "How can the computational study of exaggeration be advanced through the creation and analysis of a cross-lingual dataset and the application of deep learning techniques?": 103,
    "Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA which predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.": 104,
    "How can the features of pre-trained language models be effectively adapted to new domains and languages without fine-tuning the models themselves, while simultaneously improving the robustness of self-training methods against noisy pseudo-labels?": 105,
    "Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.": 106,
    "How can grammatical error correction systems be effectively evaluated and improved for open-domain text with low error density, moving beyond traditional high error density learner essays?": 107,
    "Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al., 2019b) and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach advances the state-of-the-art results by a large margin. Specifically, we improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between theory and the loss function used in the original work Zhang et al.(2019b), and thereby significantly boosts the performance. Our numerical results also indicate that VAT can remarkably improve the generalization performance of both domains for various domain adaptation approaches.": 108,
    "How can a well-performing model be learned for a target domain with unavailable labels by leveraging labeled data from a source domain, particularly in cross-lingual text labeling scenarios?": 109,
    "Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.": 110,
    "How can reading comprehension models be made more robust to adversarial attacks and simultaneously improve their generalization capabilities across different domains and languages?": 111,
    "We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to train a good target model. A straightforward solution is to fine-tune a pre-trained source model by using those limited labeled target data, but it usually cannot work well due to the considerable difference between the data distributions of the source and target domains. Moreover, the availability of multiple modalities (i.e., images, questions and answers) in VQA poses further challenges in modeling the transferability between various modalities. In this paper, we address the above issues by proposing a novel supervised multi-modal domain adaptation method for VQA to learn joint feature embeddings across different domains and modalities. Specifically, we align the data distributions of the source and target domains by considering those modalities both jointly and separately. Extensive experiments on the benchmark VQA 2.0 and VizWiz datasets demonstrate that our proposed method outperforms the existing state-of-the-art baselines for open-ended VQA in this challenging domain adaptation setting.": 112,
    "How can visual question answering models effectively transfer knowledge from a data-rich source domain to a target domain with limited labeled data, especially when significant distribution differences exist across multiple modalities?": 113,
    "Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level identification by treating the task as either single-word classification or sequential labelling without explicitly modelling the interaction between the metaphor components. On the other hand, while existing relation-level approaches implicitly model this interaction, they ignore the context where the metaphor occurs. In this work, we address these limitations by introducing a novel architecture for identifying relation-level metaphoric expressions of certain grammatical relations based on contextual modulation. In a methodology inspired by works in visual reasoning, our approach is based on conditioning the neural network computation on the deep contextualised features of the candidate expressions using feature-wise linear modulation. We demonstrate that the proposed architecture achieves state-of-the-art results on benchmark datasets. The proposed methodology is generic and could be applied to other textual classification problems that benefit from contextual interaction.": 114,
    "The automation of metaphor identification is challenging, particularly for relation-level expressions, due to the need to comprehend underlying comparisons and explicitly model the interaction between metaphor components within their context. How can a novel architecture address these limitations by effectively integrating deep contextualized features of candidate expressions with the wider sentence context for relation-level metaphor identification?": 115,
    "Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between distant domains (e.g., movie subtitles and research papers), however, cannot be performed effectively due to mismatches in vocabulary; it will encounter many domain-specific words (e.g., “angstrom”) and words whose meanings shift across domains (e.g., “conductor”). In this study, aiming to solve these vocabulary mismatches in domain adaptation for neural machine translation (NMT), we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pretrained NMT model to the target domain. Prior to fine-tuning, our method replaces the embedding layers of the NMT model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space. Experimental results indicate that our method improves the performance of conventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En translation, respectively.": 116,
    "How can vocabulary mismatches, including domain-specific words and semantic shifts, be effectively addressed in neural machine translation when adapting models between distant domains?": 117,
    "Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacy-aware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.": 118,
    "How can transfer learning be leveraged to enhance recommendation performance while ensuring the protection of private user attributes in the source domain?": 119,
    "One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets.": 120,
    "How can dialogue systems and user simulators be jointly trained to effectively converse using natural language and adapt to new, low-resource dialogue scenarios through self-play?": 121,
    "Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. In this paper, we introduce MultiMET, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents. MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. Moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. MultiMET will be released publicly for research.": 122,
    "How can multimodal information, integrating text and image data, be utilized to enhance the automatic understanding and detection of metaphors?": 123,
    "Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word. Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling. In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts. We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts. We evaluate our method on the VUA, MOH-X and TroFi datasets. Our method gets competitive results compared with state-of-the-art approaches.": 124,
    "How can computers accurately distinguish between the literal and metaphorical senses of a verb within natural language, moving beyond traditional token-level classification?": 125,
    "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.": 126,
    "How can knowledge be effectively transferred from a trained source model to an unlabeled target domain for unsupervised domain adaptation when access to source data is restricted and the target model may have a different network architecture?": 127,
    "Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspect–opinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of contextual information w.r.t. the aspect and opinion are further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks.": 128,
    "How can aspect-based sentiment analysis models achieve a comprehensive understanding of contextual information and explicitly leverage relations between subtasks to accurately identify sentiment polarities, especially when dealing with multiple aspect-opinion pairs in a sentence?": 129,
    "To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries. In this work, we introduce a transformation matrix that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for quotation and another for mapped-query). Furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. Experiments on two datasets in English and Chinese show that our model outperforms previous state-of-the-art models.": 130,
    "How can quotation recommendation systems be improved by directly modeling the relationship between user queries and suitable quotations, especially when they reside in different semantic spaces?": 131,
    "In this paper, we challenge the assumption that political ideology is inherently built into text by presenting an investigation into the impact of experiential factors on annotator perceptions of political ideology. We construct an annotated corpus of U.S. political discussion, where in addition to ideology labels for texts, annotators provide information about their political affiliation, exposure to political news, and familiarity with the source domain of discussion, Reddit. We investigate the variability in ideology judgments across annotators, finding evidence that these experiential factors may influence the consistency of how political ideologies are perceived. Finally, we present evidence that understanding how humans perceive and interpret ideology from texts remains a challenging task for state-of-the-art language models, pointing towards potential issues when modeling user experiences that may require more contextual knowledge.": 132,
    "How do experiential factors of human annotators influence the perception and labeling of political ideology in text, and how well can state-of-the-art language models mirror these human perceptions?": 133,
    "The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model on a source domain and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP tasks, the output categories substantially differ between domains, making adaptation even more challenging. This, for example, happens in the task of aspect extraction, where the aspects of interest of reviews of, e.g., restaurants or electronic devices may be very different. This paper presents a new fine-tuning scheme for BERT, which aims to address the above challenges. We name this scheme DILBERT: Domain Invariant Learning with BERT, and customize it for aspect extraction in the unsupervised domain adaptation setting. DILBERT harnesses the categorical information of both the source and the target domains to guide the pre-training process towards a more domain and category invariant representation, thus closing the gap between the domains. We show that DILBERT yields substantial improvements over state-of-the-art baselines while using a fraction of the unlabeled data, particularly in more challenging domain adaptation setups.": 134,
    "How can pre-trained language models be effectively adapted to new target domains for tasks like aspect extraction, particularly when there is a substantial shift in output categories between domains and limited labeled data in the target domain?": 135,
    "Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse information. In this paper, we propose a model augmented with hierarchical contextualized representation to extract more information from both sentence-level and discourse-level. At the sentence level, we leverage the metaphor information of words that except the target word in the sentence to strengthen the reasoning ability of our model via a novel label-enhanced contextualized representation. At the discourse level, the position-aware global memory network is adopted to learn long-range dependency among the same words within a discourse. Finally, our model combines the representations obtained from these two parts. The experiment results on two tasks of the VUA dataset show that our model outperforms every other state-of-the-art method that also does not use any external knowledge except what the pre-trained language model contains.": 136,
    "How can sequential metaphor identification models be augmented to extract more comprehensive information from both sentence-level and broader discourse-level contexts, moving beyond solely relying on contextual text features within a single sentence or external linguistic features?": 137,
    "Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS tagging, presenting a fine-grained domain adaption method to model the gaps accurately. We measure the gaps by one simple and intuitive metric, and adopt it to develop a pseudo target domain corpus based on fine-grained subdomains incrementally. A novel domain-mixed representation learning model is proposed accordingly to encode the multiple subdomains effectively. The whole process is performed progressively for both corpus construction and model training. Experimental results on a benchmark dataset show that our method can gain significant improvements over a vary of baselines. Extensive analyses are performed to show the advantages of our final domain adaption model as well.": 138,
    "How can the quality of pseudo-training instances in self-training be improved for Chinese lexical processing tasks like joint word segmentation and POS tagging, especially when target domain sentences are significantly different from the source domain?": 139,
    "Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CATE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained model to obtain a contextual representation of target words and employs a contrastive objective to promote an increased distance between target words’ literal and metaphorical senses based on linguistic theories. Furthermore, we propose a simple strategy to collect large-scale candidate instances from the general corpus and generalize the model via self-training. Extensive experiments show that CATE achieves better performance against state-of-the-art baselines on several benchmark datasets.": 140,
    "How can metaphor detection models effectively discriminate between literal and metaphorical meanings of words in context, especially when limited labeled data is available?": 141,
    "Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a framework to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of humor and non-humor. We present the largest dataset to date with labeled humor on 785K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in social media by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of humor on social media.": 142,
    "How can a large-scale dataset with reliable humor labels be collected, given the substantial individual and cultural differences in humor perception that make manual annotation difficult and existing automatic methods limited in scope or accuracy?": 143,
    "While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and relevance. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters/objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in several metrics (and human evaluation) for multiple datasets. Finally, we provide an analysis of the linguistic and visuo-spatial information. Code and data: https://github.com/adymaharana/VLCStoryGan.": 144,
    "How can story visualization models effectively integrate linguistic, visuospatial, and commonsense structures to generate high-quality, consistent, and relevant image sequences from narrative text?": 145,
    "In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA). While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between target domain and synthetic data distribution, and reduces model overfitting to source domain. We run UDA experiments on question generation and passage retrieval from the Natural Questions domain to machine learning and biomedical domains. We find that back-training vastly outperforms self-training by a mean improvement of 7.8 BLEU-4 points on generation, and 17.6% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation dataset - MLQuestions containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs.": 146,
    "How can unsupervised domain adaptation be effectively achieved for tasks like question generation and passage retrieval, mitigating issues such as model overfitting and distributional shift inherent in traditional self-training approaches?": 147,
    "Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible.": 148,
    "How can the robustness of dialog models be improved when faced with input dialog contexts that exhibit the use of figurative language?": 149,
    "Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task.": 150,
    "Given a strong general-purpose neural machine translation model, how can its performance be optimized on multiple new, diverse domains of interest without compromising its existing generic domain performance?": 151,
    "Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from \"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including metaphor and metonymy.": 152,
    "How can a finite lexicon be adapted to express an unbounded set of emerging ideas, specifically by predicting the formation of novel verb-noun compositions through time?": 153,
    "Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our model achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.": 154,
    "How can zero-shot cross-domain slot filling methods effectively transfer knowledge to the target domain and improve performance on unseen slots, rather than merely fitting the distribution of seen slots?": 155,
    "Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question-answer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when the text corpora is of limited size. This is problematic as many niche domains rely on small text corpora, which naturally restricts the amount of synthetic data that can be generated. In this paper, we propose a novel framework for domain adaptation called contrastive domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from question generation and domain-invariant learning to answer out-of-domain questions in settings with limited text corpora. Here, we train a QA system on both source data and generated data from the target domain with a contrastive adaptation loss that is incorporated in the training objective. By combining techniques from question generation and domain-invariant learning, our model achieved considerable improvements compared to state-of-the-art baselines.": 156,
    "How can question answering systems be effectively adapted to new domains, especially when only limited text corpora are available for the target domain, without relying on large amounts of synthetically generated datasets or costly computational resources?": 157,
    "With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.": 158,
    "How can pre-trained language models effectively transfer knowledge across tasks from distant domains with different class label sets, without suffering from negative transfer?": 159,
    "Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein Selective Transfer Learning (WSTL) method. Specifically, the proposed method considers a reinforced selector to select helpful data for transfer learning. We further use a Wasserstein-based discriminator to maximize the empirical distance between the selected source data and target data. The TL module is then trained to minimize the estimated Wasserstein distance in an adversarial manner and provides domain invariant features for the reinforced selector. We adopt an evaluation metric based on the performance of the TL module as delayed reward and a Wasserstein-based metric as immediate rewards to guide the reinforced selector learning. Compared with the competing TL approaches, the proposed method selects data samples that are closer to the target domain. It also provides better state features and reward signals that lead to better performance with faster convergence. Extensive experiments on three real-world text mining tasks demonstrate the effectiveness of the proposed method.": 160,
    "How can negative transfer in instance-based transfer learning be alleviated, especially when previous methods struggle with learning clean domain-invariant feature representations and providing effective reward signals for data selection?": 161,
    "This paper focuses on utilizing metaphor interpretation to enhance metaphor detection . Considering that existing approaches to metaphor interpretation are limited by ambiguous meanings of the metaphorical substitute words, this paper proposes a novel interpretation mechanism that utilizes glosses to interpret metaphorical words. Since there is no dataset annotated for both metaphor detection and metaphor interpretation, we enhance three datasets TroFi, VUA, and PSUCMC from the ﬁeld of metaphor detection with gloss annotations. Accordingly, we develop a model for jointly conducting metaphor detection and gloss-based interpretation (named MDGI-Joint for short). Experimental results demonstrate that MDGI-Joint outperforms state-of-the-art models on all the three enhanced datasets and that gloss-based metaphor interpretation beneﬁts metaphor detection. 1": 162,
    "How can the ambiguity of metaphorical substitute words be overcome to provide precise interpretations, and how can these improved interpretations be leveraged to enhance metaphor detection?": 163,
    "Pre-trained language models (PTLMs) acquire domain-independent linguistic knowledge through pre-training with massive textual resources. Additional pre-training is effective in adapting PTLMs to domains that are not well covered by the pre-training corpora. Here, we focus on the static word embeddings of PTLMs for domain adaptation to teach PTLMs domain-specific meanings of words. We propose a novel fine-tuning process: task-adaptive pre-training with word embedding regularization (TAPTER). TAPTER runs additional pre-training by making the static word embeddings of a PTLM close to the word embeddings obtained in the target domain with fastText. TAPTER requires no additional corpus except for the training data of the downstream task. We confirmed that TAPTER improves the performance of the standard fine-tuning and the task-adaptive pre-training on BioASQ (question answering in the biomedical domain) and on SQuAD (the Wikipedia domain) when their pre-training corpora were not dominated by in-domain data.": 164,
    "How can pre-trained language models be effectively adapted to specific domains and tasks, especially when the target domain is not well-represented in their initial pre-training corpora, without requiring large additional in-domain datasets?": 165,
    "Supervised learning methods have proven to be effective for Aspect-Based Sentiment Analysis (ABSA). However, the lack of ﬁne-grained labeled data hinders their effectiveness in many domains. To address this issue, unsupervised domain adaptation methods are desired to transfer knowledge from a labeled source domain to any unlabeled target domain. In this paper, we propose a new domain adaptation paradigm called cross-domain review generation (CDRG), which aims to generate target-domain reviews with ﬁne-grained annotation based on the source-domain labeled reviews. To achieve this goal, we pro-pose a two-step approach as a concrete realization of CDRG. It ﬁrst converts a source-domain review to a domain-independent review by masking its source-speciﬁc attributes, and then converts the domain-independent review to a target-domain review with a masked language model pre-trained in the target domain. We further propose two ways to leverage the generated target-domain reviews for two cross-domain ABSA tasks. Extensive experiments demonstrate the superiority of our CDRG-based approaches over the state-of-the-art domain adaptation methods.": 166,
    "How can the lack of fine-grained labeled data for Aspect-Based Sentiment Analysis (ABSA) in new domains be addressed through unsupervised domain adaptation, enabling the transfer of knowledge from a labeled source domain to an unlabeled target domain?": 167,
    "We propose a simple and effective few-shot model for slot tagging. Recent work shows that it is promising to extend standard few-shot classiﬁcation methods to sequence labeling with CRF-speciﬁc augmentations. Such methods show strengths in encoding slot name semantics and slot dependencies. However, we ﬁnd these strengths can be obtained by a much simpler method, which casts slot tagging into machine reading comprehension (MRC). We ﬁne-tune a standard BERT-based MRC model with a mixture of source domain and (few-shot) target domain data. Such simple method outperforms state-of-the-art methods by a large margin on the SNIPS dataset.": 168,
    "How can few-shot slot tagging be performed effectively when labeled data is scarce in new domains, especially given the task's sentence-level and extractive nature?": 169,
    "Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are urgently needed. Existing datasets are limited to providing the degree of idiomaticity of expressions along with the literal and, where applicable, (a single) non-literal interpretation of MWEs. This work presents a novel dataset of naturally occurring sentences containing MWEs manually classified into a fine-grained set of meanings, spanning both English and Portuguese. We use this dataset in two tasks designed to test i) a language model's ability to detect idiom usage, and ii) the effectiveness of a language model in generating representations of sentences containing idioms. Our experiments demonstrate that, on the task of detecting idiomatic usage, these models perform reasonably well in the one-shot and few-shot scenarios, but that there is significant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we find that pre-training is not always effective, while fine-tuning could provide a sample efficient method of learning representations of sentences containing MWEs.": 170,
    "How can pre-trained language models be improved to effectively capture the nuanced meanings of multiword expressions, particularly idioms, given their current reliance on compositionality?": 171,
    "In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We ﬁrst identify the lottery subnet-work structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only ﬁne-tune the lottery subnetwork, a small fraction of the whole parameters, on the annotated target domain data for adaptation. To obtain more adaptable sub-networks, we introduce self-attention attribution to weigh parameters, beyond simply pruning the smallest magnitude parameters, which can be seen as combining structured pruning and unstructured magnitude pruning softly. Experimental results show that our method outperforms the full model ﬁne-tuning adaptation on four out of ﬁve domains when only a small amount of annotated data available for adaptation. Moreover, introducing self-attention attribution reserves more parameters for important attention heads in the lottery subnetwork and improves the target domain model performance. Our further analyses reveal that, besides exploiting fewer parameters, the choice of subnetworks is critical to the effectiveness. 1": 172,
    "How can large, pre-trained language models be efficiently and effectively adapted to new domains with only a small amount of annotated data, without incurring the high costs of full model fine-tuning or extensive data collection?": 173,
    "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained\r\nlanguage models for downstream tasks. We introduce UDALM, a fine-tuning\r\nprocedure, using a mixed classification and Masked Language Model loss, that\r\ncan adapt to the target domain distribution in a robust and sample efficient\r\nmanner. Our experiments show that performance of models trained with the mixed\r\nloss scales with the amount of available target data and the mixed loss can be\r\neffectively used as a stopping criterion during UDA training. Furthermore, we\r\ndiscuss the relationship between A-distance and the target error and explore\r\nsome limitations of the Domain Adversarial Training approach. Our method is\r\nevaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset,\r\nyielding $91.74\\%$ accuracy, which is an $1.11\\%$ absolute improvement over the\r\nstate-of-the-art.": 174,
    "How can pretrained language models be effectively adapted for downstream tasks in an unsupervised domain adaptation setting, where only labeled source data and unlabeled target data are available?": 175,
    "Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and cross-dialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT.": 176,
    "How can the generalization performance of deep pre-trained language models be improved for Arabic cross-domain and cross-dialect sentiment analysis under domain shift?": 177,
    "Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.": 178,
    "How can text style be transferred across multiple dimensions simultaneously while preserving content, without requiring jointly annotated data for all stylistic dimensions?": 179,
    "State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) task-adaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model’s catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task.": 180,
    "How can abstractive summarization models be effectively adapted to diverse target domains in low-resource settings, where extensive labeled data is unavailable, while mitigating the issue of catastrophic forgetting during adaptation?": 181,
    "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.": 182,
    "This paper investigates whether large pre-trained language models encode metaphorical knowledge and, if so, how this information is distributed within their representations and how well it generalizes across different datasets and languages.": 183,
    "Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models. As GPT-3 appears, prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks. However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the \\operatorname{[MASK]} token in different domains, thus making underuse of the prompt tuning technique. In this paper, we propose a novel Adversarial Soft Prompt Tuning method (AdSPT) to better model cross-domain sentiment analysis. On the one hand, AdSPT adopts separate soft prompts instead of hard templates to learn different vectors for different domains, thus alleviating the domain discrepancy of the \\operatorname{[MASK]} token in the masked language modeling task. On the other hand, AdSPT uses a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain. Experiments on a publicly available sentiment analysis dataset show that our model achieves the new state-of-the-art results for both single-source domain adaptation and multi-source domain adaptation.": 184,
    "How can prompt tuning be effectively utilized for cross-domain sentiment analysis to overcome the challenges posed by differing data distributions across domains?": 185,
    "Unlike literal expressions, idioms’ meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model, Transformer, by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language.When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expressions. This manifests in idioms’ parts being grouped through attention and in reduced interaction between idioms and their context.In the decoder’s cross-attention, figurative inputs result in reduced attention on source-side tokens. These results suggest that Transformer’s tendency to process idioms as compositional expressions contributes to literal translations of idioms.": 186,
    "In what way do Transformer models process idioms, and does this processing contribute to their tendency for compositional, literal translations?": 187,
    "While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.": 188,
    "How can the abstract language understanding and complex analogical reasoning abilities of large language models be effectively evaluated beyond surface-level linguistic patterns?": 189,
    "Idioms are unlike most phrases in two important ways. First, the words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we implement them using BERT (Devlin et al., 2019) and XLNet(Yang et al., 2019). We show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that special machinery to handle idioms may not be warranted.": 190,
    "How can idioms be characterized and distinguished from other multi-word expressions in terms of conventionality and contingency without needing idiom-specific machinery?": 191,
    "Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding. However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an English dataset consisting of paired sentences spanning idioms and metaphors. We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs. We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset. We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing. This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction.": 192,
    "How can natural language inference models be effectively evaluated and improved for understanding figurative language, particularly idioms and metaphors, given their non-compositional nature and the scarcity of suitable diagnostic datasets?": 193,
    "Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions. Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive. In recent years, pre-trained language models (PLMs) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs may be useful for SI and SG tasks. Nevertheless, there are few works to explore it. In this paper, we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time. The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position. In this framework, we adopt a secondary training process (Adjective-Noun mask Training) with the masked language model (MLM) loss to enhance the prediction diversity of candidate words in the masked position. Moreover, pattern ensemble (PE) and pattern search (PS) are applied to improve the quality of predicted words. Finally, automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks.": 194,
    "How can pre-trained language models be effectively leveraged to interpret and generate similes by completing incomplete simile triples, overcoming challenges like predicting diverse and high-quality simile components?": 195,
    "Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation’s innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence. In this work, we study the discourse structure of sarcastic conversations and propose a novel task – Sarcasm Explanation in Dialogue (SED). Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations. To this end, we curate WITS, a new dataset to support our task. We propose MAF (Modality Aware Fusion), a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS. The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics. Lastly, we carry out detailed analysis both quantitatively and qualitatively.": 196,
    "How can AI agents generate natural language explanations for sarcastic conversations in multi-modal, multi-party, and code-mixed dialogues?": 197,
    "We present substructure distribution projection (SubDP), a technique that projects a distribution over structures in one domain to another, by projecting substructure distributions separately. Models for the target domain can then be trained, using the projected distributions as soft silver labels. We evaluate SubDP on zero shot cross-lingual dependency parsing, taking dependency arcs as substructures: we project the predicted dependency arc distributions in the source language(s) to target language(s), and train a target language parser on the resulting distributions. Given an English tree bank as the only source of human supervision, SubDP achieves better unlabeled attachment score than all prior work on the Universal Dependencies v2.2 (Nivre et al., 2020) test set across eight diverse target languages, as well as the best labeled attachment score on six languages. In addition, SubDP improves zero shot cross-lingual dependency parsing with very few (e.g., 50) supervised bitext pairs, across a broader range of target languages.": 198,
    "How can dependency parsing be performed in target languages without any parsing examples, relying solely on human supervision from a source language, while effectively leveraging soft and many-to-one alignment information?": 199,
    "Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes’ shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification. The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.": 200,
    "How can pre-trained language models be systematically evaluated for their ability to interpret similes by inferring shared properties, and how can this ability be enhanced to bridge the gap with human performance?": 201,
    "Synthesizing QA pairs with a question generator (QG) on the target domain has become a popular approach for domain adaptation of question answering (QA) models. Since synthetic questions are often noisy in practice, existing work adapts scores from a pretrained QA (or QG) model as criteria to select high-quality questions. However, these scores do not directly serve the ultimate goal of improving QA performance on the target domain. In this paper, we introduce a novel idea of training a question value estimator (QVE) that directly estimates the usefulness of synthetic questions for improving the target-domain QA performance. By conducting comprehensive experiments, we show that the synthetic questions selected by QVE can help achieve better target-domain QA performance, in comparison with existing techniques. We additionally show that by using such questions and only around 15% of the human annotations on the target domain, we can achieve comparable performance to the fully-supervised baselines.": 202,
    "How can the usefulness of synthetic question-answering pairs be directly estimated to improve the performance of question answering models on a target domain?": 203,
    "The tasks of humor understanding and generation are challenging and subjective even for humans, requiring commonsense and real-world knowledge to master. Puns, in particular, add the challenge of fusing that knowledge with the ability to interpret lexical-semantic ambiguity. In this paper, we present the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of puns with detailed crowdsourced annotations of keywords denoting the most distinctive words that make the text funny, pun explanations describing why the text is funny, and fine-grained funniness ratings. This is the first humor dataset with such extensive and fine-grained annotations specifically for puns. Based on these annotations, we propose two tasks: explanation generation to aid with pun classification and keyword-conditioned pun generation, to challenge the current state-of-the-art natural language understanding and generation models’ ability to understand and generate humor. We showcase that the annotated keywords we collect are helpful for generating better novel humorous texts in human evaluation, and that our natural language explanations can be leveraged to improve both the accuracy and robustness of humor classifiers.": 204,
    "How can fine-grained, human-annotated data be leveraged to improve the understanding and generation of humor, specifically puns, which inherently require extensive commonsense and real-world knowledge?": 205,
    "Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most entertaining if it fits appropriately within a given context, e.g., a given situation or dialogue. In this work, we propose a new task, context-situated pun generation, where a specific context represented by a set of keywords is provided, and the task is to first identify suitable pun words that are appropriate for the context, then generate puns based on the context keywords and the identified pun words. We collect a new dataset, CUP (Context-sitUated Pun), containing 4.5k tuples of context words and pun pairs. Based on the new data and setup, we propose a pipeline system for context-situated pun generation, including a pun word retrieval module that identifies suitable pun words for a given context, and a pun generation module that generates puns from context keywords and pun words. Human evaluation shows that 69% of our top retrieved pun words can be used to generate context-situated puns, and our generation module yields successful puns 31% of the time given a plausible tuple of context words and pun pair, almost tripling the yield of a state-of-the-art pun generation model. With an end-to-end evaluation, our pipeline system with the top-1 retrieved pun pair for a given context can generate successful puns 40% of the time, better than all other modeling variations but 32% lower than the human success rate. This highlights the difficulty of the task, and encourages more research in this direction.": 206,
    "How can a system generate entertaining puns that appropriately fit within a given context, by first identifying suitable pun words and then generating the pun based on these words and the context?": 207,
    "Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.": 208,
    "How can synthetic data be leveraged as a source of supervision to enable pretrained language models to transfer learning to new target tasks and domains in spatial language processing, particularly given the scarcity of large-scale, human-annotated datasets?": 209,
    "Recent named entity recognition (NER) models often rely on human-annotated datasets requiring the vast engagement of professional knowledge on the target domain and entities. This work introduces an ask-to-generate approach, which automatically generates NER datasets by asking simple natural language questions to an open-domain question answering system (e.g., “Which disease?”). Despite using fewer training resources, our models solely trained on the generated datasets largely outperform strong low-resource models by 19.5 F1 score across six popular NER benchmarks. Our models also show competitive performance with rich-resource models that additionally leverage in-domain dictionaries provided by domain experts. In few-shot NER, we outperform the previous best model by 5.2 F1 score on three benchmarks and achieve new state-of-the-art performance.": 210,
    "How can named entity recognition datasets be automatically generated without relying on extensive human annotation or domain-specific resources?": 211,
    "Real-world politically-opinionated memes often rely on figurative language to cloak propaganda and radical ideas to help them spread. It is not only a scientific challenge to develop machine learning models to recognize them in memes, but also sociologically beneficial to understand hidden meanings at scale and raise awareness. These memes are fast-evolving (in both topics and visuals) and it remains unclear whether current multimodal machine learning models are robust to such distribution shifts. To enable future research into this area, we first present FigMemes, a dataset for figurative language classification in politically-opinionated memes. We evaluate the performance of state-of-the-art unimodal and multimodal models and provide comprehensive benchmark results. The key contributions of this proposed dataset include annotations of six commonly used types of figurative language in politically-opinionated memes, and a wide range of topics and visual styles.We also provide analyses on the ability of multimodal models to generalize across distribution shifts in memes. Our dataset poses unique machine learning challenges and our results show that current models have significant room for improvement in both performance and robustness to distribution shifts.": 212,
    "How can machine learning models effectively identify figurative language in politically-opinionated memes, especially given the fast-evolving nature of memes and the need for models to be robust to distribution shifts?": 213,
    "Building pretrained language models is considered expensive and data-intensive, but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data Selection, a statistical sentence scoring method that conditions on a representative target domain corpus. As an example, we treat the OntoNotes corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically selected subset of the Pile. On both perplexity and across several downstream tasks in the target domain, it consistently outperforms random selection with 20x less data, 3x fewer training iterations, and 2x less estimated cloud compute cost, validating the recipe of automatic document selection for LM pretraining.": 214,
    "How can efficient encoder pretraining be achieved using smaller, domain-representative data subsets instead of relying on increasingly larger training sets?": 215,
    "Paraphrase generation is a longstanding NLP task and achieves great success with the aid of large corpora. However, transferring a paraphrasing model to another domain encounters the problem of domain shifting especially when the data is sparse. At the same time, widely using large pre-trained language models (PLMs) faces the overfitting problem when training on scarce labeled data. To mitigate these two issues, we propose, LAPA, an effective adapter for PLMs optimized by meta-learning. LAPA has three-stage training on three types of related resources to solve this problem: 1. pre-training PLMs on unsupervised corpora, 2. inserting an adapter layer and meta-training on source domain labeled data, and 3. fine-tuning adapters on a small amount of target domain labeled data. This method enables paraphrase generation models to learn basic language knowledge first, then learn the paraphrasing task itself later, and finally adapt to the target task. Our experimental results demonstrate that LAPA achieves state-of-the-art in supervised, unsupervised, and low-resource settings on three benchmark datasets. With only 2% of trainable parameters and 1% labeled data of the target task, our approach can achieve a competitive performance with previous work.": 216,
    "How can paraphrase generation models effectively adapt to low-resource settings, mitigating domain shift and overfitting issues when training on scarce labeled data?": 217,
    "Training a good deep learning model requires substantial data and computing resources, which makes the resulting neural model a valuable intellectual property. To prevent the neural network from being undesirably exploited, non-transferable learning has been proposed to reduce the model generalization ability in specific target domains. However, existing approaches require labeled data for the target domain which can be difficult to obtain. Furthermore, they do not have the mechanism to still recover the model’s ability to access the target domain.In this paper, we propose a novel unsupervised non-transferable learning method for the text classification task that does not require annotated target domain data. We further introduce a secret key component in our approach for recovering the access to the target domain, where we design both an explicit and an implicit method for doing so. Extensive experiments demonstrate the effectiveness of our approach.": 218,
    "How can a neural network be trained for non-transferable text classification without requiring labeled target domain data, while also allowing for the recovery of its performance in the target domain when authorized?": 219,
    "Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain. Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions. However, fine-tuned BERT has a considerable underperformance at zero-shot when applied in a different domain. We solve this problem by proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during training. As like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during training. To generate these negative entities, we propose a simple but effective strategy that takes the domain of the golden entity into perspective. Our experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-art.": 220,
    "How can the zero-shot transfer capability of entity linking systems be explicitly improved to mitigate the substantial performance degradation observed in fine-tuned BERT models when applied to target domains with significant distribution shifts from the source domain?": 221,
    "Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario. A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains. Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model. To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces. To enhance the contextual representation with label structures, we fuse the label graph into the word embedding output by BERT. By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem. Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks. Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods.": 222,
    "How can Named Entity Recognition (NER) models be effectively adapted across different domains despite the mismatch in entity types and data scarcity?": 223,
    "We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a distribution of candidate keywords. It provides more importance to the distinctive keywords of the target domain than common keywords contrasting with the context domain. To support the representativeness of the selected keywords towards the target domain, we introduce an optimization algorithm for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.": 224,
    "How can one select a subset of target domain representative keywords from a candidate set while contrasting with a context domain?\n---": 225,
    "Simile recognition involves two subtasks: simile sentence classification that discriminates whether a sentence contains simile, and simile component extraction that locates the corresponding objects (i.e., tenors and vehicles). Recent work ignores features other than surface strings. In this paper, we explore expressive features for this task to achieve more effective data utilization. Particularly, we study two types of features: 1) input-side features that include POS tags, dependency trees and word definitions, and 2) decoding features that capture the interdependence among various decoding decisions. We further construct a model named HGSR, which merges the input-side features as a heterogeneous graph and leverages decoding features via distillation. Experiments show that HGSR significantly outperforms the current state-of-the-art systems and carefully designed baselines, verifying the effectiveness of introduced features. Our code is available at https://github.com/DeepLearnXMU/HGSR.": 226,
    "How can simile recognition systems achieve more effective data utilization and improve performance by exploring expressive features beyond surface strings, especially given the data hunger issue in current neural models?": 227,
    "We propose a unified framework to generate both homophonic and homographic puns to resolve the split-up in existing works. Specifically, we incorporate three linguistic attributes of puns to the language models: ambiguity, distinctiveness, and surprise. Our framework consists of three parts: 1) a context words/phrases selector to promote the aforementioned attributes, 2) a generation model trained on non-pun sentences to incorporate the context words/phrases into the generation output, and 3) a label predictor that learns the structure of puns which is used to steer the generation model at inference time. Evaluation results on both pun types demonstrate the efficacy of our model over strong baselines.": 228,
    "How can a unified framework be developed to generate both homophonic and homographic puns by incorporating key linguistic attributes of humor, thereby resolving the split in existing pun generation approaches?": 229,
    "Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the entire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al., 2022) proposed to transfer soft prompts pretrained on the source domain to the target domain. In this paper, we explore domain adaptation for prompt tuning, a problem setting where unlabeled data from the target domain are available during pretraining. We propose bOosting Prompt TunIng with doMain Adaptation (OPTIMA), which regularizes the decision boundary to be smooth around regions where source and target data distributions are similar. Extensive experiments demonstrate that OPTIMA significantly enhances the transferability and sample-efficiency of prompt tuning compared to strong baselines. Moreover, in few-shot settings, OPTIMA exceeds full-model tuning by a large margin.": 230,
    "How can the sample efficiency and transferability of prompt tuning be improved, especially in data-scarce regimes, by effectively leveraging unlabeled data from the target domain?": 231,
    "Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation meth-ods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation.": 232,
    "The paper addresses the challenge of data scarcity in task-oriented dialogue systems, which limits the ability to train large-scale pre-trained language models effectively. How can a unified dialogue user simulator be developed to augment large-scale task-oriented dialogue corpora, especially in low-resource or few-shot scenarios, without heavy reliance on annotated data in the target domain?": 233,
    "Cross-domain named entity recognition (NER) aims to borrow the entity information from the source domain to help the entity recognition in the target domain with limited labeled data. Despite the promising performance of existing approaches, most of them focus on reducing the discrepancy of token representation between source and target domains, while the transfer of the valuable label information is often not explicitly considered or even ignored. Therefore, we propose a novel autoregressive framework to advance cross-domain NER by first enhancing the relationship between labels and tokens and then further improving the transferability of label information. Specifically, we associate each label with an embedding vector, and for each token, we utilize a bidirectional LSTM (Bi-LSTM) to encode the labels of its previous tokens for modeling internal context information and label dependence. Afterward, we propose a Bi-Attention module that merges the token representation from a pre-trained model and the label features from the Bi-LSTM as the label-aware information, which is concatenated to the token representation to facilitate cross-domain NER. In doing so, label information contained in the embedding vectors can be effectively transferred to the target domain, and Bi-LSTM can further model the label relationship among different domains by pre-train and then fine-tune setting. Experimental results on several datasets confirm the effectiveness of our model, where our model achieves significant improvements over the state of the arts.": 234,
    "How can named entity recognition (NER) in target domains with limited labeled data be advanced by explicitly considering and improving the transferability of valuable label information from a source domain, rather than solely focusing on token representation discrepancy?": 235,
    "Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at https://github.com/UKPLab/gpl.": 236,
    "How can dense retrieval approaches effectively adapt to new domains without requiring large amounts of labeled training data, thereby overcoming performance degradation due to domain shifts?": 237,
    "We present a method to control the emotional prosody of Text to Speech (TTS) systems by using phoneme-level intermediate features (pitch, energy, and duration) as levers. As a key idea, we propose Differential Scaling (DS) to disentangle features relating to affective prosody from those arising due to acoustics conditions and speaker identity. With thorough experimental studies, we show that the proposed method improves over the prior art in accurately emulating the desired emotions while retaining the naturalness of speech. We extend the traditional evaluation of using individual sentences for a more complete evaluation of HCI systems. We present a novel experimental setup by replacing an actor with a TTS system in offline and live conversations. The emotion to be rendered is either predicted or manually assigned. The results show that the proposed method is strongly preferred over the state-of-the-art TTS system and adds the much-coveted “human touch” in machine dialogue. Audio samples from our experiments and the code are available at: https://emtts.github.io/tts-demo/": 238,
    "How can Text-To-Speech (TTS) systems accurately emulate emotions while preserving the naturalness of speech?": 239,
    "As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero/few-shot learning. Previous researches on zero/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and example ambiguity issues. To address these problems, we propose Abundant Information Slot Filling Generator (AISFG), a generative model with a novel query template that incorporates domain descriptions, slot descriptions, and examples with context. Experimental results show that our model outperforms state-of-the-art approaches in zero/few-shot slot filling task.": 240,
    "How can cross-domain slot filling be effectively performed under data scarcity conditions, particularly addressing slot type and example ambiguities?": 241,
    "As a fundamental task in opinion mining, aspect and opinion co-extraction aims to identify the aspect terms and opinion terms in reviews. However, due to the lack of fine-grained annotated resources, it is hard to train a robust model for many domains. To alleviate this issue, unsupervised domain adaptation is proposed to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we propose a new Generative Cross-Domain Data Augmentation framework for unsupervised domain adaptation. The proposed framework is aimed to generate target-domain data with fine-grained annotation by exploiting the labeled data in the source domain. Specifically, we remove the domain-specific segments in a source-domain labeled sentence, and then use this as input to a pre-trained sequence-to-sequence model BART to simultaneously generate a target-domain sentence and predict the corresponding label for each word. Experimental results on three datasets demonstrate that our approach is more effective than previous domain adaptation methods.": 242,
    "How can robust models for fine-grained aspect and opinion co-extraction be trained for many domains despite the scarcity of annotated resources, particularly by effectively transferring knowledge from a labeled source domain to an unlabeled target domain while addressing distribution discrepancies?": 243,
    "Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.": 244,
    "How can language models be effectively tested for their ability to interpret non-literal, figurative language, particularly novel metaphors, beyond simple detection or conventionalized phrase understanding?": 245,
    "Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.": 246,
    "How can a literal sentence be transformed into its hyperbolic paraphrase, effectively generating exaggerated language while preserving the original meaning?": 247,
    "In this paper, we propose a simple yet effective way to generate pun\r\nsentences that does not require any training on existing puns. Our approach is\r\ninspired by humor theories that ambiguity comes from the context rather than\r\nthe pun word itself. Given a pair of definitions of a pun word, our model first\r\nproduces a list of related concepts through a reverse dictionary. We then\r\nutilize one-shot GPT3 to generate context words and then generate puns\r\nincorporating context words from both concepts. Human evaluation shows that our\r\nmethod successfully generates pun 52\\% of the time, outperforming well-crafted\r\nbaselines and the state-of-the-art models by a large margin.": 248,
    "In what way can a computational system generate homographic pun sentences that effectively exploit the ambiguity of a word's multiple meanings, without requiring training on existing pun data?": 249,
    "The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.": 250,
    "How can efficient domain adaptation be achieved for pretrained language models across multiple diverse domains while allowing parameter sharing among related domains and avoiding negative interference?": 251,
    "This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task in the target domain. In order to effectively utilize the information from memory, MANNER uses optimal transport to retrieve and process information from memory, which can explicitly adapt the retrieved information from source domain to target domain and improve the performance in the cross domain few-shot setting. We conduct experiments on English and Chinese cross domain few-shot NER datasets, and the experimental results demonstrate that MANNER can achieve superior performance.": 252,
    "How can named entity recognition models effectively adapt knowledge from a source domain to recognize entities in a target domain with only a few labeled examples, especially when entity types across domains may be disjoint?": 253,
    "A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer’s self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter’s gains are due to its improved ability to distinguish ”none”-valued dialogue slots, compared against baselines.": 254,
    "How can Dialogue State Tracking models be effectively adapted to new domains in a zero-shot scenario without requiring any supervised data?": 255,
    "Similes occur in the creative context of describing a concept (i.e., tenor) by making a literally false yet figuratively meaningful comparison to another (i.e., vehicle). Previous efforts form simile generation as a context-free generation task, focusing on simile-style transfer or writing a simile from a given prefix. However, generated texts under such settings might be undesirable, such as hardly meeting the simile definition (e.g., missing vehicle) or difficult to address certain preferences of content as humans wish (e.g., describe the color of apples through the simile). We believe that a simile could be more qualified and user-oriented if incorporated with pre-specified constraints. To this end, we introduce controllable simile generation (CSG), a new task that requires the model to generate a simile with multiple simile elements, e.g., context and vehicle. To facilitate this task, we present GraCe, including 61.3k simile-element annotated Chinese similes. Based on it, we propose a CSG model Similor to benchmark this task, including a vehicle retrieval module Scorer to obtain the explicable comparison for a given tenor in the vehicle-unknown situation. Both statistical and experimental analyses show that GraCe is of high quality beyond all other Chinese simile datasets, in terms of the number (8 vs. 3) of annotation elements, Is-Simile accuracy (98.9% vs. 78.7%), and increasing model-performance gains for both uncontrollable and controllable simile generation. Meanwhile, Similor can serve as a strong baseline for CSG, especially with Scorer, which beats model-based retrieval methods without any re-training.": 256,
    "How can simile generation models be made more controllable and user-oriented to produce high-quality similes that meet specific content preferences and incorporate multiple simile elements, addressing the limitations of previous context-free generation approaches?": 257,
    "With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose MetaAdapt, a meta learning based approach for domain adaptive few-shot misinformation detection. MetaAdapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). In particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. Based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. As such, MetaAdapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. To demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.": 258,
    "How can misinformation detection models effectively adapt to new, emerging topics on social media, overcoming significant distributional shifts between existing training data and target domains, especially when only a limited amount of target domain data is available?": 259,
    "Non-compositional expressions present a substantial challenge for natural language processing (NLP) systems, necessitating more intricate processing compared to general language tasks, even with large pre-trained language models. Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations. This paper addresses both of these challenges. By leveraging contrastive learning techniques to build improved representations it tackles the non-compositionality challenge. Additionally, we propose a dynamic curriculum learning framework specifically designed to take advantage of the scarce available data for modeling non-compositionality. Our framework employs an easy-to-hard learning strategy, progressively optimizing the model’s performance by effectively utilizing available training data. Moreover, we integrate contrastive learning into the curriculum learning approach to maximize its benefits. Experimental results demonstrate the gradual improvement in the model’s performance on idiom usage recognition and metaphor detection tasks. Our evaluation encompasses six datasets, consistently affirming the effectiveness of the proposed framework. Our models available at https://github.com/zhjjn/CLCL.git.": 260,
    "How can natural language processing systems effectively detect non-compositional expressions, such as idioms and metaphors, given their inherent non-compositionality and the limited availability of training data resources?": 261,
    "Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.": 262,
    "How can the inadequacy of large-scale annotated data and the domain disparity issues inherent in distant supervision methods be overcome for low-resource relation extraction?": 263,
    "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor’s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.": 264,
    "How can pre-trained generative language models predict the source domain of conceptual metaphors in natural language without relying on pre-defined labels or grammatical assumptions?": 265,
    "Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a more general solution, we propose a unified bidirectional generative framework to tackle various cross-domain ABSA tasks. Specifically, our framework trains a generative model in both text-to-label and label-to-text directions. The former transforms each task into a unified format to learn domain-agnostic features, and the latter generates natural sentences from noisy labels for data augmentation, with which a more accurate model can be trained. To investigate the effectiveness and generality of our framework, we conduct extensive experiments on four cross-domain ABSA tasks and present new state-of-the-art results on all tasks. Our data and code are publicly available at https://github.com/DAMO-NLP-SG/BGCA.": 266,
    "How can knowledge be effectively transferred from a source domain with labeled data to a target domain with only unlabeled data to perform various fine-grained aspect-based sentiment analysis tasks, given that existing methods are often task-specific and struggle with domain variability?": 267,
    "Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics. Resources of HAUSER are publicly available at https://github.com/Abbey4799/HAUSER.": 268,
    "How to establish a comprehensive, efficient, and reliable automatic evaluation system for simile generation that addresses the limitations of existing task-agnostic metrics?": 269,
    "Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only under conservative shifts in data distribution and typically focus on a single component (i.e., retriever or reader) rather than an end-to-end system. This work proposes a more realistic end-to-end domain shift evaluation setting covering five diverse domains. We not only find that end-to-end models fail to generalize but that high retrieval scores often still yield poor answer prediction accuracy. To address these failures, we investigate several interventions, in the form of data augmentations, for improving model adaption and use our evaluation set to elucidate the relationship between the efficacy of an intervention scheme and the particular type of dataset shifts we consider. We propose a generalizability test that estimates the type of shift in a target dataset without training a model in the target domain and that the type of shift is predictive of which data augmentation schemes will be effective for domain adaption. Overall, we find that these interventions increase end-to-end performance by up to ~24 points.": 270,
    "How can open-domain question answering (ODQA) models be effectively adapted to new, unseen domains with varying types and degrees of data distribution shifts, especially when high retrieval scores do not always translate to accurate answer predictions?": 271,
    "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA^2LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA^2LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM.": 272,
    "How can the generation of labeled target-domain data for cross-domain Aspect-Based Sentiment Analysis be improved to overcome issues of preserving source-specific attributes, lacking fluency and coherence, and limiting data diversity in existing data augmentation methods?": 273,
    "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.": 274,
    "How can the limitations of customized retrieval models for open-domain question answering (ODQA), which suffer from poor transferability and scalability across diverse datasets, be overcome?": 275,
    "One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design. While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its contextual meaning and its basic meaning, existing work does not strictly follow this principle, typically using the aggregated meaning to approximate the basic meaning of target words. In this paper, we propose a novel metaphor detection method, which models the basic meaning of the word based on literal annotation from the training set, and then compares this with the contextual meaning in a target sentence to identify metaphors. Empirical results show that our method outperforms the state-of-the-art method significantly by 1.0% in F1 score. Moreover, our performance even reaches the theoretical upper bound on the VUA18 benchmark for targets with basic annotations, which demonstrates the importance of modelling basic meanings for metaphor detection.": 276,
    "How can metaphor detection be improved by accurately modeling the basic meanings of words and comparing these with their contextual meanings in sentences?": 277,
    "We propose a novel RoBERTa-based model, RoPPT, which introduces a target-oriented parse tree structure in metaphor detection. Compared to existing models, RoPPT focuses on semantically relevant information and achieves the state-of-the-art on several main metaphor datasets. We also compare our approach against several popular denoising and pruning methods, demonstrating the effectiveness of our approach in context denoising. Our code and dataset can be found at https://github.com/MajiBear000/RoPPT.": 278,
    "How can metaphor detection models effectively identify metaphorical expressions by focusing on semantically relevant contextual information while mitigating the noise introduced by wider sentence contexts?": 279,
    "In this paper, we propose FrameBERT, a RoBERTa-based model that can explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor detection. FrameBERT not only achieves better or comparable performance to the state-of-the-art, but also is more explainable and interpretable compared to existing models, attributing to its ability of accounting for external knowledge of FrameNet.": 280,
    "How can external conceptual knowledge, specifically semantic frames from FrameNet, be effectively incorporated into deep learning models to improve concept-level metaphor detection and enhance model explainability, overcoming the limitations of models that rely solely on shallow semantics?": 281,
    "A major open problem in neural machine translation (NMT) is the translation of idiomatic expressions, such as “under the weather”. The meaning of these expressions is not composed by the meaning of their constituent words, and NMT models tend to translate them literally (i.e., word-by-word), which leads to confusing and nonsensical translations. Research on idioms in NMT is limited and obstructed by the absence of automatic methods for quantifying these errors. In this work, first, we propose a novel metric for automatically measuring the frequency of literal translation errors without human involvement. Equipped with this metric, we present controlled translation experiments with models trained in different conditions (with/without the test-set idioms) and across a wide range of (global and targeted) metrics and test sets. We explore the role of monolingual pretraining and find that it yields substantial targeted improvements, even without observing any translation examples of the test-set idioms. In our analysis, we probe the role of idiom context. We find that the randomly initialized models are more local or “myopic” as they are relatively unaffected by variations of the idiom context, unlike the pretrained ones.": 282,
    "How can the literal translation errors of idiomatic expressions in Neural Machine Translation (NMT) models be automatically quantified and analyzed, given that their non-compositional meaning often leads to nonsensical translations?": 283,
    "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap be-tween speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches.": 284,
    "How can existing text-input referring video object segmentation models be effectively adapted to accommodate noisy speech input, bridging the semantic gap between speech and text while maintaining robust performance?": 285,
    "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.": 286,
    "How can large language models effectively adapt to new, unlabeled target domains using in-context learning, especially when in-domain demonstrations are unavailable and traditional fine-tuning methods face limitations?": 287,
    "Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.": 288,
    "How can the presence of construction artifacts and biases in existing metaphor identification datasets be systematically identified and mitigated to ensure that models learn genuine metaphoricity rather than exploiting spurious correlations?": 289,
    "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models' ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding.": 290,
    "How can language models be enabled to productively extend word meaning to denote new semantic domains, particularly those bearing regular semantic relations with existing senses, in a human-like and systematic way?": 291,
    "In code vulnerability detection tasks, a detector trained on a label-rich source domain fails to provide accurate prediction on new or unseen target domains due to the lack of labeled training data on target domains. Previous studies mainly utilize domain adaptation to perform cross-domain vulnerability detection. But they ignore the negative effect of private semantic characteristics of the target domain for domain alignment, which easily causes the problem of negative transfer. In addition, these methods forcibly reduce the distribution discrepancy between domains and do not take into account the interference of irrelevant target instances for distributional domain alignment, which leads to the problem of excessive alignment. To address the above issues, we propose a novel cross-domain code vulnerability detection framework named MNCRI. Specifically, we introduce mutual nearest neighbor contrastive learning to align the source domain and target domain geometrically, which could align the common semantic characteristics of two domains and separate out the private semantic characteristics of each domain. Furthermore, we introduce an instance re-weighting scheme to alleviate the problem of excessive alignment. This scheme dynamically assign different weights to instances, reducing the contribution of irrelevant instances so as to achieve better domain alignment. Finally, extensive experiments demonstrate that MNCRI significantly outperforms state-of-the-art cross-domain code vulnerability detection methods by a large margin.": 292,
    "How can cross-domain code vulnerability detection be accurately performed while effectively mitigating the issues of negative transfer and excessive alignment that arise from domain discrepancies?": 293,
    "As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.": 294,
    "How can the causal effects of linguistic attributes on reader perceptions be accurately estimated from natural language data, especially when the data does not meet the strong assumptions typically required for valid causal inference?": 295,
    "While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the creative language interpretation capacity of the language models. Unlike typical text, summarization of poems is a challenging task as poems carry a deeper meaning, which can be easily lost if only the literal meaning is considered. That being said, we propose a new task in the field of natural language understanding called ‘Poem Summarization’. As a starting, we propose the first-ever dataset for this task, named ‘PoemSum’, consisting of 3011 samples of poetry and its corresponding summarized interpretation in the English language. We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations. The dataset and all relevant code used in this work have been made publicly available.": 296,
    "How can language models be developed to accurately interpret the intended meaning and decipher the figurative patterns of creative language, specifically in the context of poetry summarization?": 297,
    "Few-shot named entity recognition (NER), identifying named entities with a small number of labeled data, has attracted much attention. Frequently, entities are nested within each other. However, most of the existing work on few-shot NER addresses flat entities instead of nested entities. To tackle nested NER in a few-shot setting, it is crucial to utilize the limited labeled data to mine unique features of nested entities, such as the relationship between inner and outer entities and contextual position information. Therefore, in this work, we propose a novel method based on focusing, bridging and prompting for few-shot nested NER without using source domain data. Both focusing and bridging components provide accurate candidate spans for the prompting component. The prompting component leverages the unique features of nested entities to classify spans based on soft prompts and contrastive learning. Experimental results show that the proposed approach achieves state-of-the-art performance consistently on the four benchmark datasets (ACE2004, ACE2005, GENIA and KBP2017) and outperforms several competing baseline models on F1-score by 9.33% on ACE2004, 6.17% on ACE2005, 9.40% on GENIA and 5.12% on KBP2017 on the 5-shot setting.": 298,
    "How can named entities, particularly nested ones, be accurately identified and classified with a very small number of labeled examples, without relying on extensive source domain data?": 299,
    "Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsupervised domain adaptation (UDA). Further, our model outperforms the state-of-the-art by achieving 1.4% average accuracy improvement in the adversarial domain adaptation (ADA) setting. Moreover, our model also shows its domain adaptation efficacy on a large multi-domain intent classification dataset where it attains state-of-the-art results. We release the codes publicly at \\url{https://github.com/declare-lab/remask}.": 300,
    "How can text be robustly transformed from a source domain to a target domain by effectively identifying and removing source-domain-specific information while preserving domain-agnostic context?": 301,
    "Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging scheme.": 302,
    "How can abstractive summarization models be made robust and generalizable to unseen target domains without requiring labeled data for those domains, especially given the computational complexity of existing domain generalization methods?": 303,
    "Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language models conceptualise embodied concepts to a degree that facilitates figurative language understanding.": 304,
    "How does the degree of embodiment in metaphorical actions affect language models' ability to interpret figurative language, and can this effect be isolated from other linguistic features?": 305,
    "Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often fails to effectively utilize the target supervision and cannot address distribution shifts across different domains due to the training data bias toward the source-labeled samples. In this paper, inspired by the co-learning of multiple classifiers for the computer vision tasks, we propose to decompose the SSDA framework for emotion-related tasks into two subcomponents of unsupervised domain adaptation (UDA) from the source to the target domain and semi-supervised learning (SSL) in the target domain where the two models iteratively teach each other by interchanging their high confident predictions. We further propose a novel data cartography-based regularization technique for pseudo-label denoising that employs training dynamics to further hone our models’ performance. We publicly release our code.": 306,
    "How can semi-supervised domain adaptation be effectively applied to emotion-related tasks in natural language processing when only a few labeled target data samples are available?": 307,
    "A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as\"like\"or\"as\". The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually focuses on similes in a triplet tuple (tenor, property, vehicle) or a single sentence where the tenor and vehicle are usually entities or noun phrases, which could not reflect complex simile phenomena in real scenarios. In this paper, we propose a novel and high-quality multilingual simile dialogue (MSD) dataset to facilitate the study of complex simile phenomena. The MSD is the largest manually annotated simile data ($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes. We design 3 simile tasks (recognition, interpretation, and generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each task, we provide experimental results from strong pre-trained or state-of-the-art models. The experiments demonstrate the challenge of MSD and we have released the data/code on GitHub.": 308,
    "How can the complex and unique phenomena of similes in real-life dialogue scenarios be effectively studied and modeled, given the limitations of existing simile research which primarily focuses on simpler, single-sentence or triplet-based forms?": 309,
    "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task . To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.": 310,
    "How can high-quality visual metaphors be generated from linguistic metaphors, overcoming the challenges of modeling implicit meaning and compositionality in text-to-image diffusion models?": 311,
    "Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt-tuning strategy to boost higher performance by only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44% F1) on the unseen slots.": 312,
    "How can knowledge be effectively transferred from labeled source domains to unlabeled target domains for slot filling, while improving generalization, robustness, and parameter efficiency, especially for unseen slot types?": 313,
    "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.": 314,
    "How can language models be improved to understand culturally and linguistically diverse figurative language, given that existing datasets are predominantly English-centric and figurative expressions are deeply rooted in cultural experiences?": 315,
    "Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it's highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling, and provide a benchmark for sentence-level figurative language detection, covering three common figures of speech and seven languages. Specifically, we develop a framework for figurative language detection based on template-based prompt learning. In so doing, we unify multiple detection tasks that are interrelated across multiple figures of speech and languages, without requiring task- or language-specific modules. Experimental results show that our framework outperforms several strong baselines and may serve as a blueprint for the joint modelling of other interrelated tasks.": 316,
    "How can a unified framework be developed to effectively model multilingual and multi-figurative language detection tasks at the sentence level, leveraging cross-lingual and cross-figurative knowledge transfer without requiring task- or language-specific modules?": 317,
    "Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on ﬁgurative language classiﬁcation of sar-casm , similes , idioms , and metaphors . We conduct two studies on the classiﬁcation results to provide insights into the inner workings of such models. With our ﬁrst analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four ﬁgurative language types.": 318,
    "How do Transformer-based language models (TLMs) process figurative language, and how do their feature attention behaviors compare to those of humans?": 319,
    "Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.": 320,
    "The paper addresses the challenge of improving Automatic Speech Recognition (ASR) system performance during inference by developing a multi-modal rescorer that effectively incorporates acoustic information, thereby enhancing domain generalization and robustness beyond the capabilities of text-only language models.": 321,
    "Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user’s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance.": 322,
    "How can few-shot Spoken Language Understanding (SLU) models be effectively trained in practical scenarios where access is limited to a pre-trained language model and a small number of labeled examples, without relying on data-rich source domains?": 323,
    "Metaphor detection (MD) suffers from limited training data. In this paper, we started with a linguistic rule called Metaphor Identification Procedure and then proposed a novel multi-task learning framework to transfer knowledge in basic sense discrimination (BSD) to MD. BSD is constructed from word sense disambiguation (WSD), which has copious amounts of data. We leverage adversarial training to align the data distributions of MD and BSD in the same feature space, so task-invariant representations can be learned. To capture fine-grained alignment patterns, we utilize the multi-mode structures of MD and BSD. Our method is totally end-to-end and can mitigate the data scarcity problem in MD. Competitive results are reported on four public datasets. Our code and datasets are available.": 324,
    "How can the problem of limited training data in metaphor detection be mitigated to improve model performance and generalization ability?": 325,
    "The paper presents our work on corpus annotationfor metaphor in German. Metaphors denoteentities that are similar to their literal referent,e.g., when *Licht* ‘light’ is used in the senseof ‘hope’. We are interested in the relation betweenmetaphor and register, hence, the corpusincludes material from different registers.We focussed on metaphors that can serve asregister markers and can also be reliably indentifiedfor annotation. Our results show hugedifferences between registers in metaphor usage,which we interpret in terms of specificproperties of the registers.": 326,
    "How can the relationship between metaphor usage and linguistic register be systematically investigated and characterized within a German corpus?": 327,
    "Propaganda aims to persuade an audience by appealing to emotions and using faulty reasoning, with the purpose of promoting a particular point of view. Similarly, metaphor modifies the semantic frame, thus eliciting a response that can be used to tune up or down the emotional volume of the message. Given the close relationship between them, we hypothesize that, when modeling them computationally, it can be beneficial to do so jointly. In particular, we perform multi-task learning with propaganda identification as the main task and metaphor detection as an auxiliary task. To the best of our knowledge, this is the first work that models metaphor and propaganda together. We experiment with two datasets for identifying propaganda techniques in news articles and in memes shared on social media. We find that leveraging metaphor improves model performance, particularly for the two most common propaganda techniques: loaded language and name-calling.": 328,
    "How can the computational modeling of propaganda identification be improved by jointly considering the role of metaphor?": 329,
    "Dense retrieval models have exhibited remarkable effectiveness, but they rely on abundant labeled data and face challenges when applied to different domains. Previous domain adaptation methods have employed generative models to generate pseudo queries, creating pseudo datasets to enhance the performance of dense retrieval models. However, these approaches typically use unadapted rerank models, leading to potentially imprecise labels. In this paper, we demonstrate the significance of adapting the rerank model to the target domain prior to utilizing it for label generation. This adaptation process enables us to obtain more accurate labels, thereby improving the overall performance of the dense retrieval model. Additionally, by combining the adapted retrieval model with the adapted rerank model, we achieve significantly better domain adaptation results across three retrieval datasets. We release our code for future research. 1": 330,
    "How can dense retrieval models be effectively adapted to new domains without relying on abundant labeled data, especially when existing methods struggle with label precision and full two-stage system adaptation?": 331,
    "Few-shot named entity recognition (NER) has shown remarkable progress in identifying entities in low-resource domains. However, few-shot NER methods still struggle with out-of-domain (OOD) examples due to their reliance on manual labeling for the target domain. To address this limitation, recent studies enable generalization to an unseen target domain with only a few labeled examples using data augmentation techniques. Two important challenges remain: First, augmentation is limited to the training data, resulting in minimal overlap between the generated data and OOD examples. Second, knowledge transfer is implicit and insufficient, severely hindering model generalizability and the integration of knowledge from the source domain. In this paper, we propose a framework, prompt learning with type-related features (PLTR), to address these challenges. To identify useful knowledge in the source domain and enhance knowledge transfer, PLTR automatically extracts entity type-related features (TRFs) based on mutual information criteria. To bridge the gap between training and OOD data, PLTR generates a unique prompt for each unseen example by selecting relevant TRFs. We show that PLTR achieves significant performance improvements on in-domain and cross-domain datasets. The use of PLTR facilitates model adaptation and increases representation similarities between the source and unseen domains.": 332,
    "How can few-shot named entity recognition models be effectively generalized to unseen target domains with only a limited number of labeled examples, overcoming the challenges of minimal data overlap and insufficient knowledge transfer from source domains?": 333,
    "Non-compositional expressions, by virtue of their non-compositionality, are a classic ‘pain in the neck’ for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model’s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git .": 334,
    "How can neural models effectively generate non-compositional expressions, such as idioms and metaphors, given their inherent non-compositionality and the limited availability of relevant training data?": 335,
    "Metaphorical language, such as “spending time together”, projects meaning from a source domain (here, money) to a target domain (time). Thereby, it highlights certain aspects of the target domain, such as the effort behind the time investment. Highlighting aspects with metaphors (while hiding others) bridges the two domains and is the core of metaphorical meaning construction. For metaphor interpretation, linguistic theories stress that identifying the highlighted aspects is important for a better understanding of metaphors. However, metaphor research in NLP has not yet dealt with the phenomenon of highlighting. In this paper, we introduce the task of identifying the main aspect highlighted in a metaphorical sentence. Given the inherent interaction of source domains and highlighted aspects, we propose two multitask approaches - a joint learning approach and a continual learning approach - based on a finetuned contrastive learning model to jointly predict highlighted aspects and source domains. We further investigate whether (predicted) information about a source domain leads to better performance in predicting the highlighted aspects, and vice versa. Our experiments on an existing corpus suggest that, with the corresponding information, the performance to predict the other improves in terms of model accuracy in predicting highlighted aspects and source domains notably compared to the single-task baselines.": 336,
    "How can the implicit meaning of metaphors, specifically their highlighted aspects and source domains, be computationally identified and interpreted to enhance understanding beyond explicit expressions?": 337,
    "Metaphor is a pervasive aspect of human communication, and its presence in multimodal forms has become more prominent with the progress of mass media. However, there is limited research on multimodal metaphor resources beyond the English language. Furthermore, the existing work in natural language processing does not address the exploration of categorizing the source and target domains in metaphors. This omission is significant considering the extensive research conducted in the fields of cognitive linguistics, which emphasizes that a profound understanding of metaphor relies on recognizing the differences and similarities between domain categories. We, therefore, introduce MultiCMET, a multimodal Chinese metaphor dataset, consisting of 13,820 text-image pairs of advertisements with manual annotations of the occurrence of metaphors, domain categories, and sentiments metaphors convey. We also constructed a domain lexicon that encompasses categorizations of metaphorical source domains and target domains and propose a Cascading Domain Knowledge Integration (CDKI) benchmark to detect metaphors by introducing domain-specific lexical features. Experimental results demonstrate the effectiveness of CDKI. The dataset and code are publicly available.": 338,
    "How can machines achieve a deep understanding of multimodal metaphors in non-English languages, specifically by categorizing their source and target domains, which is crucial for recognizing the underlying conceptual mappings?": 339,
    "Cross-domain Relation Extraction aims to transfer knowledge from a source domain to a different target domain to address low-resource challenges. However, the semantic gap caused by data bias between domains is a major challenge, especially in few-shot scenarios. Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce data bias based on the characteristics of each domain. This work takes a causal perspective and proposes a new framework CausalGF . By constructing a unified structural causal model, we estimate the causal effects of factors such as syntactic structure, label distribution, and entities on the outcome. CausalGF calculates the causal effects among the factors and adjusts them dynamically based on domain characteristics, enabling adaptive gap filling. Our experiments show that our approach better fills the domain gap, yielding significantly better results on the cross-domain few-shot relation extraction task.": 340,
    "How can the semantic gap caused by data bias between domains be effectively addressed to improve few-shot relation extraction in cross-domain scenarios?": 341,
    "Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas and evoke emotion. As figurative forms are often conveyed through multiple modalities (e.g., both text and images), understanding multimodal figurative language is an important AI challenge, weaving together profound vision, language, commonsense and cultural knowledge. In this work, we develop the Image Recognition of Figurative Language (IRFL) dataset. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset, and introduce two novel tasks as a benchmark for multimodal figurative language understanding. We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%). We release our dataset, benchmark, and code, in hopes of driving the development of models that can better understand figurative language.": 342,
    "How can artificial intelligence models effectively understand and interpret figurative language, such as metaphors, similes, and idioms, when it is conveyed through both text and images?": 343,
    "In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarse- to fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly and separately evaluate the performance of unseen slot types which entangled with their counterparts (i.e., seen slot types) in the previous zero-shot slot filling evaluation methods. The extensive empirical experiments on four datasets demonstrate that the proposed method achieves comparable or even better performance than the current state-of-the-art zero-shot slot filling approaches.": 344,
    "How can a model be developed for cross-domain zero-shot slot filling that possesses high generalization ability in unseen target domains, particularly for unseen slot types, without relying on annotated data in those domains?": 345,
    "Machine learning models have made incredible progress, but they still struggle when applied to examples from unseen domains. This study focuses on a specific problem of domain generalization, where a model is trained on one source domain and tested on multiple target domains that are unseen during training. We propose IMO: Invariant features Masks for Out-of-Distribution text classification, to achieve OOD generalization by learning invariant features. During training, IMO would learn sparse mask layers to remove irrelevant features for prediction, where the remaining features keep invariant. Additionally, IMO has an attention module at the token level to focus on tokens that are useful for prediction. Our comprehensive experiments show that IMO substantially outperforms strong baselines in terms of various evaluation metrics and settings.": 346,
    "How can machine learning models be made more robust and generalizable to unseen domains in text classification, particularly by mitigating the impact of spurious correlations and learning domain-invariant features?": 347,
    "Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/metaphor-understanding-challenge.": 348,
    "How can the metaphor understanding capabilities of large language models be accurately evaluated, particularly in distinguishing between target and source domain interpretations and assessing their reasoning processes?": 349,
    "Empathy is a social mechanism used to sup-001 port and strengthen emotional connection with 002 others, including in online communities. How-003 ever, little is currently known about the nature 004 of these online expressions, nor the specific fac-005 tors that may lead to their improved detection. 006 In this work, we study the role of a specific 007 and complex subcategory of linguistic phenom-008 ena, figurative language, in online expressions 009 of empathy. Our extensive experiments reveal 010 that incorporating features regarding the use of 011 metaphor, idiom, and hyperbole into empathy 012 detection models improves their performance, 013 resulting in impressive maximum F 1 scores of 014 0.942 and 0.809 for identifying posts without 015 and with empathy, respectively. 016": 350,
    "How can figurative language enhance the detection of empathy in online expressions within specialized domains?": 351,
    "Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms. These are further refined using execution-guided feedback. Experiments over multiple source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments show that FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when training data is limited.": 352,
    "How can Knowledge Base Question Answering (KBQA) systems be effectively deployed in new domains with only a few labeled examples, by leveraging a larger labeled training dataset available in a source domain?": 353,
    "Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor detection demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework called C4MMD, which utilizes a Chain-of-Thought (CoT) method for Multi-modal Metaphor Detection. Specifically, our approach designs a three-step process inspired by CoT that extracts and integrates knowledge from Multi-modal Large Language Models (MLLMs) into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor detection capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor detection tasks. The code for our method is publicly available at https://github. com/xyz189411yt/C4MMD.": 354,
    "How can multi-modal metaphor detection be accurately performed, given the challenges of deeply interpreting both textual and visual elements, the requirement for extensive common-sense knowledge, and the prevalence of low-quality textual data in internet memes?": 355,
    "Metaphor detection aims to identify whether a linguistic expression in text is metaphorical or literal. Most existing research tackles this problem either using word-pair or token-level information as input, and thus treats word-pair and token-level metaphor detection as distinct subtasks. Benefited from the simplified structure of word pairs, recent methods for word-pair metaphor detection can provide intermediate explainable clues for the detection results, which remains a challenging issue for token-level metaphor detection. To mitigate this issue in token-level metaphor detection and take advantage of word pairs, in this paper, we make the first attempt to bridge word-pair and token-level metaphor detection via modeling word pairs within a sentence as explainable intermediate information. As the central role of verb in metaphorical expressions, we focus on token-level verb metaphor detection and propose a novel explainable Word Pair based Domain Mining (WPDM) method. Our work is inspired by conceptual metaphor theory (CMT). We first devise an approach for conceptual domain mining utilizing semantic role mapping and resources at cognitive, commonsense and lexical levels. We then leverage the inconsistency between source and target domains for core word pair modeling to facilitate the explainability. Experiments on four datasets verify the effectiveness of our method and demonstrate its capability to provide the core word pair and corresponding conceptual domains as explainable clues for metaphor detection.": 356,
    "How can token-level metaphor detection be improved by integrating word-pair information and leveraging conceptual domain mining for better explainability?": 357,
    "Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F$_1$. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is rephrasing disclosures into less specific terms while preserving their utility, e.g.,\"Im 16F\"to\"I'm a teenage girl\". We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation. To help users in deciding which disclosures to abstract, we present a task of rating their importance for context understanding. Our fine-tuned model achieves 80% accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we will only release our corpus and models to researcher who agree to the ethical guidelines outlined in Ethics Statement.": 358,
    "How can users be effectively helped to identify and mitigate privacy risks associated with their online self-disclosures?": 359,
    "Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.": 360,
    "How can large language models (LLMs) effectively address subjective tasks that require interpretation, emotional response, and nuanced understanding, given their current limitations in these areas? Is it possible to leverage LLMs' strong dialogue generation capabilities to facilitate subjective reasoning, rather than relying on traditional objective reasoning pathways?": 361,
    "Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average. While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct). These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation.": 362,
    "How can the pragmatic understanding of Large Language Models (LLMs) be effectively evaluated in a generative setting, moving beyond discriminative methods that do not fully capture their ability to respond appropriately to non-literal language?": 363,
    "Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to 'unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.": 364,
    "How can the scarcity of aligned humorous and non-humorous text datasets be addressed to improve humor detection, especially given the limitations of large language models in reliably generating novel humor?": 365,
    "The illustration or visualization of figurative language, such as linguistic metaphors, is an emerging challenge for existing Large Language Models (LLMs) and multimodal models. Due to their comparison of seemingly unrelated concepts in metaphors, existing LLMs have a tendency of over-literalization, which illustrates figurative language solely based on literal objects, ignoring the underlying groundings and associations across disparate metaphorical domains. Furthermore, prior approaches have ignored the binding process between visual objects and metaphorical attributes, which further intensifies the infidelity of visual metaphors. To address the issues above, we propose GOME (Grounding-based Metaphor Binding), which illustrates linguistic metaphors from the grounding perspective elaborated through LLMs. GOME consists of two steps for metaphor illustration, including grounding-based elaboration and scenario visualization. In the elaboration step, metaphorical knowledge is integrated into systematic instructions for LLMs, which employs a CoT prompting method rooted in rhetoric. This approach specifies metaphorical devices such as vehicles and groundings, to ensure accurate and faithful descriptions consumed by text-to-image models. In the visualization step, an inference-time metaphor binding method is realized based on elaboration outputs, which register attentional control during the diffusion process, and captures the underlying attributes from the abstract metaphorical domain. Comprehensive evaluations using multiple downstream tasks confirm that, GOME is superior to isolated LLMs, diffusion models, or their direct collaboration.": 366,
    "How can large language models and multimodal models accurately illustrate figurative language, such as linguistic metaphors, without over-literalization and while faithfully binding metaphorical attributes to visual objects?": 367,
    "Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.": 368,
    "Is it possible for machines to achieve a similar multimodal understanding capability as humans, who can integrate information from various modalities to resolve textual ambiguities?": 369,
    "Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.": 370,
    "How can large language models be systematically supported to create consistent and coherent formal mathematical libraries from informal mathematical statements?": 371,
    "Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, StorySumm, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.": 372,
    "How can faithfulness in narrative summarization be accurately and reliably evaluated, especially given the challenges of subtle errors and human annotation variability?": 373,
    "Advances in multimodal models have greatly improved how interactions relevant to various tasks are modeled. Today’s multimodal models mainly focus on the correspondence between images and text, using this for tasks like image-text matching. However, this covers only a subset of real-world interactions. Novel interactions, such as sarcasm expressed through opposing spoken words and gestures or humor expressed through utterances and tone of voice, remain challenging. In this paper, we introduce an approach to enhance multimodal models, which we call Multimodal Mixtures of Experts (MMoE). The key idea in MMoE is to train separate expert models for each type of multimodal interaction, such as redundancy present in both modalities, uniqueness in one modality, or synergy that emerges when both modalities are fused. On a sarcasm detection task (MUStARD) and a humor detection task (URFUNNY), we obtain new state-of-the-art results. MMoE is also able to be applied to various types of models to gain improvement.": 374,
    "How can multimodal models be enhanced to effectively capture the diverse types of interactions between modalities, moving beyond the primary focus on redundancy to address unique and synergistic relationships?": 375,
    "Machine Translation (MT) has developed rapidly since the release of Large Language Models and current MT evaluation is performed through comparison with reference human translations or by predicting quality scores from human-labeled data. However, these mainstream evaluation methods mainly focus on fluency and factual reliability, whilst paying little attention to figurative quality. In this paper, we investigate the figurative quality of MT and propose a set of human evaluation metrics focused on the translation of figurative language. We additionally present a multilingual parallel metaphor corpus generated by post-editing. Our evaluation protocol is designed to estimate four aspects of MT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so, we observe that translations of figurative expressions display different traits from literal ones.": 376,
    "How can the quality of machine translation for metaphorical language be accurately and comprehensively evaluated, considering aspects beyond mere fluency and factual reliability?": 377,
    "The limited generalization of coreference resolution (CR) models has been a major bottleneck in the task's broad application. Prior work has identified annotation differences, especially for mention detection, as one of the main reasons for the generalization gap and proposed using additional annotated target domain data. Rather than relying on this additional annotation, we propose an alternative referential task, Major Entity Identification (MEI), where we: (a) assume the target entities to be specified in the input, and (b) limit the task to only the frequent entities. Through extensive experiments, we demonstrate that MEI models generalize well across domains on multiple datasets with supervised models and LLM-based few-shot prompting. Additionally, MEI fits the classification framework, which enables the use of robust and intuitive classification-based metrics. Finally, MEI is also of practical use as it allows a user to search for all mentions of a particular entity or a group of entities of interest.": 378,
    "How can the limited generalization of coreference resolution models across domains be addressed to enable broader application?": 379,
    "Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM’s ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the “lazy pun generation” pattern and identify the primary challenges LLMs encounter in understanding puns.": 380,
    "How can large language models (LLMs) be systematically evaluated for their ability to understand and generate puns?": 381,
    "Abstract Proverbs carry wisdom transferred orally from generation to generation. Based on the place they were recorded, this study introduces a publicly-available and machine-actionable dataset of more than one hundred thousand Greek proverb variants. By quantifying the spatial distribution of proverbs, we show that the most widespread proverbs come from the mainland while the least widespread proverbs come primarily from the islands. By focusing on the least dispersed proverbs, we present the most frequent tokens per location and undertake a benchmark in geographical attribution, using text classification and regression (text geocoding). Our results show that this is a challenging task for which specific locations can be attributed more successfully compared to others. The potential of our resource and benchmark is showcased by two novel applications. First, we extracted terms moving the regression prediction toward the four cardinal directions. Second, we leveraged conformal prediction to attribute 3,676 unregistered proverbs with statistically rigorous predictions of areas each of these proverbs was possibly registered in.": 382,
    "How can computational methods be employed to spatially explore, characterize, and geographically attribute Greek proverbs, especially given their oral transmission and linguistic variations?": 383,
    "Recent works have demonstrated the effectiveness of self-alignment in which a large language model is aligned to follow general instructions using instructional data generated from the model itself starting from a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine, finance). As a preliminary, we quantitively show the marginal effect that generic instruction-following training has on downstream expert domains' performance. To remedy this, we propose self-specialization - allowing for effective model specialization while achieving cross-task generalization by leveraging only a few labeled seeds. Self-specialization offers a data- and parameter-efficient way of\"carving out\"an expert model out of a generalist pre-trained LLM. Exploring a variety of popular open large models as a base for specialization, our experimental results in both biomedical and financial domains show that our self-specialized models outperform their base models by a large margin, and even larger models that are generally instruction-tuned or that have been adapted to the target domain by other means.": 384,
    "How can large language models effectively specialize in expert domains while maintaining cross-task generalization, using minimal human supervision and computational resources?": 385,
    "Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine-grained training. We present a demonstration of a multimodal fine-grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically, we design a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences. Furthermore, we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals, conversations, XR responses, and vision question answering. Last, we present several prevailing open-resource LLMs as benchmarks, assessing their performance with and without fine-tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments, fostering research in both AI and HCI communities.": 386,
    "How can autonomous AI agents be effectively integrated into mixed reality environments to provide fine-grained, comprehensive training assistance, particularly for complex assembly tasks?": 387,
    "Various euphemisms are emerging in social net-001 works, attracting widespread attention from 002 the natural language processing community. 003 However, existing euphemism datasets are only 004 domain-specific or language-specific. In ad-005 dition, existing approaches to the study of eu-006 phemisms are one-sided. Either only the eu-007 phemism detection task or only the euphemism 008 identification task is accomplished, lacking a 009 unified framework. To this end, we construct 010 a large-scale B ilingual M ulti-category dataset 011 of E uphemisms named BME , which covers a 012 total of 12 categories for two languages, En-013 glish and Chinese. Then, we first propose a 014 unified generative model to Joint ly conduct 015 the tasks of bilingual E uphemism D etection 016 and I dentification named JointEDI . By com-017 paring with LLMs and human evaluation, we 018 demonstrate the effectiveness of the proposed 019 JointEDI and the feasibility of unifying eu-020 phemism detection and euphemism identifica-021 tion tasks. Moreover, the BME dataset also pro-022 vides a new reference standard for euphemism 023 detection and euphemism identification. 024 Disclaimer: This paper contains discrimina-025 tory content that may be disturbing to some 026 readers. 027": 388,
    "How can a unified framework be developed to simultaneously perform euphemism detection and identification across multiple languages, addressing the limitations of existing domain-specific or language-specific approaches?": 389,
    "Although large language models (LLMs) acquire extensive world knowledge and some reasoning abilities, their proficiency in generating humorous sentences remains a challenge. Previous research has demonstrated that the humor generation capabilities of ChatGPT are confined to producing merely 25 unique jokes. In this work, we concentrate on endowing LLMs with the ability of generating puns, a particular category of humor by preference learning method. We propose a multi-stage curriculum preference learning framework to optimize both pun structure preferences and humor preferences. Specifically, we improve the Direct Preference Optimization (DPO) algorithm to address the challenge of multi-objective alignment problem. Besides, to facilitate further advancement in this field, we collect a Chinese Pun (ChinesePun) dataset, containing 2.1k puns and corresponding annotations. Experimental results on both Chinese and English benchmark datasets demonstrate that our method significantly outperforms all the baseline models.": 390,
    "How can large language models (LLMs) be effectively endowed with the ability to generate humorous sentences, specifically puns, given their current limitations in producing unique and high-quality humorous content?": 391,
    "Aspect-based Sentiment Analysis (ABSA) is extensively researched in the NLP community, yet related models face challenges due to data sparsity when shifting to a new domain. Hence, data augmentation for cross-domain ABSA has attracted increasing attention in recent years. However, two key points have been neglected in prior studies: First, target domain unlabeled data are labeled with pseudo labels by the model trained in the source domain with little quality control, leading to inaccuracy and error propagation. Second, the label and text patterns of generated labeled data are monotonous, thus limiting the robustness and generalization ability of trained ABSA models. In this paper, we aim to design a simple yet effective framework to address the above shortages in ABSA data augmentation, called Refining and Synthesis Data Augmentation (RSDA). Our framework roughly includes two steps: First, it refines generated labeled data using a natu-ral language inference (NLI) filter to control data quality. Second, it synthesizes diverse labeled data via novel label composition and para-phrase approaches. We conduct experiments on 4 kinds of ABSA subtasks, and our framework outperforms 7 strong baselines, demonstrating its effectiveness.": 392,
    "How can a data augmentation framework for cross-domain aspect-based sentiment analysis be designed to effectively address data sparsity while simultaneously improving the quality and diversity of generated labeled data?": 393,
    "Recent advancements in joint speech-text pre-training have significantly advanced the processing of natural language. However, a key limitation is their reliance on parallel speech-text data, posing challenges due to data accessibility. Addressing this, our paper introduces an innovative framework for jointly performing speech and text processing without parallel corpora during pre-training but only downstream. Utilizing pre-trained unimodal models, we extract distinct representations for speech and text, aligning them effectively in a newly defined space using a multi-level contrastive learning mechanism. A unique swap reconstruction mechanism enhances the alignment and is followed by fusion via a multi-head mechanism, seamlessly merging modality-invariant and modality-specific representations. Testing for emotion recognition (SLU task) and idiom usage detection (NLU task) demonstrates robust performance, with commendable robustness to noise in text or speech data.": 394,
    "How can speech and text processing be jointly performed without relying on large parallel corpora during pre-training, which are often difficult to acquire and limit the applicability of such models?": 395,
    "The identification of Figurative Language (FL) features in text is crucial for various Natural Language Processing (NLP) tasks, where understanding of the author's intended meaning and its nuances is key for successful communication. At the same time, the use of a specific blend of various FL forms most accurately reflects a writer's style, rather than the use of any single construct, such as just metaphors or irony. Thus, we postulate that FL features could play an important role in Authorship Attribution (AA) tasks. We believe that our is the first computational study of AA based on FL use. Accordingly, we propose a Multi-task Figurative Language Model (MFLM) that learns to detect multiple FL features in text at once. We demonstrate, through detailed evaluation across multiple test sets, that the our model tends to perform equally or outperform specialized binary models in FL detection. Subsequently, we evaluate the predictive capability of joint FL features towards the AA task on three datasets, observing improved AA performance through the integration of MFLM embeddings.": 396,
    "How can figurative language features be effectively modeled and subsequently utilized to enhance performance in authorship attribution tasks?": 397,
    "Pre-trained language models (PLMs) exhibit promising retrieval performance in various domains. However, they struggle in domains un-seen during training, since the word distribution can shift significantly. To remedy this, GPL, a generative domain adaptation (DA) method, was proposed to generate pseudo queries and labels for documents in unseen domains to further train the retriever model. However, the pseudo queries often do not resemble real queries from the target domains, as they do not integrate the domain’s distributional information. we propose Distribution-Aware Domain Adaptation (DADA) to guide the model to incorporate the term distributions at both the document-level and the corpus-level, which we refer to as observation-level and domain-level feedback, respectively. Empirical results on five distinct datasets demonstrate that our method effectively adapts the model to target domains and expands document representation to unseen gold query terms. 1": 398,
    "How can pre-trained language models be effectively adapted for information retrieval in out-of-domain scenarios, given that existing methods struggle to integrate target domain distributional information into pseudo-query generation?": 399,
    "This paper serves as a foundational step towards the development of a linguistically motivated and technically relevant evaluation suite for Greek NLP. We initiate this endeavor by introducing four expert-verified evaluation tasks, specifically targeted at natural language inference, word sense disambiguation (through example comparison or sense selection) and metaphor detection. More than language-adapted replicas of existing tasks, we contribute two innovations which will resonate with the broader resource and evaluation community. Firstly, our inference dataset is the first of its kind, marking not just \\textit{one}, but rather \\textit{all} possible inference labels, accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we demonstrate a cost-efficient method to obtain datasets for under-resourced languages. Using ChatGPT as a language-neutral parser, we transform the Dictionary of Standard Modern Greek into a structured format, from which we derive the other three tasks through simple projections. Alongside each task, we conduct experiments using currently available state of the art machinery. Our experimental baselines affirm the challenging nature of our tasks and highlight the need for expedited progress in order for the Greek NLP ecosystem to keep pace with contemporary mainstream research.": 400,
    "How can a linguistically motivated and technically relevant evaluation suite be developed for Modern Greek NLP, given the scarcity of high-quality resources and the limitations of existing multilingual models for under-resourced languages?": 401,
    "Euphemisms are found across the world’s languages, making them a universal linguistic phenomenon. As such, euphemistic data may have useful properties for computational tasks across languages. In this study, we explore this premise by training a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic “categories” such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.": 402,
    "How can multilingual transformer models be effectively utilized to disambiguate potentially euphemistic terms across multiple languages, and what properties of euphemisms facilitate or hinder cross-lingual knowledge transfer in such models?": 403,
    "Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) – identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the target domain, observing further TD transfer gains. Finally, we demonstrate that the gains are robust to the choice of the OIE system.": 404,
    "How can the robustness of domain transfer for event trigger detection be enhanced, particularly when transferring from high-resource to low-resource domains, to mitigate significant performance drops?": 405,
    "Keyphrase extraction is the task of identifying a set of keyphrases present in a document that captures its most salient topics. Scientific domain-specific pre-training has led to achieving state-of-the-art keyphrase extraction performance with a majority of benchmarks being within the domain. In this work, we explore how to effectively enable the cross-domain generalization capabilities of such models without requiring the same scale of data. We primarily focus on the few-shot setting in non-scientific domain datasets such as OpenKP from the Web domain & StackEx from the StackExchange forum. We propose to leverage topic information intrinsically available in the data, to build a novel clustering-based sampling approach that facilitates selecting a few samples to label from the target domain facilitating building robust and performant models. This approach leads to large gains in performance of up to 26.35 points in F1 when compared to selecting few-shot samples uniformly at random. We also explore the setting where we have access to labeled data from the model’s pretraining domain corpora and perform gradual training which involves slowly folding in target domain data to the source domain data. Here we demonstrate further improvements in the model performance by up to 12.76 F1 points.": 406,
    "How can keyphrase extraction models effectively generalize to new, non-scientific domains in a few-shot setting without requiring large-scale data?": 407,
    "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \\texttt{LLM-CL} model obtains new state-of-the-art performance.": 408,
    "How can Large Language Models effectively learn new domain-specific knowledge for Aspect-based Sentiment Analysis while simultaneously retaining previously acquired knowledge and mitigating catastrophic forgetting?": 409,
    "Metaphor, as an advanced form of cognition, is challenging to understand their meaning. Current metaphor detection tasks only provide labels (i.e., metaphor or literal) without interpreting how to understand them. In this paper, we improve the metaphor detection task and explore the reason of metaphor. To the best of our knowledge, we are the first work to reason about metaphor using mainstream Large Language Models (LLMs). Specifically, we utilized ChatGPT3.5 to expand the mainstream datasets in current metaphor detection, including VUA ALL, TroFi, and MOH-X. We input the original sentence, target word, and usage (metaphor or literal) into ChatGPT, guiding it to generate corresponding metaphor reason. Then, we designed supervised baseline experiments (e.g., RoBERTa, GPT-2) and zero-shot experiments with LLMs (e.g., LLaMA3). For the results generated by the above experiments, we provided the case study. We devised four methods that include manual evaluation to evaluate the reason performance of the model, and discussed extensively the advantages and disadvantages of these evaluation methods. Our code is available at https://github.com/yc-cy/Metaphorical-Reasoning.": 410,
    "How can the understanding of metaphor be deepened beyond mere detection to include reasoning about its usage and meaning?": 411,
    "Metaphors are a common communication tool used in our day-to-day life. The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored. Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts. As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos. Hence, we introduce a new VL task of describing the metaphors present in the videos in our work. To facilitate this novel task, we construct and release a manually created dataset with 705 videos and 2115 human-written captions, along with a new metric called Average Concept Distance (ACD), to automatically evaluate the creativity of the metaphors generated. We also propose a novel low-resource video metaphor captioning system: GIT-LLaVA, which obtains comparable performance to SoTA video language models on the proposed task. We perform a comprehensive analysis of existing video language models on this task and publish our dataset, models, and benchmark results to enable further research.": 412,
    "How can videos containing metaphors be accurately captioned to convey the metaphorical meaning present in the video?": 413,
    "Pre-trained Language Models (PLMs) exhibit good accuracy and generalization ability across various tasks using self-supervision, but their large size results in high inference latency. Early Exit (EE) strategies handle the issue by allowing the samples to exit from classifiers attached to the intermediary layers, but they do not generalize well, as exit classifiers can be sensitive to domain changes. To address this, we propose Unsupervised Domain Adaptation in EE framework (DAdEE) that employs multi-level adaptation using knowledge distillation. DAdEE utilizes GAN-based adversarial adaptation at each layer to achieve domain-invariant representations, reducing the domain gap between the source and target domain across all layers. The attached exits not only speed up inference but also enhance domain adaptation by reducing catastrophic forgetting and mode collapse, making it more suitable for real-world scenarios. Experiments on tasks such as sentiment analysis, entailment classification, and natural language inference demonstrate that DAdEE consistently outperforms not only early exit methods but also various domain adaptation methods under domain shift scenarios. The anonymized source code is available at https://github.com/Div290/DAdEE.": 414,
    "How can Early Exit Pre-trained Language Models be effectively adapted to diverse target domains in an unsupervised setting, ensuring robust generalization and maintaining inference efficiency despite domain shifts?": 415,
    "We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for example, we observe improvements of up to 13.9% over target domain gold data when the latter is augmented with our generated examples.": 416,
    "How can large language models (LLMs) reliably generate content-grounded, multi-turn question-answer conversations while effectively mitigating their propensity to hallucinate, particularly in closed-domain settings?": 417,
    "In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-{3.5, 4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.": 418,
    "How can pre-trained large language models be effectively adapted as agents for target domains with large action spaces, such as web navigation, without requiring direct and costly fine-tuning on the entire model?": 419,
    "The identification of metaphor is a crucial prerequisite for many downstream language tasks, such as sentiment analysis, opinion mining, and textual entailment. State-of-the-art systems of metaphor detection implement heuristic principles such as Metaphor Identification Procedure (MIP) and Selection Preference Violation (SPV). We propose an innovative approach that leverages the cognitive information of embodiment that can be derived from word embeddings, and explicitly models the process of sensorimotor change that has been demonstrated as essential for human metaphor processing. We showed that this cognitively motivated module is effective and can improve metaphor detection, compared with the heuristic MIP that has been applied previously.": 420,
    "How can computational models for metaphor detection effectively leverage cognitively informed sensorimotor information and its dynamic changes in context?": 421,
    "Domain adaptation from labeled source domains to the target domain is important in practical summarization scenarios. However, the key challenge is domain knowledge dis-entanglement. In this work, we explore how to disentangle domain-invariant knowledge from source domains while learning specific knowledge of the target domain. Specifically, we propose a hypernetwork-assisted encoder-decoder architecture with parameter-efficient fine-tuning. It leverages a hypernetwork instruction learning module to generate domain-specific parameters from the encoded inputs accompanied by task-related instruction. Further, to better disentangle and transfer knowledge from source domains to the target domain, we introduce a meta-knowledge distillation strategy to build a meta-teacher model that captures domain-invariant knowledge across multiple domains and use it to transfer knowledge to students. Experiments on three dialogue summarization datasets show the effectiveness of the proposed model. Human evaluations also show the superiority of our model with regard to the summary generation quality.": 422,
    "How can domain-invariant knowledge be disentangled from source domains while simultaneously learning specific knowledge of the target domain in practical summarization scenarios?": 423,
    "Based on the assumption that samples in the source and target domains are freely accessible during training, unsupervised domain adaptation (UDA) of question answering (QA) aims to transfer knowledge learned from labeled source datasets to similar tasks in the unlabeled target domains. However, such assumption can easily lead to privacy violation issues in real-world applications, especially when the source domain data involves privacy-intensive domains such as finance and healthcare. In this paper, we introduce Source-Free Domain Adaptation Framework for QA (denoted as SFQA), which only allows access to trained source models for target learning, making data privacy protection more promising. Specifically, the proposed SFQA model consists of a feature extractor module (Bert Encoder) and a classifier module (Answer Classifier). We first transfer the trained source model to the target model while keeping the source classifier module frozen. Then we adopt the question generation model to generate questions and answers for the target domain. Taking the generated questions and target domain context as inputs, and the generated answers as pseudo-labels, we train the target model with joint entropy to learn a target domain-specific feature extractor. Experimental results demonstrate the superiority and effectiveness of the proposed SFQA, and show that SFQA outperforms the state-of-the-art methods.": 424,
    "How can a Question Answering model be effectively adapted to unlabeled target domains in a source-free setting, particularly by improving its generalizability through enhanced source domain training?": 425,
    "Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task-agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.": 426,
    "How can large language models be effectively compressed for domain-specific applications while simultaneously preserving their general linguistic capabilities and domain-specific knowledge?": 427,
    "Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5% absolute value.": 428,
    "How can a named entity recognition model effectively transfer knowledge from a source domain to a target domain while mitigating the mislabeling of source entities as target entities, especially when both entity types co-occur in the same document and target domain data is scarce?": 429,
    "Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in a situational context, human expectations vary depending on the relevant cultural common ground. As languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs “know” limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a “culture gap” in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticulturAl Proverbs and Sayings) for proverb understanding with conversational context for six different languages.": 430,
    "How well do multilingual large language models (mLLMs) understand and apply cultural common ground when interpreting proverbs and sayings in conversational contexts, and do \"\"culture gaps\"\" exist in their cross-cultural reasoning abilities?": 431,
    "Metaphor detection is a challenging task for natural language processing (NLP) systems. Previous works failed to sufficiently utilize the internal and external semantic relationships between target words and their context. Furthermore, they have faced challenges in tackling the problem of data sparseness due to the very limited available training data. To address these two challenges, we propose a novel model called MiceCL. By leveraging the difference between the literal meaning of the target word and the meaning of the sentence as the sentence external difference, MiceCL can better handle the semantic relationships. Additionally, we propose a curriculum learning framework for automatically assessing difficulty of the sentence with a pre-trained model. By starting from easy examples and gradually progressing to more difficult ones, we can ensure that the model will not deal with complex data when its ability is weak so that to avoid wasting limited data. Experimental results demonstrate that MiceCL achieves competitive performance across multiple datasets, with a significantly improved convergence speed compared to other models.": 432,
    "How can natural language processing systems effectively detect metaphors by better utilizing internal and external semantic relationships within sentences and addressing the problem of data sparseness due to limited available training data?": 433,
    "Metaphorical language is a pivotal element inthe realm of political framing. Existing workfrom linguistics and the social sciences providescompelling evidence regarding the distinctivenessof conceptual framing for politicalideology perspectives. However, the nature andutilization of metaphors and the effect on audiencesof different political ideologies withinpolitical discourses are hardly explored. Toenable research in this direction, in this workwe create a dataset, originally based on newseditorials and labeled with their persuasive effectson liberals and conservatives and extend itwith annotations pertaining to metaphorical usageof language. To that end, first, we identifyall single metaphors and composite metaphors.Secondly, we provide annotations of the sourceand target domains for each metaphor. As aresult, our corpus consists of 300 news editorialsannotated with spans of texts containingmetaphors and the corresponding domains ofwhich these metaphors draw from. Our analysisshows that liberal readers are affected bymetaphors, whereas conservatives are resistantto them. Both ideologies are affected differentlybased on the metaphor source and targetcategory. For example, liberals are affected bymetaphors in the Darkness & Light (e.g., death)source domains, where as the source domain ofNature affects conservatives more significantly.": 434,
    "How can the nature and utilization of metaphors and their effect on audiences of different political ideologies within political discourses be explored, particularly concerning their persuasive impact in news editorials?": 435,
    "Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.": 436,
    "How can pre-trained language models be fine-tuned to novel domains in a way that increases their sensitivity and awareness towards domain-specific terms, mitigating the impact of disparities between source and target domains?": 437,
    "State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method’s performance and to identify promising areas for further improvements.": 438,
    "How can effective and diverse synthetic training data be automatically generated to finetune a modern neural ranker for a new domain in an unsupervised manner, especially when zero-shot neural ranking is sub-optimal and acquiring large, high-quality target training data is costly?": 439,
    "A new giant outburst of the Be X-ray binary RX J0520.5–6932 was detected and subsequently observed with several space-borne and ground-based instruments. This study presents a comprehensive analysis of the optical and X-ray data, focusing on the spectral and timing characteristics of selected X-ray observations. A joint fit of spectra from simultaneous observations performed by the X-ray telescope (XRT) on the Neil Gehrels Swift Observatory (Swift) and Nuclear Spectroscopic Telescope ARray (NuSTAR) provides broad-band parameter constraints, including a cyclotron resonant scattering feature (CRSF) at $32.2_{-0.7}^{+0.8}$ keV with no significant energy change since 2014, and a weaker Fe line. Independent spectral analyses of observations by the Lobster Eye Imager for Astronomy, Einstein Probe (EP), Swift–XRT, and NuSTAR demonstrate the consistency of parameters across different bands. Luminosity variations during the current outburst were tracked. The light curve of the Optical Gravitational Lensing Experiment (OGLE) aligns with the X-ray data in both 2014 and 2024. Spin evolution over 10 yr is studied after adding Fermi Gamma-ray Burst Monitor data, improving the orbital parameters, with an estimated orbital period of 24.39 d, slightly differing from OGLE data. Despite intrinsic spin-up during outbursts, a spin-down of $\\sim$0.04 s over 10.3 yr is suggested. For the new outburst, the pulse profiles indicate a complicated energy-dependent shape, with decreases around 15 and 25 keV in the pulsed fraction, a first for an extragalactic source. Phase-resolved NuSTAR data indicate variations in parameters such as flux, photon index, and CRSF energy with rotation phase.": 440,
    "How can large language models be effectively instructed to infer the underlying reasoning process for metaphor detection, particularly by integrating explainable concepts from metaphor theories and their linguistic manifestations?": 441,
    "Cross-domain few-shot Relation Extraction (RE) aims to transfer knowledge from a source domain to a different target domain to address low-resource problems.Previous work utilized label descriptions and entity information to leverage the knowledge of the source domain.However, these models are prone to confusion when directly applying this knowledge to a target domain with entirely new types of relations, which becomes particularly pronounced when facing similar relations.In this work, we propose a relation-aware prompt learning method with pre-training.Specifically, we empower the model to clear confusion by decomposing various relation types through an innovative label prompt, while a context prompt is employed to capture differences in different scenarios, enabling the model to further discern confusion. Two pre-training tasks are designed to leverage the prompt knowledge and paradigm.Experiments show that our method outperforms previous sota methods, yielding significantly better results on cross-domain few-shot RE tasks.": 442,
    "How can models effectively differentiate between similar relation types in entirely new target domains during cross-domain few-shot relation extraction, especially when direct application of source domain knowledge leads to confusion?": 443,
    "There are several linguistic claims about situations where words are more likely to be used as metaphors.However, few studies have sought to verify such claims with large corpora.This study entails a large-scale, corpus-based analysis of certain existing claims about verb metaphors, by applying metaphor detection to sentences extracted from Common Crawl and using the statistics obtained from the results.The verification results indicate that the direct objects of verbs used as metaphors tend to have lower degrees of concreteness, imageability, and familiarity, and that metaphors are more likely to be used in emotional and subjective sentences.": 444,
    "In what way can large-scale automatic metaphor identification be leveraged to verify linguistic claims about metaphors?": 445,
    "Universal domain adaptation (UniDA) aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain under domain shift and category shift. Without prior category overlap information, it is challenging to simultaneously align the common categories between two domains and separate their respective private categories. Additionally, previous studies utilize the source classifier's prediction to obtain various known labels and one generic \"unknown\" label of target samples. However, over-reliance on learned classifier knowledge is inevitably biased to source data, ignoring the intrinsic structure of target domain. Therefore, in this paper, we propose a novel two-stage UniDA framework called MATHS based on the principle of mutual nearest neighbor contrast and hybrid prototype discrimination. In the first stage, we design an efficient mutual nearest neighbor contrastive learning scheme to achieve feature alignment, which exploits the instance-level affinity relationship to uncover the intrinsic structure of two domains. We introduce a bimodality hypothesis for the maximum discriminative probability distribution to detect the possible target private samples, and present a data-based statistical approach to separate the common and private categories. In the second stage, to obtain more reliable label predictions, we propose an incremental pseudo-classifier for target data only, which is driven by the hybrid representative prototypes. A confidence-guided prototype contrastive loss is designed to optimize the category allocation uncertainty via a self-training mechanism. Extensive experiments on three benchmarks demonstrate that MATHS outperforms previous state-of-the-arts on most UniDA settings.": 446,
    "How can knowledge be effectively transferred from a labeled source domain to an unlabeled target domain under both domain shift and category shift, without requiring prior information about the target label space?": 447,
    "Universal domain adaptation (UniDA) aims to transfer the knowledge learned from a labeled source domain to an unlabeled target domain without any constraints on the label sets. However, domain shift and category shift make UniDA extremely challenging, mainly attributed to the requirement of identifying both shared “known” samples and private “unknown” samples. Previous methods barely exploit the intrinsic manifold structure relationship between two domains for feature alignment, and they rely on the softmax-based scores with class competition nature to detect underlying “unknown” samples. Therefore, in this paper, we propose a novel evidential neighborhood contrastive learning framework called TNT to address these issues. Specifically, TNT first proposes a new domain alignment principle: semantically consistent samples should be geometrically adjacent to each other, whether within or across domains. From this criterion, a cross-domain multi-sample contrastive loss based on mutual nearest neighbors is designed to achieve common category matching and private category separation. Second, toward accurate “unknown” sample detection, TNT introduces a class competition-free uncertainty score from the perspective of evidential deep learning. Instead of setting a single threshold, TNT learns a category-aware heterogeneous threshold vector to reject diverse “unknown” samples. Extensive experiments on three benchmarks demonstrate that TNT significantly outperforms previous state-of-the-art UniDA methods.": 448,
    "How can knowledge be effectively transferred from a labeled source domain to an unlabeled target domain when the relationship between their label sets is unknown and both shared and private categories must be identified?": 449,
    "In this paper, based on an asymptotic analysis of the Softmax layer, we show that when training neural networks for classification tasks, the weight vectors corre sponding to each class of the Softmax layer tend to converge to the class-wise means computed at the representation layer (for specific choices of the representation activation). We further show some consequences of our findings to the context of transfer learning, essentially by proposing a simple yet effective initialization procedure that significantly accelerates the learning of the Softmax layer weights as the target domain gets closer to the source one. Experiments are notably performed on the datasets: MNIST, Fashion MNIST, Cifar10, and Cifar100 and using a standard CNN architecture.": 450,
    "How can the classification mechanism of neural networks be understood through the behavior of their Softmax layer representations, and how can this understanding be leveraged to enhance the efficiency and practicality of transfer learning?": 451,
    "Unsupervised domain adaptation has recently emerged as an effective paradigm for generalizing deep neural networks to new target domains. However, there is still enormous potential to be tapped to reach the fully supervised performance. In this paper, we present a novel active learning strategy to assist knowledge transfer in the target domain, dubbed active domain adaptation. We start from an observation that energy-based models exhibit free energy biases when training (source) and test (target) data come from different distributions. Inspired by this inherent mechanism, we empirically reveal that a simple yet efficient energy-based sampling strategy sheds light on selecting the most valuable target samples than existing approaches requiring particular architectures or computation of the distances. Our algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of target data that incorporate both domain characteristic and instance uncertainty into every selection round. Meanwhile, by aligning the free energy of target data compact around the source domain via a regularization term, domain gap can be implicitly diminished. Through extensive experiments, we show that EADA surpasses state-of-the-art methods on well-known challenging benchmarks with substantial improvements, making it a useful option in the open world. Code is available at https://github.com/BIT-DA/EADA.": 452,
    "How can an efficient and practical sampling strategy be designed for domain adaptation that effectively selects informative target samples to boost accuracy and generalization while minimizing annotation costs?": 453,
    "This paper studies a weakly supervised domain adaptation (WSDA) problem, where we only have access to the source domain with noisy labels, from which we need to transfer useful information to the unlabeled target domain. Although there have been a few studies on this problem, most of them only exploit unidirectional relationships from the source domain to the target domain. In this paper, we propose a universal paradigm called GearNet to exploit bilateral relationships between the two domains. Specifically, we take the two domains as different inputs to train two models alternately, and a symmetrical Kullback-Leibler loss is used for selectively matching the predictions of the two models in the same domain. This interactive learning schema enables implicit label noise canceling and exploit correlations between the source and target domains. Therefore, our GearNet has the great potential to boost the performance of a wide range of existing WSDA methods. Comprehensive experimental results show that the performance of existing methods can be significantly improved by equipping with our GearNet.": 454,
    "How can useful information be effectively transferred from a source domain with noisy labels to an unlabeled target domain in weakly supervised domain adaptation?": 455,
    "Domain adaptation aims to leverage source domain knowledge to predict target domain labels. Most domain adaptation methods tackle a single-source, single-target scenario, whereas source and target domain data can often be subdivided into data from different distributions in real-life applications (e.g., when the distribution of the collected data changes with time). However, such subdomains are rarely given and should be discovered automatically.   To this end, some recent domain adaptation works seek separations of hidden subdomains, w.r.t. a known or fixed number of subdomains.   In contrast, this paper introduces a new subdomain combination method that leverages a variable number of subdomains.   Precisely, we propose to use an inter-subdomain divergence maximization criterion to exploit hidden subdomains.   Besides, our proposition stands in a target-to-source domain adaptation scenario, where one exploits a pre-trained source model as a black box; thus, the proposed method is model-agnostic.  By providing interpretability at two complementary levels (transformation and subdomain levels), our method can also be easily interpreted by practitioners with or without machine learning backgrounds.  Experimental results over two fraud detection datasets demonstrate the efficiency of our method.": 456,
    "How can domain adaptation be effectively performed in real-life industrial applications where a pre-trained source model is given as a black-box and cannot be retrained, especially when source and target domains contain hidden subdomains with unknown distributions?": 457,
    "Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN.": 458,
    "How can state-of-the-art combinatorial optimization solvers be enhanced with data-driven insights to overcome the limitations of hand-crafted heuristics that often ignore common patterns within problem instance distributions?": 459,
    "Item representation learning is crucial for search and recommendation tasks in e-commerce. In e-commerce, the instances (e.g., items, users) in different domains are always related. Such instance relationship across domains contains useful local information for transfer learning. However, existing transfer learning based approaches did not leverage this knowledge. In this paper, we report on our experience designing and deploying Prior-Guided Transfer Learning (PGTL) to bridge this gap. It utilizes the instance relationship across domains to extract prior knowledge for the target domain and leverages it to guide the fine-grained transfer learning for e-commerce item representation learning tasks. Rather than directly transferring knowledge from the source domain to the target domain, the prior knowledge can serve as a bridge to link both domains and enhance knowledge transfer, especially when the domain distribution discrepancy is large. Since its deployment on the Taiwanese portal of Taobao in Aug 2020, PGTL has significantly improved the item exposure rate and item click-through rate compared to previous approaches": 460,
    "How can item representation learning in e-commerce be enhanced, particularly in scenarios with insufficient data or significant domain distribution discrepancies, to improve search and recommendation tasks?": 461,
    "Formal response organizations perform rapid damage assessments after natural and human-induced disasters to measure the extent of damage to infrastructures such as roads, bridges, and buildings. This time-critical task, when performed using traditional approaches such as experts surveying the disaster areas, poses serious challenges and delays response. This paper presents an AI-based system that leverages citizen science to collect damage images reported on social media and perform rapid damage assessment in real-time. Several image processing models in the system tackle non-trivial challenges posed by social media as a data source, such as high-volume of redundant and irrelevant content. The system determines the severity of damage using a state-of-the-art computer vision model. Together with a response organization in the US, we deployed the system to identify damage reports during a major real-world disaster. We observe that almost 42% of the images are unique, 28% relevant, and more importantly, only 10% of them contain either mild or severe damage. Experts from our partner organization provided feedback on the system's mistakes, which we used to perform additional experiments to retrain the models. Consequently, the retrained models based on expert feedback on the target domain data helped us achieve significant performance improvements.": 462,
    "How can citizen-generated imagery content from social networks be utilized in real-time for rapid and accurate disaster damage assessment, despite challenges posed by high data volume, redundancy, and irrelevant content?": 463,
    "Most existing light field (LF) disparity estimation algorithms focus on handling occlusion, texture-less or other areas that harm LF structure to improve accuracy, while ignoring other potential modeling ideas. In this paper, we propose a novel idea called Bad Pixel (BadPix) correction for method modeling, then implement a general post-refinement network for LF disparity estimation: Bad-pixel Correction Network (BpCNet). Given an initial disparity map generated by a specific algorithm, we assume that all BadPixs on it are in a small range. Then BpCNet is modeled as a fine-grained search strategy, and a more accurate result can be obtained by evaluating the consistency of LF images in this limited range. Due to the assumption and the consistency between input and output, BpCNet can perform as a  general  post-refinement network, and can work on almost all existing algorithms iteratively.  We demonstrate the feasibility of our  theory through extensive experiments, and achieve remarkable performance on the HCI 4D Light Field Benchmark.": 464,
    "How can the accuracy of light field disparity estimation be further improved by specifically addressing and correcting \"\"bad pixels\"\" that are often overlooked by existing algorithms?": 465,
    "Domain shift across crowd data severely hinders crowd counting models to generalize to unseen scenarios. Although domain adaptive crowd counting approaches close this gap to a certain extent, they are still dependent on the target domain data to adapt (e.g. finetune) their models to the specific domain. In this paper, we instead target to train a model based on a single source domain which can generalize well on any unseen domain. This falls into the realm of domain generalization that remains unexplored in crowd counting. We first introduce a dynamic sub-domain division scheme which divides the source domain into multiple sub-domains such that we can initiate a meta-learning framework for domain generalization. The sub-domain division is dynamically refined during the meta-learning. Next, in order to disentangle domain-invariant information from domain-specific information in image features, we design the domain-invariant and -specific crowd memory modules to re-encode image features. Two types of losses, i.e. feature reconstruction and orthogonal losses, are devised to enable this disentanglement. Extensive experiments on several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show the strong generalizability of our method. Our code is available at: https://github.com/ZPDu/Domain-general-Crowd-Counting-in-Unseen-Scenarios": 466,
    "How can a crowd counting model be trained effectively on a single source domain to generalize robustly to any unseen target domain without requiring additional target data or model updates?": 467,
    "Domain adaptive semantic segmentation aims to exploit the pixel-level annotated samples on source domain to assist the segmentation of unlabeled samples on target domain. For such a task, the key is to construct reliable supervision signals on target domain. However, existing methods can only provide unreliable supervision signals constructed by segmentation model (SegNet) that are generally domain-sensitive. In this work, we try to find a domain-robust clue to construct more reliable supervision signals. Particularly, we experimentally observe the domain-robustness of optical flow in video tasks as it mainly represents the motion characteristics of scenes. However, optical flow cannot be directly used as supervision signals of semantic segmentation since both of them essentially represent different information. To tackle this issue, we first propose a novel Segmentation-to-Flow Module (SFM) that converts semantic segmentation maps to optical flows, named the segmentation-based flow (SF), and then propose a Segmentation-based Flow Consistency (SFC) method to impose consistency between SF and optical flow, which can implicitly supervise the training of segmentation model. The extensive experiments on two challenging benchmarks demonstrate the effectiveness of our method, and it outperforms previous state-of-the-art methods with considerable performance improvement. Our code is available at https://github.com/EdenHazardan/SFC.": 468,
    "How can reliable supervision signals be constructed for domain adaptive video semantic segmentation, given that traditional segmentation models are inherently domain-sensitive and produce unreliable pseudo-labels on target domains?": 469,
    "Domain generalization in semantic segmentation aims to alleviate the performance degradation on unseen domains through learning domain-invariant features. Existing methods diversify images in the source domain by adding complex or even abnormal textures to reduce the sensitivity to domain-specific features. However, these approaches depends heavily on the richness of the texture bank and training them can be time-consuming. In contrast to importing textures arbitrarily or augmenting styles randomly, we focus on the single source domain itself to achieve the generalization. In this paper, we present a novel adaptive texture filtering mechanism to suppress the influence of texture without using augmentation, thus eliminating the interference of domain-specific features. Further, we design a hierarchical guidance generalization network equipped with structure-guided enhancement modules, which purpose to learn the domain-invariant generalized knowledge. Extensive experiments together with ablation studies on widely-used datasets are conducted to verify the effectiveness of the proposed model, and reveal its superiority over other state-of-the-art alternatives.": 470,
    "How can semantic segmentation models learn domain-invariant features to generalize effectively to unseen target domains when trained on only a single source domain, without relying on arbitrary texture augmentation?": 471,
    "Despite the excellent performance, deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Besides, these examples are often transferable among different models. In other words, the same adversarial example can fool multiple models with different architectures at the same time. Based on this property, many black-box transfer-based attack techniques have been developed. However, current transfer-based attacks generally focus on the cross-architecture setting, where the attacker has access to the training data of the target model, which is not guaranteed in realistic situations. In this paper, we design a Cross-Domain Transfer-Based Attack (CDTA), which works in the cross-domain scenario. In this setting, attackers have no information about the target model, such as its architecture and training data. Specifically, we propose a contrastive spectral training method to train a feature extractor on a source domain (e.g., ImageNet) and use it to craft adversarial examples on target domains (e.g., Oxford 102 Flower). Our method corrupts the semantic information of the benign image by scrambling the outputs of both the intermediate feature layers and the final layer of the feature extractor. We evaluate CDTA with 16 target deep models on four datasets with widely varying styles. The results confirm that, in terms of the attack success rate, our approach can consistently outperform the state-of-the-art baselines by an average of 11.45% across all target models. Our code is available at https://github.com/LiulietLee/CDTA.": 472,
    "How can deep neural networks be effectively attacked in a strict black-box, cross-domain setting where attackers have no information about the target model's architecture, training data, or label space?": 473,
    "Photo-realistic style transfer aims at migrating the artistic style from an exemplar style image to a content image, producing a result image without spatial distortions or unrealistic artifacts. Impressive results have been achieved by recent deep models. However, deep neural network based methods are too expensive to run in real-time. Meanwhile, bilateral grid based methods are much faster but still contain artifacts like overexposure. In this work, we propose the Adaptive ColorMLP (AdaCM), an effective and efficient framework for universal photo-realistic style transfer. First, we find the complex non-linear color mapping between input and target domain can be efficiently modeled by a small multi-layer perceptron (ColorMLP) model. Then, in AdaCM, we adopt a CNN encoder to adaptively predict all parameters for the ColorMLP conditioned on each input content and style image pair. Experimental results demonstrate that AdaCM can generate vivid and high-quality stylization results. Meanwhile, our AdaCM is ultrafast and can process a 4K resolution image in 6ms on one V100 GPU.": 474,
    "How can universal photo-realistic style transfer be achieved in real-time for high-resolution images while avoiding spatial distortions and unrealistic artifacts?": 475,
    "With the continuous emergence of various social media platforms frequently used in daily life, the multimodal meme understanding (MMU) task has been garnering increasing attention. MMU aims to explore and comprehend the meanings of memes from various perspectives by performing tasks such as metaphor recognition, sentiment analysis, intention detection, and offensiveness detection.  Despite making progress, limitations persist due to the loss of fine-grained metaphorical visual clue and the neglect of multimodal text-image weak correlation. To overcome these limitations, we propose a multi-granular multimodal clue fusion model (MGMCF) to advance MMU. Firstly, we design an object-level semantic mining module to extract object-level image feature clues, achieving fine-grained feature clue extraction and enhancing the model's ability to capture metaphorical details and semantics. Secondly, we propose a brand-new global-local cross-modal interaction model to address the weak correlation between text and images. This model facilitates effective interaction between global multimodal contextual clues and local unimodal feature clues, strengthening their representations through a bidirectional cross-modal attention mechanism. Finally, we devise a dual-semantic guided training strategy to enhance the model's understanding and alignment of multimodal representations in the semantic space. Experiments conducted on the widely-used MET-MEME bilingual dataset demonstrate significant improvements over state-of-the-art baselines. Specifically, there is an 8.14% increase in precision for offensiveness detection task, and respective accuracy enhancements of 3.53%, 3.89%, and 3.52% for metaphor recognition, sentiment analysis, and intention detection tasks. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing MMU.": 476,
    "How can multimodal meme understanding be advanced by overcoming the limitations of losing fine-grained metaphorical visual clues and neglecting the weak correlation between text and image modalities": 477,
    "Unsupervised domain adaptation (UDA) aims to learn a model trained on source domain and performs well on unlabeled target domain. In medical image segmentation field, most existing UDA methods depend on adversarial learning to address the domain gap between different image modalities, which is ineffective due to its complicated training process. In this paper, we propose a simple yet effective UDA method based on frequency and spatial domain transfer under multi-teacher distillation framework. In the frequency domain, we first introduce non-subsampled contourlet transform for identifying domain-invariant and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs unchanged while replacing the DVFs of the source domain images with that of the target domain images to narrow the domain gap. In the spatial domain, we propose a batch momentum update-based histogram matching strategy to reduce the domain-variant image style bias. Experiments on two commonly used cross-modality medical image segmentation datasets show that our proposed method achieves superior performance compared to state-of-the-art methods.": 478,
    "How can the performance of medical image segmentation models be improved on unlabeled target domains by effectively reducing the domain gap between different image modalities?": 479,
    "In few-shot generative model adaptation, the model for target domain is prone to the mode-collapse. Recent studies attempted to mitigate the problem by matching the relationship among samples generated from the same latent codes in source and target domains. The objective is further extended to image patch-level to transfer the spatial correlation within an instance. However, the patch-level approach assumes the consistency of spatial structure between source and target domains. For example, the positions of eyes in two domains are almost identical. Thus, it can bring visual artifacts if source and target domain images are not nicely aligned. In this paper, we propose a few-shot generative model adaptation method free from such assumption, based on a motivation that generative models are progressively adapting from the source domain to the target domain. Such progressive changes allow us to identify semantically coherent image regions between instances generated by models at a neighboring training iteration to consider the spatial correlation. We also propose an importance-based patch selection strategy to reduce the complexity of patch-level correlation matching. Our method shows the state-of-the-art few-shot domain adaptation performance in the qualitative and quantitative evaluations.": 480,
    "How can generative models be effectively adapted to new target domains with extremely limited data samples (few-shot setting) while preserving high image quality, diversity, and identity, especially when the source and target domains exhibit significant spatial misalignments?": 481,
    "We propose a novel solution for unpaired image-to-image (I2I) translation. To translate complex images with a wide range of objects to a different domain, recent approaches often use the object annotations to perform per-class source-to-target style mapping. However, there remains a point for us to exploit in the I2I. An object in each class consists of multiple components, and all the sub-object components have different characteristics. For example, a car in CAR class consists of a car body, tires, windows and head and tail lamps, etc., and they should be handled separately for realistic I2I translation. The simplest solution to the problem will be to use more detailed annotations with sub-object component annotations than the simple object annotations, but it is not possible. The key idea of this paper is to bypass the sub-object component annotations by leveraging the original style of the input image because the original style will include the information about the characteristics of the sub-object components. Specifically, for each pixel, we use not only the per-class style gap between the source and target domains but also the pixel’s original style to determine the target style of a pixel. To this end, we present Style Harmonization for unpaired I2I translation (SHUNIT). Our SHUNIT generates a new style by harmonizing the target domain style retrieved from a class memory and an original source image style. Instead of direct source-to-target style mapping, we aim for source and target styles harmonization. We validate our method with extensive experiments and achieve state-of-the-art performance on the latest benchmark sets. The source code is available online: https://github.com/bluejangbaljang/SHUNIT.": 482,
    "How can unpaired image-to-image (I2I) translation realistically handle complex images with diverse objects and sub-object components, given that existing methods struggle to maintain original style characteristics when applying global or class-level style mappings?": 483,
    "Domain adaptation for 3D point cloud has attracted a lot of interest since it can avoid the time-consuming labeling process of 3D data to some extent. A recent work named xMUDA leveraged multi-modal data to domain adaptation task of 3D semantic segmentation by mimicking the predictions between 2D and 3D modalities, and outperformed the previous single modality methods only using point clouds. Based on it, in this paper, we propose a novel cross-modal contrastive learning scheme to further improve the adaptation effects. By employing constraints from the correspondences between 2D pixel features and 3D point features, our method not only facilitates interaction between the two different modalities, but also boosts feature representations in both labeled source domain and unlabeled target domain. Meanwhile, to sufficiently utilize 2D context information for domain adaptation through cross-modal learning, we introduce a neighborhood feature aggregation module to enhance pixel features. The module employs neighborhood attention to aggregate nearby pixels in the 2D image, which relieves the mismatching between the two different modalities, arising from projecting relative sparse point cloud to dense image pixels. We evaluate our method on three unsupervised domain adaptation scenarios, including country-to-country, day-to-night, and dataset-to-dataset. Experimental results show that our approach outperforms existing methods, which demonstrates the effectiveness of the proposed method.": 484,
    "How can domain adaptation for 3D semantic segmentation be improved by effectively leveraging multi-modal data, specifically 2D images and 3D point clouds, to address domain shift and reduce the need for extensive 3D data labeling?": 485,
    "Current domain adaptation methods for face anti-spoofing leverage labeled source domain data and unlabeled target domain data to obtain a promising generalizable decision boundary. However, it is usually difficult for these methods to achieve a perfect domain-invariant liveness feature disentanglement, which may degrade the final classification performance by domain differences in illumination, face category, spoof type, etc. In this work, we tackle cross-scenario face anti-spoofing by proposing a novel domain adaptation method called cyclically disentangled feature translation network (CDFTN). Specifically, CDFTN generates pseudo-labeled samples that possess: 1) source domain-invariant liveness features and 2) target domain-specific content features, which are disentangled through domain adversarial training. A robust classifier is trained based on the synthetic pseudo-labeled images under the supervision of source domain labels. We further extend CDFTN for multi-target domain adaptation by leveraging data from more unlabeled target domains. Extensive experiments on several public datasets demonstrate that our proposed approach significantly outperforms the state of the art. Code and models are available at https://github.com/vis-face/CDFTN.": 486,
    "How can a robust and generalizable decision boundary for face anti-spoofing be achieved across different scenarios, given labeled source domain data and unlabeled target domain data, while effectively disentangling domain-invariant liveness features from domain-specific content features?": 487,
    "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by transferring knowledge from an informative source domain to the target domain, which inevitably proposes stern challenges to data privacy and transferability during the transfer process. A small amount of recent CDR works have investigated privacy protection, while they still suffer from satisfying practical requirements (e.g., limited privacy-preserving ability) and preventing the potential risk of negative transfer. To address the above challenging problems, we propose a novel and unified privacy-preserving federated framework for dual-target CDR, namely P2FCDR. We design P2FCDR as peer-to-peer federated network architecture to ensure the local data storage and privacy protection of business partners. Specifically, for the special knowledge transfer process in CDR under federated settings, we initialize an optimizable orthogonal mapping matrix to learn the embedding transformation across domains and adopt the local differential privacy technique on the transformed embedding before exchanging across domains, which provides more reliable privacy protection. Furthermore, we exploit the similarity between in-domain and cross-domain embedding, and develop a gated selecting vector to refine the information fusion for more accurate dual transfer. Extensive experiments on three real-world datasets demonstrate that P2FCDR significantly outperforms the state-of-the-art methods and effectively protects data privacy.": 488,
    "How can the long-standing data sparsity problem in recommender systems be alleviated through cross-domain knowledge transfer while simultaneously addressing stern challenges related to data privacy, transferability, and ensuring mutual benefits for all participating business partners?": 489,
    "Privacy-preserving cross-domain recommendation (PPCDR) refers to preserving the privacy of users when transferring the knowledge from source domain to target domain for better performance, which is vital for the long-term development of recommender systems. Existing work on cross-domain recommendation (CDR) reaches advanced and satisfying recommendation performance, but mostly neglects preserving privacy. To fill this gap, we propose a privacy-preserving generative cross-domain recommendation (PPGenCDR) framework for PPCDR. PPGenCDR includes two main modules, i.e., stable privacy-preserving generator module, and robust cross-domain recommendation module. Specifically, the former isolates data from different domains with a generative adversarial network (GAN) based model, which stably estimates the distribution of private data in the source domain with ́Renyi differential privacy (RDP) technique. Then the latter aims to robustly leverage the perturbed but effective knowledge from the source domain with the raw data in target domain to improve recommendation performance. Three key modules, i.e., (1) selective privacy preserver, (2) GAN stabilizer, and (3) robustness conductor, guarantee the cost-effective trade-off between utility and privacy, the stability of GAN when using RDP, and the robustness of leveraging transferable knowledge accordingly. The extensive empirical studies on Douban and Amazon datasets demonstrate that PPGenCDR significantly outperforms the state-of-the-art recommendation models while preserving privacy.": 490,
    "How can knowledge be effectively transferred across different domains in recommender systems while ensuring the privacy of user interaction data?": 491,
    "Cross-domain graph few-shot learning attempts to address the prevalent data scarcity issue in graph mining problems. However, the utilization of cross-domain data induces another intractable domain shift issue which severely degrades the generalization ability of cross-domain graph few-shot learning models. The combat with the domain shift issue is hindered due to the coarse utilization of source domains and the ignorance of accessible prompts. To address these challenges, in this paper, we design a novel Cross-domain Task Coordinator to leverage a small set of labeled target domain data as prompt tasks, then model the association and discover the relevance between meta-tasks from the source domain and the prompt tasks. Based on the discovered relevance, our model achieves adaptive task selection and enables the optimization of a graph learner using the selected fine-grained meta-tasks. Extensive experiments conducted on molecular property prediction benchmarks validate the effectiveness of our proposed method by comparing it with state-of-the-art baselines.": 492,
    "How can graph few-shot learning models effectively generalize to new tasks in data-scarce target domains when meta-training and meta-testing tasks are sampled from different, distinct distributions, thereby addressing the significant domain shift issue?": 493,
    "The ability to understand and generate similes is an imperative step to realize human-level AI. However, there is still a considerable gap between machine intelligence and human cognition in similes, since deep models based on statistical distribution tend to favour high-frequency similes. Hence, a large-scale symbolic knowledge base of similes is required, as it contributes to the modeling of diverse yet unpopular similes while facilitating additional evaluation and reasoning. To bridge the gap, we propose a novel framework for large-scale simile knowledge base construction, as well as two probabilistic metrics which enable an improved understanding of simile phenomena in natural language. Overall, we construct MAPS-KB, a million-scale probabilistic simile knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB corpora. We conduct sufficient experiments to justify the effectiveness and necessity of the methods of our framework. We also apply MAPS-KB on three downstream tasks to achieve state-of-the-art performance, further demonstrating the value of MAPS-KB. Resources of MAPS-KB are publicly available at https://github.com/Abbey4799/MAPS-KB.": 494,
    "How can a large-scale symbolic knowledge base be constructed to overcome the limitations of pre-trained language models in understanding and generating diverse similes, particularly those requiring common-sense knowledge or rarely expressed?": 495,
    "Measuring and alleviating the discrepancies between the synthetic (source) and real scene (target) data is the core issue for domain adaptive semantic segmentation. Though recent works have introduced depth information in the source domain to reinforce the geometric and semantic knowledge transfer, they cannot extract the intrinsic 3D information of objects, including positions and shapes, merely based on 2D estimated depth. In this work, we propose a novel Geometry-Aware Network for Domain Adaptation (GANDA), leveraging more compact 3D geometric point cloud representations to shrink the domain gaps. In particular, we first utilize the auxiliary depth supervision from the source domain to obtain the depth prediction in the target domain to accomplish structure-texture disentanglement. Beyond depth estimation, we explicitly exploit 3D topology on the point clouds generated from RGB-D images for further coordinate-color disentanglement and pseudo-labels refinement in the target domain. Moreover, to improve the 2D classifier in the target domain, we perform domain-invariant geometric adaptation from source to target and unify the 2D semantic and 3D geometric segmentation results in two domains. Note that our GANDA is plug-and-play in any existing UDA framework. Qualitative and quantitative results demonstrate that our model outperforms state-of-the-arts on GTA5->Cityscapes and SYNTHIA->Cityscapes.": 496,
    "How can domain adaptive semantic segmentation effectively leverage intrinsic 3D geometric information to alleviate discrepancies between synthetic and real-world data, especially for small objects, when only 2D depth information is available in the source domain?": 497,
    "Mixed Integer programs (MIPs) are typically solved by the Branch-and-Bound algorithm. Recently, Learning to imitate fast approximations of the expert strong branching heuristic has gained attention due to its success in reducing the running time for solving MIPs. However, existing learning-to-branch methods assume that the entire training data is available in a single session of training. This assumption is often not true, and if the training data is supplied in continual fashion over time, existing techniques suffer from catastrophic forgetting. In this work, we study the hitherto unexplored paradigm of Lifelong Learning to Branch on Mixed Integer Programs. To mitigate catastrophic forgetting, we propose LIMIP, which is powered by the idea of modeling an MIP instance in the form of a bipartite graph, which we map to an embedding space using a bipartite Graph Attention Network. This rich embedding space avoids catastrophic forgetting through the application of knowledge distillation and elastic weight consolidation, wherein we learn the parameters key towards retaining efficacy and are therefore protected from significant drift. We evaluate LIMIP on a series of NP-hard problems and establish that in comparison to existing baselines, LIMIP is up to 50% better when confronted with lifelong learning": 498,
    "How can learning-to-branch methods for Mixed Integer Programs be adapted to continually learn from sequentially arriving data without suffering from catastrophic forgetting of previously acquired knowledge?": 499,
    "We develop an algorithm to improve the predictive performance of a pre-trained model under \\textit{concept shift} without retraining the model from scratch when only unannotated samples of initial concepts are accessible. We model this problem as a domain adaptation problem, where the source domain data is inaccessible during model adaptation. The core idea is based on consolidating the intermediate internal distribution, learned to represent the source domain data, after adapting the model. We provide theoretical analysis and conduct extensive experiments on five benchmark datasets to demonstrate that the proposed method is effective.": 500,
    "How can a pre-trained model adapt to concept shift in a new target domain without access to the original source domain data, thereby enabling sequential model adaptation?": 501,
    "Survival analysis is the branch of statistics that studies the relation between the characteristics of living entities and their respective survival times, taking into account the partial information held by censored cases. A good analysis can, for example, determine whether one medical treatment for a group of patients is better than another. With the rise of machine learning, survival analysis can be modeled as learning a function that maps studied patients to their survival times. To succeed with that, there are three crucial issues to be tackled.   First, some patient data is censored: we do not know the true survival times for all patients. Second, data is scarce, which led past research to treat different illness types as domains in a multi-task setup. Third, there is the need for adaptation to new or extremely rare illness types, where little or no labels are available. In contrast to previous multi-task setups, we want to investigate how to efficiently adapt to a new survival target domain from multiple survival source domains.   For this, we introduce a new survival metric and the corresponding discrepancy measure between survival distributions. These allow us to define domain adaptation for survival analysis while incorporating censored data, which would otherwise have to be dropped. Our experiments on two cancer data sets reveal a superb performance on target domains, a better treatment recommendation, and a weight matrix with a plausible explanation.": 502,
    "How can survival analysis models efficiently adapt to new or extremely rare illness types, especially when labeled data is scarce and partial information from censored cases must be effectively utilized?": 503,
    "There has been a surge of interest in learning optimal decision trees using mixed-integer programs (MIP) in recent years, as heuristic-based methods do not guarantee optimality and find it challenging to incorporate constraints that are critical for many practical applications. However, existing MIP methods that build on an arc-based formulation do not scale well as the number of binary variables is in the order of 2 to the power of the depth of the tree and the size of the dataset. Moreover, they can only handle sample-level constraints and linear metrics. In this paper, we propose a novel path-based MIP formulation where the number of decision variables is independent of dataset size. We present a scalable column generation framework to solve the MIP. Our framework produces a multiway-split tree which is more interpretable than the typical binary-split trees due to its shorter rules. Our framework is more general as it can handle nonlinear metrics such as F1 score, and incorporate a broader class of constraints. We demonstrate its efficacy with extensive experiments. We present results on datasets containing up to 1,008,372 samples while existing MIP-based decision tree models do not scale well on data beyond a few thousand points. We report superior or competitive results compared to the state-of-art MIP-based methods with up to a 24X reduction in runtime.": 504,
    "How can optimal decision trees be learned scalably and with flexible constraints, overcoming the limitations of existing mixed-integer programming methods that struggle with large datasets and complex requirements?": 505,
    "Cold-start problem is one of the most challenging problems for recommender systems. One promising solution to this problem is cross-domain recommendation (CDR) which leverages rich information from an auxiliary source domain to improve the performance of recommender system in the target domain. In particular, the family of embedding and mapping methods for CDR is very effective, which explicitly learn a mapping function from source embeddings to target embeddings to transfer user’s preferences. Recent works usually transfer an overall source embedding by modeling a common or personalized preference bridge for all users. However, a unified user embedding cannot reflect the user’s multiple interests in auxiliary source domain. In this paper, we propose a novel framework called reinforced multi-interest transfer for CDR (REMIT). Specifically, we first construct a heterogeneous information network and employ different meta-path based aggregations to get user’s multiple interests in source domain, then transform different interest embeddings with different meta-generated personalized bridge functions for each user. To better coordinate the transformed user interest embeddings and the item embedding in target domain, we systematically develop a reinforced method to dynamically assign weights to transformed interests for different training instances and optimize the performance of target model. In addition, the REMIT is a general framework that can be applied upon various base models in target domain. Our extensive experimental results on large real-world datasets demonstrate the superior performance and compatibility of REMIT.": 506,
    "How can recommender systems effectively leverage multiple user interests from an auxiliary source domain to address the cold-start problem in a target domain, especially when a unified user embedding fails to capture diverse preferences?": 507,
    "Discriminability and transferability are two goals of feature learning for domain adaptation (DA), as we aim to find the transferable features from the source domain that are helpful for discriminating the class label in the target domain. Modern DA approaches optimize discriminability and transferability by adopting two separate modules for the two goals upon a feature extractor, but lack fully exploiting their relationship. This paper argues that by letting the discriminative module and transfer module help each other, better DA can be achieved. We propose Cooperative and Adversarial LEarning (CALE) to combine the optimization of discriminability and transferability into a whole, provide one solution for making the discriminative module and transfer module guide each other. Specifically, CALE generates cooperative (easy) examples and adversarial (hard) examples with both discriminative module and transfer module. While the easy examples that contain the module knowledge can be used to enhance each other, the hard ones are used to enhance the robustness of the corresponding goal. Experimental results show the effectiveness of CALE for unifying the learning of discriminability and transferability, as well as its superior performance.": 508,
    "How can discriminability and transferability be jointly optimized in domain adaptation to achieve better performance, overcoming the limitations of separate optimization?": 509,
    "Unsupervised Domain Adaptation (UDA) methods can reduce label dependency by mitigating the feature discrepancy between labeled samples in a source domain and unlabeled samples in a similar yet shifted target domain. Though achieving good performance, these methods are inapplicable for Multivariate Time-Series (MTS) data. MTS data are collected from multiple sensors, each of which follows various distributions. However, most UDA methods solely focus on aligning global features but cannot consider the distinct distributions of each sensor. To cope with such concerns, a practical domain adaptation scenario is formulated as Multivariate Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose SEnsor Alignment (SEA) for MTS-UDA to reduce the domain discrepancy at both the local and global sensor levels. At the local sensor level, we design the endo-feature alignment to align sensor features and their correlations across domains, whose information represents the features of each sensor and the interactions between sensors. Further, to reduce domain discrepancy at the global sensor level, we design the exo-feature alignment to enforce restrictions on the global sensor features. Meanwhile, MTS also incorporates the essential spatial-temporal dependencies information between sensors, which cannot be transferred by existing UDA methods. Therefore, we model the spatial-temporal information of MTS with a multi-branch self-attention mechanism for simple and effective transfer across domains. Empirical results demonstrate the state-of-the-art performance of our proposed SEA on two public MTS datasets for MTS-UDA. The code is available at  https://github.com/Frank-Wang-oss/SEA": 510,
    "How can unsupervised domain adaptation methods be developed for multivariate time-series data to effectively mitigate feature discrepancies at both global and individual sensor levels, while also capturing and transferring essential spatial-temporal dependencies across domains?": 511,
    "Transfer learning refers to the transfer of knowledge or information from a relevant source domain to a target domain. However, most existing transfer learning theories and algorithms focus on IID tasks, where the source/target samples are assumed to be independent and identically distributed. Very little effort is devoted to theoretically studying the knowledge transferability on non-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we propose rigorous generalization bounds and algorithms for cross-network transfer learning from a source graph to a target graph. The crucial idea is to characterize the cross-network knowledge transferability from the perspective of the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a novel Graph Subtree Discrepancy to measure the graph distribution shift between source and target graphs. Then the generalization error bounds on cross-network transfer learning, including both cross-network node classification and link prediction tasks, can be derived in terms of the source knowledge and the Graph Subtree Discrepancy across domains. This thereby motivates us to propose a generic graph adaptive network (GRADE) to minimize the distribution shift between source and target graphs for cross-network transfer learning. Experimental results verify the effectiveness and efficiency of our GRADE framework on both cross-network node classification and cross-domain recommendation tasks.": 512,
    "How can knowledge be effectively transferred between non-independent and identically distributed (non-IID) graph-structured data, specifically across different networks, to improve prediction performance?": 513,
    "Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model's generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-Level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets.": 514,
    "How can deep networks trained on a source domain maintain high performance when tested on unseen target domain data, particularly for generalized semantic segmentation, despite significant domain shifts?": 515,
    "Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain without requiring the same label sets of both domains. The existence of domain and category shift makes the task challenging and requires us to distinguish \"known\" samples (i.e., samples whose labels exist in both domains) and \"unknown\" samples (i.e., samples whose labels exist in only one domain) in both domains before reducing the domain gap. In this paper, we consider the problem from the point of view of distribution matching which we only need to align two distributions partially. A novel approach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is proposed to conduct partial distribution alignment for UniDA. In training phase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT to reweight source prototypes and target samples, and design reweighted entropy loss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\" samples. Experiments on four benchmarks show that our method outperforms the previous state-of-the-art UniDA methods.": 516,
    "How can knowledge be effectively transferred from a labeled source domain to an unlabeled target domain in Universal Domain Adaptation (UniDA) settings, where both domains may have common and private classes, while accurately distinguishing \"\"known\"\" from \"\"unknown\"\" samples and mitigating negative transfer?": 517,
    "In few-shot unsupervised domain adaptation (FS-UDA), most existing methods followed the few-shot learning (FSL) methods to leverage the low-level local features (learned from conventional convolutional models, e.g., ResNet) for classification. However, the goal of FS-UDA and FSL are relevant yet distinct, since FS-UDA aims to classify the samples in target domain rather than source domain. We found that the local features are insufficient to FS-UDA, which could introduce noise or bias against classification, and not be used to effectively align the domains. To address the above issues, we aim to refine the local features to be more discriminative and relevant to classification. Thus, we propose a novel task-specific semantic feature learning method (TSECS) for FS-UDA. TSECS learns high-level semantic features for image-to-class similarity measurement. Based on the high-level features, we design a cross-domain self-training strategy to leverage the few labeled samples in source domain to build the classifier in target domain. In addition, we minimize the KL divergence of the high-level feature distributions between source and target domains to shorten the distance of the samples between the two domains. Extensive experiments on DomainNet show that the proposed method significantly outperforms SOTA methods in FS-UDA by a large margin (i.e., ~10%).": 518,
    "How can a model effectively classify unlabeled data in a target domain using only a few labeled samples from a source domain, especially when there is a significant domain shift between the source and target domains?": 519,
    "Existing domain generalization aims to learn a generalizable model to perform well even on unseen domains. For many real-world machine learning applications, the data distribution often shifts gradually along domain indices. For example, a self-driving car with a vision system drives from dawn to dusk, with the sky gradually darkening. Therefore, the system must be able to adapt to changes in ambient illuminations and continue to drive safely on the road. In this paper, we formulate such problems as Evolving Domain Generalization, where a model aims to generalize well on a target domain by discovering and leveraging the evolving pattern of the environment. We then propose Directional Domain Augmentation (DDA), which simulates the unseen target features by mapping source data as augmentations through a domain transformer. Specifically, we formulate DDA as a bi-level optimization problem and solve it through a novel meta-learning approach in the representation space. We evaluate the proposed method on both synthetic datasets and real-world datasets, and empirical results show that our approach can outperform other existing methods.": 520,
    "How can a generalizable model be developed to perform effectively on unseen target domains when the data distribution shifts gradually in a non-stationary environment?": 521,
    "Domain generalization (DG) aims to train a model to perform well in unseen domains under different distributions. This paper considers a more realistic yet more challenging scenario, namely Single Domain Generalization (Single-DG), where only a single source domain is available for training. To tackle this challenge, we first try to understand when neural networks fail to generalize? We empirically ascertain a property of a model that correlates strongly with its generalization that we coin as \"model sensitivity\". Based on our analysis, we propose a novel strategy of Spectral Adversarial Data Augmentation (SADA) to generate augmented images targeted at the highly sensitive frequencies. Models trained with these hard-to-learn samples can effectively suppress the sensitivity in the frequency space, which leads to improved generalization performance. Extensive experiments on multiple public datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 2.55%. The source code is available at https://github.com/DIAL-RPI/Spectral-Adversarial-Data-Augmentation.": 522,
    "How can neural networks be trained to generalize effectively to unseen domains, particularly in a challenging single source domain setting, by addressing their inherent sensitivity to out-of-distribution data?": 523,
    "Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain\r\nfeature alignment, but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers\r\nby a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a\r\ndomain pair tend to express similar semantic patterns especially on the people’s attitudes (e.g., supporting or denying) towards the same category of rumors, the discrepancy between a pair of source domain and target domain will be decreased. We conduct experiments on four groups of cross-domain datasets and show that our proposed model achieves state-of-the-art performance.": 524,
    "How can rumor detection models effectively overcome domain shift and perform well in unsupervised cross-domain scenarios where labeled target data is unavailable?": 525,
    "Few-shot slot tagging is an important task in dialogue systems and attracts much attention of researchers. Most previous few-shot slot tagging methods utilize meta-learning procedure for training and strive to construct a large number of different meta tasks to simulate the testing situation of insufficient data. However, there is a widespread phenomenon of overlap slot between two domains in slot tagging. Traditional meta tasks ignore this special phenomenon and cannot simulate such realistic few-shot slot tagging scenarios. It violates the basic principle of meta-learning which the meta task is consistent with the real testing task, leading to historical information forgetting problem. In this paper, we introduce a novel domain-transfer meta task design paradigm to tackle this problem. We distribute a basic domain to each target domain based on the coincidence degree of slot labels between these two domains. Unlike classic meta tasks which only rely on small samples of target domain, our meta tasks aim to correctly infer the class of target domain query samples based on both abundant data in basic domain and scarce data in target domain. To accomplish our meta task, we propose a Task Adaptation Network to effectively transfer the historical information from the basic domain to the target domain. We carry out sufficient experiments on the benchmark slot tagging dataset SNIPS and the name entity recognition dataset NER. Results demonstrate that our proposed model outperforms previous methods and achieves the state-of-the-art performance.": 526,
    "How can few-shot slot tagging effectively leverage historical information from source domains to address the widespread phenomenon of overlap slots and mitigate the problem of historical information forgetting?": 527,
    "The buzz around Transformer-based language models (TLM) such as BERT, RoBERTa, etc. is well-founded owing to their impressive results on an array of tasks. However, when applied to areas needing specialized knowledge (closed-domain), such as medical, finance, etc. their performance takes drastic hits, sometimes more than their older recurrent/convolutional counterparts. In this paper, we explore zero-shot capabilities of large LMs for extractive QA. Our objective is to examine performance change in the face of domain drift i.e. when the target domain data is vastly different in semantic and statistical properties from the source domain and attempt to explain the subsequent behavior. To this end, we present two studies in this paper while planning further experiments later down the road. Our findings indicate flaws in the current generation of TLM limiting their performance on closed-domain tasks.": 528,
    "How do large language models (LLMs) generalize their zero-shot question answering capabilities, particularly when applied to closed-domain tasks characterized by significant domain drift?": 529,
    "Jokes are intentionally written to be funny, but not all jokes are created the same. While recent work has shown impressive results on humor detection in text, we instead investigate the more nuanced task of detecting humor subtypes, especially of the more adult variety. To that end, we introduce a novel jokes dataset filtered from Reddit and solve the subtype classification task using a finetuned Transformer dubbed the Naughtyformer. Moreover, we show that our model is significantly better at detecting offensiveness in jokes compared to state-of-the-art methods.": 530,
    "How can computational models accurately understand and classify nuanced humor subtypes, particularly adult humor, and effectively detect offensiveness within jokes?": 531,
    "Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios. However, previous deep models merely focused on extracting the semantics of log sequences in the same domain, leading to poor generalization on multi-domain logs. To alleviate this issue, we propose a unified Transformer-based framework for Log anomaly detection (LogFormer) to improve the generalization ability across different domains, where we establish a two-stage process including the pre-training and adapter-based tuning stage. Specifically, our model is first pre-trained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer such knowledge to the target domain via shared parameters. Besides, the Log-Attention module is proposed to supplement the information ignored by the log-paring. The proposed method is evaluated on three public datasets and one real-world dataset. Experimental results on multiple benchmarks demonstrate the effectiveness of our LogFormer with fewer trainable parameters and lower training costs.": 532,
    "How can log anomaly detection models be made more generalizable across diverse log domains and efficient in real-world industrial scenarios, avoiding costly retraining for new domains?": 533,
    "Temporal sentence localization (TSL) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant yet expensive manual annotations for training. Moreover, these trained data-dependent models usually can not generalize well to unseen scenarios because of the inherent domain shift. To facilitate this issue, in this paper, we target another more practical but challenging setting: unsupervised domain adaptative temporal sentence localization (UDA-TSL), which explores whether the localization knowledge can be transferred from a fully-annotated data domain (source domain) to a new unannotated data domain (target domain). Particularly, we propose an effective and novel baseline for UDA-TSL to bridge the multi-modal gap across different domains and learn the potential correspondence between the video-query pairs in target domain. We first develop separate modality-specific domain adaptation modules to smoothly balance the minimization of the domain shifts in cross-dataset video and query domains. Then, to fully exploit the semantic correspondence of both modalities in target domain for unsupervised localization, we devise a mutual information learning module to adaptively align the video-query pairs which are more likely to be relevant in target domain, leading to more truly aligned target pairs and ensuring the discriminability of target features. In this way, our model can learn domain-invariant and semantic-aligned cross-modal representations. Three sets of migration experiments show that our model achieves competitive performance compared to existing methods.": 534,
    "Is it possible to transfer localization knowledge from a fully-annotated source domain to a new unannotated target domain in temporal sentence localization tasks?": 535,
    "Universal domain adaptation (UniDA) is a practical but challenging problem, in which information about the relation between the source and the target domains is not given for knowledge transfer. Existing UniDA methods may suffer from the problems of overlooking intra-domain variations in the target domain and difficulty in separating between the similar known and unknown class. To address these issues, we propose a novel Mutual Learning Network (MLNet) with neighborhood invariance for UniDA. In our method, confidence-guided invariant feature learning with self-adaptive neighbor selection is designed to reduce the intra-domain variations for more generalizable feature representation. By using the cross-domain mixup scheme for better unknown-class identification, the proposed method compensates for the misidentified known-class errors by mutual learning between the closed-set and open-set classifiers. Extensive experiments on three publicly available benchmarks demonstrate that our method achieves the best results compared to the state-of-the-arts in most cases and significantly outperforms the baseline across all the four settings in UniDA. Code is available at https://github.com/YanzuoLu/MLNet.": 536,
    "How can universal domain adaptation effectively transfer knowledge between domains when the relationship between source and target label sets is unknown, particularly in distinguishing between similar known and unknown classes while reducing intra-domain variations?": 537,
    "Video semantic segmentation has achieved conspicuous achievements attributed to the development of deep learning, but suffers from labor-intensive annotated training data gathering. To alleviate the data-hunger issue, domain adaptation approaches are developed in the hope of adapting the model trained on the labeled synthetic videos to the real videos in the absence of annotations. By analyzing the dominant paradigm consistency regularization in the domain adaptation task, we find that the bottlenecks exist in previous methods from the perspective of pseudo-labels. To take full advantage of the information contained in the pseudo-labels and empower more effective supervision signals, we propose a coherent PAT network including a target domain focalizer and relation-aware temporal consistency. The proposed PAT network enjoys several merits. First, the target domain focalizer is responsible for paying attention to the target domain, and increasing the accessibility of pseudo-labels in consistency training. Second, the relation-aware temporal consistency aims at modeling the inter-class consistent relationship across frames to equip the model with effective supervision signals. Extensive experimental results on two challenging benchmarks demonstrate that our method performs favorably against state-of-the-art domain adaptive video semantic segmentation methods.": 538,
    "How can domain adaptive video semantic segmentation models effectively leverage pseudo-labels to bridge the domain gap and ensure temporal consistency in the absence of target domain annotations?": 539,
    "In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.": 540,
    "How can object detectors be trained effectively on data from a single source domain to generalize robustly to multiple distinct and unseen target domains, particularly when facing significant domain shifts and the risk of overfitting to spurious correlations?": 541,
    "Gaze estimation aims to accurately estimate the direction or position at which a person is looking. With the development of deep learning techniques, a number of gaze estimation methods have been proposed and achieved state-of-the-art performance. However, these methods are limited to within-dataset settings, whose performance drops when tested on unseen datasets. We argue that this is caused by infinite and continuous gaze labels. To alleviate this problem, we propose using gaze frontalization as an auxiliary task to constrain gaze estimation. Based on this, we propose a novel gaze domain generalization framework named Gaze Frontalization-based Auxiliary Learning (GFAL) Framework which embeds the gaze frontalization process, i.e., guiding the feature so that the eyeball can rotate and look at the front (camera), without any target domain information during training. Experimental results show that our proposed framework is able to achieve state-of-the-art performance on gaze domain generalization task, which is competitive with or even superior to the SOTA gaze unsupervised domain adaptation methods.": 542,
    "How can the performance degradation of gaze estimation models on unseen datasets be alleviated, given that gaze labels are infinite and continuous, leading to overfitting?": 543,
    "Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.": 544,
    "How can adversarial examples be generated with strong transferability across both unknown target domains and unseen model architectures in a strict black-box setting?": 545,
    "Unsupervised domain adaptive hashing is a highly promising research direction within the field of retrieval. It aims to transfer valuable insights from the source domain to the target domain while maintaining high storage and retrieval efficiency. Despite its potential, this field remains relatively unexplored. Previous methods usually lead to unsatisfactory retrieval performance, as they frequently directly apply slightly modified domain adaptation algorithms to hash learning framework, or pursue domain alignment within the Hamming space characterized by limited semantic information. In this paper, we propose a simple yet effective approach named Comparative Prototype Hashing (CPH) for unsupervised domain adaptive image retrieval. We establish a domain-shared unit hypersphere space through prototype contrastive learning and then obtain the Hamming hypersphere space via mapping from the shared hypersphere. This strategy achieves a cohesive synergy between learning uniformly distributed and category conflict-averse feature representations, eliminating domain discrepancies, and facilitating hash code learning. Moreover, by leveraging dual-domain information to supervise the entire hashing model training process, we can generate hash codes that retain inter-sample similarity relationships within both domains. Experimental results validate that our CPH significantly outperforms the state-of-the-art counterparts across multiple cross-domain and single-domain retrieval tasks. Notably, on Office-Home and Office-31 datasets, CPH achieves an average performance improvement of 19.29% and 13.85% on cross-domain retrieval tasks compared to the second-best results, respectively. The source codes of our method are available at: https://github.com/christinecui/CPH.": 546,
    "How can unsupervised domain adaptive hashing models be improved to achieve satisfactory retrieval performance by effectively transferring semantic information and knowledge from a labeled source domain to an unlabeled target domain?": 547,
    "Domain adaptation has become an attractive learning paradigm, as it can leverage source domains with rich labels to deal with classification tasks in an unlabeled target domain. A few recent studies develop domain adaptation approaches for graph-structured data. In the case of node classification task, current domain adaptation methods only focus on the closed-set setting, where source and target domains share the same label space. A more practical assumption is that the target domain may contain new classes that are not included in the source domain. Therefore, in this paper, we introduce a novel and challenging problem for graphs, i.e., open-set domain adaptive node classification, and propose a new approach to solve it. Specifically, we develop an algorithm for efficient knowledge transfer from a labeled source graph to an unlabeled target graph under a separate domain alignment (SDA) strategy, in order to learn discriminative feature representations for the target graph. Our goal is to not only correctly classify target nodes into the known classes, but also classify unseen types of nodes into an unknown class. Experimental results on real-world datasets show that our method outperforms existing methods on graph domain adaptation.": 548,
    "How can knowledge be effectively transferred from a labeled source graph to an unlabeled target graph for node classification when the target domain may contain new classes not present in the source domain?": 549,
    "Understanding the emotional polarity of multimodal content with metaphorical characteristics, such as memes, poses a significant challenge in Multimodal Emotion Recognition (MER). Previous MER researches have overlooked the phenomenon of metaphorical alignment in multimedia content, which involves non-literal associations between concepts to convey implicit emotional tones. Metaphor-agnostic MER methods may be misinformed by the isolated unimodal emotions, which are distinct from the real emotions blended in multimodal metaphors. Moreover, contextual semantics can further affect the emotions associated with similar metaphors, leading to the challenge of maintaining contextual compatibility. To address the issue of metaphorical alignment in MER, we propose to leverage a conditional generative approach for capturing metaphorical analogies. Our approach formulates schematic prompts and corresponding references based on theoretical foundations, which allows the model to better grasp metaphorical nuances. In order to maintain contextual sensitivity, we incorporate a disentangled contrastive matching mechanism, which undergoes curricular adjustment to regulate its intensity during the learning process. The automatic and human evaluation experiments on two benchmarks prove that, our model provides considerable and stable improvements in recognizing multimodal emotion with metaphor attributes.": 550,
    "How can the implicit emotional tones conveyed through non-literal associations in multimodal metaphorical content be accurately understood and recognized?": 551,
    "Current training of motion style transfer systems relies on consistency losses across style domains to preserve contents, hindering its scalable application to a large number of domains and private data. Recent image transfer works show the potential of independent training on each domain by leveraging implicit bridging between diffusion models, with the content preservation, however, limited to simple data patterns. We address this by imposing biased sampling in backward diffusion while maintaining the domain independence in the training stage. We construct the bias from the source domain keyframes and apply them as the gradient of content constraints, yielding a framework with keyframe manifold constraint gradients (KMCGs). Our validation demonstrates the success of training separate models to transfer between as many as ten dance motion styles. Comprehensive experiments find a significant improvement in preserving motion contents in comparison to baseline and ablative diffusion-based style transfer models. In addition, we perform a human study for a subjective assessment of the quality of generated dance motions. The results validate the competitiveness of KMCGs.": 552,
    "How can motion style transfer systems be developed to scale efficiently to a large number of diverse domains while preserving data privacy and ensuring high-fidelity content preservation, especially for complex human movements like dance?": 553,
    "Existing semi-supervised domain adaptation (SSDA) models have exhibited impressive performance on the target domain by effectively utilizing few labeled target samples per class (e.g., 3 samples per class). To guarantee an equal number of labeled target samples for each class, however, they require domain experts to manually recognize a considerable amount of the unlabeled target data. Moreover, as the target samples are not equally informative for shaping the decision boundaries of the learning models, it is crucial to select the most informative target samples for labeling, which is, however, impossible for human selectors. As a remedy, we propose an EFfective Target Labeling (EFTL) framework that harnesses active learning and pseudo-labeling strategies to automatically select some informative target samples to annotate. Concretely, we introduce a novel sample query strategy, called non-maximal degree node suppression (NDNS), that iteratively performs maximal degree node query and non-maximal degree node removal to select representative and diverse target samples for labeling. To learn target-specific characteristics, we propose a novel pseudo-labeling strategy that attempts to label low-confidence target samples accurately via clustering consistency (CC), and then inject information of the model uncertainty into our query process. CC enhances the utilization of the annotation budget and increases the number of “labeled” target samples while requiring no additional manual effort. Our proposed EFTL framework can be easily coupled with existing SSDA models, showing significant improvements on three benchmarks": 554,
    "How can semi-supervised domain adaptation models effectively utilize a limited budget of labeled target samples to improve performance without extensive manual annotation effort?": 555,
    "Ensuring fairness in machine learning (ML) is crucial, particularly in applications that impact diverse populations. The majority of existing works heavily rely on the availability of protected features like race and gender. However, practical challenges such as privacy concerns and regulatory restrictions often prohibit the use of this data, limiting the scope of traditional fairness research. To address this, we introduce a Shared Latent Space-based Debiasing (SLSD) method that transforms data from both the target domain, which lacks protected features, and a separate source domain, which contains these features, into correlated latent representations. This allows for joint training of a cross-domain protected group estimator on the representations. We then debias the downstream ML model with an adversarial learning technique that leverages the group estimator. We also present a relaxed variant of SLSD, the R-SLSD, that occasionally accesses a small subset of protected features from the target domain during its training phase. Our extensive experiments on benchmark datasets demonstrate that our methods consistently outperform existing state-of-the-art models in standard group fairness metrics.": 556,
    "How can fairness in machine learning be achieved without relying on protected demographic attributes?": 557,
    "Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embedding space into two distinct representations, one for encapsulating domain-invariant behaviors and the other for delineating the factors that induce domain variations in the behaviors. Our DuSkill framework enhances the diversity of skills learned offline, thus enabling to accelerate the learning procedure of high-level policies for different domains. Through experiments, we show that DuSkill outperforms other skill-based imitation learning and RL algorithms for several long-horizon tasks, demonstrating its benefits in few-shot imitation and online RL.": 558,
    "How can skill-based reinforcement learning approaches be made robust to domain shifts, enabling effective policy learning for tasks in target domains that differ from the datasets used for skill pretraining?": 559,
    "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA\".": 560,
    "How can server-side models be effectively extracted in Split Federated Learning during the training phase, especially when the target model is continuously changing and producing inconsistent gradients?": 561,
    "Efficiently utilizing rich knowledge in pretrained models has become a critical topic in the era of large models. This work focuses on adaptively utilize knowledge from multiple source-pretrained models to an unlabeled target domain without accessing the source data. Despite being a practically useful setting, existing methods require extensive parameter tuning over each source model, which is computationally expensive when facing abundant source domains or larger source models. To address this challenge, we propose a novel approach which is free of the parameter tuning over source backbones. Our technical contribution lies in the Bi-level ATtention ENsemble (Bi-ATEN) module, which learns both intra-domain weights and inter-domain ensemble weights to achieve a fine balance between instance specificity and domain consistency. By slightly tuning source bottlenecks, we achieve comparable or even superior performance on a challenging benchmark DomainNet with less than 3% trained parameters and 8 times of throughput compared with SOTA method. Furthermore, with minor modifications, the proposed module can be easily equipped to existing methods and gain more than 4% performance boost. Code is available at https://github.com/TL-UESTC/Bi-ATEN.": 562,
    "How can knowledge from multiple source-pretrained models be efficiently and adaptively utilized for an unlabeled target domain in a source-free setting, without requiring extensive parameter tuning over each source model?": 563,
    "Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training (ST) in learning from unlabeled data and adapt ST for test-time adaptation. However, ST alone is prone to over adaption which is responsible for the poor performance under continual domain shift. Hence, we propose to improve self-training under continual domain shift by regularizing model updates with an anchored loss. The final TTA model, termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm layers. We evaluate TRIBE on four datasets representing real-world TTA settings. TRIBE consistently achieves the state-of-the-art performance across multiple evaluation protocols.  The code is available at https://github.com/Gorilla-Lab-SCUT/TRIBE.": 564,
    "How can pre-trained models be effectively adapted to real-world testing data at inference time, particularly when facing challenging scenarios such as non-i.i.d. data streams, continual domain shifts, and globally class-imbalanced testing sets?": 565,
    "Multi-source transfer learning is an effective solution to data scarcity by utilizing multiple source tasks for the learning of the target task. However, access to source data and model details is limited in the era of commercial models, giving rise to the setting of multi-source-free (MSF) transfer learning that aims to leverage source domain knowledge without such access. As a newly defined problem paradigm, MSF transfer learning remains largely underexplored and not clearly formulated. In this work, we adopt an information theoretic perspective on it and propose a framework named H-ensemble, which dynamically learns the optimal linear combination, or ensemble, of source models for the target task, using a generalization of maximal correlation regression. The ensemble weights are optimized by maximizing an information theoretic metric for transferability. Compared to previous works, H-ensemble is characterized by: 1) its adaptability to a novel and realistic MSF setting for few-shot target tasks, 2) theoretical reliability, 3) a lightweight structure easy to interpret and adapt. Our method is empirically validated by ablation studies, along with extensive comparative analysis with other task ensemble and transfer learning methods. We show that the H-ensemble can successfully learn the optimal task ensemble, as well as outperform prior arts.": 566,
    "How can knowledge be effectively transferred from multiple black-box source models to a data-scarce target task without access to the original source data or detailed model structures?": 567,
    "Black-box domain adaptation (BDA) targets to learn a classifier on an unsupervised target domain while assuming only access to black-box predictors trained from unseen source data. Although a few BDA approaches have demonstrated promise by manipulating the transferred labels, they largely overlook the rich underlying structure in the target domain. To address this problem, we introduce a novel separation and alignment framework for BDA. Firstly, we locate those well-adapted samples via loss ranking and a flexible confidence-thresholding procedure. Then, we introduce a novel graph contrastive learning objective that aligns under-adapted samples to their local neighbors and well-adapted samples. Lastly, the adaptation is finally achieved by a nearest-centroid-augmented objective that exploits the clustering effect in the feature space. Extensive experiments demonstrate that our proposed method outperforms best baselines on benchmark datasets, e.g. improving the averaged per-class accuracy by 4.1% on the VisDA dataset. The source code is available at: https://github.com/MingxuanXia/SEAL.": 568,
    "How can black-box domain adaptation effectively learn a classifier on an unsupervised target domain when only a source API is available, overcoming limitations of existing methods such as reliance on a fixed budget of easy samples and insufficient utilization of target domain topological information?": 569,
    "Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (MIP), for which we propose a highly efficient algorithm based on the cutting plane method. Experiments demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the MIP and its linear program relaxation. Further experiments highlight the competitive performance of FairWASP in reducing disparities while preserving accuracy in downstream classification settings.": 570,
    "How can disparities in classification datasets be reduced through pre-processing while minimizing alterations to the original data distribution and ensuring compatibility with various downstream models?": 571,
    "The visual prompts have provided an efficient manner in addressing visual cross-domain problems. Previous works introduce domain prompts to tackle the classification Test-Time Adaptation (TTA) problem by placing image-level prompts on the input and fine-tuning prompts for each target domain. However, since the image-level prompts mask out continuous spatial details in the prompt-allocated region, it will suffer from inaccurate contextual information and limited domain knowledge extraction, particularly when dealing with dense prediction TTA problems. To overcome these challenges, we propose a novel Sparse Visual Domain Prompts (SVDP) approach, which applies minimal trainable parameters (e.g., 0.1%) to pixels across the entire image and reserves more spatial information of the input. To better apply SVDP in extracting domain-specific knowledge, we introduce the Domain Prompt Placement (DPP) method to adaptively allocates trainable parameters of SVDP on the pixels with large distribution shifts. Furthermore, recognizing that each target domain sample exhibits a unique domain shift, we design Domain Prompt Updating (DPU) strategy to optimize prompt parameters differently for each sample, facilitating efficient adaptation to the target domain. Extensive experiments were conducted on widely-used TTA and continual TTA benchmarks, and our proposed method achieves state-of-the-art performance in both semantic segmentation and depth estimation tasks.": 572,
    "How can pre-trained models efficiently adapt to diverse and continually changing target domains for dense prediction tasks without access to source data, while preserving critical spatial information?": 573,
    "In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Neural Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.": 574,
    "How can predictive models generalize effectively to unseen, time-evolving target domains without access to target data, especially when considering the complex evolving dynamics between domains?": 575,
    "For addressing the data privacy and portability issues of domain adaptation, Domain Adaptation of Black-box Predictors (DABP) aims to adapt a black-box source model to an unlabeled target domain without accessing both the source-domain data and details of the source model. Although existing DABP approaches based on knowledge distillation (KD) have achieved promising results, we experimentally find that these methods all have the minority class forgetting issue, which refers that the trained model completely forgets some minority classes. To address this issue, we propose a method called Reviewing the Forgotten Classes (RFC), which including two main modules. Firstly, we propose a simple but effective component called selection training (ST). ST selects classes that the model tends to forget according to the learning status of the model and obtains clean samples of the selected classes with the small-loss criterion for enhanced training. ST is orthogonal to previous methods and can effectively alleviate their minority class forgetting issue. Secondly, we find that neighborhood clustering (NC) can help the model learn more balanced than KD so that further alleviate the minority class forgetting issue. However, NC is based on the fact that target features from the source model already form some semantic structure, while DABP is unable to obtain the source model. Thus, we use KD and ST to warm up the target model to form a certain semantic structure. Overall, our method inherits the merits of both ST and NC, and achieves state of the art on three DABP benchmarks.": 576,
    "How can black-box predictors be effectively adapted to unlabeled target domains while specifically addressing the issue of minority classes being forgotten during the adaptation process?": 577,
    "In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation. In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance. Previous methods employed prototype methods for domain adaptation on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at https://github.com/Hehxcf/CPC/.": 578,
    "How can unsupervised domain adaptation be effectively performed in noisy environments, where models are prone to overfitting noisy labels and struggle to classify similar classes?": 579,
    "Multi-source domain adaptation (MSDA) aims to transfer knowledge from multiple source domains to the unlabeled target domain. In this paper, we propose a cycle self-refinement domain adaptation method, which progressively attempts to learn the dominant transferable knowledge in each source domain in a cycle manner. Specifically, several source-specific networks and a domain-ensemble network are adopted in the proposed method. The source-specific networks are adopted to provide the dominant transferable knowledge in each source domain for instance-level ensemble on predictions of the samples in target domain. Then these samples with high-confidence ensemble predictions are adopted to refine the domain-ensemble network. Meanwhile, to guide each source-specific network to learn more dominant transferable knowledge, we force the features of the target domain from the domain-ensemble network and the features of each source domain from the corresponding source-specific network to be aligned with their predictions from the corresponding networks. Thus the adaptation ability of source-specific networks and the domain-ensemble network can be improved progressively. Extensive experiments on Office-31, Office-Home and DomainNet show that the proposed method outperforms the state-of-the-art methods for most tasks.": 580,
    "How can knowledge be effectively transferred from multiple labeled source domains to an unlabeled target domain, particularly by leveraging dominant transferable knowledge at an instance level rather than solely at a domain level?": 581,
    "Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. In this paper, we study the confidence set prediction problem in the OOD generalization setting. Split conformal prediction (SCP) is an efficient framework for handling the confidence set prediction problem. However, the validity of SCP requires the examples to be exchangeable, which is violated in the OOD setting. Empirically, we show that trivially applying SCP results in a failure to maintain the marginal coverage when the unseen target domain is different from the source domain. To address this issue, we develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method. Finally, we conduct experiments on simulated data to empirically verify the correctness of our theory and the validity of our proposed method.": 582,
    "How can confident prediction sets be constructed for out-of-distribution data, ensuring coverage guarantees even when the test distribution differs from the training distributions?": 583,
    "Cross-domain named entity recognition (NER) tasks encourage NER models to transfer knowledge from data-rich source domains to sparsely labeled target domains. Previous works adopt the paradigms of pre-training on the source domain followed by fine-tuning on the target domain. However, these works ignore that general labeled NER source domain data can be easily retrieved in the real world, and soliciting more source domains could bring more benefits. Unfortunately, previous paradigms cannot efficiently transfer knowledge from multiple source domains. In this work, to transfer multiple source domains' knowledge, we decouple the NER task into the pipeline tasks of mention detection and entity typing, where the mention detection unifies the training object across domains, thus providing the entity typing with higher-quality entity mentions. Additionally, we request multiple general source domain models to suggest the potential named entities for sentences in the target domain explicitly, and transfer their knowledge to the target domain models through the knowledge progressive networks implicitly. Furthermore, we propose two methods to analyze in which source domain knowledge transfer occurs, thus helping us judge which source domain brings the greatest benefit. In our experiment, we develop a Chinese cross-domain NER dataset. Our model improved the F1 score by an average of 12.50% across 8 Chinese and English datasets compared to models without source domain data.": 584,
    "How to efficiently transfer knowledge from multiple data-rich source domains to a sparsely labeled target domain for Named Entity Recognition (NER) tasks, overcoming challenges like catastrophic forgetting and entity type mismatches across domains.": 585,
    "Euphemisms are commonly used on social media and darknet marketplaces to evade platform regulations by masking their true meanings with innocent ones. For instance, “weed” is used instead of “marijuana” for illicit transactions. Thus, euphemism identification, i.e., mapping a given euphemism (“weed”) to its specific target word (“marijuana”), is essential for improving content moderation and combating underground markets. Existing methods employ self-supervised schemes to automatically construct labeled training datasets for euphemism identification. However, they overlook the text-text domain gap caused by the discrepancy between the constructed training data and the test data, leading to performance deterioration. In this paper, we present the text-text domain gap and explain how it forms in terms of the data distribution and the cone effect. Moreover, to bridge this gap, we introduce a feature alignment network (FA-Net), which can both align the in-domain and cross-domain features, thus mitigating the domain gap from training data to test data and improving the performance of the base models for euphemism identification. We apply this FA-Net to the base models, obtaining markedly better results, and creating a state-of-the-art model which beats the large language models.": 586,
    "How can the text-text domain gap in euphemism identification, arising from discrepancies between self-supervised training data and real-world test data, be effectively mitigated to improve identification accuracy?": 587,
    "The prompt-based method has been proven effective in improving the performance of pre-trained language models (PLMs) on sentence-level few-shot tasks. However, when applying prompting to token-level tasks such as Named Entity Recognition (NER), specific templates need to be designed, and all possible segments of the input text need to be enumerated. These methods have high computational complexity in both training and inference processes, making them difficult to apply in real-world scenarios. To address these issues, we redefine the NER task as a Machine Reading Comprehension (MRC) task and incorporate prompting into the MRC framework. Specifically, we sequentially insert boundary markers for various entity types into the templates and use these markers as anchors during the inference process to differentiate entity types. In contrast to the traditional multi-turn question-answering extraction in the MRC framework, our method can extract all spans of entity types in one round. Furthermore, we propose word-based template and example-based template that enhance the MRC framework's perception of entity start and end positions while significantly reducing the manual effort required for template design. It is worth noting that in cross-domain scenarios, PMRC does not require redesigning the model architecture and can continue training by simply replacing the templates to recognize entity types in the target domain. Experimental results demonstrate that our approach outperforms state-of-the-art models in low-resource settings, achieving an average performance improvement of +5.2% in settings where access to source domain data is limited. Particularly, on the ATIS dataset with a large number of entity types and 10-shot setting, PMRC achieves a performance improvement of +15.7%. Moreover, our method achieves a decoding speed 40.56 times faster than the template-based cloze-style approach.": 588,
    "How can Named Entity Recognition (NER) be performed efficiently and effectively in low-resource and cross-domain settings, overcoming limitations of existing methods such as high computational complexity, sensitivity to manually designed templates, and the need for model re-configuration or retraining for new entity categories?": 589,
    "To translate well, machine translation (MT) systems and general-purposed language models (LMs) need a deep understanding of both source and target languages and cultures. Therefore, idioms, with their non-compositional nature, pose particular challenges for Transformer-based systems, as literal translations often miss the intended meaning. Traditional methods, which replace idioms using existing knowledge bases (KBs), often lack scale and context-awareness. Addressing these challenges, our approach prioritizes context-awareness and scalability, allowing for offline storage of idioms in a manageable KB size. This ensures efficient serving with smaller models and provides a more comprehensive understanding of idiomatic expressions. We introduce a multilingual idiom KB (IdiomKB) developed using large LMs to address this. This KB facilitates better translation by smaller models, such as BLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms' figurative meanings. We present a novel, GPT-4-powered metric for human-aligned evaluation, demonstrating that IdiomKB considerably boosts model performance. Human evaluations further validate our KB's quality.": 590,
    "How can machine translation systems and general-purpose language models effectively translate idiomatic expressions, given their non-compositional nature, without relying on costly large language models for offline or real-time scenarios?": 591,
    "Cross-domain text classification aims to transfer models from label-rich source domains to label-poor target domains, giving it a wide range of practical applications. Many approaches promote cross-domain generalization by capturing domaininvariant features. However, these methods rely on unlabeled samples provided by the target domains, which renders the model ineffective when the target domain is agnostic. Furthermore, the models are easily disturbed by shortcut learning in the source domain, which also hinders the improvement of domain generalization ability. To solve the aforementioned issues, this paper proposes TACIT, a target domain agnostic feature disentanglement framework which adaptively decouples robust and unrobust features by Variational Auto-Encoders. Additionally, to encourage the separation of unrobust features from robust features, we design a feature distillation task that compels unrobust features to approximate the output of the teacher. The teacher model is trained with a few easy samples that are easy to carry potential unknown shortcuts. Experimental results verify that our framework achieves comparable results to state-of-the-art baselines while utilizing only source domain data.": 592,
    "How can cross-domain text classification models achieve robust generalization without relying on target domain data, especially when the target domain is entirely unknown or agnostic?": 593,
    "Chinese idioms pose a significant challenge for machine reading comprehension due to their metaphorical meanings often diverging from their literal counterparts, leading to metaphorical inconsistency. Furthermore, the same idiom can have different meanings in different contexts, resulting in contextual inconsistency. Although deep learning-based methods have achieved some success in idioms reading comprehension, existing approaches still struggle to accurately capture idiom representations due to metaphorical inconsistency and contextual inconsistency of idioms. To address these challenges, we propose a novel model, Multi-Semantic Contrastive Learning Method (MSCLM), which simultaneously addresses metaphorical inconsistency and contextual inconsistency of idioms. To mitigate metaphorical inconsistency, we propose a metaphor contrastive learning module based on the prompt method, bridging the semantic gap between literal and metaphorical meanings of idioms. To mitigate contextual inconsistency, we propose a multi-semantic cross-attention module to explore semantic features between different metaphors of the same idiom in various contexts. Our model has been compared with multiple current latest models (including GPT-3.5) on multiple Chinese idiom reading comprehension datasets, and the experimental results demonstrate that MSCLM outperforms state-of-the-art models.": 594,
    "How can machine reading comprehension systems accurately capture idiom representations despite the metaphorical inconsistency between literal and metaphorical meanings and the contextual inconsistency of idioms across different contexts?": 595,
    "Domain-independent dynamic programming (DIDP), a model-based paradigm based on dynamic programming, has shown promising performance on multiple combinatorial optimization problems compared with mixed integer programming (MIP) and constraint programming (CP). The current DIDP solvers are based on heuristic search, and the state-of-the-art solver, complete anytime beam search (CABS), uses beam search. However, the current DIDP solvers cannot utilize multiple threads, unlike state-of-the-art MIP and CP solvers. In this paper, we propose three parallel beam search algorithms and develop multi-thread implementations of CABS. With 32 threads, our multi-thread DIDP solvers achieve 9 to 39 times speedup on average and significant performance improvement over the sequential solver, finding the new best solutions for two instances of the traveling salesperson problem with time windows. In addition, our solvers outperform multi-thread MIP and CP solvers in four of the six combinatorial optimization problems evaluated.": 596,
    "How can Domain-Independent Dynamic Programming (DIDP) solvers, particularly those based on beam search, be enhanced to effectively utilize multiple threads and achieve significant performance improvements for combinatorial optimization problems?": 597,
    "In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use cases, provides a theoretical explanation of the success of model reprogramming, and concludes with a discussion on open-ended research questions and opportunities.": 598,
    "How can machine learning models be effectively reused across vastly different domains, especially in resource-limited settings, without incurring high development and training costs?": 599,
    "The population characteristics of the datasets related to the same task may vary significantly and merging them may harm performance. In this paper, we propose a novel method of domain adaptation called \"cross-adaptation\". It allows for implicit adaptation to the target domain without the need for any labeled examples across this domain. We test our approach on 9 datasets for SARS-CoV-2 detection from complete blood count from different hospitals around the world. Results show that our solution is universal with respect to various classification algorithms and allows for up to a 10pp increase in F1 score on average.": 600,
    "How can a machine learning model adapt to multiple target domains without requiring labeled examples from those domains?": 601,
    "We propose a simple and general online method to measure the search progress within the Branch-and-Bound algorithm, from which we estimate the size of the remaining search tree. We then show how this information can help solvers algorithmically at runtime by designing a restart strategy for MixedInteger Programming (MIP) solvers that decides whether to restart the search based on the current estimate of the number of remaining nodes in the tree. We refer to this type of algorithm as clairvoyant. Our clairvoyant restart strategy outperforms a state-of-the-art solver on a large set of publicly available MIP benchmark instances. It is implemented in the MIP solver SCIP and will be available in future releases.": 602,
    "How can the performance of Branch-and-Bound (B&B) search algorithms, particularly in Mixed-Integer Programming (MIP) solvers, be improved by dynamically deciding when to restart the search based on an online estimation of the remaining search tree size?": 603,
    "Deep neural networks excel at learning from large-scale labeled training data, but cannot well generalize the learned knowledge to new domains or datasets. Domain adaptation studies how to transfer models trained on one labeled source domain to another sparsely labeled or unlabeled target domain. In this paper, we investigate the unsupervised domain adaptation (UDA) problem in image emotion classification. Specifically, we develop a novel cycle-consistent adversarial model, termed CycleEmotionGAN, by enforcing emotional semantic consistency while adapting images cycleconsistently. By alternately optimizing the CycleGAN loss, the emotional semantic consistency loss, and the target classification loss, CycleEmotionGAN can adapt source domain images to have similar distributions to the target domain without using aligned image pairs. Simultaneously, the annotation information of the source images is preserved. Extensive experiments are conducted on the ArtPhoto and FI datasets, and the results demonstrate that CycleEmotionGAN significantly outperforms the state-of-the-art UDA approaches.": 604,
    "How can deep neural networks generalize learned knowledge from a labeled source domain to an unlabeled target domain for image emotion classification, especially when significant domain shift or dataset bias exists?": 605,
    "In this paper, a unified approach is presented to transfer learning that addresses several source and target domain labelspace and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.": 606,
    "How can knowledge be effectively transferred between domains when their label spaces are disjoint and target supervision is limited or absent?": 607,
    "Recently, considerable effort has been devoted to deep domain adaptation in computer vision and machine learning communities. However, most of existing work only concentrates on learning shared feature representation by minimizing the distribution discrepancy across different domains. Due to the fact that all the domain alignment approaches can only reduce, but not remove the domain shift. Target domain samples distributed near the edge of the clusters, or far from their corresponding class centers are easily to be misclassified by the hyperplane learned from the source domain. To alleviate this issue, we propose to joint domain alignment and discriminative feature learning, which could benefit both domain alignment and final classification. Specifically, an instance-based discriminative feature learning method and a center-based discriminative feature learning method are proposed, both of which guarantee the domain invariant features with better intra-class compactness and inter-class separability. Extensive experiments show that learning the discriminative features in the shared feature space can significantly boost the performance of deep domain adaptation methods.": 608,
    "Existing deep domain adaptation methods primarily focus on minimizing distribution discrepancy, which can reduce but not fully remove domain shift, leading to misclassification of target domain samples near cluster edges or far from their class centers. How can deep domain adaptation be improved to learn more discriminative and robust feature representations that mitigate these misclassifications?": 609,
    "We propose a simple yet effective method for unsupervised domain adaptation. When training and test distributions are different, standard supervised learning methods perform poorly. Semi-supervised domain adaptation methods have been developed for the case where labeled data in the target domain are available. However, the target data are often unlabeled in practice. Therefore, unsupervised domain adaptation, which does not require labels for target data, is receiving a lot of attention. The proposed method minimizes the discrepancy between the source and target distributions of input features by transforming the feature space of the source domain. Since such unilateral transformations transfer knowledge in the source domain to the target one without reducing dimensionality, the proposed method can effectively perform domain adaptation without losing information to be transfered. With the proposed method, it is assumed that the transformed features and the original features differ by a small residual to preserve the relationship between features and labels. This transformation is learned by aligning the higher-order moments of the source and target feature distributions based on the maximum mean discrepancy, which enables to compare two distributions without density estimation. Once the transformation is found, we learn supervised models by using the transformed source data and their labels. We use two real-world datasets to demonstrate experimentally that the proposed method achieves better classification performance than existing methods for unsupervised domain adaptation.": 610,
    "How can supervised learning methods be made robust to domain shifts, enabling accurate predictions on unlabeled target data when training data is only available from a different source distribution?": 611,
    "Unsupervised domain adaptation is the problem setting where data generating distributions in the source and target domains are different and labels in the target domain are unavailable. An important question in unsupervised domain adaptation is how to measure the difference between the source and target domains. Existing discrepancy measures for unsupervised domain adaptation either require high computation costs or have no theoretical guarantee. To mitigate these problems, this paper proposes a novel discrepancy measure called source-guided discrepancy (S-disc), which exploits labels in the source domain unlike the existing ones. As a consequence, S-disc can be computed efficiently with a finitesample convergence guarantee. In addition, it is shown that S-disc can provide a tighter generalization error bound than the one based on an existing discrepancy measure. Finally, experimental results demonstrate the advantages of S-disc over the existing discrepancy measures.": 612,
    "How can the difference between source and target domains be accurately and efficiently measured in unsupervised domain adaptation when target domain labels are unavailable?": 613,
    "Aspect-level sentiment classification (ASC) aims at identifying sentiment polarities towards aspects in a sentence, where the aspect can behave as a general Aspect Category (AC) or a specific Aspect Term (AT). However, due to the especially expensive and labor-intensive labeling, existing public corpora in AT-level are all relatively small. Meanwhile, most of the previous methods rely on complicated structures with given scarce data, which largely limits the efficacy of the neural models. In this paper, we exploit a new direction named coarse-to-fine task transfer, which aims to leverage knowledge learned from a rich-resource source domain of the coarse-grained AC task, which is more easily accessible, to improve the learning in a low-resource target domain of the fine-grained AT task. To resolve both the aspect granularity inconsistency and feature mismatch between domains, we propose a Multi-Granularity Alignment Network (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary task can help the AC task modeling at the same finegrained level with the AT task. To alleviate the feature false alignment, a contrastive feature alignment method is adopted to align aspect-specific feature representations semantically. In addition, a large-scale multi-domain dataset for the AC task is provided. Empirically, extensive experiments demonstrate the effectiveness of the MGAN.": 614,
    "How can sentiment polarities towards aspects in sentences be accurately identified when fine-grained aspect term (AT) level data is scarce, by leveraging more abundant coarse-grained aspect category (AC) level data?": 615,
    "Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that “source-domain engineering” becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weaklysupervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.": 616,
    "How can knowledge be effectively transferred from a source domain with noisy labels or features to an unlabeled target domain, while simultaneously addressing both the sample noises and the distribution shift across domains?": 617,
    "For unsupervised domain adaptation, the process of learning domain-invariant representations could be dominated by the labeled source data, such that the specific characteristics of the target domain may be ignored. In order to improve the performance in inferring target labels, we propose a targetspecific network which is capable of learning collaboratively with a domain adaptation network, instead of directly minimizing domain discrepancy. A clustering regularization is also utilized to improve the generalization capability of the target-specific network by forcing target data points to be close to accumulated class centers. As this network learns and specializes to the target domain, its performance in inferring target labels improves, which in turn facilitates the learning process of the adaptation network. Therefore, there is a mutually beneficial relationship between these two networks. We perform extensive experiments on multiple digit and object datasets, and the effectiveness and superiority of the proposed approach is presented and verified on multiple visual adaptation benchmarks, e.g., we improve the state-ofthe-art on the task of MNIST→SVHN from 76.5% to 84.9% without specific augmentation.": 618,
    "The paper addresses the challenge in unsupervised domain adaptation where learning domain-invariant representations can be overly influenced by labeled source data, leading to the neglect of specific characteristics of the target domain. How can the performance of inferring target labels be improved by specializing a model to the target domain while leveraging knowledge from a domain adaptation network?": 619,
    "Recent years have witnessed the great success of deep learning models in semantic segmentation. Nevertheless, these models may not generalize well to unseen image domains due to the phenomenon of domain shift. Since pixel-level annotations are laborious to collect, developing algorithms which can adapt labeled data from source domain to target domain is of great significance. To this end, we propose self-ensembling attention networks to reduce the domain gap between different datasets. To the best of our knowledge, the proposed method is the first attempt to introduce selfensembling model to domain adaptation for semantic segmentation, which provides a different view on how to learn domain-invariant features. Besides, since different regions in the image usually correspond to different levels of domain gap, we introduce the attention mechanism into the proposed framework to generate attention-aware features, which are further utilized to guide the calculation of consistency loss in the target domain. Experiments on two benchmark datasets demonstrate that the proposed framework can yield competitive performance compared with the state of the art methods.": 620,
    "How can deep learning models for semantic segmentation generalize effectively to unseen image domains when pixel-level annotations are scarce or unavailable in the target domain?": 621,
    "Cross-domain sentiment classification refers to utilizing useful knowledge in the source domain to help sentiment classification in the target domain which has few or no labeled data. Most existing methods mainly concentrate on extracting common features between domains. Unfortunately, they cannot fully consider the effects of the aspect (e.g., the battery life in reviewing an electronic product) information of the sentences. In order to better solve this problem, we propose an Interactive Attention Transfer Network (IATN) for crossdomain sentiment classification. IATN provides an interactive attention transfer mechanism, which can better transfer sentiment across domains by incorporating information of both sentences and aspects. Specifically, IATN comprises two attention networks, one of them is to identify the common features between domains through domain classification, and the other aims to extract information from the aspects by using the common features as a bridge. Then, we conduct interactive attention learning for those two networks so that both the sentences and the aspects can influence the final sentiment representation. Extensive experiments on the Amazon reviews dataset and crowdfunding reviews dataset not only demonstrate the effectiveness and universality of our method, but also give an interpretable way to track the attention information for sentiment.": 622,
    "How to effectively transfer sentiment across domains, particularly when target domains lack sufficient labeled data, by leveraging both sentence and aspect information?": 623,
    "While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only labeled data from source domains, have been actively studied in recent years, most algorithms and theoretical results focus on Single-source Unsupervised Domain Adaptation (SUDA). However, in the practical scenario, labeled data can be typically collected from multiple diverse sources, and they might be different not only from the target domain but also from each other. Thus, domain adapters from multiple sources should not be modeled in the same way. Recent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA) algorithms focus on extracting common domain-invariant representations for all domains by aligning distribution of all pairs of source and target domains in a common feature space. However, it is often very hard to extract the same domain-invariant representations for all domains in MUDA. In addition, these methods match distributions without considering domain-specific decision boundaries between classes. To solve these problems, we propose a new framework with two alignment stages for MUDA which not only respectively aligns the distributions of each pair of source and target domains in multiple specific feature spaces, but also aligns the outputs of classifiers by utilizing the domainspecific decision boundaries. Extensive experiments demonstrate that our method can achieve remarkable results on popular benchmark datasets for image classification.": 624,
    "How can a domain adaptation framework effectively handle multiple diverse source domains and a target domain, ensuring robust classification performance despite shifts not only between sources and target but also among the source domains themselves?": 625,
    "Many idiomatic expressions can be used figuratively or literally depending on the context. A particular challenge of automatic idiom usage recognition is that idioms, by their very nature, are idiosyncratic in their usages; therefore, most previous work on idiom usage recognition mainly adopted a “per idiom” classifier approach, i.e., a classifier needs to be trained separately for each idiomatic expression of interest, often with the aid of annotated training examples. This paper presents a transferred learning approach for developing a generalized model to recognize whether an idiom is used figuratively or literally. Our work is based on the observation that most idioms, when taken literally, would be somehow semantically at odds with their context. Therefore, a quantified notion of semantic compatibility may help to discern the intended usage for any arbitrary idiom. We propose a novel semantic compatibility model by adapting the training of a Continuous Bag-of-Words (CBOW) model for the purpose of idiom usage recognition. There is no need to annotate idiom usage examples for training. We perform evaluative experiments on two corpora; results show that the proposed generalized model achieves competitive results compared to state of-the-art per-idiom models.": 626,
    "How can a generalized model be developed to recognize whether an idiomatic expression is used figuratively or literally, overcoming the limitations of previous \"\"per idiom\"\" approaches that require separate training and annotated examples for each expression?": 627,
    "Humans convey their intentions through the usage of both verbal and nonverbal behaviors during face-to-face communication. Speaker intentions often vary dynamically depending on different nonverbal contexts, such as vocal patterns and facial expressions. As a result, when modeling human language, it is essential to not only consider the literal meaning of the words but also the nonverbal contexts in which these words appear. To better model human language, we first model expressive nonverbal representations by analyzing the fine-grained visual and acoustic patterns that occur during word segments. In addition, we seek to capture the dynamic nature of nonverbal intents by shifting word representations based on the accompanying nonverbal behaviors. To this end, we propose the Recurrent Attended Variation Embedding Network (RAVEN) that models the fine-grained structure of nonverbal subword sequences and dynamically shifts word representations based on nonverbal cues. Our proposed model achieves competitive performance on two publicly available datasets for multimodal sentiment analysis and emotion recognition. We also visualize the shifted word representations in different nonverbal contexts and summarize common patterns regarding multimodal variations of word representations.": 628,
    "How can human language models account for the dynamic variations in word meaning that arise from accompanying nonverbal behaviors during face-to-face communication?": 629,
    "Transfer learning for deep neural networks has achieved great success in many text classification applications. A simple yet effective transfer learning method is to fine-tune the pretrained model parameters. Previous fine-tuning works mainly focus on the pre-training stage and investigate how to pretrain a set of parameters that can help the target task most. In this paper, we propose an Instance Weighting based Finetuning (IW-Fit) method, which revises the fine-tuning stage to improve the final performance on the target domain. IW-Fit adjusts instance weights at each fine-tuning epoch dynamically to accomplish two goals: 1) identify and learn the specific knowledge of the target domain effectively; 2) well preserve the shared knowledge between the source and the target domains. The designed instance weighting metrics used in IW-Fit are model-agnostic, which are easy to implement for general DNN-based classifiers. Experimental results show that IW-Fit can consistently improve the classification accuracy on the target domain.": 630,
    "How can the fine-tuning stage of transfer learning for deep neural networks be revised to improve final performance on the target domain for text classification?": 631,
    "Inertial information processing plays a pivotal role in egomotion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose MotionTransformer - a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.": 632,
    "How can accurate inertial tracking be achieved across different sensor placements and motion dynamics without requiring extensive labeled data for every possible domain?": 633,
    "Heterogeneous Transfer Learning (HTL) aims to solve transfer learning problems where a source domain and a target domain are of heterogeneous types of features. Most existing HTL approaches either explicitly learn feature mappings between the heterogeneous domains or implicitly reconstruct heterogeneous cross-domain features based on matrix completion techniques. In this paper, we propose a new HTL method based on a deep matrix completion framework, where kernel embedding of distributions is trained in an adversarial manner for learning heterogeneous features across domains. We conduct extensive experiments on two different vision tasks to demonstrate the effectiveness of our proposed method compared with a number of baseline methods.": 634,
    "How can knowledge be effectively transferred between domains with heterogeneous feature types, especially when labeled data in the target domain is scarce, while simultaneously addressing the challenges of precisely measuring distribution distances and capturing intrinsic low-dimensional structures?": 635,
    "Recently, learning to hash has been widely studied for image retrieval thanks to the computation and storage efficiency of binary codes. Most existing learning to hash methods have yielded significant performance. However, for most existing learning to hash methods, sufficient training images are required and used to learn precise hashing codes. In some real-world applications, there are not always sufficient training images in the domain of interest. In addition, some existing supervised approaches need a amount of labeled data, which is an expensive process in terms of time, labor and human expertise. To handle such problems, inspired by transfer learning, we propose a simple yet effective unsupervised hashing method named Optimal Projection Guided Transfer Hashing (GTH) where we borrow the images of other different but related domain i.e., source domain to help learn precise hashing codes for the domain of interest i.e., target domain. In GTH, we aim to learn domain-invariant hashing functions. To achieve that, we propose to minimize the error matrix between two hashing projections of target and source domains. We seek for the maximum likelihood estimation (MLE) solution of the error matrix between the two hashing projections due to the domain gap. Furthermore, an alternating optimization method is adopted to obtain the two projections of target and source domains. By doing so, two projections can be progressively aligned. Extensive experiments on various benchmark databases for cross-domain visual recognition verify that our method outperforms many state-of-the-art learning to hash methods. The source code is available at https://github.com/liuji93/GTH.": 636,
    "How can precise hashing codes be learned for image retrieval when there are insufficient training images or limited labeled data in the domain of interest, especially when facing distribution disparities between domains?": 637,
    "Recent studies show significant progress in image-to-image translation task, especially facilitated by Generative Adversarial Networks. They can synthesize highly realistic images and alter the attribute labels for the images. However, these works employ attribute vectors to specify the target domain which diminishes image-level attribute diversity. In this paper, we propose a novel model formulating disentangled representations by projecting images to latent units, grouped feature channels of Convolutional Neural Network, to disassemble the information between different attributes. Thanks to disentangled representation, we can transfer attributes according to the attribute labels and moreover retain the diversity beyond the labels, namely, the styles inside each image. This is achieved by specifying some attributes and swapping the corresponding latent units to “swap” the attributes appearance, or applying channel-wise interpolation to blend different attributes. To verify the motivation of our proposed model, we train and evaluate our model on face dataset CelebA. Furthermore, the evaluation of another facial expression dataset RaFD demonstrates the generalizability of our proposed model.": 638,
    "How can image-to-image translation models effectively transfer multiple attributes simultaneously while preserving the inherent image-level style diversity beyond explicit attribute labels?": 639,
    "Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs (LP), and submodular optimization. However, these previous applications have uniformly focused on problems from specific classes with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a Mixed Integer Linear Program (MIP), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a MIP by employing a cutting planes solution approach, which is an exact algorithm that iteratively adds constraints to a continuous relaxation of the problem until an integral solution is found. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and prescription separately, as well as a baseline approach of simply applying decision-focused learning to the LP relaxation of the MIP.": 640,
    "How can decision-focused learning be extended to Mixed Integer Programs, given their discrete and discontinuous nature which typically prevents differentiation?": 641,
    "We study the fair allocation of a cake, which serves as a metaphor for a divisible resource, under the requirement that each agent should receive a contiguous piece of the cake. While it is known that no finite envy-free algorithm exists in this setting, we exhibit efficient algorithms that produce allocations with low envy among the agents. We then establish NP-hardness results for various decision problems on the existence of envy-free allocations, such as when we fix the ordering of the agents or constrain the positions of certain cuts. In addition, we consider a discretized setting where indivisible items lie on a line and show a number of hardness results extending and strengthening those from prior work. Finally, we investigate connections between approximate and exact envy-freeness, as well as between continuous and discrete cake cutting.": 642,
    "How can one achieve fair allocation of a divisible resource (cake) among agents while ensuring each agent receives a contiguous piece?\n---": 643,
    "Given datasets from multiple domains, a key challenge is to efficiently exploit these data sources for modeling a target domain. Variants of this problem have been studied in many contexts, such as cross-domain translation and domain adaptation. We propose AlignFlow, a generative modeling framework that models each domain via a normalizing flow. The use of normalizing flows allows for a) flexibility in specifying learning objectives via adversarial training, maximum likelihood estimation, or a hybrid of the two methods; and b) learning and exact inference of a shared representation in the latent space of the generative model. We derive a uniform set of conditions under which AlignFlow is marginally-consistent for the different learning objectives. Furthermore, we show that AlignFlow guarantees exact cycle consistency in mapping datapoints from a source domain to target and back to the source domain. Empirically, AlignFlow outperforms relevant baselines on image-to-image translation and unsupervised domain adaptation and can be used to simultaneously interpolate across the various domains using the learned representation.": 644,
    "Given datasets from multiple domains, how can a generative modeling framework efficiently exploit these data sources to model a target domain, learn and align shared representations, and enable accurate cross-domain translation while ensuring robust and consistent mappings?": 645,
    "Unpaired image-to-image domain translation involves the task of transferring an image in one domain to another domain without having pairs of data for supervision. Several methods have been proposed to address this task using Generative Adversarial Networks (GANs) and cycle consistency constraint enforcing the translated image to be mapped back to the original domain. This way, a Deep Neural Network (DNN) learns mapping such that the input training distribution transferred to the target domain matches the target training distribution. However, not all test images are expected to fall inside the data manifold in the input space where the DNN has learned to perform the mapping very well. Such images can have a poor mapping to the target domain. In this paper, we propose to perform Langevin dynamics, which makes a subtle change in the input space bringing them close to the data manifold, producing benign examples. The effect is significant improvement of the mapped image on the target domain. We also show that the score function estimation by denoising autoencoder (DAE), can practically be replaced with any autoencoding structure, which most image-to-image translation methods contain intrinsically due to the cycle consistency constraint. Thus, no additional training is required. We show advantages of our approach for several state-of-the-art image-to-image domain translation models. Quantitative evaluation shows that our proposed method leads to a substantial increase in the accuracy to the target label on multiple state-of-the-art image classifiers, while qualitative user study proves that our method better represents the target domain, achieving better human preference scores.": 646,
    "How can subtle changes in the input space improve the performance of image translation models, especially for test samples lying outside the training data manifold?": 647,
    "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.": 648,
    "How can domain adaptation effectively align the joint distributions of features and categories across source and target domains, especially when faced with issues like mode collapse in existing adversarial training methods?": 649,
    "Unsupervised domain adaptation aims to address the problem of classifying unlabeled samples from the target domain whilst labeled samples are only available from the source domain and the data distributions are different in these two domains. As a result, classifiers trained from labeled samples in the source domain suffer from significant performance drop when directly applied to the samples from the target domain. To address this issue, different approaches have been proposed to learn domain-invariant features or domain-specific classifiers. In either case, the lack of labeled samples in the target domain can be an issue which is usually overcome by pseudo-labeling. Inaccurate pseudo-labeling, however, could result in catastrophic error accumulation during learning. In this paper, we propose a novel selective pseudo-labeling strategy based on structured prediction. The idea of structured prediction is inspired by the fact that samples in the target domain are well clustered within the deep feature space so that unsupervised clustering analysis can be used to facilitate accurate pseudo-labeling. Experimental results on four datasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate our approach outperforms contemporary state-of-the-art methods.": 650,
    "How can unlabeled samples from a target domain be accurately classified when labeled samples are only available from a source domain, and the data distributions between these two domains differ significantly?": 651,
    "Unsupervised domain adaptation in semantic segmentation is to exploit the pixel-level annotated samples in the source domain to aid the segmentation of unlabeled samples in the target domain. For such a task, the key point is to learn domain-invariant representations and adversarial learning is usually used, in which the discriminator is to distinguish which domain the input comes from, and the segmentation model targets to deceive the domain discriminator. In this work, we first propose a novel joint adversarial learning (JAL) to boost the domain discriminator in output space by introducing the information of domain discriminator from low-level features. Consequently, the training of the high-level decoder would be enhanced. Then we propose a weight transfer module (WTM) to alleviate the inherent bias of the trained decoder towards source domain. Specifically, WTM changes the original decoder into a new decoder, which is learned only under the supervision of adversarial loss and thus mainly focuses on reducing domain divergence. The extensive experiments on two widely used benchmarks show that our method can bring considerable performance improvement over different baseline methods, which well demonstrates the effectiveness of our method in the output space adaptation.": 652,
    "How can domain-invariant representations be effectively learned for unsupervised semantic segmentation to bridge the domain shift between source (synthetic) and target (real-world) domains?": 653,
    "Multi-source unsupervised domain adaptation (MS-UDA) for sentiment analysis (SA) aims to leverage useful information in multiple source domains to help do SA in an unlabeled target domain that has no supervised information. Existing algorithms of MS-UDA either only exploit the shared features, i.e., the domain-invariant information, or based on some weak assumption in NLP, e.g., smoothness assumption. To avoid these problems, we propose two transfer learning frameworks based on the multi-source domain adaptation methodology for SA by combining the source hypotheses to derive a good target hypothesis. The key feature of the first framework is a novel Weighting Scheme based Unsupervised Domain Adaptation framework ((WS-UDA), which combine the source classifiers to acquire pseudo labels for target instances directly. While the second framework is a Two-Stage Training based Unsupervised Domain Adaptation framework (2ST-UDA), which further exploits these pseudo labels to train a target private extractor. Importantly, the weights assigned to each source classifier are based on the relations between target instances and source domains, which measured by a discriminator through the adversarial training. Furthermore, through the same discriminator, we also fulfill the separation of shared features and private features.Experimental results on two SA datasets demonstrate the promising performance of our frameworks, which outperforms unsupervised state-of-the-art competitors.": 654,
    "How can useful information from multiple labeled source domains be effectively leveraged to perform sentiment analysis in an unlabeled target domain, while avoiding limitations of existing methods such as over-reliance on shared features or weak assumptions?": 655,
    "Domain-adapted sentiment classification refers to training on a labeled source domain to well infer document-level sentiment on an unlabeled target domain. Most existing relevant models involve a feature extractor and a sentiment classifier, where the feature extractor works towards learning domain-invariant features from both domains, and the sentiment classifier is trained only on the source domain to guide the feature extractor. As such, they lack a mechanism to use sentiment polarity lying in the target domain. To improve domain-adapted sentiment classification by learning sentiment from the target domain as well, we devise a novel deep adversarial mutual learning approach involving two groups of feature extractors, domain discriminators, sentiment classifiers, and label probers. The domain discriminators enable the feature extractors to obtain domain-invariant features. Meanwhile, the label prober in each group explores document sentiment polarity of the target domain through the sentiment prediction generated by the classifier in the peer group, and guides the learning of the feature extractor in its own group. The proposed approach achieves the mutual learning of the two groups in an end-to-end manner. Experiments on multiple public datasets indicate our method obtains the state-of-the-art performance, validating the effectiveness of mutual learning through label probers.": 656,
    "How can document-level sentiment be effectively inferred on an unlabeled target domain by leveraging sentiment polarity from the target domain itself, given training data from a labeled source domain?": 657,
    "Person re-identification (Re-ID) across multiple datasets is a challenging task due to two main reasons: the presence of large cross-dataset distinctions and the absence of annotated target instances. To address these two issues, this paper proposes a domain adaptive attention learning approach to reliably transfer discriminative representation from the labeled source domain to the unlabeled target domain. In this approach, a domain adaptive attention model is learned to separate the feature map into domain-shared part and domain-specific part. In this manner, the domain-shared part is used to capture transferable cues that can compensate cross-dataset distinctions and give positive contributions to the target task, while the domain-specific part aims to model the noisy information to avoid the negative transfer caused by domain diversity. A soft label loss is further employed to take full use of unlabeled target data by estimating pseudo labels. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 benchmarks demonstrate the proposed approach outperforms the state-of-the-arts.": 658,
    "How can discriminative person re-identification representations be reliably transferred from a labeled source domain to an unlabeled target domain, effectively addressing large cross-dataset distinctions and the absence of annotated target instances?": 659,
    "Unpaired image-to-image translation is proven quite effective in boosting a CNN-based object detector for a different domain by means of data augmentation that can well preserve the image-objects in the translated images. Recently, multimodal GAN (Generative Adversarial Network) models have been proposed and were expected to further boost the detector accuracy by generating a diverse collection of images in the target domain, given only a single/labelled image in the source domain. However, images generated by multimodal GANs would achieve even worse detection accuracy than the ones by a unimodal GAN with better object preservation. In this work, we introduce cycle-structure consistency for generating diverse and structure-preserved translated images across complex domains, such as between day and night, for object detector training. Qualitative results show that our model, Multimodal AugGAN, can generate diverse and realistic images for the target domain. For quantitative comparisons, we evaluate other competing methods and ours by using the generated images to train YOLO, Faster R-CNN and FCN models and prove that our model achieves significant improvement and outperforms other methods on the detection accuracies and the FCN scores. Also, we demonstrate that our model could provide more diverse object appearances in the target domain through comparison on the perceptual distance metric.": 660,
    "How can diverse and structure-preserved images be generated across complex visual domains, such as between day and night, to effectively boost the performance of CNN-based object detectors?": 661,
    "Humor is a unique and creative communicative behavior often displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.": 662,
    "How can machine learning models effectively detect humor by understanding the complex dynamics of multimodal language, specifically integrating textual, visual, and acoustic cues within their broader context?": 663,
    "Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (\\mathcal{H}-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.": 664,
    "How can the performance drop of natural language processing models under domain shift be accurately predicted without requiring labeled examples from the target domain?": 665,
    "Metaphors allow us to convey emotion by connecting physical experiences and abstract concepts. The results of previous research in linguistics and psychology suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between metaphor and emotion within a computational framework, by proposing the first joint model of these phenomena. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.": 666,
    "How can a computational framework be developed to jointly model the relationship between metaphor and emotion, and can this joint learning mutually benefit both metaphor identification and emotion prediction tasks?": 667,
    "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.": 668,
    "How can Machine Reading Comprehension models be effectively adapted to new target domains when only unlabeled passages are available, and labeled data is scarce or non-existent in the target domain?": 669,
    "Text style transfer without parallel data has achieved some practical success. However, in the scenario where less data is available, these methods may yield poor performance. In this paper, we examine domain adaptation for text style transfer to leverage massively available data from other domains. These data may demonstrate domain shift, which impedes the benefits of utilizing such data for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to: (i) distinguish stylized information and generic content information; (ii) maximally preserve content information; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines.": 670,
    "How can text style transfer be effectively performed in scenarios with limited target domain data, especially when leveraging massively available data from other domains that may exhibit domain shift?": 671,
    "In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.": 672,
    "How can a system generate pun sentences that exhibit ambiguity and diversity without relying on a large-scale, labeled pun corpus?": 673,
    "In sequence labeling, previous domain adaptation methods focus on the adaptation from the source domain to the entire target domain without considering the diversity of individual target domain samples, which may lead to negative transfer results for certain samples. Besides, an important characteristic of sequence labeling tasks is that different elements within a given sample may also have diverse domain relevance, which requires further consideration. To take the multi-level domain relevance discrepancy into account, in this paper, we propose a fine-grained knowledge fusion model with the domain relevance modeling scheme to control the balance between learning from the target domain data and learning from the source domain model. Experiments on three sequence labeling tasks show that our fine-grained knowledge fusion model outperforms strong baselines and other state-of-the-art sequence labeling domain adaptation methods.": 674,
    "How can domain adaptation methods for sequence labeling effectively account for the diverse domain relevance of individual target domain samples and their constituent elements to prevent negative transfer and improve performance?": 675,
    "We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use word-word co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) humour classification datasets using only 10% of known labels.": 676,
    "How can effective lexical features be extracted and utilized for humor recognition, particularly in scenarios with small sample sizes or for fine-grained humor ranking, without relying on extensive external training data?": 677,
    "Unsupervised domain adaptation (UDA) is the task of training a statistical model on labeled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Existing state-of-the-art UDA approaches use neural networks to learn representations that are trained to predict the values of subset of important features called “pivot features” on combined data from the source and target domains. In this work, we show that it is possible to improve on existing neural domain adaptation algorithms by 1) jointly training the representation learner with the task learner; and 2) removing the need for heuristically-selected “pivot features.” Our results show competitive performance with a simpler model.": 678,
    "How can the performance of unsupervised domain adaptation (UDA) be improved by better integrating representation learning and task learning within neural network models?": 679,
    "Most current approaches to metaphor identification use restricted linguistic contexts, e.g. by considering only a verb’s arguments or the sentence containing a phrase. Inspired by pragmatic accounts of metaphor, we argue that broader discourse features are crucial for better metaphor identification. We train simple gradient boosting classifiers on representations of an utterance and its surrounding discourse learned with a variety of document embedding methods, obtaining near state-of-the-art results on the 2018 VU Amsterdam metaphor identification task without the complex metaphor-specific features or deep neural architectures employed by other systems. A qualitative analysis further confirms the need for broader context in metaphor processing.": 680,
    "How can broader discourse features be utilized to improve metaphor identification in text?": 681,
    "Metaphor generation attempts to replicate human creativity with language, which is an attractive but challengeable text generation task. Previous efforts mainly focus on template-based or rule-based methods and result in a lack of linguistic subtlety. In order to create novel metaphors, we propose a neural approach to metaphor generation and explore the shared inferential structure of a metaphorical usage and a literal usage of a verb. Our approach does not require any manually annotated metaphors for training. We extract the metaphorically used verbs with their metaphorical senses in an unsupervised way and train a neural language model from wiki corpus. Then we generate metaphors conveying the assigned metaphorical senses with an improved decoding algorithm. Automatic metrics and human evaluations demonstrate that our approach can generate metaphors with good readability and creativity.": 682,
    "How can a neural approach be developed to generate novel metaphors in an unsupervised manner, overcoming the limitations of previous template-based or rule-based methods that lack linguistic subtlety?": 683,
    "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., “died” and “dyed”). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., “dyed”) and the distant context, but a strong association between the alternative word (e.g., “died”) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.": 684,
    "How can novel and creative pun sentences be generated given a pair of homophones?": 685,
    "A pun is a form of wordplay for an intended humorous or rhetorical effect, where a word suggests two or more meanings by exploiting polysemy (homographic pun) or phonological similarity to another word (heterographic pun). This paper presents an approach that addresses pun detection and pun location jointly from a sequence labeling perspective. We employ a new tagging scheme such that the model is capable of performing such a joint task, where useful structural information can be properly captured. We show that our proposed model is effective in handling both homographic and heterographic puns. Empirical results on the benchmark datasets demonstrate that our approach can achieve new state-of-the-art results.": 686,
    "How can English pun detection and location be jointly performed using a sequence labeling approach?": 687,
    "Cross-domain sentiment classification aims to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. Most existing adversarial learning methods focus on aligning the global marginal distribution by fooling a domain discriminator, without taking category-specific decision boundaries into consideration, which can lead to the mismatch of category-level features. In this work, we propose an adversarial category alignment network (ACAN), which attempts to enhance category consistency between the source domain and the target domain. Specifically, we increase the discrepancy of two polarity classifiers to provide diverse views, locating ambiguous features near the decision boundaries. Then the generator learns to create better features away from the category boundaries by minimizing this discrepancy. Experimental results on benchmark datasets show that the proposed method can achieve state-of-the-art performance and produce more discriminative features.": 688,
    "How can sentiment classification models effectively transfer knowledge across different domains while accounting for domain-specific variations in emotional expression and ensuring category-level alignment of features, rather than just global distribution alignment?": 689,
    "Cross-domain Chinese Word Segmentation (CWS) remains a challenge despite recent progress in neural-based CWS. The limited amount of annotated data in the target domain has been the key obstacle to a satisfactory performance. In this paper, we propose a semi-supervised word-based approach to improving cross-domain CWS given a baseline segmenter. Particularly, our model only deploys word embeddings trained on raw text in the target domain, discarding complex hand-crafted features and domain-specific dictionaries. Innovative subsampling and negative sampling methods are proposed to derive word embeddings optimized for CWS. We conduct experiments on five datasets in special domains, covering domains in novels, medicine, and patent. Results show that our model can obviously improve cross-domain CWS, especially in the segmentation of domain-specific noun entities. The word F-measure increases by over 3.0% on four datasets, outperforming state-of-the-art semi-supervised and unsupervised cross-domain CWS approaches with a large margin. We make our data and code available on Github.": 690,
    "How can cross-domain Chinese Word Segmentation (CWS) be improved, particularly given the limited availability of annotated data in target domains?": 691,
    "The performance of a Part-of-speech (POS) tagger is highly dependent on the domain of the processed text, and for many domains there is no or only very little training data available. This work addresses the problem of POS tagging noisy user-generated text using a neural network. We propose an architecture that trains an out-of-domain model on a large newswire corpus, and transfers those weights by using them as a prior for a model trained on the target domain (a data-set of German Tweets) for which there is very little annotations available. The neural network has a standard bidirectional LSTM at its core. However, we find it crucial to also encode a set of task-specific features, and to obtain reliable (source-domain and target-domain) word representations. Experiments with different regularization techniques such as early stopping, dropout and fine-tuning the domain adaptation prior weights are conducted. Our best model uses external weights from the out-of-domain model, as well as feature embeddings, pre-trained word and sub-word embeddings and achieves a tagging accuracy of slightly over 90%, improving on the previous state of the art for this task.": 692,
    "How can the performance of Part-of-Speech (POS) tagging for noisy, user-generated text, such as German Tweets, be improved when only very limited target-domain training data is available?": 693,
    "In this paper, we automatically create sentiment dictionaries for predicting financial outcomes. We compare three approaches: (i) manual adaptation of the domain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a combination consisting of first manual, then automatic adaptation. In our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. In particular, automatic adaptation performs better than manual adaptation. In our analysis, we find that annotation based on an expert’s a priori belief about a word’s meaning can be incorrect – annotation should be performed based on the word’s contexts in the target domain instead.": 694,
    "How can sentiment dictionaries be effectively adapted to the specialized financial domain to improve the accuracy of predicting financial outcomes?": 695,
    "While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.": 696,
    "How can pragmatic reasoning, specifically learning from what a speaker chooses not to say, be integrated into the training process of neural network models to improve their language understanding capabilities, particularly in data-sparse and linguistically complex environments?": 697,
    "Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.": 698,
    "How can a large-scale Chinese cloze test dataset be developed to effectively assess the comprehension of idioms, a unique and challenging language phenomenon characterized by non-compositionality and near-synonymy?": 699,
    "Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.": 700,
    "How can training data selection for domain adaptation be made self-adaptive to data and tasks, while simultaneously integrating with model training to learn better representations and avoid predefined thresholds?": 701,
    "We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively.": 702,
    "How can an automated system be developed to act as a research assistant, capable of understanding existing scientific literature, generating novel research ideas, and incrementally drafting key elements of new scientific papers?": 703,
    "Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.": 704,
    "How can automatic poetry generation models be designed to effectively incorporate and control rhetorical elements, specifically metaphor and personification, to enhance the aesthetic quality and emotional impact of modern Chinese poetry?": 705,
    "A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to/from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data.": 706,
    "How can abstractive title generation models effectively adapt from a labeled source domain to an unlabeled or limited-labeled target domain, overcoming differences in vocabulary, grammatical styles, and concept expression?": 707,
    "End-to-end training with Deep Neural Networks (DNN) is a currently popular method for metaphor identification. However, standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification. We experiment with two DNN models which are inspired by two human metaphor identification procedures. By testing on three public datasets, we find that our models achieve state-of-the-art performance in end-to-end metaphor identification.": 708,
    "How can deep neural networks be designed to effectively identify metaphors in text by explicitly incorporating insights from linguistic theories of metaphor identification, rather than relying solely on generic sequence tagging approaches?": 709,
    "Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. We introduce a new sample-efficient method that learns the AL policy directly on the target domain of interest by using wake and dream cycles. Our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving AL policy using simulation where the current student learner acts as an imperfect annotator. We evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks. Experimental results show that our dream-based AL policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning.": 710,
    "How can an active learning policy be effectively learned directly on a target domain of interest, rather than relying on policies transferred from related source domains, to overcome limitations posed by varying data distributions and domain relatedness?": 711,
    "Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.": 712,
    "How can slot filling models be made robust to misaligned schemas and capable of zero-shot cross-domain transfer, especially in low-data settings, without requiring extensive labeled training data or complex schema alignment?": 713,
    "Although the proper use of idioms can enhance the elegance of writing, the active use of various expressions is a challenge because remembering idioms is difficult. In this study, we address the problem of idiom recommendation by leveraging a neural machine translation framework, in which we suppose that idioms are written with one pseudo target language. Two types of real-life datasets are collected to support this study. Experimental results show that the proposed approach achieves promising performance compared with other baseline methods.": 714,
    "How can pertinent Chinese idioms be automatically recommended for essay writing based on surrounding context to enhance elegance, given the difficulty of remembering and properly using them?": 715,
    "Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets.": 716,
    "How can adversarial domain adaptation be effectively applied to textual matching tasks to explicitly learn both shared and domain-specific representations, thereby improving target domain accuracy?": 717,
    "The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language—namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman’s ρ of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available.": 718,
    "How can abstract and subjective aspects of creative language, such as humorousness and metaphor novelty, be reliably quantified and ranked from human judgments, especially when direct numerical scoring is inconsistent and annotation data is sparse?": 719,
    "To address the first research question, the method employs an Intra-Branch Fusion module that uses channel attention to calculate the importance of each side of views within a branch. Features from views with fewer occlusions are selected for fusion. This involves designing the Intra-Branch Fusion module with a 3D global average pooling layer, three 1x1 convolutional layers, and a sigmoid layer to compute channel attention. The attention maps are then multiplied with the feature maps to emphasize relevant features, thereby enhancing depth estimation accuracy in occluded areas.": 720,
    "The paper addresses the challenge of reducing domain-wise variations through a novel Domain-wise Adversarial Feature Learning (LDA) scheme. Instead of performing pairwise alignment between all source domains, which can lead to excessive distributional shifts and reduced feature discriminability, the method identifies a \"\"central domain\"\" and aligns all \"\"peripheral domains\"\" towards it. The central domain is defined as the one that minimizes the total Wasserstein distance (dWS) to all other source domains, quantifying the minimal distributional shift required for alignment. Wasserstein distance, approximated using Sinkhorn's method, is chosen for its symmetry and ability to handle non-overlapping distributions. Once the central domain is determined, a domain discriminator D is trained to distinguish whether an input feature belongs to the central or a peripheral domain using a cross-entropy loss (LDA-D). Concurrently, the feature mapping network M, acting as a generator, is trained to \"\"fool\"\" the discriminator by minimizing the negative entropy of the predicted domain distributions (LDA-T). This adversarial process forces the mapping network to transform features from peripheral domains into a uniform distribution that is similar to the central domain, thereby learning domain-invariant features while minimizing the negative impact on feature discriminability by avoiding unnecessary shifts towards outlying domains.": 721,
    "The paper addresses this by introducing a Concept Preservation Loss (LP) within the Concept-Aware-Pseudo-Query (CAPQ) framework. This loss is designed to preserve previously acquired knowledge from pre-trained models, making both video and text features in the joint embedding space transferable and domain-agnostic. The process involves a Concept Selector (C) and a Hallucinator (H). The Concept Selector (C) first maps a generic video descriptor, obtained from a frozen Feature Extractor (F), to a concept distribution (y) associated with the original pretraining task (e.g., ImageNet's 1000 concepts). This provides a common signal across domains. For source domain video-text pairs, the predicted concept distribution for the source video (yS) is propagated to its paired text embedding, requiring them to map to the same concept distribution if the text describes the video. Next, a Hallucinator (H), implemented as a two-layer Multi-Layer Perceptron (MLP), takes the joint video embeddings (g_vid) and text embeddings (g_text) as input. Its goal is to generate predictions (y_hat) that match the concept selector's output (y). To accurately generate these predictions, the embeddings must retain the ability to distinguish between external concepts known to the pre-trained models. The same Hallucinator is used for both text and video embeddings, implicitly encouraging feature alignment between g_vid(v) and g_text(t). The Concept Preservation Loss (LP) is then computed as the Kullback-Leibler divergence between the concept distribution (y) and the hallucinator's predictions (y_hat). In the source domain, LP minimizes (yS, y_hat_S_v) + (yS, y_hat_S_t), preserving concepts in both video and text embeddings. In the target domain, where text queries are unavailable, LP minimizes (yT, y_hat_T_v), preserving concepts only in the target video embedding. This mechanism encourages feature alignment between source and target video embeddings and broadens the semantic coverage of the text encoder's output.": 722,
    "The paper addresses the challenge of leveraging crowd flow consistency by proposing a density isomorphism reconstruction objective. This objective is based on the assumption that density\nmaps in adjacent frames are isomorphic, meaning they are mutually transformable via bijective mapping. Instead of directly calculating mapping correspondences using potentially inaccurate density maps, the method first computes an image mapping correspondence (Mj_i) between consecutive image frames (Ij to Ii) by minimizing an image reconstruction error using a warping function. This image mapping is efficiently computed using the Gunner-Farneback algorithm, which approximates pixel transformations with quadratic polynomials. Once the image mapping matrices (Mi-d_i and Mi+d_i) are obtained, they are transformed into density mapping matrices (Gi-d_i and Gi+d_i) by linear sampling and scaling, accounting for the different dimensions of image frames and density maps. Finally, the density map of the central image Ii is reconstructed (Di^0) from its preceding (Di-d) and succeeding (Di+d) density maps using these derived density mapping matrices. This reconstructed density map then serves as a self-supervised signal for training.": 723,
    "The paper introduces a Cross-Domain Grouping Network (C), which is a learnable module designed to cluster the output probability distribution into K sub-spaces. This network consists of two 1x1 convolutional layers. The first convolution generates a 64-channel feature map, followed by a Rectified Linear Unit (ReLU) activation and batch normalization. The second convolution then produces K grouping scores, which are passed through a softmax function to yield group probabilities, denoted as Hk_l, for each pixel. These group probabilities Hk_l are then element-wise multiplied with the original pixel-wise class probability distribution Pl (output of the segmentation network G before softmax) to obtain group-specific features, Fk_l. This process allows the framework to decompose complex data distributions into K simpler ones, enabling group-level domain alignment without relying on a potentially inaccurate category classifier.": 724,
    "The category dictionary guided pseudo-box generation module learns a dictionary of representative atoms for each category using the source domain features. These dictionaries are then used to represent each candidate box in the target domain in a collaborative representation manner. The representation residual for each category indicates the quality of the candidate box, allowing for more reliable and informative pseudo-label assignment. By leveraging the learned dictionaries, the method can select pseudo-boxes that are not only confident in classification but also accurate in localization, addressing the limitations of traditional classifiers like softmax.": 725,
    "The paper addresses this by learning a \"\"prototypical distribution\"\" for the source domain in an intermediate embedding space. This distribution is modeled as a Gaussian Mixture Model (GMM) with K components, where K is the number of semantic categories. Each component of the GMM represents a semantic class, characterized by its mixture weight (pi_j), mean (mu_j), and covariance (Sigma_j). This GMM is estimated using the source domain's labeled data. Specifically, after initial training of the semantic segmentation model f_theta = h_w o v o phi_u on the source domain, the encoder phi_u and decoder v transform source input images x_s into embedding space representations. Since source labels y_s are available, the parameters of each GMM component (mean and covariance) are estimated independently via Maximum A Posteriori (MAP) estimation for each class j. To improve the quality of this estimation and mitigate the effect of misclassified samples, a confidence threshold delta is applied: only samples for which the model's predicted probability p_ijk for the ground-truth class k is greater than delta are included in the estimation for class j. This process results in a robust, parametric representation of the source domain's class structure in the embedding space, which can then be used as a surrogate for the source data during adaptation.": 726,
    "The discriminative ability of source features is preserved by introducing a Source Discriminative Constraint (SDC). This constraint ensures that the classification loss on the source domain, denoted as Lce(θt, Ds), does not increase during the adaptation process. Mathematically, this is expressed as Lce(θt, Ds) <= Lce(θt-1, Ds), where θt are the current model parameters and θt-1 are the parameters from the previous step. This condition is rephrased as an inner product constraint: hw, gsi >= 0, where w is the vector used to update the model parameters and gs is the gradient of the classification loss on the source domain (∂Lce(θt, Ds)/∂θt). This means the update direction w must have a non-negative projection onto the source classification loss gradient, preventing updates that would increase this loss. This constraint is integrated into a quadratic programming (QP) problem that determines the optimal parameter update vector.": 727,
    "The paper proposes a meta-learning based adaptor learning algorithm to effectively learn and initialize a domain-specific adaptor during training. This algorithm operates in two main steps: meta-train and meta-test, simulating the real-world scenario where a model trained on source domains needs to adapt to an unseen target domain. First, a feature extractor (F), a classification header (C), a depth estimator (D), an adaptor (A), and an autoencoder (R) are defined. The adaptor (A) is designed as a 1x1 convolution connected to the first convolution of the classifier (C) through a residual architecture, allowing it to be easily removed to revert to the original model. During the meta-train step, a meta-train domain (Dtrn) is randomly selected from the available source domains. The model parameters (F, C, D, R) are updated using supervised losses on Dtrn, including a cross-entropy classification loss (LCls), a depth estimation loss (LDep) for auxiliary supervision, and an autoencoder reconstruction loss (LAE) to train the autoencoder (R) on source features. After this supervised update, in the meta-test step, a different meta-test domain (Dval) is randomly selected from the source domains. The adaptor (A) is then added to the classifier (C) to form a combined classifier with adaptor (Ca). The adaptor (A) is updated unsupervisedly on Dval using a proposed unsupervised adaptor loss (LAdap). Finally, a meta-optimization step updates the overall model parameters (F, C, D, A, R) based on the classification performance on the meta-test domain (Dval) after the adaptor's unsupervised update. This meta-learning process trains the adaptor to be well-initialized, enabling efficient adaptation to new, unseen domains at inference time.": 728,
    "The paper addresses this by engineering a 3-dimensional feature vector `f` that quantifies the differences in the orders of magnitude of coefficients after presolving, specific to each scaling method. First, for each of the two candidate scaling methods, Standard Scaling (ST) and Curtis-Reid (CR), the presolved matrix `A` is scaled to obtain corresponding scaling matrices `Rs` and `Cs`. Then, the minimum and maximum nonzero entries are identified for the scaled matrix coefficients (`d_coef_s`, `D_coef_s`), objective coefficients (`d_obj_s`, `D_obj_s`), and right-hand side coefficients (`d_rhs_s`, `D_rhs_s`). If all coefficients in a category are zero, their corresponding minimum/maximum entry is defined as zero. Variable finite bounds are treated as part of the right-hand side. From these minimum and maximum values, three separate logarithms are computed for each scaling method, representing the range in orders of magnitude: `L_coef_s = log10(D_coef_s / d_coef_s)`, `L_obj_s = log10(D_obj_s / d_obj_s)`, and `L_rhs_s = log10(D_rhs_s / d_rhs_s)`. These `L`-values are set to 0 if all values are equal or all zero. Finally, the feature vector `f` is constructed as the difference between the `L`-vectors of Curtis-Reid and Standard scaling: `f = (L_coef_cr - L_coef_st; L_obj_cr - L_obj_st; L_rhs_cr - L_rhs_st)`. This feature vector captures how many orders of magnitude the coefficients span for each scaling, providing a concise numerical representation for the machine learning model.": 729,
    "The paper addresses the challenge of transforming synthetic source images into realistic, condition-specific target-like images using a Condition Guided Style Transfer (CGST) module. This module learns a conditional generator, denoted as GST(XS;c), which maps source images (XS) to target-like images (XT) corresponding to a specific latent variable 'c' (representing conditions like T1 or T2). The CGST follows standard conditional Generative Adversarial Networks (GANs) principles, borrowing concepts from StarGAN and BicycleGAN for improved translation. A multi-level concatenation strategy injects the condition 'c' into GST as a translation guidance. Adversarial training enforces GST to produce realistic images with preferred styles, using a discriminator DT that distinguishes real target images from generated ones. An auxiliary condition classification loss (Lcls) further constrains the solution space by ensuring the generated images are correctly classified for their intended condition. To prevent semantic distortion or inconsistency during translation (e.g., trees becoming building materials), a semantic consistency constraint (Lsc) is incorporated, which uses a segmentation network (Fseg) to ensure that the translated image retains the semantic information of the original source image's ground truth label. The full objective for CGST combines these losses: LcGAN + Lcls + λscLsc.": 730,
    "To address the challenge of varying offsets in time series data, the paper proposes an Adaptive Segment Summarization module. This module first constructs multiple candidate segments of different lengths for each univariate time series `xi`. Specifically, for a given time series `xi`, a set of segments `fxi = {xi_t-N+1:t;...;xi_t-delta+1:t;...;xi_t:t;xi_t-N+1:t}` is generated, where `delta` represents the segment length. An independent Long Short-Term Memory (LSTM) network, parameterized by `theta_i`, is allocated for each variable `xi`. Each segment `xi_t-delta+1:t` is fed into its corresponding variable-specific LSTM to obtain a segment representation `h_i_delta`. This process results in a set of segment representations `h_i = {h_i_1;...;h_i_delta;...;h_i_N}` for each variable.\nSubsequently, a Segments Representation Selection via Intra-Variables Attention Mechanism is employed to identify the most suitable segment representations. A self-attention mechanism is used to calculate weights `alpha_i_delta` for each segment representation `h_i_delta`. These weights are computed using trainable projection parameters `W_Q` (query) and `W_K` (key) and a scaling factor `dh`, and then normalized using `sparsemax` to ensure sparsity. The weighted segment representation `Z_i` for variable `xi` is then obtained by summing the product of `alpha_i_delta` and `h_i_delta` over all possible segment lengths. To further mitigate the obstacle of offsets and ensure that the duration of segments from different domains is similar, the Maximum Mean Discrepancy (MMD) between the `alpha` weights from the source and target domains (`L_alpha = MMD(alpha_S, alpha_T)`) is minimized. This loss function restricts the segment duration distributions to be similar across domains, aiding in the extraction of transferable structures.": 731,
    "Solution 1:\nThe data generator, denoted as G(theta)(z|x), is designed to produce augmented samples `z` from an original data instance `x`. To ensure stable training and restrict the exploration space, the generator samples from a stationary distribution `Ps(z)`. This distribution is constructed by a stratified sampling process: first, an edit distance `d` is sampled based on `P(d|x)`; then, a position `o` for substitution is selected based on `P(o|d;x)`; finally, a new word `w` is filled into that position based on `P(w|o;d;x)`. The probability `P(d|x)` is defined using a reweighted and normalized count of sentences at a given edit distance, incorporating a temperature parameter `epsilon` (ε) that controls the degree of regularization and the search space. A smaller `epsilon` keeps samples closer to the original data. The position selection `P(o|d;x)` is simply `d/m` for `d` distinct positions in a sentence of length `m`. For `P(w|o;d;x)`, a BERT-based generator is employed. It masks the selected position `o` with a special `[mask]` token and uses BERT to predict the corresponding word, applying a softmax-temperature `T` to the BERT probabilities. This `T` parameter further controls the exploration degree, with a higher `T` yielding a softer probability distribution over candidate words, thus expanding the search space. The `Ps(z)` distribution can draw from both resource-rich source domains and the data-scarce target domain. In experiments, a sample size of 1 is used for the source domain (implying full source data usage) and 20 for the target domain per instance, generating semantically coherent variants close to the ground truths.": 732,
    "The framework constructs a Content Invariant Representation (CIR) by introducing an intermediate domain that shares invariant content with the source domain and has a similar data distribution to the target domain. This is achieved using an image-to-image translation network, such as CycleGAN, to transfer the style of source images to match the target domain's appearance while preserving the original content. The CIR serves as a bridge to reduce the domain gap between source and target domains, making the adaptation process more manageable.": 733,
    "The paper addresses this by first learning a set of source hypotheses, denoted as `hS_i` (where `i` ranges from 1 to M), on the source data `Ds`. These source hypotheses share a common feature extractor `S` but utilize `M` independent classifiers, `fS_i`, each trained from a different random initialization. This approach, similar to deep ensembles, is chosen for its ability to learn diverse functions from different modes of the hypothesis distribution and to produce well-calibrated uncertainty estimations, which is particularly beneficial for Hypothesis Transfer Learning (HTL). The source hypotheses are trained by minimizing a cross-entropy loss function, `Lsource`, averaged across the `M` hypotheses. Subsequently, for unsupervised hypothesis transfer, these `M` source hypotheses are adapted into a corresponding set of target hypotheses, `hT_i`, using unlabeled target data `Dt`. This adaptation is achieved by maximizing the mutual information between the target input `XT` and the predicted target label distribution `^YT` inferred by the target hypotheses. The optimization process involves jointly maximizing the expectation of mutual information over the target hypotheses. During this process, the shared feature extractor `T` for the target hypotheses is updated, while the parameters of the classifiers `fT_i` are kept fixed, leveraging the assumption that source and target domains share the same label space.": 734,
    "The paper leverages external data through a meta-learn module by adopting a strategy similar to Model-Agnostic Meta-Learning (MAML). This involves gaining experience from a multitude of auxiliary relevant tasks sampled from a distribution `p(T)` based on meta data `M`. Each sampled task `T` (referred to as an episode) is divided into two non-overlapping splits: a training split `M(tr)` and a test split `M(te)`. The base-learner, represented by the model parameters `θ_T`, is updated by minimizing a loss function `L_T` on the training split `M(tr)`. This loss function, defined in Equation (3), includes terms for reconstruction error, self-representation loss, and a regularization term, similar to the objective function of DSC-Nets but applied to meta data. Subsequently, the meta-learner, represented by the overall model parameters `θ`, is optimized by minimizing the same loss function `L_T` on the test split `M(te)`. This process, summarized in Algorithm 1, allows the model to learn a good initialization for the encoder that can quickly adapt to new subspace clustering tasks on target data.": 735,
    "The paper introduces a novel metric called Classiﬁer Determinacy Disparity (CDD) to quantify the discrepancy between the softmax probability outputs of two distinct classifiers, p1 and p2. Unlike previous methods that rely on simple L1 distance, CDD incorporates the relevance information between classes. It is defined using a Bi-classiﬁer Prediction Relevance Matrix A, which is computed as the outer product of the two probability vectors: A = p1 * p2^T. Each element Amn of this KxK matrix represents the product of the probability of classifier C1 classifying a sample into the m-th category and classifier C2 classifying it into the n-th category. The CDD loss, denoted as (p1, p2), is then calculated as the sum of all elements in matrix A minus the sum of its diagonal elements. Since the sum of all elements in A is always 1 (as p1 and p2 are valid softmax distributions), this simplifies to 1 - (sum_m Amm). This formulation ensures that the CDD value is zero only when the two probabilities are perfectly consistent and fully determined (e.g., both classifiers output [1,0,0] for a specific class), thereby promoting strong feature discriminability. By minimizing CDD, the method implicitly encourages the maximization of the diagonal elements of matrix A, which corresponds to consistent and confident predictions across the two classifiers. Conversely, the non-diagonal elements of A capture fine-grained confusion information between the classifiers, and maximizing CDD encourages these off-diagonal elements, leading to diverse predictions.": 736,
    "The paper proposes Self-Entropy Descent (SED) as a metric to search for an appropriate confidence threshold for pseudo-label generation. Initially, a pre-trained model from the source domain is used to generate pseudo-labels and corresponding confidence scores for the unlabeled target domain. The prediction uncertainty for the entire target dataset is quantified using mean self-entropy, H(Dt), defined as the average of negative sums of prediction probabilities multiplied by their logarithms across all classes and samples. The process involves iteratively fine-tuning the pre-trained model using pseudo-labels generated with a decreasing confidence threshold. After each fine-tuning step, the updated model is used to evaluate the mean self-entropy of the target dataset. The optimal confidence threshold is then selected when the mean self-entropy descends and hits its first local minimum, indicating the most reliable pseudo-label assignment.": 737,
    "The paper designs a Sample Transfer Score, denoted as `s(x)`, which estimates the confidence that a given sample `x` is from the shared label set Y. This score is a combination of two signals: the confidence in the classification label and the estimation of the probability of the sample being from the source domain. Specifically, the score is calculated as `s(x) = d(x) + max y(x)`. Here, `d(x)` is the output of the domain classifier D, representing the probability of the sample `x` being from the source domain. Higher values of `d(x)` indicate greater similarity to source domain samples. `max y(x)` is the maximum pseudo-probability from the label classifier C, `y(x) = C(F(x))`, where `F` is the feature extractor. This term reflects the classifier's confidence in its predicted label for `x`. The rationale is that target samples more similar to source domain samples (higher `d(x)`) and those for which the classifier has high confidence in a specific class prediction (higher `max y(x)`) are more likely to belong to the shared label set. The score `s(x)` ranges from 0 to 2, as both `d(x)` and `max y(x)` are in the range [0,1]. This score is then used as a threshold for applying specific losses during training and for deciding whether to classify a sample into a known class or an \"\"unknown\"\" class (`\\text{Ø}`) during deployment.": 738,
    "The paper addresses this by presenting *-CFQ (\"\"star-CFQ\"\"), a suite of large-scale datasets built upon the Compositional Freebase Questions (CFQ) semantic parsing benchmark. The design of *-CFQ incorporates two key differences from the original CFQ to facilitate scalability investigations: increased data size and an extended rule set.\nFirst, the datasets are generated following the algorithm described in Keysers et al. (2020), but with sampling run at a significantly larger scale to produce much larger datasets. For instance, the largest dataset in the suite is 41 times the size of CFQ. After initial sampling, sub-sampling and semantic and structural filtering are applied to enhance the diversity of rule combinations while maintaining complexity balance and reducing unnatural-sounding questions. A notable difference from Keysers et al. (2020) is the omission of the grounding step in Freebase, meaning generated questions contain only entity placeholders without guaranteeing a non-empty answer from Freebase. This choice, while potentially leading to semantically implausible questions, is motivated by observed performance bottlenecks and consistency of baseline ML system behavior between grounded and ungrounded datasets at larger scales. An example of such a dataset is U-CFQ (ungrounded CFQ).\nSecond, the rule set is extended to simulate coverage of a greater scope of natural language. This involves adding up to 92% more leaf rules (supporting additional Freebase types and properties or new surface forms) and up to 37% more non-leaf rules (supporting additional syntactic constructs). To explore the effect of both the number and type of new rules (leaf vs. non-leaf), a rule set lattice is prepared, with a separate large-scale dataset generated for each rule set in the lattice. The base rule set, B-CFQ, contains rules shared by all other rule sets and is designed to be close to U-CFQ. L-CFQ includes B-CFQ rules plus all additional leaf rules, while N-CFQ includes B-CFQ rules plus all additional non-leaf rules. X-CFQ contains the union of rules from L-CFQ and N-CFQ. Additionally, half-versions (e.g., half-L-CFQ) are provided with only half of the additional rules of the relevant types. All datasets in the suite include detailed instrumentation of the compositional structure of each example, similar to CFQ.": 739,
    "The paper proposes the Bidirectional Style-induced Domain Adaptation (BiSIDA) framework, which operates with two parallel branches: a supervised learning branch and an unsupervised learning branch, without requiring adversarial training. The core of this bidirectional alignment is a Continuous Style-induced Image Generator (CSIIG), which is a non-adversarial image generator built upon the Adaptive Instance Normalization (AdaIN) architecture. The CSIIG combines content and style feature maps using a continuous trade-off parameter (alpha) ranging from 0 to 1, allowing for a continuous blend of styles. In the supervised learning branch, source domain images are perturbed with brightness and contrast, then style-transferred into the style of target domain images using the CSIIG. This process, termed target-guided supervised learning, adapts the source data towards the target domain's appearance for training the student segmentation network. In the unsupervised learning branch, target domain images are perturbed and then style-transferred into the style of source domain images using the CSIIG. This source-guided style transfer generates high-dimensional perturbations on the unlabeled target images. By performing style transfer in both directions (source-to-target and target-to-source) and continuously parameterizing the style blend, BiSIDA achieves domain alignment bidirectionally, facilitating gradual adaptation and effective utilization of both labeled source and unlabeled target data without the complexities of adversarial training.": 740,
    "The DERWENT (DeEp Random Walk basEd distaNt Transfer) model addresses this by constructing a data graph G on all data points from the source, auxiliary, and target domains. Each data point corresponds to a node, and edge weights are defined using cosine similarity of their hidden feature representations, learned by a feature extraction network (denoted as φ(·)). Crucially, there are no direct edges between source and target data points due to their large discrepancy. The model then employs a deep random walk technique on this graph. The random walk can be initiated from either the source domain (Type 1) or the target domain (Type 2). For Type 1, it starts at a source node and continues until it reaches a target node or a maximum length (τ) is exceeded. For Type 2, it starts at a target node and continues until it reaches a source node or τ is exceeded. Hyperparameters η1 and η2 are used in edge weight calculations to increase the probability of finding source/target nodes depending on the random walk direction, effectively guiding the walk towards the desired destination. This process generates sequences (Si) of data points that represent explicit transfer paths, connecting distant domains through intermediate auxiliary data points.": 741,
    "The paper proposes to represent the probability distribution of a single domain (e.g., domain X) using a generative cooperative network. This network consists of two main components: an energy-based model (EBM), denoted as p(x;θX), and a latent variable model (LVM), denoted as GY!X(y;φX).\nThe EBM p(x;θX) is parameterized by a convolutional neural network (ConvNet) and defines the probability distribution of domain X. It is trained via maximum likelihood estimation (MLE) by minimizing the Kullback-Leibler (KL) divergence between the data distribution pdata(x) and the model distribution. The gradient of the MLE involves an expectation with respect to the model distribution, which is approximated by MCMC samples (e.g., using Langevin dynamics) from the current EBM.\nThe LVM GY!X(y;φX) acts as a translator, mapping examples from a source domain (e.g., domain Y) to the target domain (domain X). It is parameterized by an encoder-decoder structure. The LVM is trained via a process called MCMC teaching. In this process, the EBM p(x;θX) serves as a teacher. The LVM generates initial translated examples (e.g., ^xi = GY!X(yi;φX)). These initial examples are then refined by the EBM using Langevin revision (a finite number of MCMC steps) to obtain revised examples (e.g., ~xi). These revised examples are considered \"\"synthesized examples sampled from p(x;θX)\"\". The EBM then learns from these synthesized examples to update its parameters (θX). Crucially, the LVM (GY!X) learns to absorb the MCMC transitions from the initial examples (^xi) to the refined examples (~xi). This is achieved by treating the pairs (yi, ~xi) as training data for the LVM, and updating its parameters (φX) by minimizing a non-linear regression objective (Lteach) between GY!X(yi;φX) and ~xi. This iterative process allows the LVM to chase the EBM's distribution, effectively learning to translate examples from the source domain to match the statistical properties of the target domain as defined by the EBM. The LVM also serves as an initializer for the EBM's MCMC sampling, making the sampling more efficient.": 742,
    "The paper proposes the Latent Unified State Representation (LUSR), denoted as Sz. This representation is defined as the domain-general component of a broader latent state sz, which is itself composed of both domain-specific features (cSz) and domain-general features (Sz). The core design principle of LUSR is to include only the domain-general embedding, thereby enabling it to disregard domain-specific variations and generalize across different domains. To ensure the effectiveness of this design for reinforcement learning, the transition function (T) and reward function (R) of the Markov Decision Process (MDP) are formalized to depend exclusively on Sz, rather than on the full observation state or the domain-specific components. This dependency ensures that the underlying dynamics and reward structure of the environment remain consistent when viewed through the lens of LUSR, regardless of visual domain changes, thus facilitating successful policy training and zero-shot transfer.": 743,
    "The paper addresses the disentanglement of domain-agnostic and domain-specific features by constructing two separate probabilistic encoders within a variational autoencoder (VAE) framework. The input activity data, denoted as `xd`, is fed into two distinct probabilistic encoders: `p(z|xd)` parameterized by `θ` and `p(zd|xd)` parameterized by `φ`. These encoders learn to output the mean (`μ`) and variance (`σ^2`) for two Gaussian distributions, `q(z|xd; θ) = N(z|μ(xd), σ^2(xd))` for domain-agnostic features `z`, and `qd(zd|xd; φ) = N(zd|μd(xd), σd^2(xd))` for domain-specific features `zd`. Latent vectors `z` and `zd` are then sampled from these distributions using the reparametrization trick. To guide the disentanglement, two disentangling classifiers, `DCd` (with parameters `wd`) and `DCy` (with parameters `wy`), are incorporated. `DCd` is trained to predict the domain label `d` from the domain-specific features `zd`, while `DCy` is trained to predict the activity class label `yd` from the domain-agnostic features `z`. The objective function `LDC` is defined as the sum of cross-entropy losses for these two classifiers, encouraging `z` to contain domain-agnostic factors relevant for activity classification and `zd` to capture domain-specific factors. The overall VAE objective `Lelbo` ensures data reconstruction and regularizes the latent spaces with priors.": 744,
    "Humor Centric Features (HCF) are derived at the word level based on the theories of ambiguity and superiority. For ambiguity, ConceptNet, a large-scale semantic structure, is used to extract neighboring concepts (senses) for each word, filtering out edges with confidence scores less than 1. The GloVe embeddings of these senses are obtained, and the ambiguity score is calculated as the summation of cosine distances between all pairs of senses for a given word. For sentiment, the NRC VAD dictionary is utilized to extract valence (negative-positive), arousal (calm-excited), and dominance (submissive-dominant) scores, ranging from 0 to 1, for 20,000 English words. These extracted word-level ambiguity and sentiment scores constitute the HCF. The HCF are then processed by a dedicated Transformer encoder, referred to as the HCFEncoder, to generate a unimodal representation (Uh). This Uh is subsequently concatenated with the language representation (Ul) obtained from the ALBERT encoder to create an HCF-enriched language representation (Ul;h), thereby integrating the humor-centric knowledge into the language modality.": 745,
    "The paper addresses this by first training a base Neural Machine Translation (NMT) model on large-scale general domain data. After this initial training, a technique called Gradual Pruning (Zhu and Gupta 2018) is applied to this well-trained general model. Gradual Pruning iteratively removes low-magnitude parameters (those with small absolute values) over a series of training steps, gradually reducing the model's parameter count to a target sparsity level (e.g., 50%). Between pruning steps, the model is briefly trained on the general dataset to recover any performance loss in the resulting sub-network. This process identifies a sparse sub-network, referred to as the \"\"informative sub-network,\"\" which is capable of maintaining the general domain's translation performance with significantly fewer parameters. Once identified, the parameters of this informative sub-network are frozen, meaning they are kept fixed and are not updated during subsequent domain adaptation processes. This freezing mechanism ensures the consistent retention of general domain information, thereby mitigating catastrophic forgetting when adapting to new domains.": 746,
    "The paper proposes the Gated Bridging Mechanism (GBM) to selectively filter and fuse information from auxiliary task towers into a main task tower. For each block `i` in a task `τj` tower, GBM takes Transformer hidden states `H` from the previous block `i-1` of all subtasks as input.\nFirst, for each neighbor tower `τm` (where `m ≠ j`), a reset gate `Rm` is employed. This reset gate `Rm` is computed as `sigmoid(W_R * H_m(i-1) + b_R)`, where `W_R` and `b_R` are learned parameters. This gate controls which parts of the neighbor tower's hidden states `H_m(i-1)` are considered for the next fusion step.\nNext, the filtered hidden states `(Rm * H_m(i-1))` are passed through a non-linear projection function (tanh) to generate new current states `Cm`. This projection is `Cm = tanh(W_C * (Rm * H_m(i-1)) + b_C)`, where `W_C` and `b_C` are learned parameters. This step addresses the hypothesis that hidden states from different towers might reside in different vector spaces, projecting them into a common space for effective fusion.\nThen, an update gate `Zm` is used to control how much of the projected neighbor information `Cm` should fuse with the focused task's hidden states `H_j(i-1)`. `Zm` is computed as `sigmoid(W_Z * H_j(i-1) + V_Z * Cm + d_Z)`, where `W_Z`, `V_Z`, and `d_Z` are learned parameters.\nThe fused features `Fm` for each neighbor `m` are then calculated as a trade-off between the focused task's hidden states `H_j(i-1)` and the projected neighbor states `Cm`, controlled by `Zm`: `Fm = Zm * Cm + (1 - Zm) * H_j(i-1)`. This allows the main task to decide how much information from a neighbor tower is utilized.\nFinally, the output `G_j(i)` of GBM for the focused task `τj` in block `i` is obtained by summing all `Fm` from neighbor towers and applying a sigmoid activation function: `G_j(i) = sigmoid(W_G * (sum(Fm for m ≠ j)) + b_G)`. This output `G_j(i)` then serves as the input to the Transformer layer in the current block `i` of the focused task's tower. The design explicitly incorporates gates to filter out useless information and non-linear projections to align vector spaces before fusion.": 747,
    "The paper addresses this by introducing a Domain-Aware Adversarial Loss (Ladv) within a generative adversarial network (GAN) framework. An additional classifier (C), a simple multi-layer perceptron (MLP) network, is trained to predict whether a given domain tag (ts for source, tt for target) matches the text (x) based on the mean value of the encoded representation of each word in the sentence. The classifier's objective is to minimize Ldis, which encourages it to correctly identify tag-text matches and mismatches. Concurrently, the NMT model's encoder (E) is trained with an adversarial loss, Ladv, which aims to fool the classifier. Specifically, Ladv encourages the encoder E to generate representations for source-domain texts (xs) when paired with a target-domain tag (tt) that the classifier C incorrectly identifies as a match, effectively making E more sensitive to the target-domain tag and guiding it to produce target-domain-like representations. This process implicitly explores the latent space of the target-domain distribution, compensating for the limited coverage of diverse linguistic phenomena in small-scale target-domain corpora.": 748,
    "The Locate&Gen model utilizes a transformer encoder-decoder framework. First, the encoder processes the input text using BERT, transforming it into hidden vectors. The model then applies a softmax function over the hidden vectors to predict the insertion position. The insertion position is determined by the highest probability score. This process ensures that the chosen location maximizes contextual coherence and enhances the expressiveness of the text. The insertion position is calculated as follows: \\( i_{Ins} = \\arg\\max_i P_{Ins}(i) \\).": 749,
    "The Adaptive Hybrid Framework (AHF) integrates pseudo-label based semi-supervised learning and adversarial training. The framework comprises a domain discriminator, a student network, and a teacher network. The domain discriminator, equipped with a gradient reversal layer, learns domain-invariant features by attempting to deceive a domain classifier. The student network is trained on labeled source data and unlabeled target data with pseudo labels generated by the teacher network. The teacher network, which tracks an exponential moving average of the student network weights, generates better pseudo labels. This setup ensures that the target data can refine the task classifier while aligning feature distributions.": 750,
    "The paper addresses the challenge of generating entity-aware features by proposing an Input Layer and an Entity-Aware Attention Layer. First, for any given sentence, the Input Layer maps each word to a word-level embedding using pretrained Glove. Then, two distinct Bi-directional Long Short-Term Memory (BiLSTM) networks are employed as feature extractors: a NER-specific LSTM (hN) and an Adversarial-specific (Adv-specific) LSTM (hA). The NER-specific LSTM is designed to capture features relevant for Named Entity Recognition, implicitly encoding entity information in its hidden states. The Adv-specific LSTM extracts features intended for the adversarial training process. To make the adversarial features entity-aware, the Entity-Aware Attention Layer utilizes a Multi-Head Attention (MHA) mechanism. In this mechanism, the hidden states from the NER-specific LSTM (hN) serve as the query sequence, while the hidden states from the Adv-specific LSTM (hA) are used as the key and value. By using hN as the query, the attention mechanism is guided by the implicit entity information present in the NER-specific features. This guidance forces the Adv-specific features (hA) to focus more on entity-related aspects, thereby generating an entity-aware adversarial feature (hEA). This hEA is designed to be domain-invariant and specifically highlights entity information, making it suitable for subsequent entity-level alignment.": 751,
    "The style-guided discriminator is designed to guide the input image towards the target image style by utilizing a flexible decision boundary. This discriminator assesses whether the translated image aligns with the style characteristics of the target domain, thereby ensuring that the style attributes are appropriately reflected in the output image. This approach helps in achieving a more accurate and style-consistent image translation.": 752,
    "To establish a robust initial target model for Source-Free Unsupervised Domain Adaptation (SFUDA) without access to labeled source data, the paper remolds the Bi-Classifier Determinacy Maximization (BCDM) method. Since the original BCDM relies on labeled source samples for its cross-entropy losses (ℓ1 and ℓ2 in Steps A and B), these cannot be directly applied in SFUDA. The proposed solution replaces these losses with a Reverse Cross-Entropy (RCE) loss function. The RCE loss, defined as ℓrce = - Σ p(k|x)logq(k|x), where p is the probability distribution from the pre-trained source model (acting as soft labels) and q is the output of the current target model, allows the model to learn based on the confidence of these soft labels. This approach ensures that the model not only learns from the target domain but also maintains its performance on the source domain, as the RCE loss has a large sample gradient for high-confidence labels and a small sample gradient for lower-confidence labels, preventing model collapse. Specifically, for a target domain sample xt, the RCE losses ℓrce1 and ℓrce2 are calculated using the outputs of the source model's two branch classifiers (Fs1, Fs2) as p1(k|xt) and p2(k|xt), and the current model's branch classifiers (Fsr1, Fsr2) as q1(k|xt) and q2(k|xt). This allows the model to be optimized using adversarial training (Steps B and C of BCDM) while implicitly preserving source domain knowledge through the RCE loss, even without explicit source data.": 753,
    "The paper proposes to measure the probabilistic uncertainty of pseudo labels by calculating the Kullback–Leibler (KL) divergence between the predicted probability distribution and an ideal single-peak distribution. For a target domain image `xi` with a pseudo label `˜yi` (obtained via clustering), its feature `fi` is extracted by a mean teacher model. An external classifier, `ϕt(·|wcls t)`, is constructed where `wcls t` are the cluster centroids of `d` dimensions, serving as classifier weights. This classifier dynamically generates the predicted probability distribution `P(xi, ˜yi)` among identities using a Softmax function with a temperature parameter `α` (Eq. 2). The ideal distribution, `Q(xi, ˜yi)`, is defined as a smoothed δ distribution (Eq. 3), where the probability is `ϵ` for the assigned pseudo identity `˜yi` and `(1-ϵ)/(c-1)` for other `c-1` identities, with `ϵ` typically set to 0.99. The probabilistic uncertainty `U(xi, ˜yi)` is then formally defined as the KL divergence `DKL(Q(xi, ˜yi)||P(xi, ˜yi))` (Eq. 4). A larger uncertainty value indicates a higher likelihood that the pseudo label is incorrect, as it suggests the predicted distribution is ambiguous with multiple peaks, unlike the sharp peak of an ideal distribution. This quantitative criterion serves as a standard for identifying and potentially eliminating wrongly-labeled samples.": 754,
    "The proposed method, Divide-and-Regroup Clustering (DARC), first divides the unlabeled data into multiple camera-specific groups. Within each group, local clustering is conducted based on feature distances and temporal distances. This local clustering reduces ID switch errors by leveraging temporal continuity and ID split errors by focusing on local feature distribution. After local clustering, a voting and regrouping strategy is applied to associate local clusters across multiple cameras, ensuring that clusters belonging to the same identity are grouped together. This two-stage process enhances the accuracy of pseudo label generation.": 755,
    "The Amplitude Spectrum Transformation (AST) is implemented as an auto-encoding setup. For an input feature map `h` from a convolutional layer, AST first applies a Fourier transform (F) to decompose it into its amplitude spectrum (FA(h)) and phase spectrum (FP(h)). The amplitude spectrum is then processed through a vectorization transformation (T) and a fully-connected encoder network (Qe) to yield the AST-latent `z = Qe ◦ T ◦ FA(h)`. For decoding, the inverse transformations are applied: `ˆh = F−1(T−1 ◦ Qd(z), FP(h))`, where `Qd` is a fully-connected decoder network. A critical design choice is that the phase spectrum (FP(h)) is directly forwarded from the encoder to the decoder, ensuring that task-related content is preserved during the transformation. The vectorization (T) and inverse-vectorization (T−1) specifically handle the amplitude spectrum's symmetry by processing only the first and fourth quadrants and reconstructing the full spectrum by mirroring. AST operates independently on each channel of `h` using shared parameters, and the final `z` is a concatenation of the channel-wise latents. The AST auto-encoder is trained by minimizing a reconstruction objective: `min θQ LAST(hk,ˆhk) where LAST(hk,ˆhk) = ∥hk − ˆhk∥2`, with `θQ` representing the trainable parameters of `Qe` and `Qd`.": 756,
    "The paper addresses this by proposing the Unbiased Semantics (US) module, which centers around a Time-Category-Distribution three-dimensional paradigm (P). Given a batch of labeled source images, the framework first extracts features using a domain-shared backbone. Fine-grained sampling is performed to collect semantic patterns, specifically pixels inside object instances as category-specific foreground samples and an equal number of spatially-uniformed background samples. To aggregate these fine-grained semantic patterns across images within a batch and capture long-distance semantic dependencies, a cross-image graph is established. A nonlinear projection is applied to sampled semantic patterns to obtain node embeddings (X), which then undergo graph reasoning (similar to Eq. 1) to yield enhanced node representations (˜X). An auxiliary node classification task with Cross Entropy loss (Eq. 2) is used to train the graph parameters and enhance node semantics. To ensure unbiased estimation beyond inner-batch limitations, the paradigm P (P ∈ RT×C×D) is introduced, where T represents time (historical iterations), C represents categories, and D represents feature dimension. P not only captures current iteration category knowledge but also saves T-1 historical representations, providing robustness during optimization. An incremental update strategy (Eq. 3) is used to update P: semantics within the current batch for category 'c' (p) are computed from the aggregated graph nodes (˜X), and then a time-axis translation preserves historical semantics, updating the current state with 'p' using a cosine-based update rule. This process allows P to model semantic evolution across iterations, providing a robust, unbiased representation.": 757,
    "The paper addresses this by proposing a multi-branch architecture with self-ensemble learning. After a stem CNN, the model branches into K expert branches. To ensure diversity, these branches are equipped with different drop blocks, ResBlocks, and Generalize Mean Pooling (GeM). Data augmentation, including traditional methods like random erasing, cropping, and flipping, is further enhanced by using CycleGAN to generate images with different camera styles. This ensures each branch receives diverse inputs and learns distinct knowledge. An instance-aware router adaptively integrates the predictions of each expert branch. This router consists of a global average pooling layer and a fully connected layer, which produce an ensemble weight (w) for each branch. These weights are then used to aggregate the predictions of different branches to obtain an ensemble prediction. The temporally averaged model, whose parameters are updated using a moving momentum (α) from the current model's parameters, serves as a mean teacher model. Its integrated ensemble prediction is used as soft labels to supervise the current running model through a soft cross-entropy loss. This online label refining process allows each branch in the running model to be corrected by the more stable and high-quality ensemble predictions from the mean teacher, effectively increasing the quality of mean teacher-based soft labels and promoting diverse knowledge acquisition across branches.": 758,
    "The paper addresses this by first constructing a shape dictionary using an online dictionary learning algorithm. This process involves finding a set of K explicit shape templates, denoted as D = {d1, d2, ..., dK}, that can represent the diverse segmentation masks {yi} from the single source domain data S = {(xi, yi)}. The dictionary learning is formulated as an optimization problem: `argmin D,α (sum_i ||yi - sum_j dj αij||^2 + λ||αi||1)`, where `α` represents the coefficients for each mask `yi`, `dj` are the dictionary elements, and `λ` is a balancing parameter for regularization. Two principles guide its construction: the number of shape templates K is much smaller than the number of masks N (K << N) to prevent overfitting, and masks are represented sparsely to maximize the representation power of each template. Once the shape dictionary is learned, it is integrated into the deep segmentation network. This is achieved by embedding a regression branch onto the network. This regression branch takes the encoded features of a given sample as input and outputs shape coefficients, denoted as `α_hat`, which are then used to linearly combine the pre-extracted dictionary items `dj` to generate a reference mask `M = sum_j dj α_hat_ij`. This reference mask is subsequently concatenated with the network's features before the final convolutional block of the decoder, allowing the model to refine its segmentation predictions by jointly exploring sample features and the extracted shape priors. Direct supervision for the regression branch is provided by minimizing the cosine similarity between the predicted `α_hat` and the ground truth coefficients `α` obtained during dictionary learning, using the loss function `Lregress(xi) = 1 - (α_hat_i · α_i) / max(||α_hat_i||_2 · ||α_i||_2,ϵ)`. The overall training objective for the network in the single source domain is `L(xi) = Lseg(xi) + βLregress(xi)`, where `Lseg` is the segmentation loss and `β` is a weighting parameter.": 759,
    "The paper addresses this by proposing a Style-mixing Segmentor which integrates style-mixing layers directly into the segmentation network. This segmentor operates in two modes: evaluation and training. First, the sole target sample (xT) is fed into the segmentor in evaluation mode to extract its feature statistics (mean and standard deviation) from a specific layer (layer3, denoted as fT3). These target feature statistics are then augmented by adding a perturbation sampled from a normal distribution, creating a range of potential target domain variations. Next, a source sample (xS) is randomly chosen and fed into the segmentor in training mode. The style-mixing operation occurs by combining the source feature statistics (mean and standard deviation from layer3, fS3) with the augmented target feature statistics. Specifically, the intermediate channel-wise mean (µ) and standard deviation (σ) are calculated as a weighted sum: µ = α * µ(fS) + (1 - α) * (µ(fT) + rµ) and σ = α * σ(fS) + (1 - α) * (σ(fT) + rσ), where α is a randomly sampled weight from a uniform distribution for each image/feature pair, and rµ and rσ are perturbations sampled from normal distributions based on the absolute difference between source and target statistics. The stylized source feature (cfS) is then produced by applying these mixed statistics to the normalized source feature: cfS = ((fS - µ(fS)) / σ(fS)) * σ + µ. This process effectively stylizes the source images by transferring the feature-level statistics of the target image, allowing the segmentor to explore multiple domains around the target sample without requiring any additional learned parameters or a separate, pre-trained style-transfer module. The style-mixing is applied at both the image level (xS) and feature level (fS3).": 760,
    "Discriminative feature representations for target domain samples are learned using Prototypical Contrastive Learning (PCL). This process involves maintaining a sample memory, denoted as Vt, which stores feature vectors vi_t for each target sample xi_t. This memory is initialized using the feature extractor F(xi_t) and updated with a momentum m after each batch. K-means clustering is then applied to Vt to obtain target clusters Ct and normalized target prototypes uj_t. During training, for each target sample xi_t, its feature fi_t = F(xi_t) is computed, and a similarity distribution vector Pi_j_t is generated by comparing fi_t with the prototypes uj_t using a temperature parameter tau. The prototypical contrastive loss, LPCt(Dt), is calculated as the negative sum of log probabilities of the correct cluster assignments. The same PCL operations are performed on source samples, Ds, but using their ground-truth labels instead of clustering indices to obtain source prototypes uj_s. The overall self-supervised learning (SSL) loss, LCLU(Ds;Dt), is the sum of LPCs(Ds) and LPCt(Dt). This iterative process of clustering and representation learning ensures that features within the same cluster are brought into proximity, while features from different clusters are separated, thereby learning discriminative features useful for identifying novel categories.": 761,
    "The paper formulates backdoor search as a single-player, deterministic constraint satisfaction game. A \"\"state\"\" `S` in the MCTS tree is defined as a permutation of integer variables from the set `I`, such that its size `|S|` is at most `K` (the user-defined bound on backdoor size). The root of the MCTS tree represents an empty state `()`. An \"\"action\"\" from a non-terminal state `S` involves appending the index `i` of an integer variable (where `i` is not already in `S`) to the end of `S`, creating a new state `S'` with `|S'| = |S| + 1`. A state is considered \"\"terminal\"\" when its size reaches `K`. The MCTS framework then proceeds through its four standard steps: Selection, Expansion, Simulation, and Backpropagation. In the Selection step, nodes are chosen in a depth-first dive from the root, guided by a scoring rule. In Expansion, new child nodes are created for selected nodes that have only a subset of their potential children in the tree. Simulation involves traversing the state space from a non-terminal node to a terminal state, typically by picking actions randomly, to collect a reward. Finally, Backpropagation assigns the collected reward back to the nodes along the depth-first path that led to the observed reward. This iterative process builds a partial search tree to identify promising actions (variables) for constructing backdoors.": 762,
    "The paper addresses this by first leveraging the known source contamination factor, denoted as $\\gamma_S$, to set a threshold $\\delta_S$ on the source anomaly scores. This threshold is chosen such that the proportion of examples with an anomaly score greater than $\\delta_S$ is precisely equal to $\\gamma_S$. Once $\\delta_S$ is established, the distribution of normal scores is modeled as the distribution of scores less than or equal to $\\delta_S$. This derived distribution is then normalized so that its support is within the range [0,1] and its area integrates to 1. This normalized distribution is termed the $\\delta_{cut}$ distribution. Formally, for a random variable X with distribution p(x) and support in [0,1], the $\\delta_{cut}$ distribution, p$\\delta$(x), is defined as p($\\delta$x) multiplied by a normalization factor of 1 divided by the integral of p(y) from 0 to $\\delta$. This process effectively isolates and characterizes the distribution of scores corresponding to normal examples in the source domain, even without explicit labels.": 763,
    "The paper addresses this by introducing a novel post-training procedure for BERT, which includes two main self-supervised tasks: the Domain-distinguish Task (DDT) and the Target Domain Masked Language Model (MLM).\n*   Domain-distinguish Task (DDT): This task replaces BERT's original Next Sentence Prediction (NSP) task. It is designed to encourage BERT to model the relationship between sentences in terms of their domain origin. For this, sentence-pair inputs are constructed in the format: `[CLS] sentence A [SEP] sentence B [SEP]`.\n    *   Input Construction: 50% of the time, both sentence A and sentence B are randomly sampled from the *target domain reviews*, and the pair is labeled `TargetDomain`. The other 50% of the time, sentence A and sentence B come from the *target domain and another domain* (e.g., source domain), and this pair is labeled `MixDomain`. The order of sentences from different domains is randomized.\n    *   Objective: DDT is framed as a classification task. An output layer is added on the pooled representation (from the `[CLS]` token) to maximize the likelihood of predicting the correct domain label (`TargetDomain` or `MixDomain`). This process enables BERT to distill specific features for different domains, thereby enhancing its domain-awareness.\n*   Target Domain Masked Language Model (MLM): This task is used to inject specific knowledge from the target domain into BERT.\n    *   Procedure: 15% of tokens in sentences are randomly replaced by a `[MASK]` token. BERT is then trained to predict the original masked words based on their context. The final hidden vectors corresponding to the masked tokens are fed into a softmax layer over the vocabulary, and the likelihood of the masked token ID is maximized.\n    *   Specific Condition: To prevent noise from other domains, tokens are *only* masked in target domain sentences if the corresponding DDT label for the sentence pair was `MixDomain`. This ensures that the MLM task primarily focuses on target domain vocabulary and context.\n*   Total Loss: The overall loss for the post-training procedure is the sum of the losses from the DDT and the Target Domain MLM tasks. This combined approach adapts BERT's pre-trained weights, making it domain-aware and capable of understanding opinion text characteristics relevant to the target domain, even with limited labeled data.": 764,
    "The shared-private encoder-decoder framework includes a shared encoder-decoder for capturing domain-shared features and private encoders-decoders for each domain to capture domain-specific features. Each instance is processed through both shared and private modules. The shared module extracts common features, while the private modules focus on domain-specific characteristics. This dual-processing approach ensures that both shared and specific knowledge are utilized effectively.": 765,
    "The paper addresses this by proposing the Distant Annotation (DA) module, which comprises a Base Segmenter and a Domain-speciﬁc Words Miner. The Base Segmenter is a GCNN-CRF (Gated Convolutional Neural Network - Conditional Random Field) model trained solely on labeled source domain data, designed to recognize words common to both source and target domains. The Domain-speciﬁc Words Miner is a novel component that explores target domain-specific words from large raw text without any predefined dictionaries. It evaluates n-gram sequences extracted from segmented texts (initially segmented by the Base Segmenter) based on four factors:\n1.  Mutual Information (MI): Measures the internal tightness of a text segment by calculating the minimum mutual information between all possible sub-strings within it.\n2.  Entropy Score (ES): Measures the external flexibility of a text segment by calculating the minimum of its left and right entropy scores, based on the distribution of adjacent characters. A higher entropy indicates a richer neighboring context, suggesting an independent word.\n3.  tf-idf: A standard numerical statistic reflecting the importance of a word in a corpus, used to identify domain-specific noun entities which typically have high tf-idf scores.\n4.  Word frequency: Ensures that a valid word appears repeatedly in the corpus.\nThese factors are combined into a word probability score, `pval(ti)`, using a sigmoid function applied to the normalized sum of MI, ES, and tf-idf scores. By setting a threshold for `pval(ti)` (e.g., 0.95) and word frequency (e.g., >10), the miner constructs a domain-specific word collection (C). Finally, this collection C is used with a forward maximizing match algorithm to annotate sentences in the target domain, prioritizing words from C, and then using the Base Segmenter for the remaining parts of the sentence, thereby automatically building an annotated dataset for the target domain.": 766,
    "To effectively represent phonological characteristics at a phoneme level, the proposed method first breaks each word `wi` into a sequence of `Mi` phonemes, denoted as `R(wi) = {ri,1, ri,2, ..., ri,Mi}`. These phonemes are obtained from a dictionary, such as the CMU Pronouncing Dictionary. For each phoneme `ri,j` within a word `wi`, it is projected into a `dP`-dimensional embedding space, resulting in a trainable vector `ui,j` that represents its phonological properties. To derive a single pronunciation embedding `TPi` for the entire word `wi`, an attention mechanism, referred to as phonological attention, is applied to these phoneme embeddings. This mechanism first transforms each `ui,j` using a fully-connected hidden layer `P(·)` followed by a `tanh` activation to produce `vi,j`. Then, importance scores `αP i,j` are calculated for each `vi,j` by taking its dot product with a `dA`-dimensional context vector `vs` and normalizing with a softmax function over all phonemes in the word. Finally, the pronunciation embedding `TPi` is computed as a weighted sum of the phoneme embeddings `ui,j`, where the weights are the calculated `αP i,j` scores. This process allows the model to identify and emphasize important phonemes within a word's pronunciation.": 767,
    "The paper addresses this by proposing a Constraint Selection Algorithm (Algorithm 1). This algorithm operates on a set of candidate sentences (C) containing the alternative word. For each candidate sentence, it first identifies \"\"weak words\"\" (Wi), which are words weakly related to the alternative word. Then, for each weak word, it identifies \"\"support words\"\" (Si,w), which are words strongly related to the pun word and share the same part-of-speech tag as the weak word. The relatedness between words is evaluated using Point-wise Mutual Information (PMI).\nTo select the most reasonable pair of weak and support words for rewriting, the algorithm trains a Continuous Bag of Words (CBOW) model to obtain word embeddings. It then calculates a score (Scorei,w,s) for each combination of candidate sentence (Ci), weak word (w), and support word (s). This score is derived from cosine similarities between word vectors: `Sim(vo_s, vi_p, vi_q) - Sim(vi_w, vi_s) + Sim(vi_s, vi_s)`. Here, `vo_s` is the output vector of the support word, `vi_p` is the input vector of the pun word, `vi_q` is the average input vector of the potential context (words in the candidate sentence excluding the weak word and including the pun word), `vi_w` is the input vector of the weak word, and `vi_s` is the input vector of the support word. The first term `Sim(vo_s, vi_p, vi_q)` measures how well the support word fits the context when the pun word is introduced. The second term `Sim(vi_w, vi_s)` negatively correlates the similarity between the weak word and support word, implying that replacing a word with a very similar one might not create enough incongruity. The algorithm aims to maximize this score to find the optimal (Ci, w, s) triplet. It also explicitly states that the similarity between the pun word and its support word is negatively correlated to the final score, and verbs are not considered as weak words to avoid extensive sentence transformations.": 768,
    "The paper addresses this by proposing a dynamic data selection strategy based on a curriculum learning approach. For each sentence `s` in the monolingual corpus, a combined score `score(s)` is computed using the formula: `score(s) = λ(t)repr(s) + (1-λ(t))simp(s)`. Here, `repr(s)` measures the representativeness of the sentence to the target distribution, and `simp(s)` measures its simplicity, indicating how well the current Machine Translation (MT) models can translate it. The term `λ(t)` is a balancing function that changes with the current training epoch `t`, defined as `λ(t) = min(1, sqrt(t / T) * (1 - c0) + c0)`. This function ensures a gradual transition: at early epochs (small `t`), `λ(t)` is low, prioritizing simplicity; as training progresses, `λ(t)` increases, shifting the focus towards representativeness.\nThree metrics are proposed for `repr(s)` (Representativeness Metrics):\n1.  In-Domain Language Model Cross-Entropy (LM-in): Measures cross-entropy using an in-domain Language Model (LM) trained on target-domain monolingual data. Lower entropy indicates higher representativeness.\n2.  TF-IDF Scores (TF-IDF): Calculates the maximum cosine similarity between the TF-IDF vector of sentence `s` and TF-IDF vectors of sentences in a small in-domain dataset. Higher similarity indicates higher representativeness.\n3.  BERT Representation Similarities (BERT): Computes the maximum cosine similarity between the BERT embedding of sentence `s` (averaged hidden states from the eighth layer of multilingual BERT) and BERT embeddings of sentences in a small in-domain set. Higher similarity indicates higher representativeness.\nTwo metrics are proposed for `simp(s)` (Simplicity Metrics):\n1.  General-Domain Language Model Cross-Entropy (LM-gen): Measures cross-entropy using a general-domain LM trained on parallel training data. Higher entropy (indicating less common n-grams) suggests more difficulty, thus lower simplicity.\n2.  Round-Trip BLEU (R-BLEU): Translates sentence `s` to the other language using one MT model, then back-translates the result to reconstruct `s`. The BLEU score between the original `s` and the reconstructed `s'` is used as the simplicity metric. Higher BLEU indicates higher simplicity.\nAll representativeness and simplicity scores are normalized to a [0, 1] range before being combined. At each epoch, the top `p%` of sentences based on this combined score are selected for back-translation.": 769,
    "The paper addresses the challenge of creating a large-scale parallel corpus by employing a two-step automatic process. First, it uses distant supervision to collect self-labeled similes from social media platforms, specifically Reddit's WRITINGPROMPTS and FUNNY subreddits, by searching for comments containing the phrase \"\"like a\"\". This yields a large dataset of human-written similes. Second, these collected similes are transformed into their literal versions to form the \"\"source\"\" side of the parallel data. This transformation involves extracting the VEHICLE (the phrase after \"\"like a\"\") from the simile and feeding it as input to COMET (Commonsense Transformers for Automatic Knowledge Graph Construction), a pre-trained language model fine-tuned on ConceptNet. COMET is used to derive common sense properties associated with the VEHICLE, specifically leveraging its \"\"HasProperty\"\" relation. For a given simile, COMET provides a list of top properties sorted by probability. To select the most appropriate property for the literal version, the top five properties are used to form five possible literal sentences by appending them to the portion of the simile before \"\"like a\"\". These candidate literal sentences are then ranked using perplexity scores obtained from a pre-trained language model, GPT (Generative Pre-trained Transformer), with the lowest perplexity indicating the best literal version. Finally, a grammatical error correction model is applied to correct any errors introduced during this manipulation, resulting in a clean literal sentence paired with its original simile.": 770,
    "The dataset creation process involves several steps. First, hyperbolic sentences are collected from professional educational websites and linguistics research papers. Next, three native Chinese speakers manually produce non-hyperbolic versions of these sentences, ensuring they maintain the original meaning. Quality assessment is performed by additional annotators to verify the non-hyperbolic nature of the generated sentences. Then, more hyperbolic sentences are generated from the non-hyperbolic versions by different annotators to avoid bias. Finally, reliability checks are conducted to ensure the validity of the hyperbolic sentences. This results in a dataset named HYPO-cn, containing 4762 annotated sentences, of which 2680 are hyperbolic and 2082 are non-hyperbolic.": 771,
    "To transfer discriminative features from the pre-trained language model (PrLM) to the Feature Adaptation Module (FAM), the paper proposes Feature Self-distillation (Fd). This process treats the PrLM as a \"\"teacher\"\" and the FAM as a \"\"student,\"\" aiming to make the FAM capable of generating discriminative features similar to the PrLM. Fd is applied specifically to the target domain, where labeled data is scarce. The core mechanism for Fd is Mutual Information (MI) maximization between the features from the PrLM and the FAM. Since direct estimation of MI is difficult, its lower bound is maximized using Noise Contrastive Estimation (NCE). For distillation, the sum of the last N-layer features from the PrLM (denoted as `x_bar`) is used. The NCE loss function, `J_feat_NCE`, is calculated as `f(z, x_bar) - (1/|X_neg|) * sum(f(z, x_neg_i))` where `z` is the multi-layer representation from FAM, `x_bar` is the PrLM feature, `X_neg` is a set of negative samples, and `f(z, x*)` is a similarity function (cosine similarity) between `inf(z)` and `x*`. `inf(.)` is a trainable feed-forward neural network with tanh activation that resizes `z` to match the dimension of `x_bar`. Negative samples are obtained by randomly shuffling the batch of features. This maximization of MI encourages the FAM's output features (`z`) to be highly dependent on and similar to the PrLM's features (`x_bar`), thereby distilling the discriminative power.": 772,
    "The paper addresses the construction of a new grammatical error correction (GEC) benchmark dataset, named CWEB (Corrected Websites), by systematically collecting and annotating English texts from randomly sampled websites. The process begins with Text Extraction, where source texts are selected from CommonCrawl dumps, ensuring a wide range of online data such as blogs, magazines, and corporate/educational websites. To ensure English content, websites with country-code top-level domains (e.g., .fr, .de) are excluded. The jusText tool is used to retrieve content from HTML pages, removing boilerplate elements and splitting content into paragraphs. The data undergoes heavy filtering to remove non-English and incomplete sentences, and duplicate sentences are removed to ensure diversity. The collected sentences are then tokenized using SpaCy.\nFor Annotation, the data is corrected for grammatical errors by two expert annotators. The annotators are instructed to make minimal edits, focusing solely on grammatical correctness without rewriting for fluency, and have access to the entire paragraph context. The annotated data is automatically labeled for error types and converted into the M2 format using the ERRor ANnotation Toolkit (ERRANT). Annotator agreement is calculated at the sentence level using Cohen's Kappa to assess agreement on which sentences are erroneous.\nThe CWEB dataset is further categorized into two sub-datasets based on their source: CWEB-S (sponsored websites, e.g., .gov, .edu) representing professional writing, and CWEB-G (generic websites, e.g., .com, .info) including writing from various proficiency levels. For release, the collected paragraphs are split into sentences and shuffled to break original coherent structures and avoid copyright restrictions, a method successfully used in previous web-based corpora. The dataset is split into development and test sets, providing a fair amount of errors for evaluation.": 773,
    "The paper addresses the optimization of classification margin loss on the source domain by augmenting the standard cross-entropy loss with a Virtual Adversarial Training (VAT) term. This VAT term, denoted as `Re S`, is formulated as `max δ;||δ||≤ε KL[σ(f(ψ(xs)))||σ(f(ψ(xs + δ)))]`. This expression represents a local consistency regularization that aims to make the model's predictions robust to small perturbations. Specifically, it maximizes the Kullback-Leibler (KL) divergence between the softmax output distribution of the original input `xs` and that of a perturbed input `xs + δ`. The perturbation `δ` is an adversarial perturbation found within an epsilon-norm ball (`||δ||≤ε`) around the input, designed to maximize this divergence. For discrete language data, VAT is applied to the embedding space, meaning `e[xs]` (the embedding of the discrete input `xs`) is perturbed, rather than the raw discrete input itself. This process encourages the model to learn decision boundaries that are further away from the training data, thereby promoting larger classification margins and improving the generalization performance of the source domain. The maximization problem within the VAT term is efficiently approximated using a pair of forward- and backward-propagations. The overall objective function for the model incorporates this `Re S` term alongside the source classification loss (`LS`) and the domain discrepancy term.": 774,
    "The paper systematically generates more effective and diverse adversarial examples by introducing several novel methods that go beyond simple distractor insertion. These methods include:\n1.  AddKSentDiverse: This method extends the classic AddSentDiverse (Jia and Liang, 2017; Wang and Bansal, 2018) by generating *multiple* distractor sentences using AddSentDiverse and inserting them at independently sampled random positions within the context. This introduces multiple points of confusion, making it harder for models to rely on learnable biases.\n2.  AddAnswerPosition: This adversary retains the original answer span within a newly generated distractor sentence. The distractor is designed to maximize semantic mismatch, and the model is penalized for incorrect answer span location, especially when the distractor is inserted before the original answer.\n3.  InvalidateAnswer: This method creates unanswerable adversarial samples by removing the sentence containing the original answer span from the context and replacing it with a distractor sentence generated using AddSentDiverse. This is particularly used for SQuAD v2.0's NoAnswer-style samples.\n4.  PerturbAnswer (Semantic Paraphrasing): This involves algorithmically replacing content words (excluding named entities) within the sentence containing the answer span with synonyms. The Perturb subroutine (Alzantot et al., 2018) is adapted, where candidate sentences with perturbed words are ranked using a language model (OpenAI-GPT model) to ensure consistency.\n5.  PerturbQuestion (Syntactic Paraphrasing): This method uses a syntactic paraphrase network (Iyyer et al., 2018) to generate a syntactically different version of the original question. Ten paraphrases are generated for each question and ranked based on cosine similarity of their word embeddings.\n6.  Combinatorial Adversaries: The paper combines negative perturbations (like AddSentDiverse, AddKSentDiverse, AddAnswerPosition, InvalidateAnswer) with positive perturbations (like PerturbAnswer, PerturbQuestion) to create more complex adversaries that simultaneously challenge different aspects of the model's language understanding. These combinations consistently lead to a larger drop in performance on models trained on original unaugmented datasets.\nA semantic difference check is also performed to ensure distractors are sufficiently distinct from original sentences, requiring at least two content phrases in the original text not found in the distractor.": 775,
    "The paper reduces the domain shift of joint multi-modal embeddings by minimizing the Maximum Mean Discrepancy (MMD) between the source and target domains. The intuition behind MMD is that two distributions are identical if and only if all their moments coincide. The loss function, denoted as Lj, is defined as the expected squared MMD between the joint embeddings of source (es) and target (et) samples. By minimizing Lj, the framework enforces that the joint embeddings from both the source and target domains are projected onto a similar latent space, thereby aligning their distributions.": 776,
    "The paper addresses this by introducing a Contextual Modulator pipeline that utilizes feature-wise linear modulation (FiLM). First, the targeted expression (e.g., verb-noun pair) is embedded using 1,024-dimensional ELMo sentence embeddings, pre-trained on the One Billion Word benchmark corpus. This produces a fixed mean-pooled vector representation, cvn, capturing the deep contextualized meaning of the candidate expression. This cvn vector is then fed as input to the Contextual Modulator, which is a fully-connected feed-forward neural network (FFNN). This FFNN learns to produce two context-dependent parameters: a scaling vector, γ(cvn), and a shifting vector, β(cvn). These parameters are computed as Wγcvn + bγ and Wβcvn + bβ respectively, where Wγ, Wβ, bγ, bβ are learnable parameters. These generated γ and β vectors are designed to dynamically influence the subsequent linguistic pipeline's computations, effectively conditioning the sentence processing based on the specific contextualized features of the targeted expression.": 777,
    "The paper proposes \"\"Vocabulary Adaptation Prior to Fine-tuning\"\" as a three-step process to effectively adapt the vocabulary and embedding layers of a pre-trained NMT model to a target domain. The first step involves inducing target-domain embeddings. Word embeddings are generated from monolingual data specific to the target domain for each language. The Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) is chosen for this induction, as its embedding spaces are considered topologically similar to those used in NMT. The second step is projecting embeddings across domains. The induced target-domain embeddings for both the source and target languages are projected into the respective embedding spaces of the pre-trained NMT model's encoder and decoder. This process yields \"\"cross-domain embeddings\"\" that are designed to be compatible with the pre-trained model. The paper explores two methods for this projection: a linear transformation and a locally linear mapping (LLM). The third and final step is fine-tuning. The vocabularies and the embedding layers of the pre-trained NMT model are replaced with these newly obtained cross-domain embeddings. Following this replacement, the model undergoes fine-tuning using the target-domain parallel data. This sequence ensures that the NMT model's vocabulary and initial word representations are aligned with the target domain before any further training adjustments.": 778,
    "The framework includes two main components: the recommender and the privacy attacker. The recommender employs a source network, a target network, and a knowledge transfer unit to leverage source domain knowledge for enhancing target domain recommendations. The privacy attacker simulates potential adversaries by attempting to infer private user attributes from the transferred knowledge. Through adversarial learning, the recommender is trained to minimize the attacker's success while maximizing recommendation accuracy. This adversarial setup ensures that the transferred knowledge remains useful for recommendations but is less informative for inferring private attributes.": 779,
    "The paper proposes the Joint Optimization with a User SimulaTor (JOUST) framework, which involves two primary agents: a Dialogue System (DS) and a User Simulator (US). Both agents are designed as fully end-to-end, text-to-text models capable of interacting via natural language in complex multi-domain dialogues. The training process begins with supervised pre-training.\nThe Dialogue System (DS) architecture comprises several components:\n1.  Dialogue State Tracking (DST): An encoder-decoder model with an attention mechanism (Bahdanau et al., 2015) that processes dialogue history and the most recent user utterance to maintain a belief state. This belief state is a set of slot-value pairs representing user constraints (e.g., `{hotel_area=north, hotel_name=gonville_hotel}`). The model encodes the dialogue context and user utterance using a bi-directional Long Short-Term Memory (LSTM) (Graves et al., 2005) to obtain hidden states. A decoder then uses these states to predict slot and value tokens.\n2.  Database Query: Based on the updated belief state from the DST, the system searches a database to retrieve matched entities. A one-hot vector characterizes the query result.\n3.  Context Encoding: A hierarchical LSTM (Serban et al., 2016) encodes the dialogue context turn-by-turn. At each turn, the user utterance is encoded by an LSTM-based sentence encoder, and this embedding, along with the previous user-side context output, is fed into a context encoder LSTM to produce the next dialogue context state for the dialogue manager.\n4.  Policy: The dialogue manager determines the system's dialogue act, treated as a sequence of tokens to handle multiple actions per turn. An LSTM is used for sequence generation. Inputs to the policy decoder include the previous act token embedding, previous hidden state, attention vector over user utterance hidden states, database retrieval vector, and a summarized binary belief state. The initial hidden state is set to the context state from the context encoder.\n5.  Natural Language Generation (NLG): An LSTM-based NLG model generates the system's natural language response based on the predicted system dialogue act. It uses the previous hidden state to attend over the policy decoder's hidden states, and the resulting attention vector and previous output word embedding are inputs to generate a word sequence with delexicalized tokens, which are then replaced by retrieval results.\nThe User Simulator (US) architecture mirrors the DS in its end-to-end, text-to-text design, but with key differences:\n1.  Goal State: Instead of DST, the US maintains an internal goal state, a binary vector summarizing the user's dialogue goal (domain-slot pairs). This state is initialized with all goal slots \"\"on\"\" and updated by turning off entries when the slot appears in the previous user dialogue act (either provided by the user or requested by the US).\n2.  Context Encoding, Policy & NLG: These components follow similar LSTM-based implementations as in the DS. The US context encoder takes the system response embedding and DS context state to produce the US dialogue context state. The US policy takes the goal state and system response hidden states to generate a user dialogue act sequence. The US NLG model generates the user utterance from the policy decoder's hidden states, lexicalizing delexicalized tokens using the user goal.\nDuring supervised learning, both DS and US are trained using ground truth dialogue acts and output word sequences. The losses for the policy and NLG models are cross-entropy losses, and for the DS, the DST loss is the sum of cross-entropy losses for slot and value. The two agents are updated jointly to minimize the sum of their individual losses, with the success rate of generated dialogues serving as the stopping criterion.": 780,
    "The MultiMET dataset facilitates automatic metaphor detection by providing a large-scale, multimodal resource. The dataset consists of 10,437 text-image pairs sourced from social media and advertisements. Each pair is annotated for the occurrence of metaphors, domain relations, sentiments, and author intents. This comprehensive annotation scheme supports the development and evaluation of models that can interpret multimodal metaphors. The dataset's diversity and scale enable robust training and testing of algorithms, ensuring better generalization and performance in real-world applications.": 781,
    "The paper formulates verb metaphor detection as a relation classification task by defining \"\"contextual relations\"\" between a target verb and its context components. Instead of classifying the verb token directly, the model determines whether a metaphorical relation holds between the verb and its contexts. The representations of the verb (v) and a context component (c) are used to define the form of the relation (r). Three explicit models are adopted to capture the interactions for this relation:\n1.  Linear model: Uses a parameter vector (Vr) and a bias (br) to represent the relation, computing the probability of metaphoricity based on the concatenation of verb and context representations (v c).\n2.  Bilinear model: Employs a parameter matrix (Ar) and a bias (br), allowing for more sufficient interaction between the verb and context representations through a bilinear form (v^T Ar c).\n3.  Neural tensor model: Utilizes a tensor (Tr) and a bias (br) to capture more complex, non-linear interactions between the verb and context (v^T Tr c).\nThe probability of the relation being metaphorical is computed using a sigmoid function over these relational forms. This approach explicitly models the interactions, allowing the system to determine metaphoricity based on the relationship rather than just the individual contextual representation of the verb.": 782,
    "The paper proposes the Cross-domain Knowledge Distillation (CdKD) framework to address unsupervised domain adaptation (UDA) without source data and with flexible target model architectures. The framework utilizes a pre-trained source model (fs), acting as a teacher, and an unlabeled target dataset (Dt). The goal is to train a target model (ft) using only Dt and fs. The target model ft is allowed to have a different network architecture than fs. The core approach involves two main components: Knowledge Distillation (KD) and Joint Kernelized Stein Discrepancy (JKSD). First, KD is applied by minimizing the LKD objective, which measures the cross-entropy between the soft class probabilities produced by the source model fs(x) and the target model ft(x) for each unlabeled target sample x. A temperature parameter T is used in the softmax output layer of both models to generate \"\"softer\"\" probabilities during training. This allows the target model to learn from the \"\"knowledge\"\" embedded in the source model's predictions. Second, JKSD is introduced to explicitly match the joint distributions between the source model's output and the target data, addressing the domain shift inherent in cross-domain scenarios. This joint optimization allows the target model to adapt to the new domain while learning from the source model's expertise.": 783,
    "The paper addresses this by proposing the Aspect and Opinion Propagation module, which utilizes a transformer-decoder. This module is designed to consider the interactive relations between aspect and opinion terms while predicting a sequence of polarities. It takes the shared context representation, H, generated by a pre-trained language model, and processes it through a multi-head self-attention layer to produce `Uh`. Subsequently, `Uh` is fed into two sequential multi-head cross-attention layers. The first cross-attention layer, `CrossAttn(Uh, Za, Za)`, incorporates aspect representations (`Za`), resulting in `Ua`. The second cross-attention layer, `CrossAttn(Ua, Zo, Zo)`, then integrates opinion representations (`Zo`), yielding `Uo`. This two-step propagation mechanism, where aspect information is first propagated and then opinion information, allows the model to build a comprehensive understanding of the context relative to both aspect and opinion terms. Finally, `Uo` is passed through a single-layer feed-forward neural network to obtain the sequence of polarities, `Yp`, for the aspect-based sentiment classification task.": 784,
    "The paper addresses this by introducing a transformation matrix, denoted as M. This matrix is designed to directly map the representation of the query turn (the last turn of the conversation, `r_nc`) into the semantic space of quotations. The conversation representation `h_c` is first obtained by encoding the conversation using a hierarchical structure: an embedding layer maps words to vectors, a Transformer encoder learns turn representations (`r_t_i` from the [CLS] token's hidden state), and a Bi-directional Gated Recurrent Unit (Bi-GRU) models the sequence of turn representations to yield the final conversation representation `h_c`. Similarly, quotation representations (`r_q_k`) are extracted using an embedding layer and Transformer layers, but importantly, the parameters for these layers are not shared with the conversation modeling, acknowledging the distinct semantic spaces. The transformation matrix M is then applied to the query turn representation `r_nc` to produce `M r_nc`, which is now in the quotation semantic space. This transformed query representation is then used to calculate distances to candidate quotations by multiplying it with a combined quotation matrix Q (where each row is a quotation representation `r_q_k`), resulting in `z_c = Q × (M r_nc)`. Finally, this `z_c` along with `h_c` and `M r_nc` are concatenated and passed through a prediction layer `y = W[z_c;h_c;M r_nc] + b` to determine recommendation likelihoods.": 785,
    "The paper addresses this by introducing a Paired Ideology Ranking Task. Instead of asking annotators to label texts in isolation, which would require a high degree of political expertise to recognize specific ideological nuances, annotators are presented with two texts. For each pair, they are asked to select which text is more likely to have been authored by someone with a specified ideology (left-leaning or right-leaning). To ensure the texts are comparable and highlight ideological differences, the paired texts discuss the same entity, identified using Stanford Core NLP (Manning et al., 2014) to extract people, locations, organizations, and ideologies. Propensity score matching (Rosenbaum and Rubin, 1983) is then used to match a left-aligned text with a right-aligned text that discusses the same entity in a similar context, based on a heuristic semi-supervised labeling approach derived from subreddit participation patterns. Annotators are guided to consider cues such as attitude (evaluation for/against an entity), positioning (situating viewpoint relative to entity's), and jargon (in-group vocabulary). This comparative setup reduces the overhead in recruiting and training political annotators by shifting the task from absolute labeling to relative ranking.": 786,
    "The paper addresses this by introducing the Category-based Masked Language Modeling (CMLM) task, which is a variant of BERT's standard Masked Language Modeling (MLM) pre-training task. Unlike standard MLM, where tokens are masked randomly, CMLM strategically masks tokens based on their semantic similarity to aspect categories from both the source and target domains. The process begins by training static (non-contextualized) word embeddings on the unlabeled data collected from both the source and target domains. For each input word in a given text, its cosine similarity is computed against the word embedding of every aspect category name (from both source and target domains). After calculating these similarities, only the highest similarity score for each input word is retained. Subsequently, the top α% of input words, ranked by these highest similarity scores, are selected and masked. This targeted masking mechanism forces the BERT model to focus its learning on words that are likely to be \"\"proxy-pivots\"\"—that is, words that are highly probable aspect words in either the source or target domain. This approach aims to bridge the domain gap by making the learned representation more invariant to domain-specific variations in aspect terms.": 787,
    "The paper addresses this by proposing a novel Label-Enhanced Contextualized Representation method. This method explicitly introduces contextual metaphor information by using label embedding to map each word's label (metaphoric or literal) into the same vector space as the contextualized representation. For a word `xi` with its contextualized representation `di` (obtained from the combined BERT output, neighboring sentence representation, and global memory network) and its label `yi`, a label embedding `li` is obtained. The sum of `di` and `li` is then fed into a transformer encoder layer. To prevent label leakage during training (where the golden label `yi` is known), the standard transformer's Q (query), K (key), and V (value) matrices are modified. Specifically, Q is formed by summing `di` with a padding embedding `lpad` (which has the same dimension as `li`), ensuring Q does not contain the word's own label information. K and V are formed by summing `di` with the actual label embeddings `li` for all words in the sequence. Additionally, a mask matrix, `AttentionMask`, is applied to the self-attention mechanism, setting diagonal elements to 0. This ensures that each word ignores itself when calculating attention, further preventing the model from directly using its own golden label. The output `s` from this transformer encoder is then passed to a classifier (`Classifier_le`) to produce the final prediction `ˆyi`. In the testing stage, an Early Prediction module (`Classifierep`) first predicts a label `ˆyep_i` for `di`, which then substitutes the golden label `yi` for generating `li` for the label embedding phase.": 788,
    "The paper proposes a Fine-Grained Domain Adaption strategy to incrementally construct a high-quality pseudo-training corpus. This process is iterative and guided by an explicit distance metric: the Out-Of-Vocabulary (OOV) number of a target sentence relative to the source domain's vocabulary.\nInitially, the training dataset (T1) is set to the source corpus (S). In each iteration (i), a model (Mi) is trained on the current training dataset (Ti). This model (Mi) is then used to auto-label the remaining raw corpus of the target domain (Di), producing a pseudo-labeled corpus (ˆDi). A lexicon building process is performed where top-K confident word-POS pairs from ˆDi (discounted by sentence-level probability) are collected and added to a target lexicon (Ltgt).\nThe core of the incremental construction lies in selecting new instances (STi) for the next iteration. For each instance (ˆX, ˆY) in ˆDi, it is added to STi if it satisfies three conditions:\n1.  Coov (OOV number constraint): The number of OOV words in the sentence (ˆX) must be less than or equal to a specified threshold (Clex). This condition directly controls the \"\"distance\"\" of the selected sentences to the source domain, ensuring that instances added in earlier iterations are closer to the source. The OOV threshold is progressively relaxed in subsequent iterations.\n2.  Clex (Lexicon coverage constraint): All OOV words in the sentence (ˆX) must be present in the target lexicon (Ltgt). This ensures that even newly encountered words have some form of validation from the growing lexicon.\n3.  Cconf (Confidence constraint): The probability of the auto-labeled sequence (p(ˆY|ˆX)) must be greater than or equal to a predefined probability threshold (pthreshold). This ensures that only high-confidence predictions are included.\nOnce STi is formed, it is added to the current training dataset (Ti) to form Ti+1, and the selected instances' raw text (STi.X) are removed from the remaining raw corpus (Di) to form Di+1. This iterative process continues until convergence, gradually adapting the model to more distant sentences by incrementally incorporating pseudo-labeled data of increasing OOV numbers.": 789,
    "The model explicitly captures the semantic incongruence between literal and metaphorical meanings by incorporating a Contrastive Objective (Lco). This objective is designed to increase the distance between the contextual representations of a target word's literal and metaphorical senses in the embedding space. Formally, given a target word `wt` in a sentence `Sa` (anchor), a positive example `Sp` (another instance of `wt` belonging to the same class as `Sa` in the batch), and a negative example `Sn` (an instance of `wt` belonging to the opposite class), their contextualized features `ca`, `cp`, and `cn` are first obtained. These features are derived by averaging the hidden states of the subwords corresponding to `wt` from the BERT backbone. The contrastive objective `Lco` is then calculated as the sum of `d(ca,cp) + [gamma - d(ca,cn)]+` over all triplets `(a,p,n)` in a batch, where `d(·,·)` is the L2-normalized Euclidean distance and `[·]+` is the `max(0,x)` function. `gamma` is a margin hyperparameter. This loss function encourages representations of instances from the same class (e\ntive objective increases the distance between them, ensuring they are kept apart by at least a margin `gamma`. This mechanism directly addresses the linguistic theory that metaphoricity is characterized by a contrast between a word's literal and contextual meaning.": 790,
    "The paper addresses this by defining a Humor Score (HS) that leverages Facebook's \"\"Haha\"\" reactions. The core assumption is that a higher percentage of \"\"Haha\"\" reactions among all reactions indicates a more humorous post. To ensure reliability and account for posts with very few reactions, a `tanh` multiplier is applied, proportional to the total number of reactions. This multiplier, stretched by a factor of 50, ensures that posts with 100 or more total reactions are similarly weighted, while posts with fewer reactions receive a steeply declining weight. The formula for HS is `HS = (h/t) * tanh(t/50)`, where `h` represents the number of \"\"Haha\"\" reactions and `t` represents the total number of reactions. The constant `50` acts as a popularity stretcher, modulating the influence of low-reaction posts.": 791,
    "The paper proposes the Memory-Augmented Recurrent Tree-Transformer (MARTT) to encode captions and promote the forward flow of hierarchical information across the sequence of captions for each story. Given a sentence, MARTT processes its constituency parse tree. Each leaf embedding is formed by concatenating a word embedding and a corresponding node embedding, where node embeddings are learned during training and represent phrase label classes. Node embeddings are computed using an upward cumulative average over the nodes that the respective non-terminal leaf token is a child of, allowing the model to build a hierarchical understanding. Within the encoder, a hierarchical structure is promoted by introducing sub-tree masking for encoder self-attention, ensuring that for each word query, self-attention only accesses other members of its sub-tree at a given layer. This bottom-up approach, combined with node embeddings, induces compositionality. To handle a sequence of trees for story visualization, MARTT ties a series of Tree Transformers together by introducing memory cells and memory updater modules in each layer of self-attention. At each time step, the input query matrix within the self-attention layer attends over the current memory state and the current caption's hidden states, and the memory state is updated recurrently. This allows MARTT to encode the sequence of constituency parse trees while maintaining narrative flow.": 792,
    "The paper introduces \"\"back-training\"\" as an alternative to self-training for unsupervised domain adaptation (UDA). Unlike self-training, which samples natural inputs from the target domain distribution (PT) and generates noisy outputs using a supervised model trained on the source domain (PS), back-training samples natural outputs from the target domain distribution (PT) and generates noisy inputs using a source-trained supervised model (PS).\nFor Question Generation (QG), where the goal is to generate a question (q) given a passage (p), back-training proceeds as follows: Unsupervised questions (qu) are sampled from the target domain's question distribution (PT(q)). A Passage Retrieval (IR) model (PS(p|q)), which is trained on the source domain to retrieve a passage given a question, is then used to generate a noisy passage (ˆp) conditioned on the sampled target domain question (qu). The resulting pairs (ˆp, qu) are then used as synthetic training data to adapt the QG model (PS(q|p)) to the target domain.\nConversely, for Passage Retrieval (IR), where the goal is to retrieve a passage (p) given a question (q), back-training samples unsupervised passages (pu) from the target domain's passage distribution (PT(p)). A Question Generation (QG) model (PS(q|p)), trained on the source domain to generate a question given a passage, is used to generate a noisy question (ˆq) conditioned on the sampled target domain passage (pu). These (ˆq, pu) pairs form the synthetic training data for adapting the IR model (PS(p|q)) to the target domain. This approach ensures that the synthetic data's outputs are natural, reducing the gap between the target domain's true output distribution and the learned output distribution, and mitigating model overfitting to the source domain.": 793,
    "The paper addresses the identification of figurative language through a two-pronged approach focusing on metaphors and idioms. For metaphor detection, a classifier named Cmet bert is employed. This classifier is based on fine-tuning the bert-base-uncased (BERT) checkpoint from Wolf et al. (2019) on the VUA corpus (Steen et al., 2010; Gao et al., 2018). After training, each test utterance in the dialog dataset is passed through Cmet bert to obtain a probability (pmet) of it being metaphorical. To ensure high reliability and account for potential domain shift from the VUA corpus, only utterances with a pmet greater than 0.9 are selected, forming the set of automatically detected metaphorical utterances (Dmet auto). For idiom detection, a lexicon of 2048 commonly used idioms is curated from an online source. Utterances containing at least one entry from this lexicon as a substring are identified, creating the set of automatically detected idiomatic utterances (Didiom auto). Finally, these two sets are unified to form Dfig auto (Dmet auto ∪ Didiom auto), representing the overall set of automatically detected figurative language instances in the dialog contexts.": 794,
    "The paper addresses the challenge of mitigating catastrophic forgetting and improving new domain performance using a simple, data-driven strategy called data mixing. This method involves interleaving weight updates derived from the new domain data gradients with weight updates based on gradients from the original training data. Practically, this is implemented by concatenating a sample of the original training data to the domain-specific adaptation set. The importance of the original training data sample can be adjusted by varying its size, thereby influencing the ratio of training data to domain data. This ratio allows for control over the trade-off between maintaining generic performance and improving domain-specific performance. Unlike some prior data mixing approaches, this method does not require knowledge of the domain at test time, making the models domain-agnostic. To prevent overfitting to the training data sample during adaptation, the domain-specific adaptation set is upsampled (e.g., 20x), and a training sample of the increased size is concatenated to maintain a desired ratio (e.g., 1:1).": 795,
    "The paper formalizes cognitive mechanisms of chaining through two classes of deep categorization models: the Deep Prototype Model (SFEM-DPM) and the Deep Exemplar Model (SFEM-DEM). Both models operate on time-dependent hidden representations (h(t)n) of nouns, derived from multimodal knowledge.\nThe SFEM-DPM, inspired by prototypical networks, computes a prototype vector (c(t)f) for each verb-syntactic frame (f) by averaging the hidden representations of all its support nouns (ns) at time t. The likelihood of a query noun (n*) extending to a frame is then determined by a softmax distribution over the l2 distances between the query noun's representation and the frame's prototype. This models a prototype-based view of categorization, where new items are compared to a central representation of a category.\nIn contrast, the SFEM-DEM, resembling memory-augmented matching networks, formalizes the exemplar theory of categorization. Instead of a single prototype, this model depends on the l2 distances between the query noun (n*) and *every* individual support noun (ns) within a given frame (f). The likelihood of extension is calculated as a sum of exponentials of negative l2 distances to all support nouns, normalized across all support nouns of all frames. This approach emphasizes the semantic neighborhood profile, where new items are compared to individual stored exemplars. Both models are trained incrementally at each time period by minimizing the negative log-likelihood of the joint probability of a query noun and a frame.": 796,
    "The paper addresses the refinement of the slot prototype semantic space through Prototypical Contrastive Learning (PCL). This strategy is introduced to overcome the problem of chaotic and dense distributions of slot name embeddings, which typically hinder the establishment of correct relationships between slot values and their corresponding prototypes. The core mechanism involves dynamically updating the slot prototype matrix. This matrix is obtained by encoding the original slot name embeddings using an MLP layer. For a given slot value, denoted as `rk`, and its corresponding ground truth label `yk`, the model maps `rk` into the refined semantic space. A prototypical contrastive loss, `Lpc`, is then computed. This loss function is formally defined as `Lpc (rk, yk) = -log(exp(rk · zyk/τ) / ΣC c=1 exp(rk · zc/τ))`, where `zc` represents the representation of a slot prototype with label `c`, `τ` is a temperature factor, and `C` is the total number of slot categories. By optimizing this objective function, the PCL mechanism ensures that the embedding of a slot value `rk` is pulled closer to its correct slot prototype `zyk` in the semantic space. Simultaneously, it pushes `rk` away from all other incorrect slot prototypes `zc`. This process effectively enhances the precision of the mapping function from the feature space (where slot values are represented) to the semantic space (where slot prototypes reside), and it also reduces the density of slot prototype distribution, leading to better separation and clearer boundaries between different slot types.": 797,
    "The paper addresses this by proposing a novel contrastive adaptation loss (Lcon) that is incorporated into the overall training objective of the Question Answering (QA) model. This loss operates on the token level, defining two classes: answer tokens and combined context/question tokens. The core idea is two-fold: (i) to decrease the discrepancy among tokens within the same class (intra-class discrepancy) and (ii) to enlarge the discrepancy between tokens of different classes (inter-class discrepancy). The discrepancy is measured using Maximum Mean Discrepancy (MMD), which quantifies the distance between two data distributions based on samples. Empirically, MMD is computed as the distance between mean embeddings in a reproducing kernel Hilbert space, using a Gaussian kernel with multiple bandwidths. The Lcon is formulated as: Lcon(X) = (1/|X|^2) * Σi Σj k(φ(x(i)a), φ(x(j)a)) + (1/|X|^2) * Σi Σj k(φ(x(i)cq), φ(x(j)cq)) - (1/|X|^2) * Σi Σj k(φ(x(i)a), φ(x(j)cq)), where x(i)a represents the mean vector of answer tokens, x(i)cq represents the mean vector of context/question tokens, and φ is the feature extractor (the BERT-encoder). The first two terms minimize intra-class discrepancy, while the third term (negative) maximizes inter-class discrepancy. This contrastive adaptation loss is combined with the standard cross-entropy loss (Lce) used for training the BERT-QA model to predict correct answer spans, forming the overall objective Lqa(X) = Lce(X) + βLcon(X), where β is a hyperparameter. During training, mixed mini-batches are used, and Gaussian noise is applied to token embeddings to learn a smooth and generalized feature space.": 798,
    "The paper addresses this by employing a Task Representation Learning (TRL) procedure to obtain a collection of prototype vectors for each task. For each task Ti, a pre-trained task encoder performs a one-pass scan over its training set Di. Each task Ti is represented as a collection of prototypical vectors, Pi = {(cid:126)pi,j}, where (cid:126)pi,j is the j-th prototypical embedding vector corresponding to the j-th class in Di. This vector is computed as the average of the embeddings of all instances belonging to that class: (cid:126)pi,j = (1/|Di,j|) * sum(E(xi,j,ci,j)) for xi,j in Di,j, where Di,j is the subset of Di with instances assigned to the j-th class label, and E(·,·) is an embedding function that encodes both the textual input xi,j and its class label ci,j using PLMs. The input to the pre-trained encoder is formatted as \"\"[CLS]xi,j[SEP]ci,j[SEP]\"\" for single-text classification, allowing the text and class label to attend to each other, fusing label information into text representations. The average pooled output of the last encoder layer is used as E(xi,j,ci,j).\nAfter obtaining Pi for all tasks, a prototype-based Meta Knowledge Graph (Meta-KG), denoted as G(V,L), is constructed. In G, each prototypical vector (cid:126)pi,j is treated as a node in V. Edges li,j,m,n ∈ L denote the similarity between two prototypical vectors (cid:126)pi,j and (cid:126)pm,n, with the edge weight w(li,j,m,n) defined as the cosine similarity between the two vectors. This Meta-KG implicitly describes the relations among tasks and classes, considering their class semantics. During the Multi-task Meta-learner Training (MMT) phase, for each input instance xi,j, a meta-knowledge score mi,j is generated by querying xi,j in G. This score represents the degree of knowledge transferability. It is composed of two parts: instance-level meta-knowledge αi,j and class-level meta-knowledge βi,j. The instance-level meta-knowledge αi,j is calculated as the maximum cosine similarity between the embedding of the current instance E(xi,j,ci,j) and any prototypical vector (cid:126)pm,n from tasks other than Ti (i.e., (cid:126)pm,n∈ ˜Pi). The class-level meta-knowledge βi,j is calculated similarly, but using the prototypical vector of the current instance's class (cid:126)pi,j instead of E(xi,j,ci,j). Finally, mi,j is computed as the average of αi,j and βi,j (mi,j = (αi,j+βi,j)/2).": 799,
    "The Wasserstein distance-based discriminator (fϕ) is integrated into the framework to serve two primary functions. First, it is trained to distinguish between features from the source and target domains, utilizing the Wasserstein distance as a robust measure of domain discrepancy. This measure is preferred for its ability to provide more stable gradients, even when the distributions are far apart. The discriminator's objective is to maximize an empirical Wasserstein distance approximation, enforced to be 1-Lipschitz through a gradient penalty mechanism. Second, this discriminator plays a crucial role in guiding the Transfer Learning (TL) module (fω) and the Reinforced Data Selector (fθ). For the TL module, the discriminator engages in an adversarial training process: while the discriminator attempts to differentiate source from target data, the TL module simultaneously aims to \"\"fool\"\" the discriminator by minimizing the estimated Wasserstein distance. This adversarial dynamic compels the TL module to learn feature representations that are inherently domain-invariant, making them more transferable and discriminative for downstream prediction tasks. For the Reinforced Data Selector, the discriminator provides a novel, stable, and immediate reward signal. This immediate reward (Rimm) is formulated based on the Wasserstein distance metric, specifically `Lwd(xe_i,xt*) = aid(i,j*)`, where `xe_i` is a source instance, `xt*` is its most similar instance in the target batch, and `d(i,j*)` is the Euclidean distance between their shared encoder outputs. The selector is rewarded for choosing source instances that are semantically closer to the target domain, addressing the issue of sparse and high-variance rewards in traditional reinforcement learning.": 800,
    "Metaphorical words are interpreted unambiguously by selecting the most appropriate gloss from a set of candidate glosses. For a target word (wi) in a sentence, a set of candidate glosses (Gi) is collected from an existing dictionary. A dedicated Gloss Encoder, which is a fine-tuned BERT model, processes each gloss (gj) in the set. Similar to sentence encoding, each gloss is formatted as a token sequence (\"\"[CLS] gloss_tokens [SEP]\"\") and fed into BERT. The 768-dimensional vector corresponding to the \"\"[CLS]\"\" token of each gloss is extracted as its gloss representation (pj). To predict the best gloss, an attention mechanism is applied. This mechanism computes a probability (αj) for each gloss (gj) based on the dot product similarity between the contextual representation (hi) of the target word (wi) and the gloss representation (pj). The gloss with the highest probability is then predicted as the interpretation of the target word.": 801,
    "The paper addresses this by introducing the \"\"Additional Training of fastText\"\" stage. First, a fastText model is trained using only the training data of the downstream task. This fastText model is initialized with publicly available fastText embeddings. Since fastText is designed to handle subword information, it can generate embeddings for the subword units present in the Pre-trained Language Model's (PTLM) vocabulary. This process yields a set of target domain-specific word embeddings, denoted as F, for the PTLM's vocabulary VLM. This stage is computationally much faster than training a PTLM and does not alter the PTLM's original vocabulary. These F embeddings then serve as the target for regularization in the subsequent PTLM pre-training phase.": 802,
    "The paper proposes a two-step approach as a concrete realization of the Cross-Domain Review Generation (CDRG) paradigm. The first step is Domain Generalization, which converts a source-domain review into a domain-independent review. This is achieved by identifying and masking the source-specific attributes within the original review. The second step is Domain Specification, where the domain-independent review is transformed into a target-domain review. This transformation is performed using a masked language model that has been pre-trained on the target domain's unlabeled data. The domain-independent review serves as a crucial bridge, establishing word-to-word alignments between the original source review and the newly generated target review. This alignment allows for the direct transfer of the fine-grained annotation (labels) from the source review to the corresponding positions in the generated target review, thereby producing a labeled target-domain review.": 803,
    "Few-shot slot tagging is formulated as a Machine Reading Comprehension (MRC) problem by converting original dataset samples (split sentence tokens and slot types) into a set of <question, answer, context> triples. For a given sentence, it serves as the context (C). A question (Q) is generated for each slot type using a unified template that combines the domain name and the slot type (e.g., \"\"Book Restaurant, restaurant type ?\"\"). The answer is the corresponding text span from the context. If no answer (span) is available in the given sentence for a particular question (slot type), a special \"\"NO ANSWER\"\" token, concatenated to the context, is designated as the extractable answer. The BERT-based MRC model takes the token sequence in the form of `[CLS],q1,q2,..,qL,[SEP],c1,c2,...,cM` as input. The model then predicts the start token (Cstart) and end token (Cend) of the answer span by computing probabilities Pstart(i) and Pend(i) for each token position `i` using softmax functions applied to learned representations (Wsvi and Wevi). The tokens between the highest Pstart(i) and Pend(j) are predicted as the slot content for the asked slot type.": 804,
    "The paper constructs a novel dataset called AStitchInLanguageModels by collecting naturally occurring sentences containing multiword expressions (MWEs) along with their two surrounding sentences. This collection process involved 12 judges who gathered 7 to 10 examples for each meaning (\"\"Idiomatic,\"\" \"\"Non-Idiomatic,\"\" \"\"Proper Noun,\"\" and \"\"Meta Usage\"\") of a given MWE, aiming for 20 to 30 total examples per MWE. \"\"Meta Usage\"\" is defined as the literal use of an MWE in a metaphor. Judges were also instructed to identify novel usages and add new meanings or flag them for expert review. The MWEs focused on noun compounds sourced from the Noun Compound Senses (NCS) dataset.\nEach meaning of an MWE was then paraphrased by language experts. Idiomatic paraphrases concisely convey the idiom's meaning (e.g., \"\"cutting edge\"\" to \"\"most advanced\"\"), while literal paraphrases involve minimal lexical alteration to shift away from the idiomatic meaning (e.g., \"\"cutting edge\"\" to \"\"slicing edge\"\"). This \"\"adversarial paraphrasing\"\" is designed to test a model's nuanced understanding. Finally, each example was annotated with a label and corresponding paraphrase by two judges, with disagreements resolved through discussion. The final dataset comprises 4,558 English examples (223 MWEs) and 1,872 Portuguese examples (113 MWEs). The data is divided into training, development, and test splits, with the test and development sets containing MWEs that may or may not appear in the training set, enabling zero-shot, one-shot, and few-shot evaluation scenarios. The zero-shot setup includes MWEs not present in development or test sets, while one-shot and few-shot setups provide limited examples of MWEs found in development/test sets.": 805,
    "The paper identifies the adaptable lottery subnetwork using a gradual pruning algorithm that incorporates self-attention head importance. This process begins with an initial pruning mask set to all ones for the source domain model, which is a Transformer-based pre-trained language model (e.g., BERT-base). The method focuses on pruning the parameter matrices of the linear projections and feed-forward networks within the Transformer blocks, while keeping other parameters intact.\nThe identification process, detailed in Algorithm 1, proceeds iteratively over a set number of pruning steps (N). In each step `n`:\n1.  Estimate Attention Head Importance: The importance scores (I) for all attention heads are estimated using the self-attention attribution (ATTATTR) method (Hao et al., 2020), which runs an integrated gradients procedure over attention links. A higher attribution score indicates a greater contribution to the model's prediction.\n2.  Normalize Importance Scores: The estimated importance scores are then scaled using MinMax normalization, incorporating an importance factor (λ). The formula used is `ˆIn ← λ + (1 − λ) * (In − min(In)) / (max(In) − min(In))`. A smaller λ value indicates a stronger influence of the importance scores on parameter selection.\n3.  Trim Magnitudes with Normalized Importance: The magnitudes of the parameters are scaled according to these normalized importance scores. This step is crucial as it can re-rank parameters, prioritizing those associated with important attention heads even if their raw magnitudes are not the largest. Since parameters of an attention head are distributed across four parameter matrices, the same importance scores are applied to each matrix, and slices corresponding to the same head are scaled identically within a layer.\n4.  Determine Sparsity: A square sparsity scheduling is used to gradually increase the sparsity of the model, with the target sparsity `sn` for step `n` calculated as `s - s * (1 - n/N)^2`, where `s` is the final target sparsity.\n5.  Prune Parameters: The lowest magnitude parameters (after being scaled by importance scores) are pruned in groups to reach the target sparsity `sn`. The paper employs a \"\"divide-and-conquer\"\" group pruning strategy. Instead of global pruning (where all parameters are pooled) or local pruning (where parameters are compared within each matrix), this strategy divides parameter matrices into groups based on their mean magnitudes. Pruning is then performed locally *inter-group* (meaning different groups can have different pruning fractions) and globally *intra-group* (meaning parameters within a group are pooled for pruning). This approach aims to balance component importance with parameter magnitudes.\n6.  Update Pruning Mask: The pruning mask (M) is updated to reflect the pruned parameters.\n7.  Train Model: The model is trained for a fixed number of steps (∇t) to allow it to recover from the pruning-induced performance degradation.\nThis iterative process continues until the desired target sparsity is reached, resulting in the final pruning mask M, which defines the lottery subnetwork.": 806,
    "The paper addresses this by implementing a Domain Pretraining (DPT) step. This involves taking a generally pretrained BERT model, such as BERTBASE (uncased), and continuing its pretraining on an unsupervised set of in-domain (target domain) data. During this DPT phase, the model is trained solely using the Masked Language Modeling (MLM) objective. The MLM objective consists of predicting the most probable tokens for randomly masked positions within the input text. Specifically, 15% of WordPiece tokens in the target sentences are randomly masked, with 80% replaced by a [MASK] token, 10% by a random token, and 10% remaining unchanged. This process allows the BERT model to adapt its internal representations and language understanding to the specific linguistic characteristics and vocabulary of the target domain without requiring any labeled data from that domain. This adapted model then serves as the initialization for the subsequent fine-tuning step.": 807,
    "The paper proposes a method that leverages contextualized word embeddings to better capture the linguistic characteristics of various Arabic dialects and domains. These embeddings are trained to understand the context within which words appear, making them particularly suited for handling the complexities of Arabic. The method involves training these embeddings on a large corpus of Arabic text that includes multiple dialects and domains, allowing the model to learn domain-specific features without explicit labels.": 808,
    "The paper addresses this by employing a Transformer-based encoder-decoder model, which is initialized using a pre-trained Transformer Language Model (LM). This pre-trained LM is initially trained with a Masked Language Modeling (MLM) objective on a large generic corpus (English Wikipedia), equipping it with a broad understanding of language by predicting masked words based on bidirectional context. For the style transfer module, the encoder is initialized directly with this pre-trained LM. Crucially, the decoder is initialized with the same pre-trained LM but further fine-tuned on the target style using a Causal Language Modeling (CLM) objective. This CLM fine-tuning aligns the decoder to the distribution of the target style, making it more adept at generating stylized outputs and speeding up the fine-tuning process. This fine-tuned decoder LM is also repurposed to serve as a discriminator for stylistic feedback. The encoder-decoder setup is then fine-tuned with a Denoising Autoencoder (DAE) loss. Under the DAE objective, the encoder takes a noisy, masked version of an input text (where tokens are dropped or masked with probabilities `p_drop` and `p_mask`) and attempts to fill in the mask tokens. The decoder then re-creates a stylistic version of the original sentence from this noisy output. For multi-style transfer, a randomized mixture of target-domain corpora from each of the target styles is used for DAE fine-tuning. The overall training objective for the encoder-decoder model (with parameters `θG`) is `L_DAE(θG) = E_x~T[-logP_θG(x|˜x)]`, where `x` is the original sentence and `˜x` is its noisy version. This setup ensures content preservation by reconstructing the original text while simultaneously instilling style awareness through the decoder's initialization and the DAE process.": 809,
    "The paper addresses this by implementing Source Domain Pre-Training (SDPT). This involves continuing the pre-training of a pre-trained generative model, specifically BART (Lewis et al., 2019), using substantial training samples from a rich-resource source domain. The News domain, particularly the XSum dataset (Narayan et al., 2018), is chosen as the source due to its abundance of labeled summarization data. Unlike BART's original pre-training objective of sentence reconstruction, SDPT utilizes the supervisions from the source domain summarization data to train BART directly on the summarization task. The objective is to inject task-specific knowledge into the pre-trained language model, allowing it to quickly adapt to the same summarization task in diverse low-resource target domains. After this second pre-training phase, the model is then fine-tuned on the specific target domain summarization task.": 810,
    "The presence and layer-wise distribution of metaphorical knowledge in pre-trained language models (PLMs) are probed using two methods: edge probing and Minimum Description Length (MDL) probing. For both methods, the PLM parameters are frozen, and classifiers are trained on top of the PLM's word representations. Edge probing involves feeding PLM-obtained word representations, projected to 256-dimensional vectors, as inputs to a classifier. This method uses two pooler sections: one for pooling representations across words in a span and another for pooling across layers, allowing for evaluation of overall metaphoricity encoding quality. MDL probing, based on information theory, combines the classifier's quality with the effort needed to achieve that quality. The \"\"online coding\"\" method of MDL is employed, where the classifier is gradually trained on different portions of the dataset, and the code length is computed as the sum of cross-entropies. The results are reported as \"\"compression\"\" scores, which indicate the quality and extractability of the knowledge, with higher numbers signifying better quality. This layer-wise analysis helps determine at which depth metaphorical information is most concentrated within the PLM's multi-layer representations.": 811,
    "The paper adapts prompt tuning by utilizing separate soft prompts for different domains, moving beyond fixed hard templates. Each soft prompt consists of multiple learnable vectors, denoted as `h0,...,hk-1`, which are embedded into the input sequence alongside the original sentence embeddings `e(x)`, the [MASK] token embedding `e(\"\"[MASK]\"\")`, and positional tokens `e(\"\"[CLS]\"\")` and `e(\"\"[SEP]\"\")`. This allows each domain to have its own independent soft prompt, enabling the model to learn and represent domain-specific information and acquire domain-aware knowledge. This mechanism is designed to mitigate the domain discrepancy observed in the feature distribution of the [MASK] token, which is crucial for the Masked Language Model (MLM) head classifier. The sentiment classification objective, `Lclass`, is minimized using binary cross-entropy, and this optimization process updates the parameters of the Pre-trained Language Model (PLM), the learnable soft prompt vectors, and the MLM head function.": 812,
    "The paper addresses this by developing a heuristic annotation method applied to translations of the MAGPIE corpus. Sentences from the MAGPIE corpus, containing potentially idiomatic expressions (PIEs), are translated by Transformer models using beam search with a beam size of five. The heuristic labels a translation as a \"\"word-for-word translation\"\" if at least one of the idiom's keywords is literally present in the translation. Literal translations of keywords are extracted from both the model's output and Google Translate. If no literally translated keyword is present, the translation is labeled as a \"\"paraphrase.\"\" This method is then verified through human evaluation, where native speakers annotate a subset of samples, focusing on PIE keywords, to determine if the English word was copied, literally translated, or paraphrased. The agreement between the heuristic and human annotators is computed to assess the method's quality, particularly for figurative-paraphrased and literal-word-for-word categories.": 813,
    "The paper addresses this by introducing the ePiC (Employing Proverbs in Context) dataset, which is created through a multi-step crowdsourcing process. First, a candidate set of English proverbs is collected by scraping websites like 'The Phrase Finder' and WikiQuotes, followed by manual pruning to remove lexical variations, resulting in 250 unique proverbs. Second, narratives are collected using Amazon Mechanical Turk (AMT). For each proverb, 10 narratives are collected from distinct turkers, totaling 2500 proverb-narrative pairs. Turkers are instructed to write short, realistic stories (preferably within 100 words) and are explicitly encouraged to minimize lexical overlap with the proverb and avoid mentioning the proverb or its parts in the narrative. This constraint ensures that models must go beyond surface-level cues to succeed. Third, fine-grained span alignment annotations are solicited. Turkers are presented with proverb-narrative pairs and asked to identify contiguous spans in the narrative that align well with contiguous spans in the proverb, submitting up to 5 pairs of aligned spans per proverb-narrative pair. These annotations highlight the grounding of the proverb's meaning within the narrative and are crucial for verifying reasoning capabilities and adding interpretability.": 814,
    "The conventionality measure calculates the degree to which words in a phrase are interpreted in a canonical way. For each word, the average embedding from contexts where the word appears outside the target phrase is computed. This serves as a proxy for the conventional meaning. The conventionality score is then calculated as the negative average distance between this proxy and the embeddings of the word in instances of the target phrase. Pre-trained language models like BERT provide context-aware embeddings for this calculation. Separate scores are computed for head and non-head words to account for potential asymmetries in idioms.": 815,
    "A large-scale, semi-automatically generated dataset of figurative and literal sentence pairs is constructed using \"\"silver methods\"\" that leverage existing corpora and dictionary definitions. For idioms, the process begins by collecting idiomatic expressions (IEs) from corpora like MAGPIE, PIE, and SemEval2013 Task 5. Definitions for these IEs are extracted from online idiom dictionaries and then manually corrected by annotators to ensure they are correct literal interpretations and syntactically fit the same environments as the original IE. These corrected definitions are then substituted into original sentences from the corpora: replacing them into figurative contexts yields entailment relations, while replacing them into literal contexts yields non-entailments. As a second method for generating non-entailment pairs, annotators create novel, \"\"adversarial definitions\"\" for IEs that are not entailed by the true meaning but seem plausible. These adversarial definitions are then inserted into figurative sentences. For metaphors, the process starts with a collection of minimal metaphoric expressions (MEs) from Tsvetkov et al. (2014), annotated as literal or metaphoric. Annotators replace a word in the ME to make it literal in a neutral context (e.g., \"\"drop prices\"\" to \"\"reduce prices\"\"). This literal replacement is then substituted into original figurative sentences to create entailment pairs. Additionally, sentences containing these original MEs are identified from the CommonCrawl dataset, and the metaphoric word is replaced with its literal counterpart to generate more silver pairs. All silver methods employ \"\"syntactic postprocessing\"\" to overcome issues like phrases having different syntactic patterns in figurative vs. literal use, or not forming full constituents. This involves requiring the expression in context to begin with the same part of speech as the definition and ensuring it does not end inside another phrase, as well as matching verb conjugations.": 816,
    "The paper addresses the challenge of enhancing prediction diversity by introducing a secondary pre-training stage called Adjective-Noun Mask Training (ANT). This process fine-tunes a pre-trained language model (PLM) with a specially designed dataset. First, sentences containing `amod` dependencies (adjectival modifier of a noun) are extracted from the BookCorpus using `trankit`, a lightweight transformer-based toolkit for multilingual natural language processing. Second, instead of random masking, words at the end of `amod` relations (i.e., nouns or adjectives) are specifically masked. To prevent bias towards high-frequency words, the number of times any single word is masked is limited to no more than five. Finally, the PLM is fine-tuned on this constructed dataset using the standard Masked Language Model (MLM) loss. This targeted training encourages the model to generate a wider range of diverse and novel words, moving beyond the common, high-frequency words typically favored by standard MLM objectives.": 817,
    "The paper addresses this by proposing Multimodal Context-Aware Attention (MCA2). Unlike traditional cross-modal attention that directly uses textual representations as queries against multimodal representations, MCA2 first generates multimodal information-conditioned key (ˆK) and value (ˆV) vectors. This is done before performing the standard scaled dot-product attention. To control the integration of multimodal information and retention of textual modality, MCA2 employs a gating mechanism, learning values for λk and λv. These gates are computed using a sigmoid function applied to a linear combination of the original textual key/value vectors (K, V) and the multimodal context (C). This learned gating allows the model to dynamically decide the contribution of each modality. Finally, the query (Q) derived from the textual representation interacts with these multimodal-informed ˆK and ˆV to produce acoustic-information-infused (HA) and visual-information-infused (HV) representations.": 818,
    "The paper proposes Substructure Distribution Projection (SUBDP) to project dependency arc and label distributions from a source language (L1) to a target language (L2). This process involves three main steps:\n1.  Source Parser Training: A bi-affine dependency parser, denoted as P1, is first trained on annotated sentences in the source language L1. This parser learns to produce arc probabilities P1(sj|si) (probability of word sj being the head of word si) and label probabilities P1(ℓ|sj→si) (probability of label ℓ for the arc from sj to si). For convenience in projection, a dummy word (s|s|+1) is introduced whose head is itself, and for which P1(si|s|s|+1)=0 and P1(s|s|+1|s|s|+1)=1. For label probabilities, a uniform distribution is added for dummy positions.\n2.  Annotation Projection: For a given bitext pair (s, t) where s is an L1 sentence and t is an L2 sentence, word alignment matrices are generated using SimAlign. These matrices, ˜A (binary alignment), are then processed to create right-stochastic source-to-target (As→t) and target-to-source (At→s) alignment matrices by adding dummy positions and applying row normalization. The arc probability distribution ˆP2(tq|tp) in the target language is then computed as a weighted sum of source arc probabilities: ˆP2(tq|tp) = Σi Σj At→sp,i * P1(sj|si) * As→tj,q. Similarly, the label probability distribution ˆP2(ℓ|tq→tp) is computed as ˆP2(ℓ|tq→tp) = Σi Σj At→sp,i * P1(ℓ|sj→si) * At→sq,j. These projected distributions serve as \"\"soft silver labels\"\" for the target language.\n3.  Target Parser Training: A second bi-affine dependency parser, P2, is trained for the target language L2. Instead of using hard labels, P2 is trained by minimizing the cross-entropy loss between its own predicted probabilities and the projected soft silver labels (ˆP2). This process effectively transfers the learned syntactic patterns from the source language to the target language through the soft distributions.": 819,
    "The paper designs a novel task called Simile Property Probing to estimate the ability of PLMs in simile interpretation. This task is formulated as a particular masked-word-prediction probing task in the form of multiple-choice questions. Specifically, it masks the explicit property of a closed simile and then requires the PLMs to discriminate it among four candidate options. To ensure the questions are convincing and challenging, the distractors are designed to be both true-negative (introducing logical errors if filled) and semantically close to the correct answer.\nThe probing data collection involves four steps:\n1.  Data Sources: Closed similes with explicit properties are collected from two distinct sources:\n    *   General Corpus: Sentences matching the syntax \"\"ADJ as (a, an, the) NOUN\"\" are extracted from the British National Corpus (BNC) and iWeb corpora.\n    *   Teacher-Designed Quizzes: Complete closed simile sentences are retrieved from questions and answers on Quizizz, an educational platform.\n2.  Human Annotation: Three annotators are required to decide if extracted sentences are similes and to annotate their corresponding simile components (topic, vehicle, event, comparator, property). Inter-annotator agreement is measured using Fleiss’ Kappa score. All properties are converted to single-token synonyms using WordNet and ConceptNet.\n3.  Distractor Design: Three distractors are designed against the original property for each simile, adhering to criteria of being true-negative and challenging. This involves three phases:\n    *   Distractor Generation: Candidates are generated from four semantic-related components (topic, vehicle, event, property). Antonyms of the original property are harvested from WordNet and ConceptNet. For other components, properties are retrieved using the \"\"HasProperty\"\" relation from ConceptNet and COMET. Additionally, top adjectives or adverbs concerning each component are selected from Wikipedia and BookCorpus based on frequency.\n    *   Distractor Selection: To select the most challenging distractors, the cosine similarity between the original sentence (with correct property) and sentences with distractors is measured. RoBERTaLARGE is used to extract two types of features: context embedding (sentence embedding of [CLS]) and word embedding (token embedding of the answer/distractors). These embeddings are concatenated to compute cosine similarity. The top 3 distractors with the highest similarities are selected.\n    *   Human Confirmation: Three human annotators label each selected distractor to ensure it is true-negative. If more than two annotators are uncertain, the distractor is replaced.": 820,
    "The paper designs a Question Value Estimator (QVE), denoted as `eγ`, which is a BERT-based model tasked with predicting the usefulness of a synthetic question-answering (QA) example. This example comprises a context (`c`), a question (`q`), and an answer (`a`). The QVE outputs a real-valued score, `vl`, which signifies the \"\"value\"\" or potential of the example to enhance target-domain QA performance when utilized as a training sample. The input to the QVE is constructed by concatenating the context, question, and answer using special delimiters: `[CLS]q[ANS]a[SEP]c`. A BERT model then processes this sequence, and the hidden representation corresponding to the `[CLS]` token, denoted as `h`, serves as the primary feature vector. To further improve training convergence and performance, the QVE incorporates additional features: the start and end answer probabilities (`ps`, `pe`) obtained from a separately pretrained QA model. These probabilities are concatenated with a transformed version of the `h` representation. The final value `vl` is computed through a series of linear layers with `tanh` activation functions. Specifically, `h` is first transformed into `h'` via `σ(W1h + b1)`. Then, `h'` is concatenated with `ps` and `pe`, and this combined vector is transformed into `h''` via `σ(W3(h' ⊕ ps ⊕ pe) + b3)`, where `⊕` denotes concatenation. Finally, `vl` is derived from `h''` using a linear transformation: `W4h'' + b4`. `W1`, `W2`, `W3`, `W4` are weight matrices, and `b1`, `b2`, `b3`, `b4` are bias vectors, all of which are trainable parameters, and `σ` is the `tanh` activation function.": 821,
    "The paper constructs the ExPUNations (ExPUN) dataset by augmenting the existing SemEval2017 Task 7 dataset. This augmentation involves collecting detailed crowdsourced annotations for 1,999 text samples (834 heterographic puns, 1,074 homographic puns, and 91 non-puns). Five annotations are collected per sample from a professional team of 10 full-time annotators. The annotation fields (AF) are:\n*   AF1 (Understandability): A binary label indicating whether the annotator understands the text, regardless of perceived funniness. If not understood, subsequent annotations are skipped.\n*   AF2 (Offensiveness): A binary label indicating if the text is offensive or inappropriate. If offensive, subsequent annotations can optionally be skipped.\n*   AF3 (Is a joke?): A binary label indicating if the annotator thinks the text is intended to be a joke. If not a joke, subsequent annotations are skipped.\n*   AF4 (Funniness): A Likert scale rating from 1 (very not funny) to 5 (very funny).\n*   AF5 (Explanation): A concise natural language explanation of why the text is funny. This includes external or commonsense knowledge required and, for puns, an explanation of how the wordplay works.\n*   AF6 (Joke keywords): A sparse list of keyword phrases copied verbatim from the joke that are related to the punchline or the reason the joke is funny.\nAnnotation quality is ensured through a kick-off meeting, three pilot rounds for calibration with iterative feedback, and seven rounds of annotation with manual quality checks and re-doing batches as necessary. Inter-annotator agreement is reported using Cohen's kappa and Spearman's correlation for numeric ratings, and BLEU-4 and METEOR for text fields.": 822,
    "The paper proposes a retrieve-and-rank strategy to select relevant pun word pairs (pw, aw) for a given context (C). This is achieved by applying a classifier to all available pun word pairs in a predefined set (Pw, Aw) and retrieving those classified as suitable. These suitable instances are then ranked based on the model’s confidence, and the top-k instances are selected as the final retrieved pun word pairs. The paper experiments with two main approaches for building this retrieval module. The first is Supervised Neural Modeling, where pretrained language models such as BERT-base, RoBERTa-base, DeBERTa-base, RoBERTa-large-NLI, and BART-large-MNLI are finetuned on the XCUP dataset. The input to these models is formatted as a sentence matching task, treating context C as \"\"sentence 1\"\" and the pun word pair (pw, aw) as \"\"sentence 2,\"\" with the output label indicating compatibility. For Natural Language Inference (NLI) models, context words serve as the premise and the pun word pair as the hypothesis. After classification, suitable pun pairs are ranked by the model's confidence (output from the last layer after softmax) to retrieve the top-k. The second approach is an Unsupervised Embedding-based Method, which leverages semantic similarity by calculating the average Euclidean distance between the GloVe embeddings of the pun word (pw), alternative word (aw), and each context word (ci) in the context C. All candidate (pw, aw) pairs are then ranked based on these distance scores, and the k pairs with the smallest distances are retrieved.": 823,
    "The SPARTUN dataset is constructed through a multi-stage process. First, scene graphs are generated from NLVR (Natural Language for Visual Reasoning) images, with spatial relations between objects computed based on their coordinates. To expand beyond NLVR's 2D limitations, additional dimensions like FRONT and BEHIND are introduced by randomly changing LEFT/RIGHT relations, and relations are randomly assigned to blocks while ensuring spatial constraints are maintained, forming a new scene graph with entities as nodes and spatial relations as directed edges. Second, for Question Selection, a logical spatial reasoner (implemented in Prolog) validates paths between entities in the generated scene graph by applying Horn clauses (spatial rules such as Inverse, Symmetry, Transitivity, and Combination) to infer all possible direct and indirect spatial relations. Questions are then generated from paths requiring the most reasoning steps, focusing on the spatial relationship between the path's head and tail entities. The triplets within these paths serve as supporting facts for the story, augmented with additional \"\"extra triplets\"\" to increase task complexity. Third, for Text Generation, scene descriptions are generated from the selected story triplets using a context-free grammar (CFG) from SPARTQA-AUTO, enhanced with an extended vocabulary of entity properties and diverse spatial relation expressions (e.g., \"\"above,\"\" \"\"over,\"\" \"\"north\"\" for ABOVE). This flexible generation produces a richer corpus. Fourth, for Finding Answers, the spatial reasoner identifies the final answer by searching entities based on their descriptions in the story. Finally, for SpRL Annotations, the described spatial configurations are automatically annotated with spatial roles (trajector, landmark, spatial indicator) and relations (spatial type, triplet, entity IDs), providing free annotations for the SPRL task based on a pre-existing annotation scheme.": 824,
    "The framework addresses this by implementing a multi-step process. First, in the Query Formulation step, the specific need for recognizing named entities is translated into simple natural language questions. A template \"\"Which [TYPE]?\"\" is used, where [TYPE] is substituted by the desired entity type (e.g., \"\"Which disease?\"\"). This question is then encoded into a question vector using a question encoder. Second, in the Retrieval step, an open-domain question answering (QA) model, specifically a phrase retrieval model called DensePhrases (Lee et al., 2021a), is employed. This model takes the encoded question vector and retrieves relevant phrases (candidate entities) and their corresponding evidence sentences from a large-scale open-domain corpus, such as Wikipedia. The retrieved phrases form a pseudo-dictionary (˜V), and the sentences containing these phrases become the unlabeled training sentences (˜Xtrain). The system retrieves the top `k` unique sentences for each sub-question that contain each phrase, gathering a total of `k1 + ... + kL` sentences across all `L` sub-questions. This process effectively mines potential entities and their contexts without requiring pre-existing domain-specific dictionaries or human-annotated training data.": 825,
    "The FigMemes dataset is constructed by systematically crawling memes from the 4chan/pol/ (politically incorrect) board between January 2017 and December 2021. Unlike previous approaches, no keywords are used to filter the data, ensuring a natural and wide range of topics and visual styles. Duplicate images are removed using the difference hash (dHash) followed by manual inspection, and sexually-explicit content is manually removed. For annotation, a simple interface built with Python PyQt5 is used, applying a black and white filter to protect annotators. A data-driven approach identifies suitable labels: 200 random memes are free-text annotated for common figurative language, prioritizing previously studied types. Overlapping categories are then grouped, resulting in six major categories: Allusion, Exaggeration/Hyperbole, Irony/Sarcasm, Anthropomorphism/Zoomorphism, Metaphor/Simile, and Contrast. The task is defined as multi-label classification: \"\"What are the types of figurative language used in this meme?\"\". Three annotators provide labels, with the majority vote determining the final label. Text within memes is extracted using the Google Vision API. The final dataset contains 5141 memes, with 70% annotated with at least one figurative language type and 8.5% being image-only. The dataset is randomly partitioned into train, validation, and test sets with proportions of 60%, 10%, and 30% respectively, referred to as the standard evaluation split.": 826,
    "The paper extends Cynical Data Selection (CynDS), an existing statistical sentence scoring method, to the document level. Originally, CynDS greedily ranks sentences from a general text corpus by computing their score against a representative target domain corpus. This score is based on the information gained by selecting the sentence, using the cross-entropy of the selected text against the representative text. The method also biases towards shorter sentences and those containing words with high probability in the representative text. To extend this to documents, sentences within each document are still scored individually using the original CynDS formula, which calculates the delta (effect) of a given sentence on the cross-entropy. However, instead of selecting individual sentences, the *average sentence-level gain* (the average of these delta scores for all sentences within a document) is computed to determine the overall information gain of that document. These document-level scores are then used to rank all documents in the large general corpus, allowing for the selection of the top-k percentage of documents.": 827,
    "The paper proposes LAPA (Learning to Adapt to Low-Resource Paraphrase Generation), a three-stage learning paradigm that combines pre-trained language models (PLMs), adapter modules, and meta-learning. The first stage involves pre-training a backbone model, specifically BART (Lewis et al., 2020b), on large unsupervised corpora (Dpre) to acquire basic language knowledge, resulting in parameters denoted as θpre. This step establishes a strong foundation of general linguistic understanding.\nIn the second stage, an adapter layer is inserted into each transformer layer of the pre-trained BART model. An adapter layer is a bottlenecked feed-forward network consisting of a down-project layer, a non-linearity function (ReLU), and an up-project layer, along with a skip connection from input to output. During this meta-training stage, the backbone model's parameters (θpre) are frozen, and only the newly inserted adapter layers' parameters (Φ) and normalization layers are trainable. This stage uses MAML (Model-Agnostic Meta-Learning) on related source corpora (Dsrc) to help the adapters learn an optimal initialization (Φsrc) suitable for the paraphrase generation task. MAML's objective is to find initial parameters that can quickly adapt to new tasks with a few gradient steps. By freezing the majority of the PLM parameters and only training the small adapter layers, the approach retains the prior knowledge of the PLM, preventing negative transfer effects and mitigating overfitting, especially when the training data is scarce. This smaller-scale parameter updating also helps prevent gradient explosion or diminishing problems often associated with MAML in deeper models or with scarce data.\nFinally, in the third stage, the adapter model is initialized with the meta-learned parameters ([θpre, Φsrc]). This model is then fine-tuned on a small amount of target domain labeled data (Dtgt). Similar to meta-training, only the adapter layers' parameters (Φ) are updated, while the backbone PLM parameters (θpre) remain frozen. This fine-tuning process quickly adapts the model to the specific target task, resulting in the final target model f[θpre, Φtgt]. This three-stage process ensures that the model first learns general language knowledge, then the fundamental paraphrase generation task from source domains, and finally adapts efficiently to the specific target domain with minimal data.": 828,
    "The Unsupervised Non-Transferable Learning (UNTL) approach trains a model to perform well on a source domain while degrading its performance on an unlabeled target domain. It utilizes a BERT-based model, denoted as `ψ`, as a feature extractor, where the final hidden state of the `[CLS]` token serves as the feature representation. A simple feed-forward network (FFN) acts as the classifier for predicting labels. The overall objective function for UNTL is `LUNTL = LCE + β·LDC + λ·LMMD`.\nThe `LCE` (Cross-Entropy Loss) term, defined as `E(x,y)∼Ds[CE(FFN(ψ(x)),y)]`, is applied to the source domain dataset (`Ds`) to ensure the model maintains good performance on authorized data.\nTo enlarge the distance between the feature representations of the source and target domains, two additional loss terms are employed. The `LMMD` (Maximum Mean Discrepancy) loss, formulated as `−min(c,dS,T)`, aims to maximize the discrepancy (`dS,T`) between the source (`S`) and target (`T`) data distributions. The `dS,T` metric is calculated as `||Ex∼p[ψ(x)]−Ex′∼q[ψ(x′)]||2Hk`, where `Hk` is a reproducing kernel Hilbert space with a Gaussian kernel `k(z,z′)=e−||z−z′||2`. The negative sign and the `min(c, dS,T)` function ensure that the distance is maximized only up to an upper bound `c`, preventing it from dominating the entire loss.\nAdditionally, the `LDC` (Domain Classification) loss, defined as `ExS∼S[CE(FFNdc(ψ(xS)),0)]+ExT∼T[CE(FFNdc(ψ(xT)),1)]`, introduces an adversarial component. A separate feed-forward network (`FFNdc`) functions as a domain classifier, trained to distinguish between source (labeled 0) and target (labeled 1) domain representations. By optimizing this loss, the model is encouraged to learn more distinct feature representations for different domains, thereby facilitating non-transferable learning by clearly drawing a boundary between them. The hyperparameters `β` and `λ` control the scaling of the `LDC` and `LMMD` terms, respectively.": 829,
    "The paper proposes the Transformational Biencoder (T-Biencoder) which integrates learnable transformation matrices into the BERT architecture of the Biencoder. The standard Biencoder's BERT encoders, E_theta_m for mentions and E_theta_e for entities, are conceptually split into early (E_theta1_m, E_theta1_e) and later (E_theta2_m, E_theta2_e) transformer layers. The early layers, E_theta1_m and E_theta1_e, are responsible for mapping the mention `m` and entity `e` into a common representation space `Z`, yielding `Zm` and `Ze` respectively.\nTo address the domain shift, learnable transformation matrices, `Am` for mentions and `Ae` for entities, are introduced. These matrices are applied to the representations `Zm` and `Ze` to produce transformed representations `Z'm` and `Z'e` using the additive transformation: `Z'm = Zm + AmZm` and `Z'e = Ze + AeZe`. The purpose of `Am` and `Ae` is to approximate the ideal transformations (denoted as `bar_Am` and `bar_Ae`) that would shift the source domain distribution to the target domain distribution.\nThese transformed representations, `Z'm` and `Z'e`, are then fed into the later encoder layers, E_theta2_m and E_theta2_e, to generate the final representations `H'm` and `H'e`. The model is trained using a total loss function, `L_total`, which is formulated as `min_theta_m,theta_e [L(theta_m,theta_e) + max_||Am||,||Ae||<=epsilon L'(theta_m,theta_e,Am,Ae)]`. Here, `L` is the standard Noise-Contrastive Estimation (NCE) loss, and `L'` is a transformational loss function that follows the same definition as `L`. The `max` term in `L_total` is crucial: it aims to find the \"\"worst-case\"\" transformation matrices `Am` and `Ae` within a norm constraint `epsilon`. By optimizing against these worst-case transformations, the model (specifically, the encoders E_theta_m and E_theta_e) is encouraged to learn representations that are robust and generalize well even under domain shifts. This ensures that if the encoders can perform well with representations transformed by `Am` and `Ae`, they can also handle representations transformed by the true, unknown domain shifts (`bar_Am` and `bar_Ae`). Importantly, during inference, the transformation matrices `Am` and `Ae` are not applied; mentions and entities are directly fed into the encoders E_theta_e and E_theta_m, maintaining efficiency.": 830,
    "The solution involves constructing label graphs for both the source and target domains, where each graph represents the label relationships as a probability distribution. These graphs are then integrated into the word embeddings produced by BERT. This integration allows the model to leverage the structured information from the label graphs, enhancing the contextual representation of words and thereby improving the adaptability of the NER model across domains with different entity types.": 831,
    "The mixture model mechanism assumes the target corpus is generated from a mixture of two unigram language models: a known background model computed from the context corpus and a target domain model that needs estimation. The log-likelihood value (LLV) of generating the corpus from this mixture model is calculated. The Expectation-Maximization (EM) algorithm updates the probabilities to estimate the target domain model. This process ensures that the generated distribution highlights distinctive keywords of the target domain.": 832,
    "The paper addresses this by constructing and encoding a heterogeneous graph for each input sentence. First, input-side features are identified: Part-of-Speech (POS) tags are used to distinguish nouns from other words, dependency trees provide long-range word-to-word dependencies, and word definitions (for nouns) are obtained via a word sense analyzer. A heterogeneous graph G=(V,E) is then built. The node set V includes word nodes (nouns and non-nouns, differentiated by POS tags) and two subsentence nodes, corresponding to the left and right parts of the sentence split by the comparator word. The edge set E consists of two main types: \"\"sd-edges\"\" (dependency arcs), which capture syntactic relationships, and \"\"ns-edges\"\" (noun-subsentence edges), which connect noun nodes to subsentence nodes, labeled as \"\"con\"\" or \"\"not-con\"\" based on whether the subsentence contains the noun. The initial state for word nodes is derived from their corresponding BERT output, while subsentence nodes are initialized by average pooling over the hidden states of words within that subsentence. Edge labels are randomly initialized. The graph is then encoded using multiple Graph Attention Network (GAT) layers, which sequentially conduct graph attention and gating mechanisms to update node states. This process aggregates information from neighboring nodes based on attention scores, effectively merging the heterogeneous input-side features into rich node representations.": 833,
    "The system identifies and selects two key elements: a context word that supports the meaning of the pun word (pw) and a phrase that is characteristic of the alternative word (aw) while remaining compatible with the pun word. For phrase selection, given a pun word-alternative word pair (pw-aw), the system first extracts multiple phrases (N1=20) containing the alternative word from a large non-pun corpus (Wikipedia and Gutenberg Book Corpus). These phrases are then ranked by how well they exhibit the semantics of the pun pair. This is achieved by replacing the alternative word with a `<mask>` token and using a RoBERTa-Large model to obtain the probability of the alternative word in the masked position. Less probable phrases are filtered out to ensure they are characteristic of the alternative word. Subsequently, a similar mask-infilling procedure is performed for the pun word, and a middle-ranking phrase is selected to avoid phrases that are too general or too incompatible with the pun word. This two-step ranking process ensures the selected phrase elicits surprise when the pun word is substituted for the alternative word, yet still makes sense. For context word selection, the system retrieves sentences containing the pun word from the same non-pun corpus. Keywords are extracted from these sentences using RAKE (Rapid Automatic Keyword Extraction). Based on their TF-IDF values, the top N2 (N2=20) words that uniquely co-occur with the target pun word are identified, and one is randomly sampled to encourage creativity.": 834,
    "The paper proposes a targeted regularization technique within the OPTIMA framework. This technique encourages a smooth decision boundary only in areas where the source-domain and target-domain data are similar. This is achieved by defining a perturbation $\\delta^*$ that maximizes a combined objective function. This objective includes a KL divergence term, $\\ell_{KL}(\\delta, p, x_s)$, which measures how much the model prediction changes when the perturbation $\\delta$ is applied to a source input $x_s$, thereby capturing the smoothness of the decision boundary. Additionally, it includes an adversarial loss term, $\\ell_{adv}(\\delta, x_s)$, which, when maximized, causes a domain discriminator network (parameterized by $\\theta_d$) to mistake the perturbed source example $x_s + \\delta$ as coming from the target domain. By maximizing $\\ell_{adv}$, the perturbation $\\delta^*$ is constrained to regions where data from the two domains are similar. The optimization of $\\delta^*$ is performed using Projected Gradient Ascent (PGA), ensuring $\\delta^*$ remains within an $\\epsilon$-radius ball. This dual objective for $\\delta^*$ ensures that the smoothness constraint is applied precisely where the domains overlap, making the regularization targeted and effective for domain adaptation.": 835,
    "The paper develops a unified user simulator by pre-training a T5 (text-to-text transformer model) backbone model on a collection of publicly available task-oriented dialogue datasets. This pre-training process, termed Domain Adaptive Pre-training (DAPT), is conducted in two stages to enhance knowledge transfer. First, the user simulator is trained on datasets that lack user goal annotations (the \"\"Top group\"\" in Table 3). Second, it continues training on datasets that include user goal or user dialogue acts annotations (the \"\"Middle group\"\"). For datasets without explicit user goal annotations, user dialogue acts are automatically deduced and converted into structured goals, which are then transformed into natural language templates. To increase language diversity and generalization ability, these template-generated goals are further paraphrased using an online paraphrasing tool. The user simulator is trained to decode a goal-grounded response auto-regressively, taking as input the concatenation of a task description (auto-generated from domain names), the user goal, and the dialogue context (utterances up to the current turn). After this extensive pre-training, the model is fine-tuned on a small amount of target domain data (e.g., 5% or 10% of the original training set) to quickly adapt to the specific target scenario.": 836,
    "The paper addresses this by introducing a novel Label Encoder and a Bi-Attention module. First, a randomly initialized label lookup table (U) is constructed, where each unique label in the source or target domains is associated with an embedding vector. For a given token `xi`, a bidirectional LSTM (Bi-LSTM), denoted as `fre(·)`, is employed as the Label Encoder to process the sequence of embeddings of previously predicted labels (`y1:i-1`). The output of the Bi-LSTM for each previous label `k` is `ek`, which captures the contextual information of the previous labels.\nNext, a Bi-Attention module is used to merge the token representation (`hi`) from a pre-trained input sequence encoder (e.g., BERT) with the label features (`ek`) from the Bi-LSTM. This module performs two types of attention to generate label-aware information (`zi`).\n1.  Label Background Information (`hbi`): The last hidden state of the Bi-LSTM (`ei-1`) is used as a query vector. This `ei-1` is first projected into the same dimension as the token representations (`hi`) using a fully connected layer (`e'i-1 = W2 · ei-1 + b2`). Attention weights (`abi`) are then computed between `e'i-1` and all token representations (`h1:N`) from the input sequence encoder using a softmax function. A weighted sum of these token representations, guided by `abi`, produces `hbi`. This `hbi` represents the relationship between the current token's label and the entire input sequence.\n2.  Label Context Information (`eci`): To capture the relationship between the current token `xi` and previously predicted labels (`y1:i-1`), a comprehensive intermediate state (`h'i`) is formed by concatenating the current token representation (`hi`) and the label background information (`hbi`), followed by a linear transformation (`h'i = W3 · (hi ⊕ hbi) + b3`). An attention mechanism then computes weights (`aci`) between `h'i` and the outputs of the Bi-LSTM for previous labels (`e1, ..., ei-1`). A weighted sum of these `ek` vectors, guided by `aci`, yields `eci`. This `eci` represents the context derived from previous labels relevant to the current token.\nFinally, the label background information (`hbi`) and label context information (`eci`) are concatenated to form the final label-aware information (`zi = hbi ⊕ eci`). This `zi` is then concatenated with the current token representation `hi` to form a unified representation `ui = hi ⊕ zi`, which is then used by a label predictor to determine the NER label for `xi`.": 837,
    "To generate fine-grained relevance signals for synthetic query-passage pairs, the Generative Pseudo Labeling (GPL) method employs a \"\"Pseudo Labeling via Cross-Encoder\"\" module. For each generated query, a positive passage (the original passage from which the query was generated) and multiple negative passages (hard negatives retrieved by an existing dense retrieval model) are identified. An existing, pre-trained cross-encoder (specifically, `ms-marco-MiniLM-L-6-v2` trained on the MSMARCO dataset) is then used to score each (query, passage) pair. A cross-encoder is a model that takes the concatenation of a query and a passage as input and predicts a relevance score, leveraging cross-attention between the two. The fine-grained relevance signal, or pseudo-label, is derived as the margin (δ) between the cross-encoder's score for the (query, positive passage) pair and its score for a (query, negative passage) pair, calculated as δ = CE(Q, P+) - CE(Q, P-), where CE is the cross-encoder's score, Q is the query, P+ is the positive passage, and P- is the negative passage. These continuous pseudo-labels provide a more nuanced understanding of relevance compared to simple binary labels, allowing for more robust training.": 838,
    "The Differential Scaling (DS) module determines how best to vary the output of the Emotional Variance Adaptor (EVA) to bring about the desired change in emotion. This is achieved by estimating the difference in variances necessary for a pronounced effect of the target emotion relative to its neutral counterpart. The DS module facilitates linear transformations in the VAD space and ensures strong disentangling of emotional prosody from other acoustic features. The process involves predicting intermediate features for a given phoneme sequence at two different sets of VAD values—one for neutral emotion and one for the target emotion—and taking the difference of these predictions as the direction for varying variances.": 839,
    "The paper proposes the Abundant Information Slot Filling Generator (AISFG), a generative template-based zero-shot slot filling framework that frames slot filling as a conditional sequence generation task. This task is solved in a sequence-to-sequence manner, utilizing a pre-trained generative language model, specifically BART (Lewis et al., 2020), as its backbone. The core idea is to make the slot filling task consistent with the pre-training objectives of the pre-trained language model (PLM) by generating responses in a natural language style. During training, for an input sentence with multiple slots, the system constructs a training pair consisting of a natural language query and a corresponding natural language response. The response is constructed by concatenating the target entities with commas (e.g., \"\"y1,y2,...,yi\"\"). The model is then fine-tuned to maximize the log likelihood for predicting these gold responses. In the inference stage, a query is built for each candidate slot, and the response is generated in an auto-regressive manner, where the model selects the token with the highest probability at each time step. Although the model does not explicitly restrict response tokens to originate from the input sentence, it generally extracts them from the input. Finally, the generated response is split by commas to recover the original format for evaluation.": 840,
    "The paper addresses this by employing a \"\"Domain-Specific Segment Mask\"\" module. First, it extracts domain-specific segments from both the labeled source dataset (DS) and the pseudo-labeled target dataset (DTP). This is achieved using a frequency-ratio method, where an n-gram segment `w`'s relative frequency `s(w, Dv)` in a dataset `Dv` is calculated based on its frequency in `Dv` versus its frequency in other domains, smoothed by a parameter `λ`. Segments are filtered based on a specified threshold `δ` to identify those occurring more frequently in one domain, thus defining them as domain-specific features. Once the set of domain-specific segments `M` is obtained, a Forward Maximum Matching algorithm is used to identify and replace these segments within a given review sentence with `[mask]` tokens. If any word of a domain-specific aspect or opinion phrase is matched, the entire phrase is masked. This process transforms a sentence `X` with its label sequence `L` into a masked tuple `(˜X, L)`, where `˜X` is the masked sentence.": 841,
    "The paper addresses this by introducing Fig-QA, a Winograd-style non-literal language understanding task. This benchmark consists of 10,256 paired figurative phrases with divergent meanings and their corresponding literal implications. The dataset was crowdsourced using Amazon Mechanical Turk (MTurk) workers from the United States with high approval ratings. Workers were specifically instructed to generate \"\"rare or creative metaphors\"\" that would not commonly appear in existing text corpora but could still be easily understood by humans. To guide workers, examples of valid and invalid pairs were provided, and the task employed principles from Cognitive Load Theory, such as \"\"randomness as genesis\"\" and \"\"narrow limits of change,\"\" by showing workers three random words (selected based on metaphorical frames from Lakoff and Johnson) to encourage diversity, though their use was not mandatory. The collected data underwent a rigorous manual validation process by three authors, where examples were excluded if they were nonsensical given the figurative expression, contained significant grammar or spelling errors, or did not adhere to the specified format. This meticulous creation and validation process ensured the novelty and quality of the figurative expressions, making it suitable for testing advanced language understanding.": 842,
    "The paper addresses the scarcity of hyperbolic data by constructing HYPO-XL, a large-scale English hyperbole corpus, through a two-step weakly supervised process. First, a BERT-based binary classifier is fine-tuned on the existing, smaller HYPO dataset to detect hyperbole. This initial model is then used to retrieve a large set of \"\"pseudo-hyperbolic\"\" sentences from Sentencedict.com, an online sentence dictionary. Second, to improve the precision of the detection model, a subset of 5,000 of these pseudo-hyperbolic sentences is randomly sampled and manually labeled by human annotators, forming HYPO-L. Only sentences with unanimous judgments from two annotators are retained for reliability. The BERT-based detection model is then re-trained using this human-annotated HYPO-L data. Finally, this refined detection model is applied to the entire retrieval corpus again, and sentences with prediction probabilities for the positive class exceeding a certain threshold (0.8) are selected to form the HYPO-XL corpus, resulting in 17,862 hyperbolic sentences.": 843,
    "The system addresses the challenge of identifying monosemous words for each sense by leveraging a Reverse Dictionary (Qi et al., 2020; Zhang et al., 2020). Given the two sense definitions (S1 and S2) of a target pun word (p), the Reverse Dictionary is utilized. This tool takes a descriptive query as input and generates multiple related words whose semantic meaning aligns with the provided description. For each sense definition, the system generates five such related words. This initial step is crucial for differentiating the two senses of the polysemous pun word by representing each sense with a set of words that are unambiguous in their meaning, thereby circumventing the inherent ambiguity of the pun word itself in subsequent processing steps.": 844,
    "The hierarchical adapter structure is built upon a tree where each node represents a set of adapter weights. During training, the adapters along the path leading to a leaf node are activated for specializing the language model to the corresponding domain. This structure allows parameter sharing among related domains, reducing redundancy and improving efficiency. The hierarchical tree is constructed either manually or automatically using unsupervised clustering methods based on Gaussian Mixture Models (GMMs). For inference, the hierarchical structure enables averaging over multiple paths through the tree, enhancing generalization for unseen domains.": 845,
    "The paper proposes MANNER, which introduces an external memory module (M) to store token representations of entity types from the source domain. This memory module contains key-value pairs where keys are entity types and values are token representations belonging to those types. For each entity type `k` in a target domain few-shot task `τ`, MANNER first retrieves the most similar entity types `k*` from the memory. This retrieval is based on the Optimal Transport (OT) distance between the contextualized representations of tokens for entity type `k` in the support set (`Hk`) and the token representations of entity type `k'` stored in memory (`Mk'`). The OT distance `W(Mk', Hk)` is computed as the minimum cost `⟨C,T⟩` over joint probabilities `T`, where `C` is a cost matrix based on the squared Euclidean distance between token representations. After identifying `k*`, the retrieved information `Mk*` is then adapted to the target domain using optimal transport (as detailed in Solution 2). Finally, this adapted memory information (`ˆHk`) is combined with the token representations from the support set (`Hk`) to infer the prototype distributions for entity type `k`. This augmentation provides additional background knowledge, making the prototypes more accurate and representative than those learned from the support set alone.": 846,
    "The paper proposes a Slot Prompt Generation mechanism that creates a slot-specific prompt (S) by fusing information from the slot's natural language description with a global prompt. Instead of simply concatenating the description to the input, which often leads to superficial understanding in zero-shot settings, this method uses a cross-attention mechanism. The global prompt (G), a learnable vector of size N x d (where N is empirically set to 10 and d is the model dimension), acts as the query. The embedding of the slot description (E), of size K x d (where K is the length of the slot description), acts as the key and value. The calculation is performed as S = ((G Wq)(E Wk)⊤)(E Wv), where Wq, Wk, and Wv are query, key, and value weight matrices for the cross-attention mechanism. This approach ensures that all slot prompts share the same initial structure from the global prompt, promoting stable training, while the cross-attention allows the slot description to modify this global prompt, reflecting changes specific to the slot label and addressing domain shift. A key advantage is that the final slot prompt (S) has a fixed length (N x d), independent of the varying lengths of slot descriptions.": 847,
    "To construct a high-quality, fine-grained annotated dataset named GraCe, the paper employs a four-step pipeline. First, in Step 1: Rule-based Filtering Method, candidate sentences are tokenized using Jieba and filtered to retain only those containing comparator-related words, which are considered the hallmark of a simile. In Step 2: Model-based Filtering Method, a binary classifier based on RoBERTa Large is trained to identify and filter out non-simile sentences that might still contain comparator words but do not express a true simile (e.g., literal comparisons or personified sentences). This step ensures that only genuine similes are retained. Step 3 focuses on labeling the core simile elements: a sequence labeling model, also based on RoBERTa Large and trained on the CCL2018 dataset, is used to automatically annotate the tenor and vehicle within each simile. The topic is then annotated as the span between the tenor and comparator, representing the tenor and its supplementary description. Finally, Step 4 aims at annotating the ground and cognitive properties of the tenor and vehicle. This involves querying the Cogbank dataset to obtain cognitive properties for both the tenor and vehicle. Their shared properties are then used to fuzzily match property-related clauses within the simile, identifying them as the explicit ground. If the ground is implicit, the shared cognitive properties between the tenor and vehicle help interpret their relationship. This meticulous process expands the annotated elements from three to eight, including context, ensuring a comprehensive and high-quality dataset for controllable simile generation.": 848,
    "The paper addresses this by proposing a meta-adaptation framework, specifically a bi-level optimization problem, to learn an optimal initial parameter set (θ) for the misinformation detection model. In the inner-level optimization, the initial model (parameterized by θ) is updated upon multiple batches of sampled source data examples, referred to as \"\"source tasks.\"\" This update uses a first-order gradient descent step: `ϕ = θ - α∇θL(θ, X)`, where `α` is the task learning rate and `ϕ` represents the updated parameters for a specific source task. In the outer-level optimization, these updated models (`ϕ`) are evaluated on the few-shot target examples, termed the \"\"metatask\"\" (X't), to compute a \"\"metaloss.\"\" The framework then derives second-order meta-gradients with respect to the original initial parameters (θ) by differentiating through the inner-level optimization process. This allows the model to \"\"learn to adapt\"\" by finding an initial parameter set that, when fine-tuned with a small number of target examples, yields optimal performance on the target domain.": 849,
    "The paper applies contrastive learning to generate distinct representations for figurative and literal senses of non-compositional expressions. For a given non-compositional expression, its figurative and literal meanings are considered semantically different. The approach defines positive pairs as sentences containing the same non-compositional expression used in the same sense (either both figurative or both literal) and negative pairs as sentences containing the same expression used in different senses (one figurative, one literal). The model is trained using a contrastive objective, denoted as L_cts, which aims to pull the embeddings of positive pairs closer and push the embeddings of negative pairs further apart in the embedding space. Specifically, for an anchor example Y_i, a positive example Y_i+, and a negative example Y_i-, the contrastive loss L_cts is formulated as the negative logarithm of the ratio of the distance function f(x_i, x_i+) to the sum of f(x_i, x_i+) and f(x_i, x_i-), where x represents the embedding of the sentence. This objective is combined with a standard cross-entropy classification loss (L_cls) based on the ground truth label of the expression's sense in Y_i, forming the final loss L = L_cts + L_cls. This training process encourages the model to group embeddings of the same sense together while separating those of different senses, thereby creating two distinct clusters in the embedding space for the figurative and literal meanings.": 850,
    "The paper addresses this by building an LLM-based synthesizer that adapts to the target domain and automatically generates large quantities of relational training data. The process involves two critical choices: 1) linearizing relational statements into natural language sequences where entity pairs are indicated by special marker tokens (e.g., `[Sub]`, `[\\Sub]`, `[Obj]`, `[\\Obj]`); and 2) resorting to unconditional generation instead of label-conditioned ones. This unconditional approach relaxes the requirements for strict label-semantic correspondence and increases sample availability and diversity. For standard-sized generative LLMs like GPT-2, the synthesizer is first fine-tuned on a few training instances to adapt to the target domain. For very large LLMs like GPT-3.5, In-Context Learning is directly applied by prepending 5-shot exemplars and providing specific instructions to generate more examples with domain, format, and diversity constraints. After fine-tuning or in-context learning, the LLM is prompted to generate synthetic data, which is then filtered to ensure it conforms to the relational structure (e.g., presence of four exact special markers and correct marker order).": 851,
    "The paper addresses this question by employing GPT-3, specifically the davinci-002 and curie-001 model variants, to predict the source domain of a metaphor given a natural language sentence and its target domain. The task is framed as a text-to-text generation problem, where the model completes a prompt to identify the source domain. For example, given \"\"Sentence: You are wasting my time\"\" and \"\"Target Domain: Time\"\", the model is expected to generate \"\"Source Domain: <<model completion>>\"\", with \"\"TIME IS MONEY\"\" or \"\"TIME IS A RESOURCE\"\" being correct predictions.\nTwo primary strategies are used to elicit predictions from GPT-3:\n1.  Few-shot prompting: The model is provided with a prompt that includes a varying number of labeled examples (2, 4, 6, 8, or 12) of correct source domain mappings before the new sample to be predicted. In this approach, the model's weights remain unchanged; it leverages the in-context examples for guidance. The temperature parameter for text generation is set to 0, ensuring deterministic and repeatable outputs by always generating the most likely next word.\n2.  Fine-tuning: The model's weights are optimized by training it on input/output task samples. Two fine-tuning variants are explored:\n    *   One variant is fine-tuned using all 132 sentences from the training set.\n    *   Another variant is fine-tuned using a smaller subset of 34 sentences from the training set, ensuring one sample per unique source domain.\n    After fine-tuning, the model does not require few-shot examples in the prompt and can directly classify a sample from the validation set.": 852,
    "The paper unifies various aspect-based sentiment analysis (ABSA) tasks into a sequence-to-sequence format, referred to as the text-to-label direction. This approach takes a sentence as input and outputs a sequence of sentiment tuples extracted from that sentence. To ensure a valid and unambiguous output format, predefined tagger tokens are used, such as `<aspect>`, `<opinion>`, `<pos>`, `<neu>`, and `<neg>`. For example, in Aspect Sentiment Triplet Extraction (ASTE), the format is `<pos> aspect <opinion> opinion`, where `aspect` and `opinion` are the extracted terms. The model is trained on labeled source domain data (DS) by minimizing the standard maximum likelihood loss. After training, it performs inference on unlabeled target domain data (DT) to extract sentiment tuples, denoted as `ˆyT`. During inference, constrained decoding is employed to ensure that each generated token is either from the input sentence or one of the predefined tagger tokens, preventing invalid outputs and ensuring relevance to the specific domain. This initial prediction, though potentially noisy due to the domain gap, serves as the input for the subsequent stage.": 853,
    "The overall quality of generated similes is measured by combining three sub-criteria: relevance, logical consistency, and sentiment consistency.\n1.  Relevance (r): This score quantifies how relevant the topic and vehicle components of a simile are, based on their tendency to co-occur in simile sentences and share properties. It leverages a large-scale probabilistic simile knowledge base, MAPS-KB (Million-scale Probabilistic Simile Knowledge Base), which contains millions of simile triplets (topic, property, vehicle) along with frequency and plausibility metrics. The relevance score for a simile is calculated as the average plausibility of all (topic, property, vehicle) triplets extracted from it, where plausibility indicates the probability that the topic and vehicle share the property. An approximation method also allows calculating relevance based on the frequency of topic-vehicle pairs in large-scale simile sentences.\n2.  Logical Consistency (cl): This criterion assesses whether the generated simile maintains the original sentence's semantics without contradiction. It uses a pre-trained Multi-Genre Natural Language Inference (MNLI) model. The literal sentence and the generated simile are input as a sentence pair into the MNLI model, which determines their relationship (entailment, neutral, or contradiction). The logical consistency score is defined as 1 minus the probability that the model predicts the relationship to be a contradiction.\n3.  Sentiment Consistency (cs): This measures if the generated simile enhances or maintains the sentiment polarity of the original sentence. A model fine-tuned on the GLUE SST-2 dataset is used to classify the sentiment (positive or negative) of both the literal sentence and the simile. The sentiment consistency score is calculated as the probability that the simile's sentiment matches the literal sentence's sentiment, minus the probability that the literal sentence's sentiment matches its own predicted sentiment. To handle sentences with multiple topic-vehicle pairs, only the text up to the first vehicle in the simile or the first event in the literal sentence is input into the sentiment model.\nFinally, for a set of simile candidates generated from a single literal text, the individual relevance, logical consistency, and sentiment consistency scores are normalized to range from 0 to 1. The final quality score (Q) for each simile is then computed as a weighted combination of these normalized scores: Q = α * r' + β * cl' + γ * cs', where α, β, and γ are hyperparameters.": 854,
    "The paper proposes a Generalizability Test to assess the type and degree of diversity in a new domain using only a few examples from the target domain as an evaluation set. This test conceptualizes adapting or fine-tuning a pre-trained source model within a Bayesian framework, where the source model acts as a prior. The test characterizes the shift in Open-Domain Question Answering (ODQA) as a two-step process:\n1.  Input/Retriever Distribution Assessment: This step determines if the input distribution contains informative signals with respect to the target evaluation set. It computes the joint question and context distribution using un-normalized (energy) scores from a dense retriever, which quantifies compatibility between a question (q) and a context (c) via R(q,c). These scores are normalized over a subset of contexts from the entire corpus. For each question, the Wasserstein distance (wtu) is computed between the input distribution and a uniform distribution, and averaged over target evaluation examples. Similarly, the distance between the gold (oracle) distribution and the input distribution (wtg) is computed. If wtu > wtg, it is concluded that the target distribution is far from uniform and closer to gold, indicating compatibility of the source model with the target input distribution.\n2.  Output/Reader Distribution Assessment: This step tests whether the output distributions match by computing the likelihood of generating the oracle answer given a question (q) and relevant contexts (Cq). The output distribution is computed using a sub-sample of answers (A). The Wasserstein distance (vtu) between the uniform and output distribution is averaged over the target evaluation set. To address the empirical finding that output distribution is always closer to uniform than oracle, a reference answer conditional distribution is used to de-bias likelihood scores with a threshold. This threshold is obtained by considering the source distribution as a reference and computing the distance between the output distribution evaluated on source evaluation examples and the uniform distribution (vru). If vru - vtu is close to 0, it is assessed that the target is far from uniform and the source model is compatible with the target dataset.\nThese two assessments (input and output distribution compatibility) are then combined into a decision tree (Figure 2) to identify the type of dataset shift: No shift, Concept shift, Covariate shift, or Full shift.": 855,
    "The paper proposes a novel Domain-Adaptive Language Model (DALM) that unifies the process of data generation and fine-grained annotation. For each training sample, the DALM takes two input sequences: an input token sequence and an input label sequence. The input token sequence `xin` is constructed by inserting a special `⟨BOS⟩` token at the beginning, followed by a domain-specific token (e.g., `[source]` or `[target]`) to distinguish the domain, and then the sentence tokens. The input label sequence `yin` starts with `⟨BOL⟩` (initial state of label sequence), followed by `y⟨BOS⟩` (which is 'O'), and then the labels of the preceding tokens in the sentence.\nA decoder, such as an LSTM or a pre-trained GPT-2 model, processes the input token sequence `xin` to obtain token representations `ewt`. Simultaneously, a label embedding layer processes the input label sequence `yin` to obtain label representations `eyt`. At each timestep `t`, the token representation `ewt` and the label representation `eyt-1` (from the previous timestep) are added together to produce a token and label-aware representation `et`. This combined representation `et` is then fed into two distinct full-connected softmax layers. One softmax layer predicts the probability of the next token `wt+1` (P(wt+1|w≤t,y≤t-1)), and the other simultaneously predicts the probability of the current token's label `yt` (P(yt|w≤t,y≤t-1)). The model is optimized by minimizing the combined cross-entropy losses for both the output token sequence and the output label sequence. This unified architecture allows the DALM to learn the shared distribution of words and labels across domains, enabling it to generate both tokens and their corresponding fine-grained labels simultaneously.": 856,
    "The paper proposes a novel modularization parameterization inspired by sparse Transformer models to mitigate task interference. Instead of sharing a single text encoder across all tasks, which can lead to negative transfer, the Chain-of-Skills (COS) model mixes skill-specific Transformer blocks with shared ones. A typical Transformer block consists of a Multi-Head Attention (MHA) sub-layer and a Feed-Forward Network (FFN) sub-layer. While previous mixture-of-expert models often specialize the FFN sub-layer, the authors hypothesize that FFN specialization hinders knowledge sharing because most reasoning skills require similar world knowledge. Instead, they propose and investigate MHA specialization. In this approach, for skill-specific Transformer blocks, a specialized MHA sub-layer is selected from a pool of parallel sub-layers based on the input skill, meaning different skill inputs are processed independently by their dedicated MHA experts. This design is found to be more effective in reducing task interference and is more parameter-efficient. The expert configuration is adjusted from a naive setup (which would have 8 experts) to save computation and address data sparsity. For example, the context expert for both single and expanded query retrievers is merged, and the expert for expanded queries and reranker inputs are merged due to their similarity. During self-supervised pretraining, the expert for single and expanded queries is further shared for efficiency.": 857,
    "The solution involves creating a model that learns the basic meanings of words directly from literal annotations provided in the training dataset. This model captures the fundamental, non-metaphorical meanings of words, which are then stored as reference points. When a new sentence is processed, the model retrieves the basic meaning of the target word from its learned database and compares it with the word's contextual meaning within the sentence. This comparison helps in determining whether the word is used metaphorically.": 858,
    "The paper addresses this by introducing a novel Target-oriented Parse Tree Pruning Module. This module generates a flat, target-oriented tree structure through a three-step process:\n1.  Reshaping the original parse tree: The process begins by taking an ordinary dependency parse tree, which can be generated by existing parsers like spaCy or Biaffine.\n2.  Re-rooting the tree at the target word: Unlike standard parse trees rooted at the main verb or sentence head, this step re-roots the entire tree at the specific target word for which metaphoricity is being assessed. This re-orientation ensures that the focus of the tree structure is on the target word and its direct connections.\n3.  Pruning the tree based on neighbor range: After re-rooting, the tree is pruned. This pruning is performed according to the \"\"neighbor range,\"\" which is defined as the distance between leaves and the new root (the target word). Words outside a predefined `neighbor_range_con` threshold (e.g., 4) are discarded, retaining only the most semantically relevant neighbors of the target word. This resulting flat, target-oriented tree structure then facilitates a simpler encoding process for the downstream model.": 859,
    "The paper addresses this by introducing a Conceptual Encoder, which is a RoBERTa-based model specifically fine-tuned on the FrameNet dataset. The primary objective of this fine-tuning is to enable the model to classify semantic frame labels. Given an input sentence, the Conceptual Encoder leverages the CLS token's hidden state (hcls) to predict the overall sentence frame distribution (ˆyfcls) using a sigmoid activation function on a linear transformation (W0hcls+b0). Simultaneously, it uses the contextualized hidden states of the target word (H) to predict the target word's frame distribution (ˆyf) via a softmax activation on another linear transformation (W1H+b1). The model is trained by minimizing a combined loss function, L = λ ∗ Lcls + Ltarget, where Ltarget is the cross-entropy loss for target word frame prediction, and Lcls is the binary cross-entropy loss for sentence frame classification across all possible frame classes. This pre-training process allows the Conceptual Encoder to learn and provide rich, concept-level frame embeddings (hS,t, ht, hcls) that capture the deeper semantic information necessary for metaphor detection.": 860,
    "The paper proposes LitTER (Literal Translation Error Rate), a novel metric designed to automatically measure the frequency of literal translation errors. This metric overcomes the limitation of prior methods, which required hand-crafted blocklists, by automatically creating word blocklists for a given expression. The method is based on two key ideas:\n1.  Candidate Error Generation: For each word in an annotated source idiom span (s = ⟨s1, s2, ..., sN⟩), all its possible word translations into the target language are obtained using a bilingual word dictionary (specifically, MUSE (Lample et al., 2018) is used in this work). These translations form a candidate blocklist for each source word (bi = ⟨t1, t2, ..., tM⟩), resulting in a list of blocklists for the entire idiom (Bs = ⟨b1, b2, ..., bN⟩).\n2.  Blocklist Filtering: The generated candidate blocklists are then filtered using the reference translation (R). For each word in the reference translation, the system checks if it occurs in any of the blocklists (bi). If a reference word is found in a blocklist, that corresponding blocklist (bi) is removed from Bs. This step is crucial to prevent false positives, ensuring that the blocklist is not triggered when the correct translation of an idiom is, in fact, literal.\n3.  Error Checking: Finally, the hypothesis translation is checked to see if it contains any of the words remaining in the filtered blocklists. If any blocklisted word is found in the hypothesis, that hypothesis is marked as having a literal translation error. The final LitTER score is the percentage of translations that trigger the blocklist.": 861,
    "The paper addresses this by introducing the Semantic Jitter Suppression (SJS) module. This module first mimics noisy text embeddings, denoted as `g't`, by applying semantic jitters to the original clean text embeddings `gt`. This jitter is implemented as a linear perturbation function: `g't = m ◦ gt + δ`. Here, `m` is a binary masking operation (either word-level or channel-level) and `δ` is random noise. The SJS module then employs a learnable block, `φ(·)`, which is a cascaded transformer encoder followed by a global average pooling layer, to suppress this introduced jitter. This process generates a proper object query `q` for subsequent mask decoding, effectively training the R-VOS model to handle and recover from noisy or incomplete linguistic guidance.": 862,
    "The paper addresses the construction and integration of semantically rich contexts from an unlabeled target domain by employing a retrieval-augmented approach. Given an input sentence from the source domain (xS), the method first searches for semantically similar examples from the unlabeled target domain (DT). This process is analogous to retrieval and re-ranking based on a search query. For the Sentiment Analysis (SA) task, SimCSE (Simple Contrastive Learning of Sentence Embeddings) is used as the retrieval model, which produces semantically meaningful sentence embeddings, and cosine similarity is applied to retrieve the top-ranked (most similar) examples. For the Named Entity Recognition (NER) task, BERTScore is utilized, as it provides a metric for each sentence based on the similarity of token representations, which is more crucial for NER. Specifically, for a source sentence xS, the system retrieves top-k relevant chunks of texts from the target unlabeled dataset DT, denoted as xT = {xT1, ..., xTk}. These retrieved examples then serve as the context to enrich the semantics for the source input. For encoder-only models, these retrieved sentences are concatenated at the end of the source input as [xS; <SEP>; xT1; ...; xTk]. For decoder-only models, the retrieved target contexts (xT) are prepended before the source input (xS) within the prompt structure.": 863,
    "To reveal construction artifacts and biases, the paper designs and applies two incomplete information baselines using a BERT-base model. The default setting provides the model with the full sentence, where the potentially metaphorical expression (PME) is explicitly marked (e.g., \"\"The latest developments move us closer to a <PME>dark</PME> age.\"\"). The first baseline, \"\"Only PME,\"\" tests performance when the model only sees the PME itself (e.g., \"\"dark\"\"), completely hiding the context. The second baseline, \"\"Masked PME,\"\" tests performance when the PME is masked, but the context is provided (e.g., \"\"The latest developments move us closer to a <masked> age.\"\"). By comparing the performance of these baselines against the default setting, the paper assesses the extent to which models can identify metaphorical expressions without complete information, thereby indicating the presence and type of dataset biases. The BERT-base model is fine-tuned for binary classification by adding a classification layer on top of the pre-trained model. Hyperparameter optimization is performed using the Bayesian Optimization with Hyperband (BOHB) algorithm.": 864,
    "The systematic word meta-sense extension (SWORME) task is formally defined and operationalized through a process involving meta-sense definition, systematic alternation identification, meaning-based word type partitioning, and a token substitution evaluation task. First, a \"\"meta-sense\"\" is defined as a group of word senses sharing high-level semantic features. A \"\"meta-alternation\"\" occurs when a word form has senses from two meta-sense categories, and it is deemed \"\"systematic\"\" if a large set of words instantiate the same alternation. The CoreLex ontology, built on WordNet, is used as the meta-sense inventory, providing 39 basic meta-senses. WordNet synsets are mapped to CoreLex meta-senses, and this categorization is extended to verbs and adjectives by assigning them the same meta-sense label as their syntactic noun objects.\nTo operationalize meaning extension, a polysemous word type `w` that instantiates a systematic meta-alternation `(m, m')` is partitioned into two hypothetical tokens: `t(w,m)` and `t(w,m')`. `t(w,m)` replaces all mentions of `w` in a sense-annotated corpus that exhibit meta-sense `m`, and `t(w,m')` replaces `w` for sentences signifying meta-sense `m'`. This partitioning allows a language model to compute meaning representations for usages of `w` with meta-sense `m'` using `t(w,m')` without explicitly knowing that `w` can express `m'`.\nFinally, SWORME is evaluated as a token substitution task. Given a usage sentence `u` containing `t(w,m')`, the model is presented with a list of candidate paraphrase tokens `T`, which includes the ground-truth substitution `t(w,m)` and 99 negative alternatives. The model computes the contextualized embedding `h(t,u)` for each token `t` in `T` within context `u` (where `t(w,m')` is replaced by `t`). The model then chooses the best paraphrase token `t*` that minimizes the Euclidean distance `||h(t,u) - h(t(w,m'),u)||2`. The meaning extension of `t(w,m)` to `m'` is considered successful if `t*` equals `t(w,m)`.": 865,
    "The paper addresses this question through a Mutual Nearest Neighbor Contrastive Learning paradigm, which forms a core part of its geometric domain alignment strategy. First, a feature extractor, denoted as f(·), maps each code sample (xi) into an embedding representation (zi). To identify semantically consistent instances across domains, the method retrieves the k nearest neighbors for each target domain sample (xti) within the source domain (NNsi), and similarly, for each source domain sample (xsj), its k nearest neighbors (NNtj) are found in the target domain. A mutual nearest neighbor pair is then established: a source sample xsj and a target sample xti are considered mutual nearest neighbors if xsj is among xti's k nearest neighbors in the source domain, and xti is among xsj's k nearest neighbors in the target domain. This relationship is formalized in a relationship matrix Mst, where Mst[i,j] is 1 if xti and xsj are a mutual nearest neighbor pair, and 0 otherwise. Additionally, an in-domain relationship matrix Mss is constructed for the source domain based on ground-truth labels to leverage supervised information. Finally, a contrastive loss function (LCon) is applied. For each training sample xi, its positive sample set Pi and negative sample set Ni are obtained from a memory bank using the Mss and Mst matrices. The LCon aims to pull the feature representations of mutual nearest neighbor pairs closer in the feature space while pushing non-geometrically similar samples apart. This process ensures that common semantic characteristics are aligned across domains, while private semantic characteristics unique to each domain are effectively separated, thereby mitigating the problem of negative transfer.": 866,
    "The paper addresses this by introducing TEXT-TRANSPORT, a method that frames causal effect estimation as a distribution shift problem. It defines a causal estimator that transports a causal effect from a causally valid source domain (PR), such as a randomized experiment, to a target domain (PT), which may not fulfill the assumptions required for valid causal inference. The core mechanism involves using the density ratio between the two distributions as an importance weight. Specifically, the average response in the target domain, denoted as µ(PT), is estimated by weighting the observed responses Yi(Xi) from the source domain by the density ratio dPT/dPR(Xi) for each text Xi. This is expressed as ˆµ(PT) = (1/n) Σ (dPT/dPR(Xi)) Yi(Xi). The causal effect in the target domain, τT, is then estimated as the difference between two such average responses: ˆτT = ˆµ(PT1) - ˆµ(PT0), where PT1 and PT0 represent distributions for the linguistic attribute of interest being 1 or 0, respectively. This approach leverages the intuition that texts similar to those in the target domain, but observed in the source domain, should contribute more to the estimate, while texts representative of the source domain should have their contribution reduced. The estimator, referred to as ˆµR→T, is shown to be an unbiased estimator of µ(PT), asymptotically normal, and has a closed-form variance, allowing for quantification of statistical uncertainty.": 867,
    "The PoemSum dataset was curated to expedite research in poem summarization by providing a diverse range of poems along with their summaries. Each sample within the dataset includes the poem's title, the poet's name, the full poem text, the source website of the poem, and its corresponding summary. Data collection involved a two-pronged approach: summaries were primarily collected via web scraping from the 'PoemAnalysis' website, recognized for its extensive collection of high-quality poem summaries. Since this website often hosted only the summaries, the corresponding poem texts were manually collected from numerous other internet sources. To ensure reliability and accuracy in this manual data collection, four undergraduate students majoring in Computer Science and Engineering from a reputable university were hired, selected for their technical skills in data acquisition. Following collection, a rigorous dataset cleaning process was implemented to ensure uniformity. This involved addressing discrepancies such as duplicate records, garbage values, unnecessary whitespaces, corrupt values, and HTML tags. The cleaned records underwent further manual verification to confirm the elimination of these inconsistencies. A crucial step in data verification involved establishing strict quality standards for the collected summaries. Summaries were systematically discarded if they were limited to explaining only the rhyming scheme, contained solely the poet’s biography without insights into the poem, discussed only other works of the poet without context, merely copied lines from the poem without extracting deeper meaning, or simply rewrote poem lines in simple language without analysis. This extensive cleaning and verification process reduced the initial collection of approximately 3,500 samples to the final 3,011 samples comprising the PoemSum dataset.": 868,
    "The generation and filtering of high-quality candidate spans are achieved through two sequential components: the Focusing stage and the Bridging stage.\nIn the Focusing stage, the input text is processed to identify \"\"entity-concentrated parts,\"\" which are the longest continuous segments where named entities are adjacent. This is accomplished by constructing an IO tagging module. Each token in the input text is fed into a pre-trained language model (PLM), specifically BERT (Bidirectional Encoder Representations from Transformers), to obtain its representation. For each token, its representation is formed by concatenating the mean-pooled subtoken representation and the representation of the [CLS] token. This combined representation is then passed through a multilayer perceptron (MLP) for binary classification, predicting whether the token is part of an entity (I-tag) or not (O-tag). Continuous tokens marked with the I-tag are concatenated to form an entity-concentrated part. This stage acts as an initial filter, significantly reducing the number of potential spans to consider.\nIn the Bridging stage, for each entity-concentrated part identified in the Focusing stage, all possible spans within that part are enumerated to obtain candidate nested spans. To refine these candidates, a boundary scoring mechanism is employed. For each token within an entity-concentrated part, its probability of being a left or right boundary of an entity is calculated. This is done by taking the mean pooling of all tokens' representations within the part (part-representation) and concatenating it with the individual token's representation. These combined representations are then fed into separate MLPs (MLPleft and MLPright) to predict the probabilities of being a left or right boundary, respectively. The boundary score for each candidate span is calculated by element-wise multiplication of the probability of its left boundary token being a left boundary and the probability of its right boundary token being a right boundary. Finally, candidate nested spans are sorted by their boundary scores, and those with low scores or partial overlapping are discarded, resulting in a filtered set of high-quality candidate spans.": 869,
    "The paper addresses this by introducing Over-The-Top (OTT) Masking, which is the second step in its three-step domain obfuscation process. After an initial frequency-based masking (Base Masking), OTT masking leverages the encoded knowledge within language models to identify additional token-domain associations. This is achieved by training a domain classifier, denoted as `fd: D -> D`, which is a fine-tuned RoBERTa-base model, to classify an input text `XD` to its domain label `d`. For identifying tokens to mask, the method utilizes an attention-based architecture for `fd`. Specifically, it defines the attention-based domain affinity of a word `w` as `mb(w;l) := ||αlwvlw||`, where `||·||` computes the Euclidean norm, `vlw` is the word's representation at the output of layer `l`, and `αlw` specifies how much the classification token (e.g., `<s>` for RoBERTa) in layer `l` attends to `w`. Words whose `mb` score is higher than a predefined threshold `τ2` are masked. This approach allows for context-aware masking, reconsidering words that failed initial frequency-based scoring and generalizing the notion of a domain-specific word by exploiting learned word-word co-occurrences, thereby improving recall in domain-specific token extraction.": 870,
    "The paper addresses this by utilizing prefix tuning to train a separate prefix for each source domain. Prefix tuning modifies the transformer attention mechanism by adding tunable prefixes, `hK` (key prefix) and `hV` (value prefix), to the key (`K`) and value (`V`) matrices. Specifically, `K` is modified to `K' = [hK; K]` and `V` to `V' = [hV; V]`. These prefixes, `hjK` and `hjV` for source domain `DSj`, are modeled using a two-layer Multi-Layer Perceptron (MLP) as `hjK = WjK,2f(WjK,1Ej+bjK,1)+bjK,2` and `hjV = WjV,2f(WjV,1Ej+bjV,1)+bjV,2`. Here, `W` and `b` are trainable weights and biases, and `Ej` is a trainable embedding matrix with `C` as the prefix length. The pre-trained language model's parameters remain frozen, and only these prefixes are trained in an end-to-end fashion on their corresponding source domain data. This approach ensures parameter efficiency and lightweight adaptation.": 871,
    "A specialized subcorpus, CEmb, is constructed from the Fig-QA dataset to study the effect of embodiment. This process begins by identifying verbs within the metaphorical phrases of the Fig-QA training and development data using spaCy for part-of-speech tagging and lemmatization. The lemmatized verbs are then matched with embodiment scores from the dataset by Sidhu et al. (2014), which provides ratings for 687 English verbs based on the degree to which their meaning involves the human body on a 1-7 scale. If a metaphorical sentence contains more than one verb, the average of their embodiment scores is assigned to the phrase. This results in the CEmb subcorpus containing 1,438 entries, each augmented with an embodiment rating for its action. A control subcorpus, CNoE, of the same size is also constructed, comprising metaphorical phrases that do not contain an embodied verb, allowing for comparative analysis. Only phrases where the verb is contained within the Winograd pair are kept for both subcorpora.": 872,
    "The framework includes an unsupervised domain adaptation (UDA) component that leverages labeled source data and unlabeled target data to train one classifier. This classifier aims to bridge the domain gap by learning domain-invariant features, ensuring that the learned representation can generalize well to the target domain. The UDA component employs techniques like domain adversarial training to minimize distribution shifts between the source and target domains. This enables the classifier to perform well on the target domain despite the lack of labeled data.": 873,
    "The paper constructs the Multilingual Simile Dialogue (MSD) dataset through a rigorous data collection and annotation process. First, for data collection, existing open-domain dialogue corpora are utilized: Reddit Dialogue dataset for English, and PchatbotW and LCCC for Chinese. A multi-stage pipeline is then applied to extract simile candidates. The first step involves selecting dialogue examples where responses contain specific comparators (e.g., \"\"like,\"\" \"\"as...as\"\" in English; \"\"像...一样\"\" in Chinese). Second, a length control is applied, selecting dialogues with context lengths between 15 and 30 words. Third, machine translation is used to verify that the comparator persists when translated into another language, ensuring the presence of a true comparator. Fourth, a semantic check is performed using a semantic dependency tool to locate candidate tenor and vehicle, and their similarity is computed. Examples with high similarity (indicating they are from the same category, thus not a simile) are removed, retaining only those with low similarity, ensuring the tenor and vehicle are from different categories.\nFor data annotation, seven English-speaking students and six well-educated native Chinese speakers (graduate students) are recruited. Annotators are provided with the \"\"dialogue context,\"\" \"\"response,\"\" \"\"comparator,\"\" and a \"\"vehicle candidate.\"\" They first determine if the response contains a simile; if not, it's labeled \"\"Literal.\"\" If it is a simile, they check the vehicle candidate's correctness and annotate the correct vehicle (word, phrase, or sentence) if needed. Finally, they annotate the tenor (word, phrase, or sentence) if it exists. The annotation process accounts for complexities like tenors existing in different sentences, occurring after the vehicle, or not existing in the dialogue at all (in which case the example is deleted). Preliminary training is conducted, and quality control involves random sampling tests of annotated files, with revisions requested for low-quality annotations. Each dialogue is annotated by three annotators, with majority agreement determining the final result, achieving a Fleiss' Kappa of 0.61.": 874,
    "The paper addresses this by proposing a novel collaboration between Large Language Models (LLMs) and Diffusion Models. Specifically, InstructGPT-3 (davinci-002) is employed with Chain-of-Thought (CoT) prompting to generate a \"\"visual elaboration\"\" of a linguistic metaphor. This visual elaboration is a textual representation that explicitly contains the implicit meaning and relevant objects of the metaphor. The CoT prompting method, inspired by prior work on Visual Blends, emphasizes the objects to be represented and the implicit meaning. For example, given \"\"My bedroom is a pig sty,\"\" the LLM generates \"\"A bedroom with clothes and garbage everywhere with a pig in the center rooting around.\"\" This detailed visual elaboration then serves as the input to diffusion-based text-to-image models, such as DALL·E2 or Stable Diffusion, enabling them to generate more accurate and contextually relevant visual metaphors by providing explicit instructions for compositionality and implicit meaning.": 875,
    "The paper proposes the Generative Zero-shot Prompt Learning (GZPL) framework, which reformulates slot filling as a language generation task. Instead of sequence labeling or machine reading comprehension (MRC), the model is designed to generate slot values directly from a structured input. Specifically, for each slot type, the input sequence is constructed by concatenating a question corresponding to that slot type (e.g., \"\"what is the slot_type?\"\"), the names of all possible slot types across all domains, and the original user query. The output sequence is the related slot value(s) extracted from the query. If a slot type does not exist in the input query, the model is trained to output a special \"\"none\"\" token. This text-to-text format allows for deep semantic interaction between slot types and slot values via the pre-trained language model (PLM), which is crucial for recognizing unseen slots that only appear in the target domain. The framework constructs question-answer (QA) pairs for each original input query, with the number of QA pairs equal to the number of slot types. The question template is kept simple, \"\"what is the?\"\", to emphasize the framework's simplicity and effectiveness, and all slot names from all domains are included in the prompt to enhance interaction between different slot types.": 876,
    "The paper addresses this by choosing seven diverse languages: Hindi (hi), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Indonesian (id), and Javanese (jv). The selection process is systematic, considering three main factors. First, the languages are chosen to represent different classes within the resource-based taxonomy of languages, as proposed by Joshi et al. (2020), subject to annotator availability. Second, languages with a sizable speaker population are prioritized. Third, the selected languages originate from five typologically diverse language families and are spoken in four different countries, thereby allowing for a wide range of linguistic and cultural diversity in the collected data.": 877,
    "The paper addresses this by formalizing the figurative language detection task as a text-to-text generation problem. For any given source sentence, the model is trained to generate a textual label (e.g., \"\"literal\"\" or \"\"idiomatic\"\"). To unify multiple detection tasks across different figures of speech and languages, a template-based prompt learning approach is employed. Specifically, a pre-defined prompt template (denoted as `pt`) is prepended to the source sentence (`x`) from any given task (`Tt`) before feeding it into the multilingual pre-trained text-to-text transformer (mT5) model (denoted as `M`). The model then produces the label (`y'`) for the input, aiming to minimize the negative log-likelihood of the output sequences. This method allows the mT5 model to be fine-tuned in a sequence-to-sequence (seq2seq) fashion using multilingual and multi-figurative samples from the dataset. The prompt templates, such as \"\"Which figure of speech does this text contain? (A) Literal. (B) [Task].| Text:[Text]\"\", act as language instructions that guide the model to perform the specific target task (e.g., hyperbole detection, idiom detection, metaphor detection) without needing task-specific parameters or modules. This design enables the model to connect multiple figures of speech and languages within a single, unified framework.": 878,
    "To explore this, the study conducts a feature importance analysis using SHAP values to identify the top 20 features each model deems important for different figurative language classes. These features are then categorized using LIWC, enabling a comparison of model and human attention patterns. The analysis reveals that BERT and XLNet focus on expected cues like positive emotion words for sarcasm and comparison words for similes, while RoBERTa exhibits divergent behavior. White-box models tend to focus more on high-frequency function words.": 879,
    "The paper addresses this by designing a Cross-modal Adaptation module. This module consists of two sub-modules: (i) a Convolutional Neural Network (CNN) based subsampling component, which balances the sequence length between the acoustic and lexical modalities, and (ii) a bottleneck adapter network, which projects the acoustic representations into the BERT encoder input space. The outputs from the adapter network (acoustic representations `a`) and lexical embeddings (`l`) are then horizontally concatenated (`a` ⌢ `l`) and passed through the BERT encoder layers to fuse the information from the two modalities. This allows the pre-trained embedding layers of BERT to serve as the text embedding module, while the intermediate encoder layers take both acoustic and lexical representations as input.": 880,
    "The paper addresses this by proposing the BERT-NSP-Prompt model, which leverages the Next Sentence Prediction (NSP) task of BERT pre-training. The NSP task is designed to predict whether two sentences are contextually related, thereby assessing if they share the same topic or similar semantics. To adapt this to intent detection, the model transforms the intent detection problem into an NSP task. The input sequence for the BERT-NSP-Prompt model is constructed in a specific format: `([CLS], TA, XA, [SEP], TB, XB, [EOS])`. Here, `XA` is the original user dialogue, `XB` is a natural language text description of a specific intent label, and `TA` and `TB` are manually designed prompt templates. `[CLS]`, `[SEP]`, and `[EOS]` are special identifiers for the task. For a given dialogue, multiple input sequences are created, one for each possible intent description. The input sequence is then encoded by BERT to obtain hidden states, particularly the hidden state vector of the `[CLS]` token (`h0`). The BERT pre-trained NSP task classifier uses `h0` to determine the relationship between sentence A (dialogue) and sentence B (intent description). The intent is then predicted by comparing the \"\"IsNext\"\" tag scores from the NSP task, with the highest score indicating the most relevant intent description. The probability distribution of the intent is calculated using a Softmax function over the output of the NSP classifier, and the loss function is a cross-entropy loss based on the intent label. This approach allows the model to utilize knowledge acquired during BERT's pre-training without adding new parameters, making it suitable for few-shot and even zero-shot scenarios.": 881,
    "The paper addresses the data scarcity problem in Metaphor Detection (MD) by proposing a novel auxiliary task called Basic Sense Discrimination (BSD). BSD is constructed from Word Sense Disambiguation (WSD), which has abundant data. The core idea is that the most commonly used lexical sense of a word, often found at the top of a WordNet inventory list, is considered a \"\"basic sense.\"\" BSD is then defined as a binary classification task that identifies whether a word in a given context is used in its basic sense or not. This makes BSD structurally similar to MD, which also involves a binary classification (literal vs. metaphorical).\nTo facilitate knowledge transfer from BSD to MD, the proposed Adversarial Multi-task Learning Framework (AdMul) is designed. AdMul uses a shared feature extractor, specifically DeBERTa, to process inputs for both tasks. The input format for both MD and BSD is standardized as `([CLS], target, [SEP], sentence, [SEP])`, where the first segment (target word alone) represents a more basic meaning, and the second segment (whole sentence) encodes the contextual meaning. This setup allows the model to compare the semantic difference between a basic meaning and the current contextual meaning, aligning with the Metaphor Identification Procedure (MIP) linguistic rule. By training on the data-rich BSD task alongside MD, AdMul can distill knowledge from BSD to alleviate data scarcity and overfitting in MD, effectively transferring general semantic understanding relevant to sense discrimination.": 882,
    "To systematically compile and characterize a German corpus for investigating metaphor usage across linguistic registers, the authors assembled a corpus comprising five distinct subcorpora, each eventually intended to be 30,000 words in length. These subcorpora were selected to represent a wide range of register variation and include: parliament speeches from the German Parlamentsreden-Korpus, news commentaries from the Potsdam Commentary Corpus, sermons, light fiction (written by amateurs for their peers), and debates from competitions of the organization `Jugend debattiert`. The corpus was characterized by explicitly varying specific dimensions of register based on Systemic-Functional Linguistics (SFL) and Biber's multi-dimensional approach. For SFL, two dimensions of tenor were varied: hierarchy versus equality, and distance versus closeness. Additionally, two mode dimensions were varied: dialogue versus monologue, and spoken versus written register. For instance, parliament speeches and sermons, despite their oral presentation, were classified as \"\"literal\"\" based on Koch and Oesterreicher's (1994) distinction of conceptual literality versus orality, due to being prepared and fixed in advance. For Biber's dimensions, the subcorpora were chosen to represent variation along \"\"situation-dependent vs. elaborated reference\"\" and \"\"overt expression of persuasion,\"\" with anticipated levels of these properties for each subcorpus. This systematic selection and characterization allowed for controlled analysis of metaphor usage in relation to specific register features.": 883,
    "The paper structures a multi-task learning (MTL) framework by training a single model jointly on propaganda identification and metaphor detection. Propaganda identification is designated as the main task, while metaphor detection serves as an auxiliary task. The model extends a single-task learning (STL) setup by incorporating an additional classifier specifically for metaphor detection. All tasks share the pre-trained RoBERTa-BASE model in a hard parameter sharing fashion, meaning the core language model parameters are common to both tasks. During fine-tuning, the best hyper-parameter configurations from the STL models are reused to facilitate comparison. The training process involves specific hyper-parameters for the MTL regime, including task sampling ratios (ra, rm) to select which task to train on at each update step, epoch sampling coefficients (ca, cm) to dynamically adjust these ratios over epochs, and loss scaling factors (sa, sm) to balance the contribution of each task's loss to the overall training objective. The model's computational budget is limited to match that of STL models, with iterations per epoch randomly selecting a task for training based on its sampling probability, and batches filled with samples from the selected task.": 884,
    "The paper addresses the challenge of noisy labels in pseudo-generated datasets during rerank model adaptation through a novel denoise-finetuning approach, which incorporates random batch warm-up and co-regularization learning. The process begins by initializing two separate cross-encoder models, denoted as {CEk}k=1,2. During the initial phase of training, specifically for the first α% of total training steps (where α is a hyperparameter, e.g., 0.1), a \"\"random batch warm-up\"\" strategy is employed. In this phase, for each of the two cross-encoder models, batches are randomly discarded with a certain probability (P), allowing the models to learn from different data distributions and preventing premature overfitting to potentially noisy labels. The models are updated using a standard binary cross-entropy (BCE) loss based on the provided labels (Llabel). Following this warm-up phase, the training transitions into a \"\"co-regularization learning\"\" stage for the remaining steps. In this stage, both cross-encoder models are jointly optimized. The total loss (L) for each model consists of two components: the task-specific loss (Llabel) and an agreement loss (Lagg). Llabel is calculated as the BCE between the model's batch prediction (ˆyk) and the ground truth labels (y). The Lagg component measures the discrepancy between a model's prediction and an \"\"aggregate prediction\"\" (agg). The aggregate prediction is computed by averaging the batch predictions from both cross-encoder models (agg = 1/2 * Σk=1,2 CEk(B)). Lagg is then calculated as the BCE between the model's prediction (ˆyk) and this aggregate prediction (agg). The influence of co-regularization is controlled by a hyperparameter γ. This co-regularization technique encourages consistency between the two models' predictions, leveraging their shared learning to alleviate the impact of noisy labels and prevent overfitting.": 885,
    "The paper proposes a mutual information-based method to identify type-related features (TRFs) from the source domain. First, for each entity type `t`, a set `St` is defined containing all sentences from the source domain where entities of type `t` appear, and `S\\St` contains sentences without entities of type `t`. A binary variable is introduced, indicating examples from `St` as 1 and `S\\St` as 0. Mutual information is calculated between all tokens and this binary variable. To ensure selected tokens are strongly associated with `St` rather than `S\\St`, a filtering condition is applied: `CS\\Si(wm) / CSi(wm) <= ρ` and `CSi(wm) > 0`. Here, `CSi(wm)` is the count of the m-gram `wm` in `Si`, `CS\\Si(wm)` is its count in all source domains except `Si`, and `ρ` is a frequency ratio hyperparameter (set to 3 in experiments). This criterion ensures that an m-gram `wm` is considered a TRF for `Si` only if its frequency in `Si` is significantly higher than in other entity types. For simplicity, only 1-gram texts are considered. This module efficiently computes mutual information for all entity types and tokens by traversing training sentences once.": 886,
    "The paper proposes a novel difficulty metric that combines two components: representation distance and perplexity score.\nFirst, the representation distance `dr(Y)` is calculated as `1 / ||l(Y) - Emb(Y)||`, where `l(·)` is the final layer of the model and `Emb(·)` is the embedding layer. This metric leverages the property that for non-compositional expressions, the meaning cannot be inferred from constituent words. A larger distance between the contextualized representation of the non-compositional expression and the original word embeddings of its constituent words indicates that the model has learned the expression's representation beyond its parts, suggesting it is \"\"easier\"\" for the model. Conversely, a smaller distance implies the expression is still difficult for the model.\nSecond, the perplexity score `dp(Y)` is used, calculated as `PPL(Y; θ) = e^(-1/t * Σ log pθ(yi|y<i))`. Perplexity is a measure of how well a probability distribution predicts a sample. For non-compositional expressions, which are rare in large corpora, pre-trained language models often assign low probabilities due to unfamiliarity. A lower perplexity score for a non-compositional expression indicates that the language model is more familiar with it, meaning it is \"\"easier\"\" for the model to generate.\nThese two metrics are combined additively to form the overall difficulty score `dn(Y) = dr(Y) + dp(Y)`. This combined metric is designed to reflect the model's familiarity and understanding of the non-compositional expressions, guiding the curriculum learning process.": 887,
    "The paper addresses this by implementing a continual learning approach, illustrated in Figure 1(a). This method involves two sequential fine-tuning phases for a Sentence-BERT model, which uses DeBERTa as its encoder. In the first phase, the model is fine-tuned to predict either the most similar source domains or highlighted aspects for a given set of metaphorical sentences. This training utilizes contrastive learning with the multiple-negatives ranking loss, which optimizes the embedding space such that the representation of the correct concept (source domain or highlighted aspect) is positioned closer to the sentence representation, while incorrect concepts are distanced. After this initial training, the best-performing model from the first phase (selected based on validation set performance) is then used as the starting point for the second phase. In the second phase, this pre-trained model is continually fine-tuned to predict the other task (e.g., if source domains were predicted first, then highlighted aspects are predicted next, or vice versa). The same contrastive learning setup and multiple-negatives ranking loss are applied in the second phase. This sequential process aims to preserve and leverage the information learned in the first task to improve performance on the second, and ultimately on both tasks.": 888,
    "The paper addresses this by constructing a domain lexicon that includes hierarchical domain categories and corresponding examples. The process begins with two broad \"\"source level\"\" categories: \"\"physical entity\"\" (concrete, tangible objects/substances) and \"\"abstraction\"\" (concepts, ideas, qualities). These form the highest level of the hierarchy. To further organize the domains, the hierarchical structure of WordNet is referenced, and domains are divided into three additional levels: macro-level, micro-level, and entity-level. At the macro-level, the source level is subdivided into 13 categories, including 9 physical entity categories (e.g., person, animal, artifact, event, location, process) and 4 abstract categories (e.g., relation, communication, attribute, psychological). This division is based on WordNet's upper/lower relations, with adjustments made to account for overlaps or similarities in Chinese (e.g., \"\"person\"\" and \"\"people\"\" are combined). The micro-level comprises category words obtained by further subdividing the macro-level categories (e.g., \"\"animals\"\" into mammals, birds, reptiles, insects, fish). The entity-level represents specific examples within the micro-level (e.g., \"\"tiger,\"\" \"\"mouse,\"\" \"\"bat\"\" as mammals). The lexicon consists of 2,755 words, including 61 category words and 2,694 specific example words. This structured lexicon is then used to annotate the source and target domain categories of metaphors at the macro-level.": 889,
    "The framework constructs a unified structural causal model (SCM) that represents the cross-domain relation extraction task. Variables such as contextualized representation (C), syntactic structure (S), entity representation (E), label description (L), and relation representation (X) are defined within the SCM. The causal effects among these variables are estimated using Transformer-based fusion of semantic information and linguistic structures. Full connectivity with nonlinear transformation obtains the causal effect of parent nodes on the outcome variable Y. This enables comprehensive causal effect estimation within the model.": 890,
    "The paper constructs the Image Recognition of Figurative Language (IRFL) dataset by developing a multi-stage pipeline for idioms and adapting it for metaphors and similes. For idioms, the process begins by collecting 628 idioms from the MAGPIE corpus. Definitions for these idioms are then crawled from online dictionaries (Wiktionary, Oxford Dictionary) and parsed into search queries. Next, candidate images are searched using Google Images for both the idioms and their definitions, with \"\"Safe-Search\"\" enabled. A crucial filtering step is applied to remove images containing textual representations of the search query using Optical Character Recognition (OCR) tools like EasyOCR and TextBlob, and further filtering out document-like images using the ViLT model. The remaining images are scored for \"\"phrase-image\"\" and \"\"definition-image\"\" matching using ViLT to select top-k literal and figurative candidates. Finally, human annotators from Amazon Mechanical Turk (AMT) label the relationship between each idiom and its candidate images using a nuanced classification system: \"\"Literal,\"\" \"\"Figurative,\"\" \"\"PartialLiteral\"\" (visualizing some literal elements), and \"\"Figurative+Literal\"\" (expressing figurative meaning while retaining literal aspects). For metaphors and similes, the pipeline is adapted due to their flexible nature; expert annotators define metaphors, while similes use the simile itself and the target concept with the shared property as search queries for figurative images, and source/target concepts (with antonyms for properties) for literal distractors. These images are then manually annotated by experts.": 891,
    "The paper proposes Hierarchical Contrastive Learning (HiCL), a coarse-to-fine contrastive learning approach. This framework first performs coarse-grained entity-level contrastive learning (CL) to optimize distributional divergence between entity-class embeddings. This step learns entity type and boundary knowledge. Subsequently, it performs fine-grained token-level CL, which combines the features learned from the entity-level CL to finely learn token-class knowledge. Both levels of CL use slot labels as supervised signals. The coarse-grained CL models the entity class distribution, while the fine-grained CL optimizes KL-divergence between token Gaussian embeddings and models the token class distribution. This hierarchical approach encourages HiCL to learn generalized feature representations that can categorize and differentiate between different entity and token classes, fostering zero-sample target domain adaptation.": 892,
    "Domain-invariant features are extracted and retained at the feature level using a parametric mask layer. For each transformer layer, a learnable mask, denoted as `ml`, is applied via element-wise multiplication to the token representations, `hli`. This mask is constructed from a trainable filtering vector, `m`, which is the element-wise product of a trainable weight vector, `r`, and a trainable pruning threshold vector, `s`. To enable differentiability for training, the non-differentiable unit step function `g(t)` used to derive a binary mask `q` from `|r|-s` is approximated by a differentiable function. A sparsity regularization term, `Lsparse = sum(exp(-si))`, is added to the training loss to encourage high thresholds, which in turn promotes the removal of more features, thereby enforcing sparsity and focusing on invariant features. During inference, these learned mask layers are retained to preserve the identified invariant features while discarding irrelevant ones.": 893,
    "The paper addresses this by creating the Metaphor Understanding Challenge Dataset (MUNCH). This dataset is built upon metaphorical sentences extracted from the VUA corpus, covering four genres (academic, news, fiction, conversation) and varying levels of novelty. Each metaphorical sentence contains a highlighted metaphor-related word (MRW) that needs interpretation. The core innovation lies in the systematic generation of both apt and inapt paraphrases. Apt paraphrases are collected via a crowdsourcing task where workers fill in a blank with a single word to semantically and grammatically paraphrase the original sentence. These crowd-sourced paraphrases are then expert-validated to select the \"\"best\"\" apt paraphrase, prioritizing those clearly within the target domain. Inapt paraphrases are manually created using WordNet. For each MRW, WordNet synsets corresponding to its more basic (literal, source domain) meaning are identified. Synonyms or hypernyms from these basic-sense synsets are then selected to form inapt paraphrases that are grammatically acceptable but semantically incorrect in the given context, specifically reflecting the source domain. This dual annotation of apt and inapt paraphrases allows for controlled testing of whether models perform full metaphor interpretation (cross-domain mapping) or resort to shallow lexical similarity. The dataset includes over 10,000 apt paraphrases and 1,492 triples (metaphorical sentence, apt paraphrase, inapt paraphrase).": 894,
    "The method employs Lai et al. (2023)'s publicly available multi-figurative language detection approach to identify idioms, metaphors, and hyperboles. This approach uses a multitask framework that incorporates template-based prompt learning using mT5 across five existing figurative language datasets. Each post is iteratively prompted for each type of figurative language, marking whether any sentences within the post contain at least one type. The identified figurative language features are then integrated into both feature-based and pretrained language model (PLM) classifiers. For PLM conditions, one-hot encoded labels for detected idioms, metaphors, and hyperboles are appended through an additional embedding layer during fine-tuning or prompting. For feature-based conditions, these one-hot encodings are directly appended to the feature vectors. This allows evaluating the impact of these linguistic elements on numerous settings by comparing model performance with and without them.": 895,
    "The relevance of retrieved Knowledge Base (KB) elements is enhanced through a multi-step process within the Retrieval Stage of FuSIC-KBQA. First, one or more off-the-shelf supervised KB-retrievers (e.g., TIARA, Pangu) are trained on the large source dataset and then fine-tuned using the limited target few-shot examples. For a given target test question, each retriever independently retrieves its top-K data-paths, entity types, and relations. Next, an LLM (Large Language Model) is employed for retrieval re-ranking and sub-selection. The top-K results for each aspect (data-paths, entity types, relations) from each retriever are provided to the LLM along with the question. The LLM is then prompted to select and rank a smaller subset of top-k results (where k is much less than K), thereby improving their target relevance. This process is performed separately for each aspect and each retriever, and the aspect-wise top-k retrieval from each retriever is then passed to the Generation Stage.": 896,
    "The paper employs a three-step Chain-of-Thought (CoT) prompting method to guide the Multi-modal Large Language Model (MLLM) in generating high-quality and informative features. This method ensures a progressive acquisition of information across modalities.\nSTEP 1: The MLLM is prompted with \"\"Question 1\"\" to focus exclusively on comprehending visual elements (objects, scenes) in the image (xI), temporarily ignoring any text within the image. This step aims to generate an image description (mI) without textual interference.\nSTEP 2: The MLLM is then prompted with \"\"Question 2\"\" to analyze the meaning of the text (xT), including potential homophonic memes and puns, while excluding any interference from image features. This step aims to generate a text analysis (mT).\nSTEP 3: Finally, the MLLM is prompted with \"\"Question 3\"\" to synthesize the results from the previous two steps (mI and mT) and further integrate the original image (xI) and text (xT) features. This step aims to obtain more profound cross-modal interaction information, generating a mixed modality description (mMix) that captures implicit meanings.": 897,
    "The method constructs literal example sentences for the focus verb's literal meaning, then maps semantic roles to form semantic groups in both the input sentence and literal examples. Conceptual domain mining leverages cognitive, commonsense, and lexical resources to mine source and target domains. Finally, token-level metaphor detection evaluates the importance of word pairs based on domain inconsistencies, giving more attention to core word pairs during training.": 898,
    "The paper addresses this by developing a Self-Disclosure Detection Model. This model is built by fine-tuning transformer-based encoder models, specifically RoBERTa-large (a robustly optimized BERT pretraining approach) and DeBERTaV3-large (an improved DeBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing), to perform self-disclosure detection as a sequence tagging task. The training data consists of a curated corpus of 2.4K Reddit posts manually annotated with 4.8K self-disclosure spans, categorized into a comprehensive taxonomy of 19 self-disclosure types (13 demographic attributes and 6 personal experiences). For infrequent categories like \"\"name\"\" and \"\"contact,\"\" the approach combines a sentence classifier (trained on the corpus to determine if a sentence contains a self-disclosure) with an existing Named Entity Recognition (NER) model (LUKE, for person names) and Microsoft Presidio (for contact information) to specifically identify self-disclosures rather than generic entities. During fine-tuning, the models are initially trained on 4,959 sentences with single annotations and then continually trained on 802 sentences with adjudicated annotations. Data processing methods include segmenting Reddit posts and comments into shorter chunks (64, 128, 256 words) or individual sentences using Ersatz (a unified approach to sentence segmentation), with sentence-level segmentation yielding the best performance. During inference, for words tokenized into multiple subword tokens, the hidden states of the first token are used to determine the label.": 899,
    "The method addresses this by first extracting task-relevant keywords and then constructing a brief, zero-shot dialogue based on these keywords. In the \"\"Keywords Extraction\"\" step, the LLM (M) is prompted to identify keywords (K) from the task description (T) and the specific question (Q). This is achieved by concatenating T, Q, and a trigger prompt P1 (e.g., \"\"First, extract keywords from the question\"\") as input to the LLM, resulting in K = M(T ⊕ Q ⊕ P1). The number of keywords typically ranges from 4 to 5. Subsequently, in the \"\"Dialogue Simulation\"\" step, the extracted keywords (K) are used to guide the LLM in constructing a scenario (S) in the form of a dialogue. This is done by providing the LLM with T, Q, K, and a prompt P2 (e.g., \"\"Then, according to the keywords, construct a scenario for the question in the form of dialogue\"\"), such that S = M(T ⊕ Q ⊕ K ⊕ P2). This process typically generates an approximately one or two-turn dialogue, designed to uncover subjective expressions relevant to the task.": 900,
    "The paper designs a novel generative evaluation framework centered on assessing LLMs' ability to produce pragmatically relevant responses to non-literal utterances. This framework is structured around a specific setup involving: a `Context (C)` which is a short narrative with two or more characters; a `Non-literal Utterance (UN1)` made by a speaker using non-literal language; a `True Intention (IT)` representing the actual intended meaning of the speaker; and an `Incorrect Literal Intention (IL)` which is a misinterpretation of the speaker's intention. The framework then requires the LLM to generate a response, denoted as `UN2`, to the `UN1` within the given `C`. To evaluate `UN2`, the framework establishes two `Reference Dialog Chains`: one based on the `True Intention (IT)` (resulting in `UT1` and `UT2`) and another based on the `Incorrect Literal Intention (IL)` (resulting in `UL1` and `UL2`). The core of the framework's evaluation is to compare the model's generated response (`UN2`) against these two reference responses (`UT2` and `UL2`) to determine if `UN2` aligns more closely with the pragmatically correct `UT2`.": 901,
    "The paper addresses the effectiveness of LLMs in \"\"unfunning\"\" by benchmarking their performance against human-curated data from the Unfun Corpus. The Unfun Corpus consists of pairs of satirical headlines and their human-edited serious counterparts. The study employs a few-shot setting, providing LLMs (GPT-4, GPT-3.5-TURBO, MISTRAL-7B-INSTRUCT, and MISTRAL-7B) with a short task description and exemplar input-output pairs (humorous text, serious text) sampled from high-quality human-rated pairs. A lightweight alternative approach, ROBERTA-SWAP, is also considered, which iteratively replaces low-probability tokens in satirical headlines with high-probability predictions from a ROBERTA masked language model, motivated by the Incongruity Theory of Humor. The quality of the synthetically generated unfunned data is evaluated through both automatic evaluations on the downstream task of Unfun detection and human evaluations. Automatic evaluations involve training binary humor classifiers (MISTRAL and ROBERTA) on the synthetic outputs and testing their ability to differentiate between actual humorous and unfunned headlines from the original Unfun dataset. Human evaluations involve recruiting university students to rate headlines as real, satirical, or neither, and to assess funniness, grammaticality, and coherence.": 902,
    "The paper addresses this by employing a Large Language Model (LLM), specifically GPT-4, to perform \"\"visual elaboration.\"\" This process transforms figurative content into visualizable textual descriptions. The LLM is instructed to act as an expert in metaphors, integrating systematic domain knowledge such as the definitions and characteristics of tenor, vehicle, and groundings, along with examples, into its system role. A Chain-of-Thought (CoT) prompting method, rooted in rhetoric, is utilized to guide the LLM. Unlike previous approaches that focused on all possible objects, GOME's method proposes to elaborate metaphors with less emphasis on provocative objects from vehicles, instead prioritizing the underlying groundings. For instance, given \"\"love is like a gust of wind,\"\" if the grounding is perceived as \"\"love is gentle,\"\" the LLM generates a visual description like \"\"two lovers embracing each other in a sunny field, their hair and clothes gently blown by a soft breeze.\"\" Conversely, if the grounding is \"\"love is a brief passage,\"\" the description becomes \"\"In a park with fallen leaves during autumn, a couple broke up. The woman left, and a man reached out his arm to grab her hand.\"\" The outputs of this elaboration step include fine-grained metaphorical elements (tenor, vehicle, groundings) and the detailed visual descriptions, which are then used for subsequent metaphor binding and image generation.": 903,
    "The Pun Grounding module operates by identifying key visual elements that are contextually relevant to the text of the pun. It then anchors the textual content to these visual cues, enhancing the model's understanding of the pun's dual meanings. This process involves a sophisticated algorithm that analyzes the image for elements that could potentially relate to the text, thereby grounding the ambiguous text in a more concrete visual context.": 904,
    "The paper addresses this by proposing Most-Similar Retrieval Augmented Generation (MS-RAG). Instead of using fixed examples, MS-RAG first retrieves a set of samples from a Knowledge Base (KB) based on a similarity relevance function, MS(s). The transformation function for autoformalization then becomes fϕ(s) = LLM(pauto, {(si,ϕi)}s, s), where pauto is a prompt for autoformalization and (si,ϕi) are the retrieved exemplars from MS(s). This approach ensures that the examples provided to the Large Language Model (LLM) are highly relevant to the input informal statement, thereby improving the quality and consistency of the generated formal statements. The BM25 algorithm is used as the primary ranking function to retrieve the top-k (k=3) most similar samples. The query for retrieval can be either the natural language textual description or the description combined with a zero-shot autoformalization result from Mistral. The index for the knowledge base can combine three content sources: natural language textual description, informalization of formal statements, and formal statements.": 905,
    "The paper constructs a new benchmark dataset called STORYSUMM with a focus on three principles: stories short enough for affordable human annotation, stories not widely trained on by LLMs to avoid bias, and summaries representative of powerful LLMs. To achieve this, 96 short stories were collected from Reddit subreddits (r/shortstories and r/shortscarystories), filtering out NSFW content and posts with fewer than three up-votes. These stories are typically less than one page long and are unlikely to have been widely summarized elsewhere, minimizing data contamination for LLMs. For each story, three different summaries were generated using various LLMs (Davinci-3, ChatGPT, Claude-2, GPT-3.5, GPT-4, Claude-3), resulting in 96 story-summary pairs. The dataset is split into a validation set (33 summaries from older models) and a test set (66 summaries from newer models) to assess the generalizability of automatic metrics. Each summary is about a paragraph long. The dataset includes over 500 sentence-level faithfulness labels and explanations, with unfaithful summaries further labeled as \"\"easy\"\" or \"\"hard\"\" to detect based on annotator agreement.": 906,
    "Multimodal data points are systematically categorized into three distinct interaction types—Redundancy (R), Uniqueness (U), and Synergy (S)—by formalizing the concept of modality agreement and disagreement using a prediction discrepancy function. First, unimodal classifiers, denoted as f1 for modality X1 and f2 for modality X2, are used to generate predictions y1 and y2, respectively, for a given data point. The paper defines a discrepancy function, δ(y1, y2), which quantifies the dissimilarity between these unimodal predictions. For tasks with a discrete label space Y, δ(y1, y2) is 0 if y1 equals y2 (modalities agree) and 1 if y1 does not equal y2 (modalities disagree). To categorize a data point, the interaction discrepancy, denoted as ∆1,2(y1, y2, ym), is calculated as the sum of discrepancies between each unimodal prediction and the ground-truth multimodal prediction (ym): ∆1,2(y1, y2, ym) = δ(y1, ym) + δ(y2, ym). Based on this, data points are categorized as follows:\n- Redundancy (R): When ∆1,2 = 0, meaning y1 = y2 = ym. Both modalities agree with each other and the ground-truth multimodal prediction, indicating redundant information.\n- Uniqueness (U): When ∆1,2 = 1, meaning either y1 = ym ≠ y2 or y2 = ym ≠ y1. The two modalities disagree, but one of them aligns with the ground-truth multimodal prediction, indicating unique dominant information in one modality.\n- Synergy (S): When ∆1,2 = 2, meaning y1 ≠ ym and y2 ≠ ym. Both unimodal predictions disagree with the ground-truth multimodal prediction, implying that synergistic information emerges only when both modalities are fused.\nIn practice, high-quality unimodal predictions (y1 and y2) are obtained using state-of-the-art foundation models (e.g., CogVLM2 for vision-only, Qwen2-72B-Instruct for text-only) in a few-shot prompting style, ensuring predictions are conditioned only on the respective modality. Ground-truth multimodal labels (ym) are typically derived from human annotations. Data filtering and rebalancing techniques are applied to ensure high-quality and balanced subsets for training, particularly by removing low-confidence unimodal predictions and undersampling majority classes.": 907,
    "The MMTE corpus is constructed by first selecting a source dataset, the MOH dataset (Mohammad et al., 2016), which contains 315 metaphorical and 332 literal English sentences. Four popular Machine Translation (MT) models—Google Cloud Translation API, Youdao Cloud Translation API, Helsinki-NLP/opus-mt, and GPT-4o—are then used to translate this English source data into Chinese and Italian. This generates initial machine translations for both metaphorical and literal expressions.\nTo create the gold standard reference translations and annotations, a multi-stage post-editing and annotation process is employed. Eighteen linguistics majors, native speakers of the target languages (Chinese and Italian) with professional English competency, are hired as annotators. These annotators are divided into six groups, each with three annotators, equally split between English-Chinese (EN-ZH) and English-Italian (EN-IT) instances. Each sample (source sentence and its four machine translations) is annotated and post-edited by three individuals. The annotation platform used is Doccano (Nakayama et al., 2018).\nThe annotators perform two main tasks:\n1. Annotation: They compare the source sentences with their translations and label them based on four criteria: Quality (Fluency, Intelligibility, Fidelity, Overall), Metaphorical Equivalence (Full-Equivalence, Part-Equivalence, Non-Equivalence, Misunderstanding, Error), Emotion (Zero, Less, Same, More), and Authenticity. These criteria are judged on a 5-point Likert scale. For non-metaphorical source instances, translations are classified as Literal, Metaphorical, or Error.\n2. Post-Editing: Annotators are asked to post-edit the machine translation results to generate a gold standard translation reference. They compare the four different machine translations and either select high-quality ones or modify low-quality ones to produce a reference translation for both metaphorical and non-metaphorical language. Three separate annotator groups work on each sample.\nFinally, a panel of expert translators cross-checks the annotation results, resolves disagreements in meetings, and performs final filtering to select the best quality edited translation as the gold reference. This rigorous quality control process ensures the quality of the dataset. The final dataset includes aligned English, Chinese, and Italian translations, with 315 metaphorical and 332 literal instances per language, totaling over 1900 instances.": 908,
    "The paper proposes Major Entity Identification (MEI) as an alternative referential task. In MEI, the input consists of the document and a predefined set of major entities, represented by designative phrases (e.g., \"\"Aladdin,\"\" \"\"Mustapha\"\"). This differs from traditional coreference resolution (CR) where such phrases are not provided as input. The task is limited to identifying mentions that refer *only* to these specified major entities, effectively disregarding minor entities or non-mentions. This formulation allows MEI to be viewed as an open-set classification task, where each text span is classified as referring to one of the major input entities or to a null class (minor entities and non-mention spans). This classification framework enables the use of robust and intuitive classification-based metrics for evaluation, which maintain consistent granularity and proportionally penalize perturbations. By focusing on major, frequently occurring entities and providing them as input, the burden of domain adaptation is shifted from training to inference, and the task inherently disregards insignificant or smaller clusters that often inflate CR metrics.": 909,
    "To address this research question, the paper introduces dual-biased prompting, where prompts are crafted to slightly lean towards either \"\"pun\"\" or \"\"non-pun.\"\" This approach helps gauge the confidence level of LLMs' responses. The prompts incorporate the definition of puns and several examples to assess their impact. By comparing the variations in true positive rate (TPR) and true negative rate (TNR) when the prompt bias shifts from pun to non-pun, the method evaluates the models' consistency and accuracy in recognizing puns. Additionally, Cohen’s Kappa is used to measure agreement between biased recognitions.": 910,
    "The paper addresses this by first collecting and processing a large dataset of Greek proverbs, then analyzing their spatial distribution, linguistic alternations, and distinctive local terms. For spatial distribution, it quantifies proverb dispersion by identifying proverbs existing in multiple locations (dispersed) versus those unique to a single location (non-dispersed). To analyze linguistic alternations, the Levenshtein distance is measured between widespread proverbs and their non-duplicate variants. Hierarchical clustering, specifically agglomerative clustering with single linkage and TFIDF (Term Frequency-Inverse Document Frequency) text representations (using n-grams from 2 to 5), is then applied to visualize linguistic \"\"paths\"\" of these alternations. For identifying distinctive local terms, each word in a proverb from a specific location is represented by its frequency within that location's proverbs, normalized by the inverse location frequency of that word (TFILF). This TFILF representation highlights words frequent in one location but rare across others, revealing unique linguistic identities. Pre-processing for TFILF involves lowercasing and filtering out features present in more than half of the locations.": 911,
    "The paper addresses this by implementing a \"\"Domain-Specific Instruction Generation\"\" phase. A base large language model (Mbase), such as MPT-30B, is prompted to produce new combinations of instructions (i) and corresponding input contexts (c). This generation process is guided by a handful of seed demonstrations (S), which are randomly sampled from an initial pool of human-authored domain-specific seeds. These seeds, though minimal (e.g., 80 for biomedical, 90 for financial), capture essential domain concepts and serve as the primary scaffold for generating new, comprehensive domain-specific instructions. The prompt used for this generation explicitly guides the model to create diverse tasks, avoid verb repetition, use varied language (questions and imperatives), and generate realistic, challenging inputs.": 912,
    "The paper designs a cerebral language agent that serves as the core intelligence for the AI assistant. This agent is built upon a Large Language Model (LLM) and is augmented with three key capabilities: memory, planning, and functional tool interaction. It is responsible for replying to trainees' requests, providing guidance, and interacting with both the Mixed Reality (MR) application and the vision-language agent. The agent can handle diverse multimodal inputs, including instruction manuals, historical conversations, and metadata from the MR environment. Based on these inputs, it generates actions, which can be either direct responses to the user or Application Programming Interface (API) calls to the MR application. This design allows the agent to make decisions based on past experiences and adapt its assistance. Its scope is defined by a system prompt, enabling it to tailor assistant services by interacting with MR applications to gradually discover business needs and to understand multimodal context through its interaction with the vision-language agent.": 913,
    "The paper proposes JointEDI, a unified generative model that uses a multi-lingual BART (mBART) as its sequence-to-sequence backbone. This framework takes an input sequence formatted as `[Task: EuphemismDetectionandIdentification; s]`, where `s` is the sentence to be analyzed. The mBART encoder processes this input, and the mBART decoder then generates a target sequence in the format `[EuphemismLabel: y; ClassLabel: c]`. Here, `y` is a binary label (0 or 1) indicating whether a potential euphemism term (PET) in the sentence is indeed a euphemism, and `c` is the category label if `y` is 1, or 'none' if `y` is 0. This generative approach allows the model to produce both the detection outcome and the identification category within a single output sequence, effectively unifying the two tasks.": 914,
    "The paper addresses this by implementing a Structure Preference Optimization module as the first stage of its multi-stage curriculum learning framework. Given a pun word (pw) and an alternative word (aw) pair, a prompt (x) is generated using a designed template. The labeled pun from the training dataset serves as the positive sample (y+). Negative samples (y−s) are produced by randomly generating puns with the same prompt using a pre-trained SFT LLM (e.g., LLaMA2). A structured discriminator then judges if the generated pun satisfies the required structure: for English homographic and Chinese homographic puns, the pun word (pw) must be present; for English homophonic puns, pw must be present without aw; and for Chinese homophonic puns, both pw and aw must be present. If a generated pun does not comply with these structural rules, it is accepted as a negative sample. These (y+, y−s) pairs are then used to perform Direct Preference Optimization (DPO) with a loss function `Lstructure-dpo(θ) = -logσ(r+(θ) - r−s(θ))`, where `r+` and `r−s` are pseudo-rewards based on the log-likelihood difference between the model being optimized (`πθ`) and the SFT model (`πsft`). This process increases the likelihood of structurally correct puns while reducing that of incorrect ones.": 915,
    "The paper addresses the quality control of pseudo-labeled data through a Natural Language Inference (NLI) filter. Initially, a label extraction model (Me) and a text generation model (Mg), both based on T5-base, are trained on source domain data. The extraction model Me then extracts a pseudo-label (l') from an unlabeled target domain sentence (t), and the generation model Mg generates a new sentence (t') aligned with this pseudo-label, forming a labeled sample (t', l'). To control quality, the NLI filter (f) is employed. It takes the original target domain text (t) as the premise and the newly generated text (t') as the hypothesis. The NLI filter determines the relationship (y) between this pair, which can be Entailment, Contradiction, or Neutral, formulated as P(y|t, t') = f(t, t'). If a contradiction relation arises between t and t', it indicates that the generated text t' is unlikely to be inferred from the original text and is thus filtered out. This process removes problematic samples, yielding a higher-quality labeled dataset denoted as Dtf.": 916,
    "The paper addresses this by first extracting unimodal representations from pre-trained models: `ria` for speech using wav2vec2 and `rit` for text using RoBERTa. These are then disentangled into two separate representations: modality-invariant and modality-specific. For modality-invariant representations, a linear layer `Einv` is used to learn `hia,inv` from `ria` and `hit,inv` from `rit`. To ensure consistency and learn these representations, semantic-level contrastive learning (`Lsemcts`) is applied, treating modality-invariant representations from matching speech and text examples as positive pairs and those from different examples within the same batch as negative pairs. For modality-specific representations, two distinct linear layers, `Ea` for speech and `Et` for text, extract `hia,spe` from `ria` and `hit,spe` from `rit`, respectively. Modality-level contrastive learning (`Lmodcts`) is then employed to ensure that modality-specific representations for the same modality contain consistent information across different examples (positive pairs), while cross-modality ones are treated as negative pairs. This process results in four distinct representations: `hia,inv`, `hit,inv`, `hia,spe`, and `hit,spe`.": 917,
    "The paper addresses this by first creating specialized binary classifiers for each of the six figurative language (FL) features: Metaphor, Simile, Sarcasm, Hyperbole, Idiom, and Irony. These binary classifiers are based on RoBERTa (Liu et al., 2019) and are trained on a combination of 13 publicly available datasets. For each binary classifier, a balanced training set is formed by selecting all positive examples and supplementing them with an equal number of negative and literal examples. Literal examples are transformed to \"\"not_feature_X\"\" labels. After training these individual binary models, they are used to automatically label the combined training dataset in a multi-label format. This process generates an augmented FL training corpus where each sentence has a list of predicted FL labels. To ensure high quality, only examples where the predicted labels are consistent with the original human annotations are retained. This multi-label dataset is then used to train the Multi-task Figurative Language Model (MFLM), which is also based on RoBERTa. The MFLM is designed to label a sentence with all applicable features in a single pass. The performance of the MFLM is then compared against the specialized binary models on 13 task-specific test sets using weighted average F1-score.": 918,
    "The method integrates observation-level and domain-level distributional information by constructing a Multi-level Feedback Vector (Runified). First, Observation-level Distribution (Robs) is generated for a document `d` using SPLADE, which expands the document's original vocabulary `Vd` into an augmented set `ˆVd`. `Robs(d)` is represented as a vector where each element `SP(d,v_i)` corresponds to the weight from SPLADE for the `i`-th term `v_i` in `ˆVd`. Second, Domain-level Distribution (Rdom) is approximated for the entire Out-Of-Domain (OOD) corpus using Inverse Document Frequency (IDF). `Rdom` is a vector of `IDF(v_i)` values for terms `v_i` originating from the OOD corpus, computed from up to 1,000 randomly selected documents to approximate the oracle distribution. To ensure scale consistency, both `Robs` and `Rdom` undergo a two-step normalization process: first, z-score normalization `Z(·)`, followed by tanh normalization `tanh(Z(·))`. Finally, these normalized distributions are merged into the `Runified(d)` vector using a dot product: `Runified(d) = Norm(Rdom) · Norm(Robs(d))`.": 919,
    "The paper addresses this by creating a handcrafted Natural Language Inference (NLI) dataset with a unique multi-label annotation scheme and detailed linguistic characterization. The dataset creation follows a three-stage process: first, each author independently writes sentence pairs and suggests a set of tags and labels; second, authors cross-annotate pairs from others with hidden tags and labels; and third, an aggregation process resolves disagreements, particularly for examples where tags or labels do not reach a majority. This ensures internal consistency and captures all possible inference relations (Entailment, Contradiction, Neutral) that may hold between a premise and hypothesis, rather than forcing a single label. Additionally, the Greek version of FraCaS (Amanaki et al., 2022) is re-annotated according to this multi-label format. Each sentence pair is \"\"adorned with a linguistic characterization in the form of tags à la SuperGlue,\"\" encompassing various phenomena that influence inference, which are intended for diagnostic analysis outside the model's input/output pipeline.": 920,
    "The paper addresses this question by fine-tuning a multilingual transformer model, specifically XLM-RoBERTa-base (XLM-R), on euphemism disambiguation data. The task involves classifying potentially euphemistic terms (PETs) as either euphemistic (1) or non-euphemistic (0) within a given context. The experiments are conducted using four languages: Mandarin Chinese (ZH), American English (EN), Spanish (ES), and Yorùbá (YO). For training, the model is configured in two primary settings: \"\"monolingual\"\" where it is trained on data from a single language, and \"\"multilingual\"\" where it is trained on combined and shuffled data from multiple languages. For each test run, 1800 examples are randomly sampled from each language, and an 80-10-10 split is used to create training, validation, and test sets. The test sets are kept constant across all settings to ensure consistent evaluation. Non-default fine-tuning parameters include a batch size of 16, a learning rate of 1e-5, a maximum of 30 epochs, and an early stopping patience of 5. The model's performance is evaluated using Macro-F1 scores averaged over 30 test runs. The study specifically investigates zero-shot cross-lingual learning by evaluating models trained on one language against test sets from other languages, and assesses multilingual synergy by comparing the performance of multilingual models against monolingual ones. Statistical significance is determined using a paired t-test (p=0.05) across the 30 runs.": 921,
    "The implicit multi-task model addresses the representational alignment by training and utilizing embeddings for the token labels of Open Information Extraction (OIE) relations. Specifically, for each of the three IOB2 (Inside, Outside, Begin) tags used for OIE relations, a randomly initialized vector is created. During training, the contextualized token embedding from the last layer of the Pre-trained Language Model (PLM), denoted as `xPLM`, is concatenated with the embedding `xOIE` corresponding to the OIE relation label of that token. This combined representation, `x = [xPLM; xOIE]`, is then fed into a standard softmax classifier, `softmax(WTclx + bcl)`, which predicts the IOB2 event trigger label for the token. The model is trained on trigger detection data in the source domain, optimizing all PLM parameters, the classifier's parameters (`Wcl` and `bcl`), and the trainable embedding matrix (`XOIE`) containing the OIE label embeddings. At inference time in the target domain, an OIE system is first run on the test sentences to obtain the OIE relation labels for tokens. These OIE labels are then used to retrieve their corresponding embeddings (learned during training), which are concatenated with the PLM's output for subsequent trigger detection inference. This approach incentivizes the model to establish contextualized associations between the two tasks within the OIE label embeddings, leveraging the domain-agnostic nature of rule-based OIE to improve recall of TD in the target domain.": 922,
    "The Clustering-based Sampling (C) approach selects high-quality few-shot samples by utilizing topic information inherent in the target domain data. Initially, for a given target domain dataset (Dt), d-dimensional sentence embeddings (Et) are generated for all documents using a SentenceTransformer model. Subsequently, KMeans clustering is applied to these embeddings to form 'c' sub-domain clusters, denoted as C = {C1, ..., Cc}. To represent the topic of each sub-domain, sub-domain embeddings (EC = {eC1, ..., eCc}) are computed by averaging all document embeddings within their respective clusters. Each document (xi) in Dt is then assigned a score (si) using a cosine-similarity-based scoring function (Equation 1). This function calculates the average cosine similarity between the document's embedding (exi) and all sub-domain embeddings (eCj), thereby assigning higher scores to samples that demonstrate high relevance across multiple sub-domain topics. Finally, the 'k' documents with the highest scores are chosen as the few-shot samples (X*), as defined by Equation 2. A refined version, Stratified Clustering-based Sampling (SC), further enhances diversity by first ranking samples using the same scoring function and then selecting a proportionately equal number of top-scoring samples from each cluster, ensuring representation across all sub-domains.": 923,
    "The paper addresses this by designing a Domain Knowledge Decoupling module. Unlike traditional LoRA (Low-Rank Adaptation) models, this module learns a domain-invariant adapter (θS) and separate domain-variant adapters (θi) for each domain. For the i-th domain, training data includes the current domain's data (Di) and replay data (DR,i), which consists of a few samples from previously learned domains. The domain-invariant adapter θS (represented by BSAS, where B and A are low-rank matrices) is trained based on the replay data DR,i using a language modeling loss (LML). Simultaneously, the domain-variant adapter θi (represented by BiAi) for the i-th domain is trained based on the current domain data Di, also using LML. To ensure that the domain-variant knowledge is distinct from the domain-invariant knowledge, an orthogonal constraint is introduced. This constraint forces the matrices Bi and Ai to be orthogonal to BS and AS, respectively, by minimizing the squared Frobenius norm of their products (∥ATiAS∥2 + ∥BTiBS∥2). The final training loss for this decoupling is a weighted sum of the invariant adapter loss, the variant adapter loss, and the orthogonal constraint loss. This mechanism ensures that the model learns shared, general sentiment knowledge (domain-invariant) separately from domain-specific, context-dependent sentiment knowledge (domain-variant).": 924,
    "The paper enhances the traditional metaphor detection task by introducing a \"\"metaphor reason\"\" component. Instead of merely classifying a target word as \"\"metaphor\"\" or \"\"literal,\"\" the improved task requires the model to provide a corresponding reason or explanation for that judgment. This is visually represented where the model first judges the usage (metaphor or literal) and then generates a textual reason to justify its decision. This new task aims to move beyond simple labeling to interpret *how* to understand the metaphor, thereby addressing the limitation of current metaphor detection tasks that only provide labels without interpretation.": 925,
    "The GIT-LLaVA model combines a pretrained video captioning model (GIT) and a Large Language Model (LLM) (Vicuna). The video representation obtained from GIT is mapped to the embedding space of Vicuna using a simple Multilayer Perceptron (MLP). This mapping network learns to align the video representation with the LLM's embedding space, allowing the LLM to generate metaphor captions conditioned on the video input. The model is pre-trained on a synthetic dataset of metaphor and normal images and then fine-tuned on a manually annotated dataset of videos containing metaphors. The fine-tuning process ensures that the model can generate creative and accurate metaphor captions despite limited training data.": 926,
    "The paper proposes a GAN-based multi-level adaptation approach to achieve domain-invariant representations across all layers of the Pre-trained Language Model (PLM). After the initial supervised training of the source backbone with attached exit classifiers, the target encoder's weights are initialized using the frozen weights of the source encoder. For each layer `i` of the PLM, a dedicated discriminator function, denoted as `Di`, is introduced. This discriminator `Di` is trained to classify whether the output features from layer `i` originate from the source domain or the target domain. The discriminator's loss, `Ldisi`, is formulated to maximize its ability to correctly identify the domain of the input features. Concurrently, the target encoder is trained to act as a generator, aiming to produce features that are indistinguishable to these discriminators. The generator loss for the `i`-th layer, `Lgeni`, is designed to minimize the discriminator's confidence in identifying the domain, thereby compelling the target encoder to generate features that are domain-invariant at that specific layer. This adversarial training process involves alternating optimization between the target encoder (generator) and the discriminators. The overall discriminator loss `Ldis` and generator loss `L` are computed as weighted averages across all `L` layers, with deeper layers assigned higher weights to reflect their role in learning more domain-specific representations.": 927,
    "The paper addresses this by introducing a Structured Chain-of-Thought (SCoT) prompting approach, which represents the complex task of generating content-grounded multi-turn question-answer (QA) conversations as a state machine. This state machine comprises four distinct states, each corresponding to an individual action in the conversation generation process. The states are: User utterance generation (uu), Question answerability classification (ac), Answer sentences selection (ss), and Agent utterance generation (au). The overall algorithm defines specific sequences of state transitions, such as `uu` to `au`, `uu` to `ac` to `au`, or `uu` to `ac` to `ss` to `au`. For each state, a Large Language Model (LLM) is few-shot prompted with task-specific instructions and exemplars to execute the corresponding action. A full conversation is generated by repeating this process N times for N turns. This modular breakdown allows for dedicated resources and actions within each state, enabling fine-grained control and optimization of the generation process.": 928,
    "The paper addresses this by employing an Intent Extractor (Mextract), which is a large language model (GPT-4 for GPT agents and Llama-3.1-405B-FP8 for Llama agents) prompted to discover intents. The intent space is constrained to very short phrases, specifically gerund + noun phrase, consisting of up to three words. This compact form ensures semantic distinction even with minor changes. The Mextract model takes the current observation (including task description), the action performed, and previous-step intents as input. It is instructed to identify the underlying intent of the current action, ensuring it is concise, contextually relevant to the final goal and previous subgoals, and does not include named entities or proper nouns. The goal is for the inferred intent, combined with the final goal, to uniquely identify the target element among HTML elements. The process is applied to each step of a demonstration trajectory to construct an intent-augmented demonstration dataset.": 929,
    "The paper addresses the reliable extraction of fine-grained sensorimotor information by developing and training 11 distinct sensorimotor regressors. Each regressor is a neural network specifically designed to map a 768-dimension BERT embedding (obtained from layer 0 of the BERT model) of a word to a single scalar value. This value quantifies how strongly the lexicalized concept is experienced by one of 11 specific sensorimotor dimensions, which include Auditory, Gustatory, Olfactory, Visual, Tactile, Interoceptive, Hand_Arm, Foot_Leg, Head, Mouth, and Torso. These regressors are trained using the Lancaster Sensorimotor Norm (Lynott et al., 2020), a dataset providing human-rated sensorimotor information for 39,707 English words. Each regressor's architecture consists of two fully connected hidden layers, with sizes of 384 and 192 respectively, both utilizing ReLU activation functions. The training process minimizes the mean squared error and employs the Adam optimizer, incorporating 5-fold cross-validation and early stopping based on validation loss. The reliability of these regressors in predicting sensorimotor values is evaluated by computing Pearson correlations between their outputs and the human ratings from the Lancaster Sensorimotor Norms.": 930,
    "The paper addresses this by employing a Hypernetwork Instruction Learning (HIL) module within a hypernetwork-assisted encoder-decoder architecture. The underlying model is a pre-trained encoder-decoder model, specifically an extension of the MAM model, which incorporates parameter-efficient submodules like prefix-tuning and adapters. The HIL module is designed to generate domain-specific adapter parameters exclusively for the decoder. This process begins by creating several task instructions (e.g., \"\"To generate a summary in such a way that the context should be present in input\"\") and domain-related instructions (e.g., \"\"This input focuses on product design in an industrial setting\"\" for the Product domain). These instructions are then converted into representations using a pre-trained HyperEncoder. The HIL module encodes all instructions via this HyperEncoder. Subsequently, an integration operation appends these encoded instructions with the encoder output of the underlying model. This integrated vector, denoted as `e`, is formed by mean pooling the instruction representation (`hI`) and the hidden representation from the encoder output (`hD`), i.e., `e = [Mean(hI); Mean(hD)]`. To ensure diverse adapter parameters across different Transformer layers, `e` is further concatenated with a learnable layer embedding (`el`). Finally, a two-layer Parameter Generator takes this combined vector as input to produce the adapter parameters (`phi`). Specifically, the process involves a ReLU-activated hidden layer `h = ReLU(Wi,0([e;el])+bi,0)` followed by an output layer `phi_i = Wi,1(h)+bi,1`, where `Wi,0`, `bi,0`, `Wi,1`, and `bi,1` are trainable parameters. These generated `phi_i` are then sliced and reshaped to form the adapter parameters `[Wiu, Wid, biu, bid]` for the i-th Transformer layer.": 931,
    "The paper integrates prompt learning into the source domain training phase by augmenting a Pre-trained Language Model (PLM), specifically the T5 architecture, with an additional soft prompt (π). This soft prompt is a sequence of trainable vectors (⟨v1, v2, ..., vk⟩) prefixed to the input (e.g., \"\"question:xxx context:xxx\"\"). During initial training on the source domain, the prompt (π) is first fine-tuned while keeping the Question Answering (QA) model (f) static. This step is designed to capture and internalize domain-agnostic QA concepts, which are crucial for broader generalizability. Following this, the QA model (f) itself is trained with the now-tuned prompt (π) remaining fixed. The training objective for both the prompt and the QA model is defined by a cross-entropy loss function, aiming to maximize the conditional probability of generating the correct answer given the context, question, and prompt. This two-step training process ensures that the prompt effectively guides the PLM to assimilate fundamental QA knowledge that is applicable across various domains, rather than being bound by the specificities of the source domain, thus preparing the model for effective source-free adaptation.": 932,
    "The importance of large language model (LLM) weights for general linguistic capabilities is assessed using a General Weight Importance module. This module operates on the hypothesis that an important weight, if removed (set to zero), will cause a larger increase in the loss value compared to less important ones. Formally, for an open-domain calibration dataset Dg = {xj, yj} with size N, the importance of each weight at index m, denoted as IWm, is approximated using a Taylor series expansion. This approximation considers the change in cross-entropy loss L(Dg) when the weight Wm is set to zero. For sufficiently trained foundational language models, the importance can be further approximated by εm = 1/2 * (Wm)^2 * [H^-1]mm, where H denotes the Hessian matrix. This εm value represents the error caused by removing the weight Wm. The method computes εm for all prunable weights and constructs a matrix of importance scores, G, with respect to general domains, having the same dimension as the weight matrix W.": 933,
    "The paper proposes a pretraining algorithm for named entity recognition in a transfer learning setting, focusing on enriching the feature space with auxiliary data derived from event mentions. This involves two main approaches for obtaining auxiliary embedding vectors:\n1.  Concatenation-based Event Embedding: This approach uses an off-the-shelf token encoder (SapBERT) pretrained on biomedical data. For an event mention, its arguments (e.g., theme, cause, product, site) are encoded using SapBERT, and the resulting vectors are concatenated to form the event representation. For nested events, a recursive compression function (averaging successive elements) is applied to maintain consistent dimensions. Missing arguments are handled by padding with random partial embeddings sampled from a Gaussian distribution.\n2.  Sentence-Encoder-based Event Embedding: This method leverages Large Language Models (LLMs) to generate templates for event types. Event types and their arguments are submitted to an LLM (ChatGPT) to construct templates. For event mentions in the source data, these templates are completed by replacing placeholders with actual arguments. The resulting passages are then processed by an off-the-shelf sentence encoder (S-PubMed-BERT) to obtain the final representation vectors.\nOnce auxiliary vectors are obtained, an Auxiliary Similarity ($\\kappa_{ij}$) is calculated between two entities $x_i$ and $x_j$ as the maximum cosine similarity between any pair of their auxiliary vectors. This captures the relatedness of their contexts.\nFinally, a Refined Multi-Similarity (RMS) Loss is used for Contrastive Grouping. This loss adapts the standard multi-similarity loss by incorporating the auxiliary similarity scores ($\\kappa_{ij}$) into the soft weights for positive and negative pairs. For positive pairs, higher weight is given to instances with smaller primary similarity (cosine similarity between encoder outputs) but higher auxiliary similarity, indicating a need for the encoder to better project these related entities. For negative pairs, higher weight is given to pairs with higher primary similarity but lower auxiliary similarity, signaling a need to revise parameters to separate them. The overall pretraining objective combines the standard cross-entropy loss for NER with this unsupervised RMS term.": 934,
    "The paper addresses this by developing the MulticulturAl Proverbs and Sayings (MAPS) dataset. This involves collecting proverbs and sayings, along with their explanations, from Wikiquote and Wiktionary across six geographically and topologically diverse languages: English, German, Russian, Bengali, Mandarin Chinese, and Indonesian. To create conversational contexts for these proverbs, a model-in-the-loop approach is employed, where GPT-3.5 (gpt-3.5-turbo-0301) is prompted with fixed templates to generate initial seed conversational contexts. Subsequently, two or more native speakers (experts or crowd annotators) for each language either accept these model-generated conversations or rewrite them if the proverb's usage is flawed or irrelevant to the context. For Russian and Bengali, contexts are initially written in English, then machine-translated and fixed by native speakers. For each proverb in its conversational context, annotators create one correct and one wrong interpretation to formulate a binary inference task. Additionally, each proverb usage is labeled as figurative or not, indicating whether its interpreted meaning differs from its literal meaning. Quality control is performed by a separate set of native speakers who assess the correctness of proverb usage and interpretations.": 935,
    "The paper addresses this question through the novel IE-SPV (Internal and External Selectional Preference Violation) module. This module is designed to identify the metaphoricity of a target word by detecting whether it is semantically reasonable within its context. Unlike previous methods that primarily focused on the difference between the target word's contextual meaning and the sentence vector, IE-SPV introduces two distinct difference measures. First, it calculates the sentence internal difference (hin) by comparing the contextual meaning vector (ht) of the target word with the overall sentence meaning vector (hs). This is achieved using a linear transformation that concatenates hs, ht, their absolute difference, and their element-wise product, followed by weights (WTin) and biases (bin). Second, it computes the sentence external difference (hex) by comparing the sentence vector (hs) with the literal meaning vector (hl) of the target word. This is also done via a linear transformation using concatenated hs, hl, their absolute difference, and their element-wise product, with weights (WTex) and biases (bex). By combining both hin and hex through concatenation to form hIE-SPV, the model gains a more comprehensive understanding of the semantic relationship between the target word and its context, enabling a better judgment of its reasonableness and thus its metaphorical usage.": 936,
    "The paper addresses this by re-annotating an existing corpus of news editorials from the New York Times (El Baff et al., 2018) through a two-level annotation process. Firstly, metaphors are identified and annotated, distinguishing between single and composite metaphors. For single metaphors, the widely-used Metaphor Identification Procedure (MIPVU) by Steen (2010) is employed, which involves steps like understanding general meaning, determining lexical units, establishing contextual meaning, identifying a more basic meaning, and checking for contrast between contextual and basic meanings while maintaining comparability. For composite metaphors, a custom procedure is devised: a pivot metaphorical word is identified using MIPVU, and then a context window of one to two words on either side of the pivot word is identified. Secondly, for each identified metaphor, annotators determine its source and target domains from a pre-defined set of concepts, which combines domains from Shutova et al. (2010) and Gordon et al. (2015), with an option for annotators to add new domains. The annotation process involved two pilot studies to refine guidelines and ensure consistency, followed by a main study with three annotators using a customized LabelStudio platform. Inter-annotator agreement was calculated using span overlaps (exact for single, full/partial for composite) and Fleiss’κ on BIO format tags, and final annotations were consolidated by selecting the most reliable annotator's output for each editorial.": 937,
    "The paper proposes a \"\"Joint ELM-BLM\"\" masking strategy that combines the benefits of Entity-level Masking (ELM) and Base-level Masking (BLM). ELM is utilized to enrich the language model (LM) with contextual knowledge necessary for discriminating targeted domain-specific terms (DS-terms) by masking named entity spans. Simultaneously, BLM is employed to preserve the pre-trained language model's (PLM) inherent domain and generic knowledge by randomly replacing a proportion of tokens with [MASK] tokens. A crucial aspect of this strategy is ensuring that the sets of tokens masked by BLM and ELM are disjoint, preventing overlap and allowing distinct learning signals. Unlike conventional approaches, the method avoids the assumption of a fixed 15% masking rate and instead explores a spectrum of rates to find an optimum. In the experimental setup, the ELM rate is established with respect to the total number of DS-terms mentioned in an input sequence (e.g., 25% ELM for 4 DS-terms means 1 DS-term is masked), rounding up decimal values to the nearest integer. The conventional 15% masking rate for BLM is initially halved to 7.5% to account for the masking budget consumed by ELM, though both rates are later varied to study their influence on performance. This joint strategy aims to strategically increase the LM's awareness of DS-terms while retaining sufficient knowledge of generic terms.": 938,
    "The paper addresses this by introducing a multi-stage \"\"Domain Document Selection\"\" process. First, for \"\"Collection Document Clustering,\"\" the full target document collection undergoes preprocessing to discard short or noisy documents. Then, a state-of-the-art text encoder, Contriever, is used to generate embeddings for each document. These embeddings are then clustered using K-Means, where the number of clusters (K) is a tunable hyper-parameter, determined optimally using the Elbow method (found to be K=1000). This clustering aims to divide the collection into topical portions, ensuring diverse topical coverage. Second, for \"\"Probabilistic Document Sampling,\"\" the method accounts for imbalanced cluster sizes by sampling more documents from larger clusters, proportional to their size. The probability of selecting a document (Di) from a cluster (clusterk) is determined by a normalized softmax function, Pr(Di|clusterk) = edi/T, where edi is an exponential value representing the cosine similarity between the document's embedding (vDi) and its cluster centroid, and T is the softmax temperature. This ensures that documents more representative of their cluster's core topic are more likely to be selected. Third, for \"\"Diversified Document Selection,\"\" to enhance selection robustness, the probabilistic sampling process is iterated multiple times (m=5) to generate different sample sets. From the pooled documents of these sets, Maximal Marginal Relevance (MMR) is applied to select the final top-Nk documents for each cluster. MMR balances similarity to the cluster centroid (Sim1) with dissimilarity to already selected documents (Sim2), using a trade-off weight (λ=1.0), thereby ensuring both representativeness and diversity in the final set of documents used for query generation.": 939,
    "The paper addresses this by manually simplifying metaphor theories and constructing a Metaphor Knowledge Graph (KG). This KG, formulated as G={E,R,F}, where E represents entities, R represents relations, and F represents facts, covers essential information from both the theory and computational aspects. On the theory side, the KG represents fundamental concepts and their relations identified in the metaphor theory. On the computational side, it includes core linguistic expressions such as words, sentences, and Part-of-Speech (POS) information, along with their connections. The entities (E) are divided into two groups: concepts (e.g., source domain, target domain, basic meaning, contextual meaning) and attributes (e.g., POS tag, sentence). Relations (R) can be directed or undirected between entities. Each fact (F) is represented as a triple (head entity, relation, tail entity). This structured representation of metaphor theories (Selectional Preference Violation, Metaphor Identification Procedure, and Conceptual Metaphor Theory) provides a clear instructional structure for the LLM.": 940,
    "The paper designs an innovative Label Prompt (PL) and a Context Prompt (PC). The Label Prompt is structured as `\"\"relation is not Si1,...,Sij...or Nrandom\"\"`, where `Sij` are relations from a set of similar relations identified by filters, and `Nrandom` is a randomly selected non-similar relation. This explicit negative phrasing aims to guide the model away from confusing labels and prevent radical predictions. The Context Prompt is created by replacing each entity in the original sentence with a special token `[BLANK]`, which helps capture scenario-specific differences. The final input instance `Xall` is formed by concatenating the correct label `L`, the original sentence `Xori`, the Context Prompt `PC`, and the Label Prompt `PL` in sequence, separated by `[SEP]` tokens. This combined prompt structure enhances the model's ability to alleviate confusion in cross-domain scenarios.": 941,
    "The study uses MisNet, a metaphor identification model incorporating MIP and SPV, to identify metaphors in sentences extracted from the CC-100 corpus. Sentences containing verb-object pairs are processed, and each pair is evaluated for concreteness, imageability, and familiarity using datasets from Ljubeši´c, Fišer, and Peti-Stanti´c (2018) and Maddela and Xu (2018). The average scores for these properties are then calculated and compared between metaphoric and literal usages.": 942,
    "The paper addresses feature alignment through a mutual nearest neighbors (MNN) contrastive learning paradigm. First, a classifier (ϕ) is trained on the labeled source domain using cross-entropy loss (Lce). To overcome domain shift and class-specific structure issues, an instance contrastive loss (LIns−Con) is designed. For any sample `i` in the target domain, its `k` nearest neighbors (`Gs_i`) are searched in the source domain. Similarly, for a sample `j` in the source domain, its `k` nearest neighbors (`Gt_j`) are found in the target domain. If an instance pair from different domains (`i` and `j`) is contained in each other's nearest neighbor set (i.e., `i ∈ Gt_j` and `j ∈ Gs_i`), they are identified as mutual nearest neighbors, forming a \"\"positive pair.\"\" An adjacency matrix `Ast` (`Rns×nt`) is constructed where `Ast_ij = 1` for positive pairs and `0` otherwise. To capture the target domain's intrinsic structure, mutual nearest neighbor pairs are also identified within the target domain, forming an adjacency matrix `Att` (`Rnt×nt`). For the source domain, an affinity matrix `Ass` (`Rns×ns`) is derived from ground-truth labels.\nDuring training, the goal is to pull these MNNs closer while pushing geometrically dissimilar samples apart. An effective memory bank `B` is employed to store diverse global features from both domains, including current minibatch features and older features. For each training sample `i`, its positive (`Pi`) and negative (`Ni`) sample sets in `B` are inferred using `Ass`, `Ast`, and `Att`. The instance contrastive loss `LIns−Con` is formulated as:\n`LIns−Con = - Σ_{i=1}^{ns+nt} Σ_{j∈Pi} log(exp(z_i z_j / τ) / (Σ_{k∈Pi} exp(z_i z_k / τ) + Σ_{l∈Ni} exp(z_i z_l / τ)))`\nwhere `z_i` represents the embedding feature of sample `i`, and `τ` is a temperature parameter. By minimizing `Lce + λLIns−Con` (where `λ` is a weight parameter), the method aims to spread out known categories in the source domain, align common categories across domains, and compact clusters within the target domain.": 943,
    "The paper addresses accurate uncertainty estimation by formulating it as a Bayesian uncertainty estimation problem through the introduction of an evidential deep learning (EDL) paradigm. EDL employs a Multinomial-Dirichlet hierarchical model to predict the distribution of class probabilities and provide associated uncertainty. Specifically, the model outputs evidence parameters (α) for each class, which are then used to derive the predictive probabilities (pk = αk / S, where S is the total evidence, S = Σαk) and the prediction uncertainty (u = Ls / S, where Ls is the number of known categories). A logarithmic evidence score, theoretically aligned with data likelihood, is proposed for \"\"unknown\"\" sample detection. This total evidence score S is a summation-based statistic, free from the competition inherent in softmax normalization, making it more suitable for distinguishing \"\"known\"\" from \"\"unknown\"\" samples.\nTo overcome the overfitting risk of EDL in a closed set and ensure proper calibration, a u-c adversarial mechanism is introduced. This mechanism defines an uncertainty versus confidence (u-c) adversarial objective (L2). The first term of L2 aims to enforce low uncertainty (u → 0) and high confidence (c → 1) for labeled source domain samples. The second term penalizes the u-versus-c homogeneity on the unlabeled target domain, forcing uncertainty and confidence to optimize in opposite directions. This encourages the model to learn a skewed and sharp Dirichlet simplex for \"\"known\"\" categories (high confidence, low uncertainty) and an unskewed and flat Dirichlet simplex for \"\"unknown\"\" samples (low confidence, high uncertainty). This adversarial calibration mechanism does not require an additional validation set during training, enhancing its flexibility.": 944,
    "The paper theoretically characterizes the asymptotic behavior of Softmax layer weights by analyzing the categorical cross-entropy loss function. Under Assumption 1, which posits that data representations follow a Gaussian Mixture Model (GMM) with class-wise means (m_l) and covariances (C_l), the loss function converges to its expectation for large datasets. The asymptotic class-weight vectors (w_hat_l) are then defined as those satisfying the condition where the expectation of the gradient of the loss with respect to w_hat_l is zero. Proposition 2 provides an implicit equation for these asymptotic class-weight vectors, showing that each w_hat_l is determined by a weighted sum of expected values involving the representations' means and covariances across all classes. Furthermore, Corollary 1 demonstrates that by carefully choosing the hyperparameters (xi_j) in the generalized labels (specifically, setting xi_j = (k * n_j)^-1 where n_j is the class proportion), the class-weight vectors can be made asymptotically independent of class proportions, which is desirable for handling unbalanced datasets. This analysis provides a foundational understanding of how Softmax weights relate to the underlying statistics of data representations.": 945,
    "The paper proposes using the intrinsic free energy of an unlabeled target sample as a surrogate metric to reflect its domain characteristic. Energy-based models (EBMs) assign lower energy to correct answers and higher energy to incorrect ones. The free energy, F(x), for an input x is defined as F(x) = -log(sum_y exp(-E(x;y))), where E(x;y) is the energy of input x with label y. The observation is that models trained only on labeled source data exhibit free energy biases, where the free energy distribution of supervised source data is lower than that of unlabeled target data. This implies that target samples with higher free energy are more dissimilar to source data and thus more representative of the target distribution. In the selection process, the first step involves computing the free energy F(x) for all unlabeled target samples. Then, a top percentage (alpha1%) of these samples with the highest free energy values are selected to form a candidate set. This ensures that the selected candidates are unique to the target distribution and complementary to the existing labeled source data, addressing the domain characteristic aspect of sample selection.": 946,
    "The paper proposes GearNet, a universal paradigm that exploits bilateral relationships between the source and target domains by training two models, fθ and ˜f˜θ, alternately. The learning strategy comprises three parts: a pre-trained process, a forward step, and a backward step, which are iteratively conducted. Initially, in the pre-trained process, model fθ is trained on the source domain data with noisy labels. This pre-trained fθ is then used to generate hard pseudo labels for the unlabeled target domain instances. Following this initialization, the forward and backward steps begin to iteratively transfer knowledge in opposite directions. The backward step trains model ˜f˜θ, aiming to transfer knowledge from the target domain (using its pseudo labels) to the source domain. Subsequently, the forward step trains model fθ, aiming to transfer knowledge from the source domain (with noisy labels) to the target domain. This alternating training enables the discovery and leveraging of pseudo supervision information from the target domain, which in turn improves the model's robustness against label noise originating from the source domain. This bilateral exchange of information helps to attenuate accumulated errors that typically arise from biased sample selection in unidirectional learning.": 947,
    "The paper addresses the discovery and separation of hidden subdomains by estimating optimal subdomain separations, denoted as X, through maximizing inter-subdomain discrepancies. This is formalized by the objective function X∗ = argmaxX (∑i̸=j D(Xt i,Xt j) + ∑i̸=j D(Xs i,Xs j)), where D(·,·) is a domain discrepancy measure. For tabular data, D(·,·) is chosen as the sum of one-dimensional Wasserstein distances over each feature. When specializing to temporal drift, the objective is redefined to maximize discrepancies only between successive subdomains: X∗=argmaxX (∑kt−1i=1 D(Xt i,Xt i+1) + ∑ks−1j=1 D(Xs j,Xs j+1)). Since this objective function is not differentiable due to categorical features, the Nelder-Mead method is employed for optimization. The maximum number of subdomains (ksup t and ksup s) is determined iteratively: starting from two subdomains, the number is gradually increased by one if each resulting subdomain contains more than a minimum number of examples (nm), otherwise the process stops.": 948,
    "The paper proposes training a Graph Neural Network (GNN) in a supervised fashion to predict variable biases for unseen instances of Binary Linear Programs (BLPs). Variable biases, denoted as (cid:22)b(I), are defined as the component-wise average over a set of feasible solutions F ∗ (I) that are close to the optimal solution (within a tolerance (cid:28) > 0 of the optimal value). This set of near-optimal solutions is collected using a solver's solution pool feature. The learning problem is set up to minimize the empirical error of a function f(cid:18) (representing the GNN model with parameters (cid:18)) that predicts these biases. Specifically, the objective is to minimize (1/|S|) * sum over I in S of `(f(cid:18)(V(I)); (cid:22)b(I))`, where S is a finite training set, V(I) is the set of variables for instance I, and ` is a loss function. To simplify training and make predictions more interpretable, the continuous bias prediction (a regression problem) is transformed into a classification problem by introducing a threshold value (cid:28) ≥ 0. If a variable's bias (cid:22)b(I)i is less than or equal to (cid:28), it is interpreted as 0; otherwise, it is interpreted as 1. The output of the Multi-Layer Perceptron (MLP) that predicts the variable bias is then a vector bp in [0,1]n.": 949,
    "The paper addresses this by designing Domain-perspective Embedding Extraction Layers. Instead of using entirely separate embedding layers for each domain, which would lead to a large number of parameters, the approach utilizes a shared embedding matrix (E) for initial feature lookups. For an input `x` with `d` types of features, the system first obtains a feature embedding `ek` for each feature `xk` from the shared embedding matrix. To introduce domain specificity without excessive parameters, two individual small attention networks, `Ga`, are designed. One attention network, `Gs`, is instantiated for source domain instances (`xs`) and prior domain instances (`xp`), while another, `Gt`, is instantiated for target domain instances (`xt`). Each `ek` is fed into its respective domain-specific attention network `Ga` to extract a domain perspective attention `ak`. This `ak` is then applied element-wise (via Hadamard product, denoted by `(cid:12)`) to `ek` to produce the domain perspective embedding `mk`. Finally, all domain perspective embeddings (`m1`, ..., `md`) are concatenated to form the final embedding `Ge(x)`. This mechanism allows for domain-specific feature extraction while sharing the base embedding parameters, thus avoiding parameter explosion.": 950,
    "The Rapid Damage Assessment (RDA) system employs an Image Deduplicator module to identify and filter image-level duplicates. This module utilizes a fine-tuned VGG16 model, pre-trained on the ImageNet dataset, to extract deep features from the penultimate fully-connected (\"\"fc2\"\") layer of each image. These extracted features are stored in a hash. When a new image is collected, its deep features are extracted, and the Euclidean distance between these new features and the features of existing images in the hash is calculated. If the Euclidean distance is less than a predetermined threshold of 20, the system considers the new image to be a duplicate or near-duplicate of an existing one, thereby filtering it out. This process addresses duplication caused by different URLs pointing to the same image, or images that have been cropped, resized, or re-shared with additional text.": 951,
    "The paper proposes the Bad-pixel Correction Network (BpCNet) as a general post-refinement network to perform a fine-grained search for bad pixel correction. Given an initial disparity map (Dc) and corresponding light field images (LFs), BpCNet assumes that all \"\"BadPixs\"\" (bad pixels) on the initial map are within a small range, denoted as δ. The network's architecture is composed of five modules that collectively implement this search strategy. First, a Hypothesis Disparity Generation module creates K hypothesis disparities, denoted as Di_h (where i ranges from 1 to K), by distributing them at equal intervals within the defined δ range around the input Dc. This generates a set of potential corrected disparity values. Second, a Feature Extraction network processes the input LFs to extract feature representations (Fu,v) for each image. Third, a Cost Volume Construction module evaluates the consistency of the LFs under each generated hypothesis disparity Di_h. This involves warping the extracted features (Fu,v) according to each Di_h to construct a cost volume (Cd). Finally, a Disparity Fusion module, which is an attention-based network, decodes the cost volume Cd to produce an attention map (A). This attention map assigns pixel-wise attention weights (Ai) to each hypothesis disparity Di_h. The final refined disparity map (Df) is then obtained by performing a weighted fusion of all hypothesis disparities Di_h, using the attention weights Ai. This entire process allows BpCNet to search for and select the most accurate disparity value within the limited δ range for each pixel.": 952,
    "The paper addresses this by designing two specialized memory modules: the Domain-Invariant Crowd Memory (DICM) and the Domain-Specific Crowd Memory (DSCM). These modules work in separate branches to re-encode image features. The DICM, located in the Domain-Invariant (DI) branch, consists of `M` memory vectors, initialized randomly. For an input image feature `f`, each pixel-level vector `fj` is used to compute similarities to these memory vectors. A Softmax function is applied to these similarities to obtain weights, which then perform a weighted combination of the memory vectors to produce a re-encoded domain-invariant feature `˜fj`. This process is iterated for every pixel to obtain the full domain-invariant feature `˜f`. The DSCM, in the Domain-Specific (DS) branch, functions similarly but has multiple sets of memory vectors, with each set responsible for re-encoding images belonging to a specific sub-domain. For an image with sub-domain label `d`, the corresponding memory set is used to re-encode its feature `z` into a domain-specific feature `˜z`. During back-propagation, only the selected memory set in DSCM is updated. To enable this disentanglement, two types of losses are devised: feature reconstruction loss and feature orthogonal loss. The feature reconstruction loss ensures that re-encoded features (`˜f` and `˜z`) maintain essential information from their pre-encoded counterparts (`f` and `z`), preventing memory vector collapse and enhancing diversification. The feature orthogonal loss enforces dissimilarity between domain-invariant features (`˜f`) and domain-specific features (`˜z`) in the embedding space, explicitly distinguishing the information learned by each module. The domain-invariant feature `˜f` is then used for crowd density estimation.": 953,
    "The paper proposes a novel Segmentation-to-Flow Module (SFM) to convert semantic segmentation maps into optical flow representations, termed \"\"segmentation-based flow\"\" (SF). The SFM is designed with the same network architecture as FlowNet, but its input differs. Instead of raw image pairs, SFM takes \"\"segmentation-based region pairs\"\" (SR) as input. An SR is derived by modulating video frames according to segmentation results, effectively splitting regions by category. Specifically, for a given frame Xt and its corresponding c-th class softmax segmentation prediction Pc t from the VSS model G, the SR for class c at timestamp t is defined as SRc t = XtP c t. This means the input to SFM contains both appearance information (from Xt) and semantic information (from Pc t). SFM then utilizes these SRs to predict a corresponding \"\"region flow\"\" (RF) for each class: RFc t = SFM(SRc t−1, SRc t). After calculating region flows for all classes, SFM generates a complete segmentation-based flow (SF) by a simple fusion of all RFc t: SFt = Σc=1 to C RFc t. This design allows SFM to generate flow information that is directly influenced by the semantic segmentation, enabling the bridging of semantic and motion domains.": 954,
    "The paper proposes an Adaptive Filtering Mechanism (AFM) to separate an input image (I) into a content-dependent component (Ic) and a texture-dependent component (It). This mechanism consists of two main modules: the Adaptive Strength Predictor (ASP) and the Texture Filtering Generator (TFG). The ASP is designed to generate an adaptive filter strength value (αa) based on the style of the input image. It achieves this by first using a feature extraction module (composed of three convolution layers) to extract features. Then, it calculates the mean (μ) and variance (σ^2) of these extracted features across the spatial dimension for each channel. These statistics (μ and σ^2), which are effective in representing image style, are concatenated and fed into a fully connected layer to produce the filter strength value (αa). During training, a small error term (ε), sampled from a normal distribution and truncated, is added to αa to improve robustness; this term is removed during testing. The generated αa is then passed to the TFG. The TFG is a U-shape network that acts as a smoothing generator, designed to filter the texture component while preserving the image structure. It uses the smoothing strength (αm) (which is αa from ASP) to generate convolution kernels. These kernels are then applied to perform structure-preserving smoothing on the input image. The TFG is pre-trained on the GTA5 dataset with a smoothing loss (Lsmooth) that regularizes the smoothed output (S) to be consistent with the input image while penalizing texture. The Lsmooth function is defined as the L2 norm of the difference between the input image I and the smoothed output S, plus a term that penalizes the gradient of the smoothed output, weighted by αm. The smoothed image S is then regarded as the content-dependent image (Ic), and the texture-dependent image (It) is derived as the difference between the original input image and the content-dependent image (It = I - Ic). Once pre-trained, the parameters of the TFG are fixed during the training of the main segmentation network.": 955,
    "The paper proposes a novel Contrastive Spectral Training method to train the feature extractor (h) to be more domain-agnostic. This method modifies conventional self-supervised contrastive learning in two key ways. First, it weakens texture-related data augmentations, such as grayscale, which are typically used in self-supervised training but can cause trained models to have unbalanced shape and texture biases. By reducing these texture-related augmentations, the method aims to balance these biases, making the feature extractor more generalizable across domains. Second, a spectral pooling layer (Rippel, Snoek, and Adams 2015) is added before the input layer of the feature extractor during training. This layer transforms the image from the spatial domain to the frequency domain using Fast Fourier Transform (FFT), cuts off high-frequency components, and then converts the remaining low-frequency signals back to the spatial domain using inverse FFT. This process removes fine details while retaining image structures and high-level semantic information, further contributing to domain-agnostic feature extraction. Once training is complete, the spectral pooling layer is removed from the network architecture. The training procedure involves applying different random augmentations to a training image (x) to obtain a positive sample pair (x1, x2). These are fed to the feature extractor (h*) and a combined MLP predictor (p). The objective is to maximize the cosine similarity (d) between the output vectors (z1 = p(h*(x1)) and z2 = p(h*(x2))) while treating the feature extractor outputs (y1 = h*(x1) and y2 = h*(x2)) as constants, thereby updating only the feature extractor combined with the MLP predictor.": 956,
    "The paper proposes using a Color Multi-Layer Perceptron (ColorMLP) to model complex non-linear color mapping relationships. The ColorMLP is designed with two hidden layers, each containing N units. The parameters of this ColorMLP, denoted as Θ, include weights and biases, forming a vector of size D = N^2 + 8N + 3. To demonstrate its fitting capability, the ColorMLP is trained to approximate given 3D Look-Up Tables (3D-LUTs). This is achieved by sampling a large number of colors as input, obtaining target colors using a 3D-LUT, and then training the ColorMLP on these generated color pairs. Experimental results show that with N=20 hidden units, the ColorMLP can accurately approximate most 3D-LUTs with an average L1 error of only 2.69 (on a 0-255 pixel value range), while using only 0.5% of the parameters required by a typical 3D-LUT (M=33). This lightweight property makes it suitable for adaptive parameter prediction.": 957,
    "To extract fine-grained metaphorical visual clues, the model employs an object-level semantic mining module. This module first uses a pretrained VGG16 convolutional neural network classifier to extract image-level features (`hc`). Subsequently, it identifies and localizes objects within the image using an object detection mechanism (Anderson et al. 2018). For each detected visual region (`Ii`) represented by a bounding box, the region is resized to a standard 224x224 pixels. This resized region is then reshaped into a sequence of embedded blocks (`Ii = {r1,...,rm}`). Each block `rj` is flattened and projected into a `dI`-dimensional vector using a trainable linear projection matrix `E`, resulting in `zj = rjE`. To incorporate contextual and positional information, a `[class]` token embedding is prepended to the patch sequence, and position embeddings (`Epos`) are appended, forming the input matrix `Zi = [z[class];z1,...,zm] + Epos`. This `Zi` is then fed into the VGG16 encoder to obtain the visual region representation `hv_i`. Finally, the overall image representation `hI` is defined as a combination of the image-level feature `hc` and the object-level features `{hv_1,...,hv_m}`.": 958,
    "The paper addresses this by introducing a Frequency Domain Transfer (FT) module that leverages Non-Subsampled Contourlet Transform (NSCT). First, a source domain image is decomposed into 15 frequency components (FCs) using a three-layer NSCT. NSCT is chosen because it can produce finer, anisotropic, and multi-directional frequency components and reduce spectrum overlapping compared to Discrete Fourier Transform (DFT) or Discrete Cosine Transform (DCT). Through empirical analysis, specific FCs are identified as Domain-Invariant Frequency Components (DIFs) and Domain-Variant Frequency Components (DVFs). For a source domain image, its DIFs are kept unchanged, while its DVFs are replaced with the corresponding DVFs from a randomly selected target domain image. This process generates adapted frequency components. Finally, an inverse NSCT is applied to these adapted frequency components to reconstruct a target-like image, which is then used to train the frequency domain transfer-based segmentation teacher network. The training uses a loss function combining cross-entropy loss and Dice loss.": 959,
    "The paper addresses the challenge of bridging the significant domain gap by introducing a Progressive Adaptation (PA) strategy. Instead of directly considering the initial source generator (Gs, denoted as G0 at the 0th iteration) and the final target generator (Gs t, denoted as GN at the Nth iteration), the adaptation process is viewed as a progressive learning with N+1 iterations. The core idea is to manage the domain gap by focusing on models at nearby training iterations, specifically Gn and Gn-α, where Gn is the generator at the nth iteration and α is a small constant. The parameters of Gn-α are efficiently approximated using Exponential Moving Average (EMA) of the current generator's parameters, meaning it's not necessary to store parameters for every iteration. This approach ensures that the domain gap between Gn and Gn-α is much smaller than the gap between G0 and GN, making it easier to identify semantically similar regions between the outputs of these two closely related generators. This progressive approach forms the foundation for the align-free spatial correlation by providing a context where pixel-level displacements between generated images are small and manageable.": 960,
    "The paper addresses this by introducing the Style Harmonization Layer (SHL), a core component within the target generator (Gy). The SHL takes three conditional inputs: the image-specific component style (sx), the class-aware memory style (ˆsy), and the semantic label (Lx). Within the SHL, convolutional layers are used to compute pixel-wise scale (γ) and shift (β) factors from both the component style (sx) and the memory style (ˆsy). Specifically, four convolutional layers extract γx, βx from sx and γy, βy from ˆsy. The key innovation is the adaptive aggregation of these two styles using class-wise alpha parameters (α1, α2, ..., αN). An alpha mask (ˆα) is generated by broadcasting these class-wise alpha parameters to their corresponding semantic regions defined by the label Lx. This ˆα mask then weights the contribution of each style: the final scale factor γc,h,w is computed as ˆαh,wγx c,h,w + (1 − ˆαh,w)γy c,h,w, and similarly for the shift factor βc,h,w as ˆαh,wβx c,h,w + (1 − ˆαh,w)βy c,h,w. These harmonized γ and β factors are then used to denormalize the input feature (f) of the current SHL, effectively transferring the adaptively determined target style. This process allows the network to decide, for each class, which style (component or memory) has more influence, enabling fine-grained, pixel-level style control that preserves original image characteristics while adapting to the target domain.": 961,
    "The paper addresses this by introducing a cross-modal contrastive learning objective that enforces consistency between 2D pixel features and 3D point features. For each sample, after projecting 3D points onto the 2D image, corresponding 2D pixel features (F2D) and 3D point features (F3D) are obtained. These matched pixel-point pairs are considered to represent the same semantic content. To align these features, they are first mapped into a joint feature space (Rd) using a 2D projection head (ϕ2D) and a 3D projection head (ϕ3D). The similarity between projected feature vectors is measured using cosine distance. The core of the contrastive learning is the NT-Xent loss (Normalized Temperature-scaled Cross-Entropy loss). For a given 2D pixel feature (fi 2D), its corresponding 3D point feature (fi 3D) is treated as a positive pair, while all other 2N-2 feature vectors (from both 2D and 3D modalities in the batch) are considered negative examples. The objective is to maximize the similarity of positive pairs and minimize the similarity of negative pairs. To ensure both modalities benefit, the final cross-modal contrastive loss (Lctr) is formulated symmetrically, combining losses where 2D features are anchors and where 3D features are anchors. Due to GPU memory constraints, a sampling strategy is employed, selecting a portion of features (e.g., 30%) with the highest prediction confidence to calculate the contrastive loss, which is found to be beneficial for learning discriminative features.": 962,
    "The paper addresses this by employing a pair of specialized encoders, a liveness encoder (EL) and a content encoder (EC), for both the source and target domains. The liveness encoder (EL) is designed to extract domain-invariant liveness features (zL), which represent the live-or-spoof attribute. Simultaneously, the content encoder (EC) extracts domain-specific content features (zC), capturing attributes like face category, background, camera, and illumination. To ensure the liveness features are truly domain-invariant, a GAN-like discriminator (DL) is used in a domain adversarial training setup. This discriminator attempts to distinguish the domain affiliation of the extracted liveness features (zL) from source and target domains. The liveness encoder (EL) is trained adversarially against DL to fool it, thereby making the liveness features indistinguishable across domains. Additionally, a classification loss (LclsL) is applied to the liveness features from the source domain using a binary classifier (C) to ensure their discriminative property for live/spoof classification. This combination of specialized encoders and adversarial training facilitates the disentanglement of liveness and content features.": 963,
    "The paper proposes P2FCDR, a privacy-preserving federated framework for dual-target Cross-Domain Recommendation. This framework is designed as a peer-to-peer federated network architecture, meaning that user-item interaction data are stored locally within each business partner's domain (e.g., Domain A and Domain B) without any third-party involvement. This peer-to-peer structure ensures that original data never leave the local devices or databases, significantly reducing the risk of privacy leakage. The framework supports dual-target recommendation, meaning it aims to improve recommendation performance simultaneously in both domains, addressing the \"\"Unidirectional Benefit Target\"\" limitation of prior works. Communication between the two domains occurs only twice per iteration to exchange model-related information, specifically transformed user embeddings and updated orthogonal mapping matrices, further minimizing exposure. This design satisfies the scenario of cooperation between companies by treating business partners as clients in a federated setting.": 964,
    "The paper addresses this by introducing a Selective Privacy Preserver (SPP) module within the Stable Privacy-Preserving Generator (SPPG). SPP ensures cost-effective privacy preservation by applying Rényi Differential Privacy (RDP) only to the gradients of the discriminator (D) in the GAN-based SPPG, as D is the only component that directly accesses the private real ratings in the source domain. The generator (G) obtains knowledge from D's feedback, and due to the post-processing property of RDP, the privacy of the remaining gradients in SPPG is inherently preserved. Specifically, SPP sanitizes the gradients of D (denoted as `gD`) during back-propagation. This involves two steps: first, clipping the gradient `g` to a maximum norm `B` (i.e., `g/max(1,∥g∥/B)`) to bound its sensitivity; second, applying a Gaussian mechanism by adding noise `N(0,σpB^2I)` to the clipped gradient. This results in a sanitized gradient `ˆgD` which replaces the original `gD` in the update process, thereby selectively preserving privacy without severely degrading performance.": 965,
    "The paper models the association and discovers relevance between candidate meta-tasks from the source domain and prompt tasks from the target domain through a process called Task Association Modeling. This involves three main steps: Task Representation Learning, constructing a Task Bipartite Graph, and Task Representation Refinement.\nFirst, for Task Representation Learning, each meta-task (τi) from the source domain and each prompt task (˜τj) from the target domain is characterized by its task examples. A task representation (pτi or p˜τj) is derived by combining class prototypes. For an N-way K-shot meta-task, N class prototypes are calculated by averaging the graph embeddings of examples in the support set for each class, learned by the graph base learner. These prototypes are then combined using a Multilayer Perceptron (MLPWa). In the experiments, for binary classification (N=2), the task representation pτi is obtained by concatenating the positive (ppos) and negative (pneg) class prototypes: pτi = MLPWa(ppos∥pneg). For a larger N, an order-invariant method like mean-pooling can be used. Prompt tasks are organized similarly from a limited set of labeled target domain data, each consisting of a support set and remaining unchanged during meta-training.\nSecond, a Task Bipartite Graph (GB) is constructed to explore the association between candidate meta-tasks and prompt tasks. This graph consists of two disjoint sets of vertices: U representing candidate meta-tasks (τi) and V representing prompt tasks (˜τj). Edges (eij) can only exist between a candidate meta-task τi ∈ U and a prompt task ˜τj ∈ V, ensuring direct interaction. The adjacency matrix A ∈ R(Ms+Mt)×(Ms+Mt) of GB is initialized based on the similarity between task representations. An element A(τi, ˜τj) is set to 1 if the Euclidean distance (||pτi − p˜τj||2) between their task representations is greater than or equal to a pre-defined threshold δ, and 0 otherwise. This initialization aims to connect task pairs with high relevance.\nThird, Task Representation Refinement is performed using a two-layer Graph Convolutional Network (GCN) on the constructed task bipartite graph. This step refines the initial task representations, as predefined similarity measurements might not accurately capture task relevance. The GCN propagates information across the graph, allowing similar or related tasks to become closer in the embedding space. The update rule for task representations P(l) at layer l is P(l) = σ( ˜D−1/2 ˜A ˜D−1/2 P(l−1)W(l)g), where ˜A = A + I (adjacency matrix with self-loops), ˜D is the diagonal degree matrix of ˜A, W(l)g is a trainable weight matrix, and σ is an activation function. P(0) is initialized with the learned task representations {pτi} and {p˜τj}. The refined task representations are expected to contain more accurate information regarding the target domain.": 966,
    "The paper addresses simile detection by employing a semi-supervised co-training approach, leveraging two distinct syntactic patterns: the \"\"like\"\" pattern (Noun1 ... BE/VB like ... Noun2) and the \"\"be\"\" pattern (Noun1 ... BE ... Noun2). These patterns are used to extract initial simile candidates from large corpora, which can be either true similes or literal comparisons. The extracted sentences form two \"\"views\"\": the like view and the be view. In a bootstrapping process, two separate BERTBASE classifiers, Mlike and Mbe, are trained. Initially, a small set of labeled data from the like view is used to initialize both classifiers. At each iteration, Mlike annotates the unlabeled subset of the be view (Ube), and Mbe annotates the unlabeled subset of the like view (Ulike). Only pseudo-labeled samples with a predicted confidence score greater than predefined thresholds (θlike and θbe) are retained as positive simile sentences. To mitigate data imbalance, pseudo-labeled negative samples are sampled to an equal number as positive ones. These high-confidence pseudo-labeled subsets are then integrated into the respective labeled sets (Lbe and Llike) for the next iteration. This iterative process, performed for T iterations, allows the models from each view to complement each other, effectively leveraging massive unlabeled sentences. After co-training, the fully trained classifiers are applied to the remaining unlabeled data, and positively labeled samples in the\nfinal round are denoted as L′be and L′like.": 967,
    "The paper addresses this by introducing the Depth-Aware Adversarial Adaptation stage. In this stage, the Geometry-Aware Network for Domain Adaptation (GANDA) first learns to disentangle structural information from single source RGB images (Xs) under the supervision of available source depth maps (Ds). This is achieved by training a 2D depth decoder alongside a 2D semantic encoder. The 2D semantic encoder is optimized to \"\"fool\"\" a discriminator, which distinguishes between source and target features, while the 2D depth decoder is trained using a depth loss (Ls_depth) that combines a Scale-Invariant Logarithmic loss (Ls_SILog) and a smoothness loss (Ls_grad) against the source ground truth depths. Once trained on the source domain, the learned depth knowledge is then applied to raw target images (Xt) to generate pseudo depth maps (^Dt). This process effectively transfers the ability to predict depth from the source to the target domain, enabling the generation of target pseudo depths without direct target depth supervision. The overall training loss for this stage (Ldepth-aware) combines the semantic segmentation loss (Ls_seg) with the depth loss and an adversarial loss (Ladv) to align domain features.": 968,
    "The paper addresses this by representing a Mixed Integer Program (MIP) instance as an edge-weighted bipartite graph, G = (V,E,C). One side of the graph contains 'n' nodes representing the variables, and the other side contains 'm' nodes representing the constraints. An edge exists between the j-th variable node and the i-th constraint node if the j-th variable appears in the i-th constraint. The weight of an edge ei,j corresponds to the coefficient value of variable vj in constraint ci. Variable features (V), edge features (E), and constraint features (C) are used as input.\nTo encode this bipartite graph, an edge-weighted bipartite Graph Attention Network (GAT) is employed. This GAT performs two levels of message passing, referred to as \"\"half-aggregations\"\":\n1.  Variable to Constraint Aggregation: Messages are passed from variable nodes to constraint nodes. For each constraint node `ci`, its embedding is updated by aggregating information from its neighbors `N(i)` (variable nodes) and its own initial embedding, weighted by attention coefficients `αi,j`. This step enriches the representation of constraint nodes.\n2.  Constraint to Variable Aggregation: Messages are then passed from constraint nodes to variable nodes. For each variable node `vj`, its embedding is updated by aggregating information from its neighbors `N(j)` (constraint nodes) and its own initial embedding, weighted by attention coefficients `αj,i`. This step generates richer representations for each variable node.\nThe attention coefficients (e.g., `αi,j`) are computed using a shared attention mechanism that considers the embeddings of the connected nodes and the edge features, passed through a neural network (MLP) and a LeakyReLU activation function, followed by a softmax normalization over neighbors. Multiple attention heads are used to stabilize training. The final representations of the candidate variable nodes, obtained after these two aggregations, are then passed through a softmax layer to produce a probability distribution over the variables, which serves as the scoring function for branching decisions.": 969,
    "The paper proposes capturing the source domain's learned knowledge by estimating its internal distribution in the embedding space using a parametric Gaussian Mixture Model (GMM). During the initial training phase on the source domain, the deep encoder transforms the source input distribution into a multimodal internal distribution, where each mode corresponds to a specific class. This internal distribution, denoted as pJ(z), is modeled as a GMM with 'k' components, where 'k' is the number of classes. The parameters of this GMM (mixture weights αj, means µj, and covariances Σj for each component 'j') are estimated using Maximum A Posteriori (MAP) estimates. Specifically, for each class 'j', the MAP estimates are computed from the support set Sj (data points belonging to class 'j' in the source training dataset) by averaging the encoder's output (ϕv(xsi)) for the means, and computing the covariance matrix from the deviations of these outputs from their respective means. This GMM effectively encodes the knowledge gained from supervised learning in the source domain, serving as a consolidated representation of the source distribution in the embedding space.": 970,
    "The paper constructs the Symmetric Discordance Index (SDI) to measure the distance between two risk functions, `r1` and `r2`, given survival data `D = {(xi,ti,δi)|i ∈ {1,...,n}}`. SDI is defined as a weighted sum of two parts: `α1` for disagreements in ranking pairs of non-censored instances, and `α2` for disagreements in ranking pairs of censored and non-censored instances. The formula is:\n`SDI(r1, r2; D) = (α1 / (α1 + α2)) * (1/Z) * Σ I[ (r1(xi) < r1(xj) ∧ r2(xi) > r2(xj)) ∨ (r1(xi) > r1(xj) ∧ r2(xi) < r2(xj)) ] + (α2 / (α1 + α2)) * (1/|Dce|) * Σ (|Cr1,xi△ Cr2,xi| / |Cr1,xi ∪ Cr2,xi|)`\nwhere `Dev` is the set of non-censored instances (`δj = 1`), `Dce` is the set of censored instances (`δj ≠ 1`), `Cr,x` is the set of instances assumed to outlive `x` according to risk function `r`, and `△` is the set symmetric difference. `α1` is `|Dev| choose 2`, and `α2` is `|Dev| * |Dce| / 2`. `Z` is a normalization factor. The symmetry of SDI is achieved by counting discordance with censored cases twice, once for each risk function while considering the other as ground truth. The paper proves that SDI is a metric satisfying the triangular inequality. The counting-based comparison in SDI is implemented using `MarginRankingLoss` (MRS) from PyTorch, specifically `MRS(exp(r(xi) - r(xj)), 0, 1)` with a margin `m=1` for the indicator function `I`.": 971,
    "The paper addresses this by proposing a novel path-based Mixed-Integer Program (MIP) formulation. Instead of modeling split conditions at nodes (arc-based), this approach explicitly models individual decision rules, or paths, from the root to the leaf nodes of a tree. The core idea involves defining a feature graph, G(V,E), which is an acyclic multi-level directed graph. In this graph, each feature corresponds to a level, and nodes within a level represent distinct feature values. Nodes of one feature are fully connected to nodes in the next level, and the graph includes a source and a sink node. A decision rule is then defined as a path from the source to the sink node in this feature graph. To allow for features to be ignored in a rule, dummy \"\"SKIP\"\" nodes are introduced for each feature, enabling paths to bypass certain features. This construction naturally embeds the hierarchical structure of a tree.\nThe MIP formulation aims to identify a subset of these paths (decision rules) to minimize prediction error. Let L be the total number of possible decision rules (paths) in the feature graph. A binary decision variable, zj, is introduced for each rule j, indicating whether rule j is selected (1) or not (0). For each sample i, a non-negative slack variable si and a positive penalty cost ci are defined. The objective function is to minimize the sum of prediction error (ξj) for selected rules and penalties for unassigned samples. The formulation includes a set partitioning constraint (Equation 1): `sum_{j=1 to L} aij * zj + si = 1` for all samples i. Here, aij is 1 if sample i satisfies the conditions specified in rule j, and 0 otherwise. This constraint ensures that each sample is ultimately assigned to exactly one rule. A cardinality constraint (Equation 2): `sum_{j=1 to L} zj <= l` is also included, stipulating that no more than l rules (leaf nodes) are active in the optimal solution, where l is a user-specified parameter defining tree complexity. This path-based modeling inherently supports multiway-splits, as a single rule (path) can represent a complex combination of feature values, and the selection of multiple such rules forms the multiway tree.": 972,
    "The paper addresses this by first constructing a Heterogeneous Information Network (HIN) that includes users, items, categories, and brands, with edges representing their relationships. From this HIN, multiple user interest embeddings, denoted as {hp1_u, hp2_u, ..., hpK_u}, are obtained for each user `u` using meta-path based aggregations (e.g., uiu, uiciu, uibiu). To transfer these K interest embeddings, the framework employs K distinct bridge functions {F1, F2, ..., FK}. Each bridge function Fj is personalized for a specific user `u` and interest `hpj_u`. This personalization is achieved by feeding the interest embedding `hpj_u` along with a weighted sum of the user's historical behavior items `Su` (represented as `Vu`) into a shared meta network `f(.)`. An attention mechanism calculates a weight vector `Aj` for `Vu` based on its correlation with `hpj_u`, resulting in a personalized sequence embedding `pj_u`. The meta network `f(pj_u; ϕ)` then generates the parameters `Wj_u` for the j-th bridge function. For simplicity, the bridge function `Fj` is defined as a linear layer, so the transformed interest embedding `tpj_u` is computed as `tpj_u = hpj_u * Wj_u`. This process results in multiple meta-generated personalized transformed user embeddings.": 973,
    "The paper addresses this through Cooperative Learning (CLE), a mechanism that enables the discriminative module (G) and the transfer module (H) to guide each other by exchanging \"\"easy examples.\"\" This process involves two main components:\n1.  Discriminability guides Transferability: To provide the transfer module with clues for more discriminable feature learning, CLE generates \"\"discriminable examples\"\" (xdisc). These examples are created by adding cooperative perturbations to original examples (x), specifically by finding `xdisc = argmin x';∥x−x'∥≤ϵ ℓdisc(x')`, where `ℓdisc` is the discriminability loss function. The transfer module (H) is then regularized by minimizing its consistency loss on these discriminable examples: `ℓCLE tran(x) = Dist.[H(F(x))∥H(F(xdisc))]`. This drives H to utilize more discriminable features to regularize the original features extracted by the feature extractor (F).\n2.  Transferability guides Discriminability: Similarly, to guide the discriminative module towards using more transferable features, CLE generates \"\"transferable examples\"\" (xtran). These are produced by adding cooperative perturbations to original examples, found by `xtran = argmin x';∥x−x'∥≤ϵ ℓtran(x')`, where `ℓtran` is the transferability loss function. The discriminative module (G) is then regularized by minimizing its consistency loss on these transferable examples: `ℓCLE disc(x) = Dist.[G(F(x))∥G(F(xtran))]`. This encourages G to rely more on domain-invariant features for its discriminative task.\nThe minimizers for generating xdisc and xtran are approximated by moving the original example x towards its negative gradient of the respective loss function: `x' = x - ∇xℓ(x)/∥∇xℓ(x)∥2`. The `Dist.` function measures the discrepancy between two distributions, such as KL-divergence or cross-entropy. By exchanging these easy examples, CLE bridges the discriminative and transfer modules, allowing them to learn from each other's strengths and collectively progress towards feature representations that are both transferable and discriminable.": 974,
    "To effectively model and transfer spatial-temporal dependencies within Multivariate Time-Series (MTS) data across domains, the proposed SEnsor Alignment (SEA) framework employs a Multi-branch Self-attention-based Graph Construction (MSGC) module in conjunction with a Graph Neural Network (GNN) and Long Short-Term Memory (LSTM). First, given an MTS sample, it is cropped into mini-pieces, which are then constructed as sequential graphs by the MSGC. The MSGC is designed to fully represent complex correlations between sensors by constructing multiple graphs using different initialized weights. In each branch of the MSGC, a query and key mechanism is adopted to learn weights between sensors, where the m-th sensor is treated as the query and the n-th sensor as the key. This process generates an edge `ei mn,T` for the i-th branch, which is then normalized using softmax to be within [0,1]. To ensure robustness, the obtained graphs from all branches are averaged to form the final sensor correlations `emn,T`. This results in sequential correlations `ET` for each time step. With these sequential graphs, a shared GNN is introduced to capture spatial dependencies by processing each graph independently using a Message Passing Neural Network (MPNN) variant. Subsequently, to capture temporal dependencies, an LSTM processes the features of each sensor across the sequential graphs. This combination of MSGC, GNN, and LSTM updates the sequential features by capturing the spatial-temporal dependency within MTS data, making this information transferable across domains for subsequent alignment steps.": 975,
    "The distribution shift between source and target graphs is measured using a novel parametric measure called Graph Subtree Discrepancy (GSD). This measure is inspired by the connection between the Weisfeiler-Lehman (WL) subtree kernel and message-passing Graph Neural Networks (GNNs). The WL subtree kernel decomposes non-parametric graph similarity into the similarity of subtrees rooted at every node, while message-passing GNNs iteratively aggregate messages from local neighborhoods to learn node representations. GSD is formally defined as the average of base domain discrepancies `db` across different depths `m` of subtree representations: `dGSD(Gs,Gt) = lim M→∞ (1/(M+1)) * Σ(m=0 to M) db(Gsm,Gtm)`. This definition recursively estimates the subtrees' distribution discrepancy between source and target graphs at different depths. The subtree representation `fm(v)` for a node `v` at depth `m` is learned by existing message-passing GNNs, such as Graph Convolutional Networks (GCNs). For instance, `fm(v)` can be computed as `σ(Σ(u∈{v}∪N(v)) bavuWmfm-1(u))`, where `bA` is the re-normalized adjacency matrix, `Wm` is a trainable matrix, and `σ` is a non-linear activation function. The base domain discrepancy `db(Gsm,Gtm)` can be instantiated using measures like the discrepancy distance, defined as `sup h,h'∈H |Ev∈Vs [L(h(fm(v)),h'(fm(v)))] - Ev∈Vt [L(h(fm(v)),h'(fm(v)))]|`, where `h` and `h'` are hypotheses from a hypothesis class `H`, and `L` is a loss function. This approach allows GSD to capture distribution shifts in the latent feature space induced by GNNs, leveraging the expressive power of GNNs and the structural insights from the WL kernel.": 976,
    "The Self-supervised Source Domain Projection (SSDP) network aims to project data onto the source domain to mitigate domain shift. Since target data is unavailable during training, the method simulates domain shift by applying data augmentation to source domain data. The SSDP network, denoted as F, learns to map augmented data back to its corresponding original data in the source domain. Specifically, for an original source image `x` and its augmented version `xa = A(x)`, both `x` and `xa` are fed into an encoder `E` of SSDP to obtain features `fx` and `fxa`. For `fxa`, instance normalization is applied to get a normalized feature `ˆfxa`, which eliminates its style information. Concurrently, the channel-wise mean `µ(fx)` and standard deviation `σ(fx)` of `fx` (containing style information of `x`) are calculated. These statistics are then used as affine parameters to transform the normalized feature `ˆfxa` into `˜fxa = σ(fx) ˆfxa + µ(fx)`. This transformed feature `˜fxa` is assumed to contain the content information of `xa` and the style information of `x`. Finally, `˜fxa` is input into a decoder `D` to reconstruct the original data `˜x`. The training of SSDP is self-supervised, minimizing an L1 reconstruction loss `Lrecon = ||x - ˜x||1`. During the test phase, when a target image `xt` is encountered, the method cannot use a paired source image. Instead, it leverages pre-trained cluster centers of mean and standard deviation from source training data features, denoted as `µS` and `σS`. For `xt`, its feature `fxt` is extracted, normalized to `ˆfxt`, and then the closest mean `ˆµ` and standard deviation `ˆσ` from `µS` and `σS` (respectively) to `µ(fxt)` and `σ(fxt)` are found using L2 distance. These `ˆµ` and `ˆσ` are then used to renormalize `ˆfxt` into `˜fxt`, which is subsequently passed through the decoder `D` to obtain the projected target data.": 977,
    "The paper proposes the mini-batch Prototypical Partial Optimal Transport (m-PPOT) to align distributions in Universal Domain Adaptation (UniDA). Instead of aligning individual source samples, m-PPOT uses a set of source domain prototypes, denoted as `ci`, which are the mean features of samples belonging to each class `i` in the source domain. This approach is more robust to sampling bias in mini-batches and reduces computational resources compared to original Partial Optimal Transport (POT). The m-PPOT is defined as `m-PPOT_epsilon(c, q) = (1/k) * sum(POT_epsilon(c, q_Bi))` for `i=1 to k`, where `c` is the empirical distribution of source prototypes, `q_Bi` is the empirical distribution of `b` random target samples in a mini-batch `Bi`, `k` is the number of mini-batches, and `epsilon` is the total mass to transport. The cost matrix `Cij` for m-PPOT is defined as the L2-distance `d(ci, f(xt_j))` between source prototype `ci` and target feature `f(xt_j)`. The m-PPOT loss, `Lot`, is then defined as the expectation of `m-PPOT_epsilon(p,q)` over all target domain data index partitions, approximated by solving `POT_epsilon(c, q_Bi)` for each mini-batch. Source prototypes `c` are updated using an exponential moving average.": 978,
    "The paper addresses this by introducing the Task-specific Semantic Embedding (TSE) module. First, in each episode, local features (L) are extracted from the convolutional network for all samples in the support set (XS) and query sets (QS, QT). These local features are then clustered using K-means to generate distinct semantic clusters for the support set and query set separately. The number of clusters (k) for each task is adaptively determined by utilizing Singular Value Decomposition (SVD), taking the number of eigenvalues greater than a certain threshold. Subsequently, a task-specific semantic feature map (F) is computed by measuring the Cosine similarity between the local features (L) and the centroids (C) of all semantic clusters. To maintain discriminative ability and spatial information, F is split into 2x2 blocks, and these four blocks are concatenated along the channel dimension to generate the final semantic features (`^F`). Furthermore, to leverage semantics from previous tasks and guide the current task's semantic feature learning, the centroids of previous clusters are used to update the initialization of current clustering centroids via cross-attention. This cross-attention mechanism, which consists of three convolution parameters to generate Query (Q), Key (K), and Value (V) for attention calculation, helps K-means clustering converge rapidly and incorporates prior knowledge.": 979,
    "The paper proposes a Directional Domain Augmentation (DDA) method, where a domain transformer, denoted as ψ, is designed to generate augmented features. This transformer takes features from historical domains and transforms them to mimic the data from the next unseen domain. Unlike simpler approaches that might only use one preceding domain, this ψ incorporates an attention module. This attention mechanism allows it to leverage information from *all* historical source domains, extracting sequential information to effectively capture evolving patterns and simulate target data.\nSpecifically, the process involves:\n1.  Similarity Score Calculation: For each sample `i` from domain `t` and sample `j` from a historical domain `t'`, a similarity score `st,t′ i,j` is calculated. This score is based on the dot product of query embeddings `ψq(zt i)` (from the current sample) and key embeddings `ψk(zt′ j)` (from historical samples), normalized by the square root of the embedding dimension. This measures how close the current sample is to historical domain samples.\n2.  Directional Transform Augmentation: The augmented feature `˜zt+1 i` is then obtained as a weighted sum of value embeddings `ψv(zt′ j)` from historical domains, combined with a skip-connection network `ψsc(zt i)`. The weights are derived from the softmax of the similarity scores, ensuring that the transformer attends to the most relevant historical samples. The skip-connection network helps stabilize learning and preserve instance-level information.\nThus, the domain transformer `ψ` is composed of `ψk`, `ψq`, `ψv`, and `ψsc`. This design ensures that the generated augmentations `˜zt+1` are \"\"directional\"\" as they are transformed based on the domain-evolving direction and historical samples, aiming for their decision boundary to align with the actual next domain's features.": 980,
    "The paper proposes an enhanced model sensitivity map to quantify a model's vulnerability to different frequency corruptions. This is achieved by first defining a Fourier basis `Ai,j` and its real-valued pixel space representation `Ui,j` (Inverse Fast Fourier Transform of `Ai,j`). Instead of uniform perturbation, the noise `Ni,j` added to the original image `x` is formulated as `ˆNi,j = r · D(i,j) · Ui,j`, where `r` is a random sign (-1 or 1), and `D(i,j)` is the `(i,j)`th entry of the averaged amplitude spectrum `D` computed from all images in the source domain. This `D(i,j)` acts as a domain prior, ensuring that low-frequency components with larger amplitudes are perturbed more significantly, reflecting their importance in natural images and domain shifts. The enhanced model sensitivity `MS(i,j)` at a specific frequency `(i,j)` is then calculated as `1 - Acc(F(x+r·D(i,j)·Ui,j),y)`, where `Acc` is the model prediction accuracy of model `F` on the perturbed source images `x+r·D(i,j)·Ui,j` with corresponding labels `y`. By aggregating these `MS(i,j)` values across the frequency space, a 2D sensitivity map is obtained, where higher values indicate greater vulnerability. This enhanced map provides a strong correlation between model performance and sensitivity, allowing for visualization and quantification of implicit regularization effects.": 981,
    "The paper addresses the challenge of unsupervised feature alignment by employing a Contrastive Learning Module (CLM) that performs both instance-wise and prototype-wise contrastive learning. Since ground-truth labels for the target domain (Dt) are unavailable, a clustering-based approach is used to produce pseudo-labels. This process begins by initializing cluster centers using class prototypes from a batch of source domain (Ds) samples. Specifically, for each class 'm', a centroid `cent_m` is computed as the average of all source samples belonging to that class within the current batch. These centroids then serve as initial centers for K-means clustering on the target domain features, associating each target sample with a pseudo-label.\nWith pseudo-labels generated for the target domain, the CLM proceeds with two main types of contrastive learning:\n1.  In-domain Contrastive Learning (LICL): This component applies instance-wise contrastive learning within both the source and target domains. For source samples, it uses the ground-truth labels to pull same-class samples closer and push different-class samples apart. For target samples, it uses the generated pseudo-labels to achieve a similar effect. The objective function `LSCL` (for source) and `LtSCL` (for target) are based on the InfoNCE loss, minimizing the distance between positive pairs (same class) and maximizing it for negative pairs (different classes) using cosine similarity and a temperature parameter. The overall `LICL` is a weighted sum of `LSCL` and `LtSCL`.\n2.  Cross-Domain Contrastive Learning (LCCL): This component aims to reduce domain shift by aligning features across domains. It includes:\n    *   Instance-wise Cross-Domain Contrastive Loss (`Lt->s CCL`): This loss treats a target sample as an anchor and forms positive pairs with source samples from the same class (based on the target's pseudo-label and source's true label). It pulls these cross-domain same-class instances closer. The formulation can also be reversed (`Ls->t CCL`).\n    *   Prototype-wise Cross-Domain Contrastive Loss (`Lt->s Pro`): This loss aligns target samples with the corresponding source class prototypes. For a given target feature, it pulls it closer to the centroid of its (pseudo-labeled) class in the source domain, while pushing it away from centroids of other classes.\nThe total loss for the CLM (`LCL`) is a weighted sum of `LICL` and `LCCL`, ensuring both within-domain discriminability and cross-domain alignment.": 982,
    "The paper introduces a novel domain-transfer meta task design paradigm. This paradigm defines a \"\"basic domain\"\" for each target domain based on the Jaccard similarity coefficient of their slot label spaces. The domain with the highest slot coincidence degree among source domains is selected as the basic domain. In this new meta task formulation, the objective is to infer the class of query set samples based on both an \"\"auxiliary set\"\" (comprising all samples from the basic domain) and a \"\"support set\"\" (few labeled samples from the target domain). This setup explicitly incorporates abundant data from a related domain, allowing the model to learn historical information about overlap slots, thereby simulating realistic scenarios where such overlaps exist and preventing the forgetting of this crucial information.": 983,
    "To address the question of TLMs' ability to generate longer answer spans, the paper conducts an \"\"Answer Length Analysis\"\" (‡1). This involves recording the average number of characters in the generated answer spans from various models on the validation sets of two distinct datasets: TechQA, which serves as the closed-domain dataset (CDD), and SQuAD (Stanford Question Answering Dataset), used as the open-domain dataset (ODD). Characters are counted instead of tokens to ensure consistency across different tokenization schemes (e.g., BPE, WordPiece). The study compares the performance of Transformer-based Language Models (TLMs) such as BERT and RoBERTa against the gold standard answer lengths. Furthermore, to ascertain whether any observed limitations are specific to TLMs or indicative of a broader issue with neural architectures, the analysis also includes two non-Transformer models: BIDAF (Bi-Directional Attention Flow), a recurrent neural network model, and QANet, a convolutional neural network model. The generated answer lengths from all models are then compared to the gold answer lengths for both SQuAD and TechQA to assess their capacity to produce spans of varying lengths, particularly the longer spans required by CDDs.": 984,
    "A comprehensive and categorized dataset of adult humor subtypes is constructed by scraping specific subreddits from Reddit and news articles from Thomson Reuters. The process begins by identifying subreddits that naturally categorize different types of humor: r/cleanjokes for Wholesome Jokes, r/darkjokes for Dark Jokes, and r/DirtyJokes for Dirty Jokes. News articles from Thomson Reuters are sourced to serve as non-joke examples. To overcome the limitations of the official Reddit API, which restricts access to only the 1000 most recent listings, the Pushshift.io API is utilized as an effective surrogate. Data collection involves sending paginated requests, incorporating sleep intervals between queries to avoid rate limiting, continuously saving queried results, and employing multiprocessing for efficiency. After obtaining the raw posts, a data processing step is performed to prune the dataset. This involves removing posts with deleted or empty bodies of text and eliminating duplicate posts using regular expressions (RegEx) to ensure robust removal and data quality. The final dataset comprises 92,153 jokes categorized into Wholesome Jokes (inoffensive), Dark Jokes (morbid, cruel, offensive to some, graphic), and Dirty Jokes (obscene, indecent, vulgar, sexist, racist, discriminatory), along with news articles.": 985,
    "The paper proposes LogFormer, a two-stage pipeline designed to transfer shared semantic knowledge across diverse log domains. In the first stage, called the pre-training stage, a model based on the Log-Attention encoder is initially trained on a source domain dataset. This pre-training aims to acquire common semantic representations of log sequences. The objective of this stage is a supervised binary classification task, and it operates without using log adapters. After pre-training, the learned parameters of the Log-Attention encoders are saved and used as the initialization for the subsequent stage. In the second stage, known as the adapter-based tuning stage, the knowledge obtained from pre-training is leveraged for target domain adaptation. Specifically, lightweight adapters are strategically plugged into the encoders of the pre-trained model. During this tuning phase, only the parameters of these newly introduced adapters are updated, while the parameters of the pre-trained Log-Attention and feedforward layers are frozen. This mechanism allows LogFormer to efficiently reuse the pre-trained model, transferring knowledge with only a few additional trainable parameters, thereby improving generalization across domains.": 986,
    "The paper proposes a Modality-Specific Domain Adaptation module that reduces domain gaps by optimizing an appropriate intermediate domain. First, the module generates initial coarse intermediate domain representations. It then defines a function P(·) to represent the distribution of each modality data in each domain. By calculating the distance relationship between the source and target domains, the module minimizes the intermediate domain loss, thereby alleviating the domain shift. This ensures that the source domain's knowledge can be smoothly transferred to guide the learning of the target domain.": 987,
    "The paper proposes Neighborhood Invariance Learning (NIL) to learn more generalizable features in the target domain by reducing intra-domain variations. This is achieved by maximizing pairwise similarities between each target sample and its reliable neighbors. The method maintains a memory bank, denoted as M, storing l2-normalized features of all target samples. For an input target sample xt_j, its feature zt_j is stored in M. Pairwise similarity sj,k between xt_j and any other target sample xt_k in the memory is computed as the dot product of their features, mjmk. These similarities are then normalized using a Softmax function to estimate the probability pjk of xt_k sharing the same class with xt_j.\nA self-adaptive neighbor search strategy is employed to define the neighborhood Nj for each target sample xt_j. Instead of a fixed K-nearest neighbors approach, Nj is defined as the set of samples k (excluding j itself) where sj,k is greater than a relative similarity ratio (ϵ times the similarity to the nearest neighbor Nj[0]). This self-adaptive approach overcomes limitations of fixed K-values and sample size imbalances.\nFurthermore, a confidence-guided representation learning technique is used, assigning different weights wjk to each neighbor in Nj. This confidence is measured using the Jaccard distance between the neighborhoods of xt_j and xt_k, specifically wjk = |Nj ∩ Nk| / |Nj ∪ Nk|. This incorporates additional information about neighborhood relationships to improve robustness. Finally, the confidence-guided invariant feature learning objective, Lnil(xt_j), is minimized. This loss function is defined as the negative sum of wjk multiplied by the logarithm of pjk for all neighbors k in Nj, effectively forcing reliable neighbors with high confidence to have similar feature representations.": 988,
    "The paper addresses this by designing a Target Domain Focalizer (TDF). The TDF takes the original prototypes (P), which are trained on the source domain, and the target domain feature map (F) as input. It then produces a set of target-aware prototypes (PT). This process involves two cross-attention stages. First, the original prototypes (P) act as queries to the feature map (F) (keys), generating initial correlation maps (ck). These maps are then normalized using a softmax operation to produce initial attention maps (ak). To mitigate noise from the domain gap, unconfident weights in these attention maps are filtered out using a predefined threshold (τ) before weighted pooling. These filtered attention maps are then used to perform weighted pooling on the feature map (F), yielding an intermediate set of prototypes (Ptemp). This Ptemp is then used in a second cross-attention stage, repeating the process (without thresholding) to generate the final target-aware prototypes (PT). This iterative refinement, inspired by Gestalt law, ensures that PT more precisely and completely activates the areas of corresponding classes in the target domain, thereby suppressing noisy pseudo-labels and highlighting reliable ones. Additionally, a target prototype bank is maintained and updated by exponential momentum averaging with momentum (θ) at each time stamp (t) to expand the coverage of the underlying target domain feature distribution.": 989,
    "The paper applies Differentiable Neural Architecture Search (NAS) to the prediction head of an object detector to find an optimal sub-architecture that enhances Out-of-Domain (OoD) generalization. The core idea involves constructing a super-net, denoted as `S(ω,α)`, which is composed of multiple computational units called cells. Each cell is defined by `p` ordered nodes `N = {n1,...,np}` and `q` candidate operations `O = {o1,...,oq}` that can connect these nodes. To make the architecture search differentiable, the binary choice variables `δ(i,j)k` (indicating whether operation `k` between node `ni` and `nj` is chosen) are converted into continuous relaxation using a softmax function, resulting in differentiable architectural parameters `α(i,j)k`. The feature representations `z` extracted by the object detector `F(θ)` initialize the first node `n0` of the first cell, with subsequent cells using the output of the preceding cell. The NAS problem is formulated as a bi-level optimization. While typically involving training and validation losses, this paper simplifies it by using the training loss (`Ltrain`) to optimize the architectural parameters `α`, as an in-domain validation set is deemed unsuitable for S-DGOD's OoD generalization objective. Once the search stage concludes, the optimal architectural parameters `α*` are determined by selecting the operation with the maximum value for each connection, and the prediction head is then reconstructed with these chosen operations.": 990,
    "The paper proposes a Gaze Frontalization Module to embed the gaze frontalization process. This module takes the extracted feature `z` from the feature extractor `F` (which processes the input face image `x`) as input. Its goal is to force the feature `z` to rotate the eyeballs in `x` so that they look at the (0,0) direction (front/camera) while keeping the head orientation unchanged. This is implemented using an image warping strategy. First, the extracted feature `z` is fed into a warping field generator `W` to obtain a warping field `M`. The warping field `M` dictates how pixels from the input image `x` are mapped to the output frontalized image `xfro`. Specifically, for every position in the output image `xfro`, `M` provides the corresponding position in the input image `x`. Then, a bilinear sampler `S` applies this warping field `M` to the input face image `x` to produce the output warped face image `xfro`. The warping field generator `W` is implemented as a ResNet-like decoder. To constrain the generation of `xfro`, a reference face image `xref` is introduced, which has eyeballs looking at (0,0) and the same head orientation and identity as `x`. The module is optimized using a loss function `Lfro`, which maximizes the Multi-Scale Structural Similarity (MS-SSIM) between `xref` and `xfro`, defined as `1 - MS-SSIM(xref, xfro)`.": 991,
    "The paper introduces the Frequency-Aware Domain Randomization (FADR) module. This module transforms a pixel-domain image into its frequency-domain components using the Discrete Cosine Transform (DCT). It then applies a random masked filtering operation specifically to the domain-variant low- and high-frequency band components, while preserving the domain-invariant mid-frequency components. Mathematically, for a source image `xs`, the transformation `TFADR(xs)` is defined as `ϕ−1(ϕ(xs + ξ) ⊙ M)`, where `ϕ` is the DCT, `ϕ−1` is the inverse DCT (IDCT), `ξ` is random noise sampled from a Gaussian distribution `N(0,σ^2I)`, and `M` is a mask matrix. The mask `M` assigns values from a Uniform distribution `U(1 − ρ, 1 + ρ)` to low-frequency (`f < fl`) and high-frequency (`f ≥ fh`) bands, and `1` to the mid-frequency band (`fl ≤ f < fh`). This selective randomization diversifies the input images fed to the perturbation generator `Gθ(·)`, making it more robust to domain shifts. The augmented image from FADR is then used as input to the generator to produce the adversarial image `x′s = P(Gθ(TFADR(xs)))`, where `P(·)` is a perturbation projector ensuring the `l∞`-budget constraint.": 992,
    "The paper addresses this by employing Prototype Contrastive Learning (PCL) within a domain-shared unit hypersphere space. First, source prototype codes (ps_c) are calculated for each category 'c' using labeled source data. This is done by averaging the feature representations (fs_i) of all source samples belonging to category 'c', obtained via a Multi-Layer Perceptron (MLP) (Eq. 1, 2). These prototypes are initialized from the first epoch and updated in successive epochs based on sampled data. Next, pseudo-labels (yt_j_hat) for target domain samples are obtained by assigning each target sample to the category whose source prototype has the highest cosine similarity with the target sample's feature representation (ft_j) (Eq. 3). Using these pseudo-labels, target prototype codes (pt_c) are calculated similarly to source prototypes, by averaging target sample feature representations belonging to each pseudo-labeled category (Eq. 4). The core of the solution is the Prototype Contrastive Loss (LPCL) (Eq. 5). This loss encourages two main objectives: prototypical alignment and prototypical uniformity. Prototypical alignment is achieved by maximizing the similarity between corresponding source and target prototypes (ps_c and pt_c) for the same category. Prototypical uniformity is achieved by minimizing the similarity between prototypes of different categories (ps_c and pt_i, or pt_c and pt_i). This process ensures that feature representations are uniformly distributed on a unit hypersphere, categories are conflict-averse, and source and target domains are aligned in the shared feature space, implicitly transforming sample relationships into more discriminative prototype relationships.": 993,
    "The paper addresses the challenge of dividing target nodes by employing an adaptive thresholding mechanism based on prediction uncertainty. For each target node, its classifier output `ft` is used to calculate its entropy, denoted as `et = H(ft)`. Entropy serves as a measure of prediction certainty, where a lower entropy value indicates higher certainty that the node belongs to a known class, and a higher entropy value suggests a greater likelihood of belonging to an unknown class. A predefined threshold `γ` is then used to categorize target nodes into two distinct groups:\n1.  Certain Group (Groupc): This group comprises target nodes whose entropy `et` is less than `γ`. These nodes are considered to have a high probability of belonging to one of the known label classes shared with the source domain.\n2.  Uncertain Group (Groupu): This group includes target nodes whose entropy `et` is greater than or equal to `γ`. These nodes are considered more likely to originate from the unknown label space, although they may also contain a minority of known-class nodes with high prediction uncertainty.\nThe threshold `γ` is specifically set to `log(|Cs|)`. This value represents the maximum possible entropy for a classifier predicting over `|Cs|` classes, ensuring a consistent and adaptive division criterion. This dynamic splitting allows for differentiated treatment of target nodes based on their predicted confidence.": 994,
    "The paper addresses this by formulating schematic prompts and corresponding references based on two metaphor theories: Conceptual Metaphor Theory (CMT) and Semantic Preference Violation (SPV). For CMT, the prompt `Pc` is defined as \"\"{The[source]in[Sm]is mapped as[target]in[Tm]}\"\", representing property transformations. For SPV, the prompt `Ps` is \"\"{The[source]breaks the[target]context of[Tc]}\"\", focusing on context breaking. Here, `[source]`, `[target]`, `[Sm]`, `[Tm]` refer to source domains, target domains, source modality, and target modality from annotations, respectively, while `Tc` is the caption text obtained via a pre-trained multimodal captioner (BLIP). These prompts are embedded using a Transformer encoder (`θgT`) initialized with parameters pre-trained for generation, yielding metaphorical embeddings (`EMT`). The model then uses an image-grounded text decoder, which has a similar architecture to encoders but with causal self-attention and cross-attention layers for vision-language interactions, to generate sequences. The optimization is achieved through Maximum Likelihood Estimation (MLE) using a standard Cross Entropy (CE) loss (`LMLE`) based on generative logits and their labels (the next token ID). This conditional generative manner enhances the model's capacity to capture metaphorical analogy and non-literal interactions.": 995,
    "The paper addresses scalability and data privacy by employing a strategy inspired by Dual Diffusion Implicit Bridges (DDIBs), which leverages two separately trained Denoising Diffusion Implicit Models (DDIMs) for motion-to-motion translation. This approach allows for independent training of diffusion models for each style domain, eliminating the need for jointly trained models or paired datasets. The motion transfer process is a two-step procedure: first, a source diffusion model (s(i)θ) encodes the source motion (x(i)) and conditional signal (c) into a latent encoding (x(l)) at the final diffusion time step (t=1). Second, a target diffusion model (s(j)θ) decodes this latent encoding to construct the target motion (x(j)) at the initial time step (t=0). Both encoding and decoding steps are defined via Ordinary Differential Equations (ODEs) and solved using DDIMs. This decoupling of source and target domain models means that each model can be trained separately on its respective domain's data, alleviating the need for simultaneous access to the entire dataset and thus upholding data privacy.": 996,
    "The paper proposes a novel active strategy called Non-maximal Degree Node Suppression (NDNS) to select representative, diverse, and low-confidence target samples. This strategy operates by first constructing a directed graph from the unlabeled target data. For each unlabeled target sample, two types of neighbors are defined: Accepted Neighbors (ADN) and Acceptive Neighbors (AEN). ADN for a sample `xt_i` are the `M1` nearest neighbors based on cosine similarity, while AEN are the `M2` nearest neighbors, with `M2` being much smaller than `M1`. An adjacency matrix `A` is then constructed where `Aij = 1` if sample `xt_j` is an ADN of `xt_i` AND `xt_i` is an AEN of `xt_j`, indicating a directed edge from `xt_i` to `xt_j`. The out-degree of a node `xt_i` (sum of its row in `A`) indicates its appearance frequency in the AEN of other nodes, suggesting its representativeness as a \"\"center point\"\" for a local cluster if its out-degree is high.\nTo inject model uncertainty into the query process, the query criteria for NDNS are modified to incorporate a probabilistic margin `mi` of the sample `xt_i` yielded by the target learning model `ft`. The margin `mi` is defined as the difference between the prediction probabilities of the first and second largest elements of the output vector, where a lower margin implies an ambiguous prediction. The query function `Q(xt_i)` is then calculated as `α * (max-normalized out-degree) + (1 - α) * (1 - mi)`, where `α` is a trade-off factor.\nNDNS iteratively performs two steps: node query and node removal. In the node query step, the sample `xt_i` with the maximal `Q(xt_i)` score on the current graph is selected. In the node removal step, to avoid querying redundant nodes and ensure diversity, the selected node `xt_i` and its `k`-order neighbor nodes (where `k` is fixed to 2) are removed from the current graph. This is implemented by setting the corresponding rows and columns in the adjacency matrix `A` to zero, effectively isolating these nodes. For subsequent query rounds, the 1-order neighbors of previously selected target samples are also marked as isolated nodes to further guarantee diversity. This iterative process continues until the pre-specified annotation budget for the round is reached.": 997,
    "The framework includes three core modules: pre-training, fine-tuning, and debiasing. First, in the pre-training phase, the source and target data are transformed into correlated latent representations using source encoder \\(E_\\vartheta\\) and target encoder \\(E_\\phi\\). The cross-domain protected group estimator \\(G_\\Psi\\) is trained on these representations to estimate group memberships. Next, in the fine-tuning phase, consistency training refines the group estimates on the target data by injecting noise into the input examples and ensuring the estimator remains invariant. Finally, in the debiasing phase, an adversarial learning technique leverages the group estimator to debias the downstream ML model.": 998,
    "The paper addresses this by introducing a Hierarchical Domain Encoder, which consists of two sub-encoders: a domain-invariant encoder (qρ) and a domain-variant encoder (qσ). The domain-invariant encoder (qρ) maps a sequence of states and actions (s, a) to a domain-invariant embedding (zρ). Subsequently, the domain-variant encoder (qσ) takes the domain-invariant embedding (zρ) and the domain parameterization (ω) as input to produce the domain-variant embedding (zσ). This hierarchical structure ensures that the domain-invariant encoder captures fundamental action sequences pertinent to achieving goals, while the domain-variant encoder specifically captures features related to domain variations. The disentanglement is optimized using a modified Evidence Lower Bound (ELBO) loss, LDHVAE, which includes regularization terms (βρDKL(p(zρ)||qρ(zρ|s,a)) and βσDKL(p(zσ|zρ)||qσ(zσ|zρ,ω))) to encourage the embeddings to adhere to unit Gaussian priors and to ensure the domain-variant encoder constructs a distinct embedding space conditioned on the domain-invariant embedding.": 999,
    "The paper addresses this by proposing the Early-learner approach. Instead of the conventional method of training the surrogate model only after the Split Federated Learning (SFL) training is completed, the Early-learner initiates the surrogate model's training right from the beginning of the SFL process (epoch 0). This strategy allows the surrogate model to continuously learn from the evolving target model. As the target model undergoes significant changes during early SFL training epochs, it produces a wider variety of synthetic samples. By training the surrogate model with these samples from the outset, it gains exposure to increased inter-epoch variety, which is crucial for handling the inconsistent gradients inherent in a dynamic training environment. The surrogate model is trained using mini-batch Stochastic Gradient Descent (SGD) from the start, effectively leveraging this \"\"easy curriculum\"\" provided by the early-stage generated outputs.": 1000,
    "The paper proposes learning intra-domain weights (α_i) within the Bi-level ATtention ENsemble (Bi-ATEN) module to address the mismatch between target bottleneck features and source classifiers. For a given target sample's bottleneck feature (ϕ_i^t) from the i-th source domain, the method first obtains all possible cross-domain outputs (O_i^t) by forwarding ϕ_i^t through all available source classifiers (g_j). To find the most compatible classifier, the method computes the similarity between the bottleneck feature and the output vectors. This is achieved by applying cosine similarity between a linearly transformed feature embedding (ϕ_i^t W_F) and linearly transformed output embeddings (θ_j^gs W_O), where W_F and W_O are linear transforms (Linear2 and Linear1 in Fig. 2) that project features and outputs into the same embedding dimension (d_emb). The resulting similarity vector (Sim_i^t) is then passed through a softmax operation to obtain the intra-domain weights (α_i). These weights determine how different source classifiers contribute to the \"\"unbiased\"\" output for that specific bottleneck feature. The goal is to make this assembled output (˜y_i^t) confident and diverse, which is encouraged by applying an Information Maximization (IM) loss (L_IM) to the softmax output of ˜y_i^t.": 1001,
    "The paper proposes a Balanced Batch Normalization (Balanced BN) layer to address the challenge of adapting to class-imbalanced testing data. This layer maintains `Kc` pairs of statistics, specifically mean (µk) and variance (σk), separately for each semantic class `k`. To update these category-wise statistics, an efficient iterative approach is applied using pseudo-labels predicted by the adapted model. The update rule for the mean (µt k) and variance (σt k) at time `t` incorporates a delta term (δk) that is proportional to the instance-level momentum coefficient (η) and the difference between the feature value and the previous mean, weighted by an indicator function `1(ˆyb = k)` which is 1 if the pseudo-label `ˆyb` for sample `b` is class `k`. This ensures that statistics are updated only by samples belonging to their respective classes. Furthermore, to enhance robustness, the Balanced BN combines this pure class-wise updating strategy with a class-agnostic updating strategy (similar to Robust BN) using a balancing parameter `γ`. When `γ = 0`, it acts as pure class-wise updating, and when `γ = 1`, it degrades to the Robust BN rule. This allows the layer to adapt without biasing towards majority classes, as the global distribution is extracted from these category-wise distributions, enabling invariant estimation of distribution under both locally and globally class-imbalanced testing data.": 1002,
    "The paper determines the optimal linear combination of source models by defining the target feature extractor, fT, as a weighted sum of individual source feature extractors, fSj, where fT = sum(αj * fsj) and sum(αj) = 1. The optimal weights, α, are found by maximizing an information-theoretic metric called H-score, which measures the transferability of the combined features. The H-score, specifically its one-sided form H(fT) = tr(cov(E[f(XT)|YT])), is used to quantify the quality of the synthesized target features. This metric can be explicitly computed from extracted features and labels, even with limited target data. The optimization problem of maximizing H(fT;α) with respect to α is proven to be a convex quadratic form, ensuring its benign property. This allows for reliable solution using gradient descent-based algorithms, specifically Projected Gradient Descent (PGD), which projects the updated weights onto the hyperplane where their sum equals one. The training process involves iteratively computing empirical conditional expectations of source features given target labels, calculating the H-score, and updating α using PGD until convergence.": 1003,
    "The framework addresses the dynamic collection of well-adapted samples through a module called Well/Under-Adapted Domain Separation. This module divides the target domain into two subdomains: the well-adapted subdomain (DW) and the under-adapted subdomain (DU). DW is composed of two types of samples:\n1.  Easily-adaptable Sample Selection (DWe): This part identifies samples that are easily adapted by the model early in training. It computes the cross-entropy loss, denoted as H(·,·), for each sample using its source-predicted label (in Onehot form, ˆzi) and the target model's output logits (pi = hT(xi)). The top-δ percent of samples with the smallest loss are selected as DWe. This subset acts as an initial pivot for model training and domain alignment.\n2.  Model-confident Sample Selection (DWc): As training progresses and the domain gap narrows, more samples become well-adapted. These newly adapted samples may still have high loss ranks but typically exhibit high prediction confidence. To capture these, SEAL employs a flexible confidence thresholding mechanism. For each class c at epoch e, a per-class threshold Te(c) is calculated as ˆae(c) · τ, where τ is a pre-defined high confidence threshold (e.g., 0.95). ˆae(c) represents the adaptation progress of class c, estimated by the number of samples in the whole target domain that have a high-confident output (maximum logits above τ) and are predicted as class c. This adaptation progress ae(c) is then scaled and normalized to ˆae(c) using a hyper-parameter γ (e.g., 0.5 for VisDA) to account for class imbalance. Samples whose maximum predicted probability (max(pi)) is greater than Te(argmax(pi)) are selected as DWc.\nFinally, the complete well-adapted subdomain DW is formed by combining these two sets: DW = DWe ∪ DWc. The remaining samples constitute the under-adapted subdomain, DU = D − DW. This dynamic and flexible approach allows SEAL to progressively identify and include more reliable samples as training advances, going beyond a static selection budget.": 1004,
    "The FairWASP method formulates fair pre-processing as an optimization problem that seeks to find a set of integer sample weights, denoted as {θi}i∈[n], for a given dataset Z = {(Di,Xi,Yi)}n i=1. The reweighted distribution of the dataset is defined as pZ;θ = (1/n) Σi=1^n θiδZi, where δZi are Dirac measures centered on Zi. The objective is to minimize the Wasserstein distance Wc(pZ;θ, pZ;e) between this reweighted distribution and the empirical distribution of the original dataset (pZ;e, where all θi=1). This measures the discrepancy between the original and reweighted datasets. To ensure fairness, the formulation includes demographic parity constraints, expressed as J(pZ;θ(y|d), pY(y)) ≤ ϵ for all sensitive feature levels d ∈ D and outcome levels y ∈ Y. Here, J(p,q) is a symmetric probability ratio measure defined as max(p/q - 1, q/p - 1), which ensures that the conditional distribution of the outcome Y given a sensitive feature D under the new weights closely aligns with the marginal distribution of Y in the original dataset. The crucial aspect is that the weights {θi} must be integers (θ ∈ In), meaning samples are duplicated or eliminated, and they must sum to n (θ ∈ ∆n). This formulation results in a Mixed-Integer Program (MIP). The paper theoretically proves that integer weights are optimal for this problem, meaning the optimal solution of the integer-weighted problem (4) is also an optimal solution for its real-valued relaxation (5).": 1005,
    "The paper proposes Sparse Visual Domain Prompts (SVDP) to address the challenge of preserving spatial information while extracting local domain knowledge. Unlike previous dense visual prompts that mask out continuous spatial details, SVDP maintains the same resolution as the input image but only inserts minimal discrete trainable parameters (e.g., 0.1%) onto specific pixels. This design ensures that more contextual information from the original input image is preserved. The reformulated image, which is the original input image `x` augmented with the SVDP `p` (expressed as `ex = x + p`), then serves as the input for the model. This sparse application of prompts allows the model to capture local domain knowledge through pixel-wise prompt parameters without significantly occluding the original image content, which is crucial for dense prediction tasks like semantic segmentation and depth estimation.": 1006,
    "The paper establishes a theoretical framework by introducing the concept of λ-consistency (Definition 4.1) for an evolving environment E. This definition quantifies how consistently the environment evolves by measuring the worst-case Kullback–Leibler (KL) divergence between an ideal forecasted domain (DK*i+1) and the actual subsequent domain (Di+1) across all domain pairs in E. The ideal mapping function K* is defined as the one that minimizes this maximum divergence. Based on this λ-consistency, Theorem 4.2 is derived, which provides a generalization bound for the risk on an unseen target domain (RDt(h)). This bound relates the target risk to the risk on the forecasted domain (RDK*t(h)) and includes terms dependent on the KL divergence between conditional distributions of real and forecasted domains, as well as the λ-consistency value. This theoretical analysis highlights that minimizing the distance between forecasted and real domain distributions is crucial for reducing the generalization bound.": 1007,
    "The paper proposes a component called Selection Training (ST) to address this. ST operates by first adaptively selecting classes that the model tends to forget. This selection is based on two criteria: the number of samples predicted for each class (fewer samples indicate higher likelihood of forgetting) and the prediction entropy of each class (higher entropy indicates lower confidence and poorer learning). These two rankings are combined to form a total ranking for each class, and a predefined number of top-ranked classes (Ncls = ⌊τ · C⌋, where τ is a hyper-parameter and C is the total number of classes) are chosen to form the selected class set (Sc).\nOnce the forgotten classes are identified, ST proceeds to obtain clean samples belonging to these selected classes. This is achieved using a small-loss criterion, where samples with the smallest Exponential Moving Average (EMA) losses are considered clean. The EMA loss for a sample is updated iteratively, preventing the need to store historical losses. To ensure sufficient samples for training, especially for extremely minority classes, a non-linear mapping function is applied to determine the sampling ratio (rc) for each class, allowing more samples to be selected from very small classes.\nFinally, these selected clean samples are used for enhanced training. The samples are augmented using both weak augmentation (random cropping and flipping) and strong augmentation (RandAugment and AutoAugment) to enlarge the sample set. The model is then trained on these augmented samples using a noise-robust loss function, which combines a standard cross-entropy loss (Lsel) with a reverse cross-entropy loss (Lrce). The reverse cross-entropy loss further mitigates the impact of any remaining noisy samples. After this training step, the EMA predictions of the selected samples are updated to prevent misclassification.": 1008,
    "The paper addresses the challenge of generating and refining robust pseudo-labels for the target domain under noisy conditions through a process called Target Domain Refinement. Initially, a prototype for each class in the source domain (P_s^k) is computed by averaging the embedded samples of that class using a feature extractor (ϕ). This source prototype is then used to initialize a shared prototype (P). For target samples (x_t^i), pseudo-labels (ŷ_t^i) are obtained by calculating the cosine similarity between their features (ϕ(x_t^i)) and the shared prototype (P), assigning the label of the most similar prototype. Subsequently, a prototype for each class in the target domain (P_t^k) is computed by averaging the embedded target samples assigned to that class. The shared prototype (P) is then updated by averaging the source and target prototypes (P = (P_s^k + P_t^k)/2), aiming to align the domains.\nTo enhance robustness against noise, the method incorporates a de-noising step. It assumes that samples farthest from their respective class prototypes are likely noisy. For each class k, the Euclidean distance (dist(x_t^i, P_k)) between each target sample and its prototype is calculated. A ratio (τ) of the farthest samples (e.g., τ=0.8, meaning 20% farthest samples are discarded) are identified and excluded from prototype calculation. A de-noised prototype (Q_t^k) is then re-calculated using only the remaining, closer samples. This de-noised prototype (Q_t^k) is used to iteratively refine the shared prototype (P_k,i) over multiple iterations, where P_k,i = (P_k,i-1 + Q_t^k)/2. This iterative process refines the pseudo-labels for the target domain, resulting in a refined target domain (D_t).": 1009,
    "The paper proposes an instance-level ensemble strategy to aggregate predictions from multiple source-specific networks for each target sample. This strategy considers three factors to estimate the importance of each source-specific network:\n1.  Prediction Confidence (cxt_ici): For a target sample `xt` and the `ith` source-specific network, prediction confidence is calculated using a cross-entropy-like function `H(p1, p2)` between the prediction of `xt` and the predictions of its similar samples (`Nxt`). The set `Nxt` is obtained by clustering the target domain features (from the domain-ensemble network) using K-means and selecting the cluster containing `xt`. This ensures confidence estimation is precise by considering similar samples.\n2.  Prediction Diversity (cxt_cdi): This measures how diversely a network predicts across categories. For `xt`, its prediction diversity from the `ith` source-specific network is estimated by comparing the prediction distribution of all clusters (`Nk`) in the target domain (obtained via K-means) with the prediction distribution of the cluster `xt` belongs to. A network that predicts diversely and has a low preference for `xt`'s category indicates higher diversity.\n3.  Ensemble Diversity Regularization (rxt_di): This measures the consistency of the `ith` source-specific network's prediction with the average prediction across all source-specific networks for `xt`. It is calculated as the negative cross-entropy between the `ith` network's prediction `pxt_i` and the average prediction `pxt_avg`. A larger value indicates the prediction is useful for ensemble learning.\nThese three importance measures (`cxt_ici`, `cxt_cdi`, `rxt_di`) are summed for each source-specific network. The resulting values are then normalized using a softmax function to obtain instance-level weights (`wxt_i`) for each source-specific network on the target sample `xt`. Finally, the instance-level ensemble prediction for `xt` is computed as a weighted sum of the predictions from all source-specific networks, using these calculated weights. This allows source-specific networks with high-confidence predictions for a particular sample to dominate its ensemble prediction.": 1010,
    "The paper defines the target distribution set, denoted as Tf,ρ(S1,...,Sd), as the set of all possible target domains T such that there exists a distribution Q within the convex hull of the source domains (CH(S1,...,Sd)) for which the f-divergence Df(T||Q) does not exceed a predefined threshold ρ. This formulation explicitly models the distributional shift between the source and target domains using f-divergence, a general measure that includes well-known metrics like KL divergence and Total Variation distance as special cases. For the nonconformity scores, the corresponding distribution set, Pf,ρ, is defined as the set of all distributions S on R (representing scores) such that there exists a distribution S0 in the convex hull of the push-forward distributions of scores from the source domains (CH(s#S1,...,s#Sd)) for which Df(S||S0) ≤ ρ. This allows the problem of finding a valid confidence set to be reduced to an optimization problem: maximizing the (1-α)-quantile over all distributions P in Pf,ρ.": 1011,
    "The paper addresses this by proposing a Decomposed Network (DN) that separates the Named Entity Recognition (NER) task into two pipeline sub-tasks: mention detection and entity typing. For mention detection, a unified training objective is adopted, using B, I, or O labels for each token to determine if it is part of an entity, without specifying the entity type. This allows the `fBERT` (pre-trained BERT encoder) and `fMD` (feed-forward network for B, I, O tagging) components to be shared across all source and target domains, bridging the gap caused by different entity type sets. For entity typing, which classifies detected mentions into predefined types, separate `fET` (feed-forward networks for entity typing) components are used for each domain, as entity types vary. The mention detection task provides higher-quality entity mentions to the entity typing task. The overall training loss combines the mention detection loss (`LMD`) and entity typing loss (`LET`) with a balancing parameter `α`.": 1012,
    "The paper addresses the challenge of making in-domain features more discriminative by introducing the In-domain Contrastive Learning Network (IC-net). This module is inspired by instance-wise contrastive learning. Given a batch of samples containing target keywords from the source domain (Ds), the IC-net first obtains their representations using the base model's extractor. To enhance discriminability, it then applies an instance-wise contrastive learning objective. This objective aims to cluster samples belonging to the same class (i.e., having the same target keyword category) closer together in the embedding space, while simultaneously pushing samples from different classes further apart. The objective function, denoted as Ls_IC, calculates the negative logarithm of the probability that a positive pair (samples from the same class) is closer than negative pairs (samples from different classes), normalized by a temperature hyper-parameter (τ). This process ensures that representations within the source domain are well-separated by category, laying a foundation for better domain alignment.": 1013,
    "The paper redefines the Named Entity Recognition (NER) task as a Machine Reading Comprehension (MRC) task and integrates prompting into this framework to enable single-round extraction of all entity types. Specifically, the method introduces special boundary tokens, `[ENT START]` and `[ENT END]`, which are sequentially inserted into templates for various entity types. These tokens are added to the vocabulary of the Pre-trained Language Model (PLM) backbone, such as BERT. A manual label vocabulary mapping, denoted as `M`, is constructed, assigning a semantically meaningful word (e.g., \"\"person\"\" for the \"\"PER\"\" label) to each entity label. For each label `li` in the predefined label set `L`, a prompt `p(li)` is designed as `{[ENT START]M(li)[ENT END]}`. All these generated prompts `P = {p1, p2, ..., pm}` are then appended to the original input sentence `X` to form an extended input sequence `X' = {x1, ..., xt, p1, ..., pm+1}`, where `t` is the length of the original sentence and `m` is the number of entity classes. An additional label word is included in the prompts to represent non-entity classes. This `X'` sequence is fed into the Encoder (the PLM), which generates contextualized representations. During the inference process, the representations of the special `[ENT START]` and `[ENT END]` tokens within the prompts serve as anchors or metric references. This design allows the model to extract all entity spans in a single pass, as opposed to traditional MRC frameworks that might require multi-turn question-answering for different entity types or enumerating all possible text segments.": 1014,
    "The paper addresses the construction of a comprehensive, multilingual idiom knowledge base, named IDIOMKB, by leveraging large language models (LLMs) through a process of symbolic knowledge distillation. To build IDIOMKB, the first step is Source Data Collection, where idioms are gathered from multiple existing datasets across three languages: English (from MAGPIE, IMIL, EPIE, PIE), Chinese (from PETCI, CCT, ChID), and Japanese (from OpenMWE, ID10M). This collation ensures broad coverage of idioms. The second step is Idiomatic Meanings Distillation from LLMs. This involves using LLMs via in-context learning to generate the figurative meanings. A specific task prompt is designed, emphasizing the non-compositional nature of idioms and instructing the LLM to provide the idiom's figurative meaning in a target language (e.g., English meaning for a Chinese idiom). Manual examples, annotated from online dictionaries, are provided within the prompt for in-context learning. For instance, for the Chinese idiom \"\"一气呵成,\"\" the LLM is prompted to generate its English figurative meaning, such as \"\"to complete a task or work in one go, without stopping or taking a break.\"\" This method is designed to be computationally efficient and scalable, avoiding the labor-intensive process of manual annotation. The resulting IDIOMKB contains a large number of idioms with their multilingual figurative meanings, distinguishing it from existing corpora that often lack multilingual coverage or figurative meanings.": 1015,
    "The paper proposes using a Variational Auto-Encoder (VAE) for feature disentanglement. A probabilistic latent variable `z` is used to encode the text representation `h`, and `h` is then decoded from `z`. The VAE loss (`Lvae`) is defined based on the evidence lower bound (ELBO), which includes a reconstruction term and a Kullback-Leibler (KL) divergence term between the posterior `q(z|h)` and the prior `p(z)`. The posterior `q(z|h)` is modeled as a normal distribution `N(µ, diag σ^2)`. Crucially, the mean `µ` and variance `σ^2` (or standard deviation `σ`) are treated as independent components. The paper uses `zµ` (derived from `µ`) to represent robust features and `zσ` (derived from `σ`) to represent unrobust features. These are modeled by two independent linear transformations applied to the base model's output. To ensure the robustness of `zµ`, a classification head (`Head(.)`) is applied to `zµ` to predict the sample labels, optimized with a cross-entropy loss (`Lce`). This setup ensures that `zµ` is effective for the classification task, while `zσ` is intended to capture task-irrelevant or unrobust information.": 1016,
    "The model addresses metaphorical inconsistency through a Metaphor Contrastive Learning Module, which aims to semantically narrow the gap between idioms' literal and metaphorical representations using a prompt-based contrastive learning method. First, a Chinese whole word mask BERT model (Cui et al. 2021) is employed to obtain the metaphorical static representation. This is achieved by constructing a prompt template in the format `[CLS]成语[MASK]的定义是D[SEP]` (meaning: `[CLS] The definition of idiom [MASK] is D [SEP]`), where 'D' represents the idiom's definition. The BERT model processes this template, and the hidden layer representation of `[MASK]` from its final layer (l-th layer) is extracted as the metaphorical static representation, denoted as `hl_mt`. Next, the contextual dynamic representation `hl_pt` (obtained from the Contextual Representation Learning Module) and the metaphorical static representation `hl_mt` are designated as positive samples. Representations of other contexts and template sentences within the same mini-batch are treated as negative samples for both `hl_pt` and `hl_mt`. The SimCSE model (Gao, Yao, and Chen 2021) is then utilized to maximize the consistency of these positive samples within the idiom semantic space. The training objective for this module, `lc`, is an additional loss value calculated using a contrastive learning formulation, which is trained simultaneously with the main idiom cloze test loss.": 1017,
    "The Shared Beam Search (SBS) algorithm addresses this by employing a concurrent hash table, specifically DashMap, to manage the set of generated states (G) and facilitate dominance checks across multiple threads. In SBS, the concurrent hash table is logically divided into multiple shards, with each shard protected by a lock. When a thread needs to access a shard, it first acquires the lock, performs the necessary operation (e.g., inserting a successor state), and then releases the lock. The specific shard for a state is determined by the hash value of its non-resource variables, ensuring that the same key consistently maps to the same shard. The algorithm processes base states in parallel to identify potential solutions. For expanding states, each thread processes states from the open list (O) in parallel. When a successor state is generated, its shard is determined, the shard is locked, and the `INSERT` function (Algorithm 2) is called. This function checks for dominance: if the new state is dominated by an existing state in the shard, it is ignored; if it dominates an existing state, the dominated state is removed. After all states in the current open list are processed, the global open list is updated by collecting states from all shards, filtering those with f-values greater than or equal to the primal bound. To handle the beam width (b), search nodes are sorted in parallel by f-values and h-values, and only the best 'b' states are retained, with a 'complete' flag indicating if any states were discarded.": 1018,
    "The paper addresses the challenge of transforming target domain data to fit a source model's input by introducing an Input Transformation Layer (Input-Transform). This layer takes a target-domain data sample `xT` of dimension `dT` and transforms it into `exT` of dimension `dS`, which is the input dimension of the pre-trained source model `fS`. The transformation function is parameterized by a set of trainable parameters `θ`. For continuous data, a simple realization is an additive input perturbation `δ` with a binary mask `M ∈ {0,1}dS`. This is expressed as `exT = Zero-Padding(xT) + M ⊙ δ`, where `Zero-Padding(·)` augments the input with zero values to match `dS`, and `⊙` denotes the Hadamard (element-wise) product. The mask `M` indicates which dimensions are trainable, typically the zero-padded ones. For instance, `xT` can be placed at the center of its zero-padded version, with the surrounding padded dimensions made trainable. If the source model's input range is bounded, a change-of-variable technique like `θ = tanh(M ⊙ W)` can be applied, where `W` are unconstrained optimization variables and `tanh` is the hyperbolic tangent function. For discrete data, the input transformation can involve inserting trainable prompt tokens at the data input or using a trainable token embedding mapping function `VT ≈ VSθ` via dictionary learning, where `VS` and `VT` are vocabulary matrices for source and target tokens, respectively.": 1019,
    "The paper proposes a novel method called cross-adaptation, which iteratively treats each domain as a source domain while treating all other domains as target domains. For each iteration, the source dataset is transformed based on the target datasets using a selected domain adaptation method. The transformed datasets are then concatenated and paired with their respective labels to train a general model. This process works regardless of the chosen domain adaptation method, allowing for implicit adaptation to the target domain without labeled examples.": 1020,
    "The paper defines a simple and general online progress measure based on a \"\"hardness\"\" value assigned to nodes in the search tree. The hardness value, denoted h(i) for a node i, is defined recursively: the root node r has a hardness h(r) = 1. For any inner node i, its hardness h(i) is the sum of the hardness values of its children. This recursive definition ensures that the hardness of a subproblem is partitioned among its subproblems. Two hardness schemes are considered: uniform (pu), where hardness is uniformly divided among children, and approximate tree balance (pk), which attempts to reflect the relative sizes of child subtrees. The progress measure for a search tree Tk is then defined as h(Fk), which is the sum of the hardness values of the final leaves (Fk) explored up to step k. This measure is proven to be non-decreasing, starting at 0 and reaching 1 when the complete tree is explored. The hardness at the leaves (h(Lk)) is shown to remain constant throughout the search and equal to 1. This approach avoids significant time or memory overhead and is invariant to changes in the primal bound.": 1021,
    "The emotional content and associated labels of source images are preserved during adaptation through the introduction of an Emotional Semantic Consistency (ESC) loss, denoted as LESC(GST). This loss is designed to minimize the difference between the predicted emotions of the original source images (xS) and their adapted counterparts (GST(xS)). Specifically, LESC(GST) is calculated as the expected value of a distance function d(·,·) applied to the predicted emotion labels from a classifier F for both the original source image and the adapted source image: ExS∼PSd(F(xS),F(GST(xS))). The paper proposes two strategies for defining the distance function d(·,·):\n1.  Symmetrized Kullback–Leibler divergence (SKL): Since the classifier's output F is a probability distribution over emotion categories, SKL(p || q) = KL(p || q) + KL(q || p) is used to measure the distance between the probability distributions p (for F(xS)) and q (for F(GST(xS))). KL(p || q) is defined as the sum over all L emotion categories of pl * ln(pl / ql).\n2.  Mikels’ Wheel distance: Inspired by emotion theory, this method uses Mikels’ Wheel to determine the relationship between two emotions. The pairwise emotion distance is defined as \"\"1 + the number of steps required to reach one emotion from another\"\" on the wheel, and similarity is its reciprocal. The distance d(·,·) is then calculated as \"\"1 - pairwise emotion similarity\"\". This loss term is incorporated into the overall CycleGAN loss, with a hyperparameter λ controlling its relative importance.": 1022,
    "The paper proposes the Common Factorised Space (CFS) layer, denoted as FC, which is a fully connected layer with a sigmoid activation function, ΨθC(·) = σ(WΦθM(·) + b). This layer is inserted between the feature output layer (the penultimate layer) from the feature extractor ΦθM and the classification layer. The CFS layer is shared between both source and target domains, forming a common latent space of dimension dC. The sigmoid activation ensures that the activations FC are within the range (0,1), allowing values near 0 or 1 to be interpreted as the absence or presence of a corresponding latent factor. This design aims to learn a latent factor representation for both domains, enabling alignment and knowledge transfer even when their label spaces are disjoint and target labels are scarce or absent.": 1023,
    "The paper proposes an Instance-Based Discriminative Loss (LId) to achieve more discriminative deep features. This loss is formulated to ensure that samples from the same class are as close as possible in the feature space, while samples from different classes are distant from each other by a large margin. Specifically, for any pair of deep features (hs_i, hs_j) from the bottleneck layer, if they belong to the same class (Cij = 1), their squared Euclidean distance (||hs_i - hs_j||^2) is penalized if it exceeds a margin m1. Conversely, if they belong to different classes (Cij = 0), their squared Euclidean distance is penalized if it is less than a margin m2 (where m2 > m1). This is achieved through a hinge-like loss function: max(0, ||hs_i - hs_j||^2 - m1)^2 for same-class pairs, and max(0, m2 - ||hs_i - hs_j||^2)^2 for different-class pairs. The total LId is the sum of these penalties over all pairs of samples in a mini-batch, with a trade-off parameter α balancing intra-class compactness and inter-class separability. This loss is differentiable and integrated into the overall training objective, allowing the network parameters to be updated via standard backpropagation to learn features that inherently possess better intra-class compactness and inter-class separability.": 1024,
    "The paper addresses the challenge of preserving the feature-label relationship during unilateral transformation by introducing a specific form for the transformation function: F(x) := (A + I)x. In this formulation, F(x) represents the transformed source features, x denotes the original source features, A is a D × D matrix to be estimated, and I is a D × D identity matrix. This design assumes that the transformed features F(x) differ from the original features x by only a small residual function, Ax. The magnitude of this residual is controlled by minimizing the Frobenius norm of A, i.e., ||A||_F, which serves as a regularization term in the overall objective function. By learning this residual function Ax with reference to the source features x, rather than directly learning an unreferenced function F(x) := Ax, the proposed method prevents drastic changes to the source features. This mechanism is critical because excessively transforming source features risks destroying the inherent relationship between features and their corresponding labels, which would otherwise lead to deteriorated performance on the target domain. The regularization term, weighted by a positive constant γ, allows for flexible control over the extent of deviation from an identity mapping, ensuring that the transformation is not overly aggressive.": 1025,
    "The paper proposes a novel discrepancy measure called source-guided discrepancy (S-disc), defined as $\\varsigma_{\\ell}^{\\mathcal{H}}(P_{D1}, P_{D2}) = \\sup_{h \\in \\mathcal{H}} |R_{\\ell}^{D1}(h, h_S^*) - R_{\\ell}^{D2}(h, h_S^*)|$. Here, $h_S^*$ represents the true risk minimizer in the source domain, $R_{\\ell}^{D}(h, h')$ is the expected loss for functions $h, h'$ under distribution $P_D$, and $\\mathcal{H}$ is the hypothesis class. Unlike existing measures like X-disc which consider the worst pair of hypotheses, S-disc fixes one hypothesis to the true risk minimizer of the source domain ($h_S^*$). This design choice allows S-disc to provide a tighter generalization error bound than X-disc, as demonstrated by the inequality $\\varsigma_{\\ell}^{\\mathcal{H}}(P_T, P_S) \\le \\text{disc}_{\\ell}^{\\mathcal{H}}(P_T, P_S)$. Furthermore, by incorporating source labels, S-disc can be computed without requiring labels from the target domain, making it suitable for unsupervised domain adaptation. This approach also leads to a lower computational cost compared to X-disc.": 1026,
    "The Multi-Granularity Alignment Network (MGAN) is proposed to facilitate knowledge transfer from a rich-resource source domain of an Aspect Category (AC) task to a low-resource target domain of an Aspect Term (AT) task. The MGAN consists of two distinct networks: one for the source AC task and one for the target AT task. To reduce task discrepancy, these networks are designed with different numbers of attention hops to maintain consistent granularity and symmetric information regarding the aspect. The source network, acting as a \"\"teacher,\"\" utilizes three levels of attention hops: Context2Aspect (C2A), Coarse2Fine (C2F), and Position-aware Sentiment (PaS). The target network, acting as a \"\"student,\"\" uses two basic attention hops: C2A and PaS. After obtaining aspect-specific representations from both networks, knowledge transfer is achieved through Contrastive Feature Alignment (CFA), which bridges the distribution gap across domains by semantically aligning features. The overall training procedure involves an initial individual training of the source network, followed by alternately optimizing the source and target networks.": 1027,
    "The paper designs a transferable curriculum using a linearly-combined weighting scheme, `w(x_s^i) = 1(l_i + lambda * tau_i <= gamma)`. This scheme determines whether a source example `x_s^i` is selected for training (weight `w` is 1) based on a threshold `gamma`. The selection criterion combines two factors: `l_i` and `tau_i`. `l_i` represents the \"\"easiness\"\" of the sample, measured by the loss `L_y(y_s^i, G_y(G_f(x_s^i)))` from the label classifier `G_y` and feature extractor `G_f`. A smaller `l_i` indicates an easier sample. `tau_i` represents the \"\"transferability\"\" of the sample to the target domain. This is measured by `tau_i = -log(1 - G_d(G_f(x_s^i)))`, where `G_d` is the domain discriminator. `G_d(G_f(x_s^i))` indicates the probability that `x_s^i` is from the source domain. Therefore, `1 - G_d(G_f(x_s^i))` indicates the probability that `x_s^i` is similar to the target domain. A smaller `tau_i` indicates higher transferability. The hyper-parameter `lambda` balances the contribution of easiness and transferability. If a source example is both easy and transferable, its combined loss `l_i + lambda * tau_i` will be small, making it highly likely to be selected into the curriculum learning procedure.": 1028,
    "The paper addresses this by introducing a dedicated Target-Specific Network (TSN) parameterized by `θT`, which has the same architecture as the Adaptation Network (AN) but is trained exclusively on unlabeled target data `Xt`. The AN, parameterized by `θA`, acts as a teacher, providing soft labels to guide the TSN. This guidance is implemented through a mutual learning term, `Lmutual(θT;Xt,θA)`, which is part of the TSN's overall loss function `LT`. Specifically, `Lmutual(θT;Xt,θA)` minimizes a similarity measure `S` (e.g., Kullback-Leibler (KL) divergence, cross entropy, mean square error) between the TSN's predicted class probability distribution `pθT(xt)` and the AN's predicted class probability distribution `pθA(xt)` for each target instance `xt`. This process allows the TSN to learn from the AN's predictions, effectively using them as soft targets, without requiring direct adversarial training for domain confusion, as the TSN's focus is solely on the target domain.": 1029,
    "The paper proposes a self-ensembling architecture comprising a student network and a teacher network, both sharing the same DeepLab-v2 architecture with a VGG-16 backbone and an integrated attention module. The student network is responsible for learning and is optimized through back-propagation. It receives source-domain images (Xs) with corresponding ground truth labels (Ys) to calculate a supervised segmentation loss (Lseg) using cross-entropy. Additionally, it processes target-domain images (Xt), which are unlabeled, to compute an unsupervised consistency loss (Lcon). This consistency loss is calculated as the mean squared error between the student network's probability map (PS(g(Xt))) and the teacher network's probability map (PT(g(Xt))) for the target-domain images. The overall loss function for the student network is a combination of these two losses: LS = Lseg(Xs) + λconLcon(Xt), where λcon is a weighting factor. The teacher network, conversely, does not participate in back-propagation. Instead, its parameters (θT) are updated using an exponential moving average (EMA) of the student network's parameters (θS) at each iteration. Specifically, at the t-th iteration, θtT = αθt-1T + (1-α)θtS, where α is a smoothing coefficient hyper-parameter (empirically set to 0.99). This EMA update allows the teacher network to maintain a more stable and robust representation, effectively acting as an ensemble of the student's past states. In the test phase, target-domain images are fed into the teacher network to obtain the final semantic segmentation maps, leveraging its more stable and domain-invariant features.": 1030,
    "The Interactive Attention Transfer Network (IATN) addresses this by employing two main components: a sentence network (S-net) and an aspect network (A-net), which interact through attention mechanisms. Both S-net and A-net process their respective inputs (sentences and aspects) through word embedding, Long Short-Term Memory (LSTM) networks for hidden state learning, and a mean pooling operation to obtain a single pooling vector (hp_s for sentences, hp_a for aspects). The core of the interactive attention transfer mechanism lies in how these pooling vectors influence each other's attention calculations. For sentence representation, the aspect pooling vector (hp_a) is connected with each sentence hidden state (hi_s) to form a combined vector (hn+1_a). This combined vector is then used to calculate attention weights (alpha_i) for each word in the sentence, allowing aspect information to guide the importance of words in the sentence. Similarly, for aspect representation, the sentence pooling vector (hp_s) is spot-multiplied with each aspect hidden state (hi_a) to calculate attention weights (beta_i) for each word in the aspect, enabling sentence context to influence aspect word importance. The final sentence representation (Sr) and aspect representation (Ar) are then computed as weighted sums of their respective hidden states using these attention weights. Finally, these two representations (Sr and Ar) are concatenated and fed into a sentiment classifier to predict the sentiment label.": 1031,
    "The paper addresses this by implementing a \"\"Domain-speciﬁc Distribution Alignment\"\" as the first stage of its framework. Instead of mapping all domains into a single common feature space, the proposed Multiple Feature Spaces Adaptation Network (MFSAN) maps each pair of source and target domains into *multiple different* specific feature spaces. This is achieved by dividing the network into two parts: a \"\"Common feature extractor\"\" (f(·)) and \"\"Domain-spespeciﬁc feature extractors\"\" (hj(·)). The common feature extractor learns shared representations for all domains. Subsequently, for each of the N source domains, there is an unshared domain-specific subnetwork hj(·) that takes the common features (f(xsj) and f(xt)) and maps them into a specific feature space for that particular source-target pair. To align the distributions within these specific feature spaces, the Maximum Mean Discrepancy (MMD) is used as the discrepancy measure. The MMD loss (Lmmd) is calculated as the average of the MMD between the features from each source domain j and the target domain in their respective specific feature spaces, i.e., ˆD(Hj(F(Xsj)),Hj(F(Xt))). This process ensures that domain-invariant representations are learned specifically for each source-target relationship, acknowledging that a single common representation might be too restrictive.": 1032,
    "To enhance context representations, the model introduces several alternatives beyond the standard Average of Context Embeddings (ACE). First, it employs a special bidirectional Long Short-Term Memory (LSTM) network to encode sequential information. Unlike standard bidirectional LSTMs, the two LSTMs gravitate toward the target words: a forward LSTM generates a hidden representation for each word *before* the target word, and a reversed LSTM generates a hidden representation for each word *following* the target word, explicitly excluding the target word itself from the LSTM input. The context representation for a target word at position 'i' is then the concatenation of the hidden representations of its two neighboring words (ci = [hi-1; hi+1]). Second, to identify critical words, two self-attention (or intra-attention) mechanisms are proposed: Global Attention and Local Attention. Global Attention uses a learnable attention vector 'v' to compute an importance score (gi = v · hi + b) for each word 'wi' based on its hidden representation 'hi'. These scores are then normalized using a softmax function (ai = egi / sum(egp)) to obtain attention weights, and the attention-based context representation (vc) is a weighted sum of the hidden states of LSTMs (vc = sum(hiai)). Local Attention, designed for interactions among words within a sentence, characterizes the strength of semantic interaction between words 'wi' and 'wj' using a diagonal relevance matrix 'A' where Ai,j = f(wi,wj) (inner product of their embeddings). The importance score for each word 'li' is the maximum relevance score with any other word in the context (li = maxj Ai,j, excluding self-relevance). Similar to global attention, these scores are normalized by softmax, and the final context representation is a weighted sum of hidden states. These attention models can also be applied to ACE by replacing 'hi' with word embeddings 'wi'.": 1033,
    "The paper addresses this by proposing the Nonverbal Sub-networks component. For each word L(i) in an utterance, the corresponding visual and acoustic subword units, V(i) and A(i) respectively, are processed. V(i) is a temporal sequence of visual frames, and A(i) is a temporal sequence of acoustic frames. To model these temporal sequences and compute nonverbal embeddings, two separate Long Short-Term Memory (LSTM) networks are employed: LSTMv for visual sequences and LSTMa for acoustic sequences. Specifically, the visual embedding h(i)v is obtained as the final state of LSTMv applied to V(i), and the acoustic embedding h(i)a is obtained as the final state of LSTMa applied to A(i). This approach captures the temporal dynamics and fine-grained structure of nonverbal behaviors within the duration of each uttered word.": 1034,
    "The paper integrates dynamic instance weighting into the fine-tuning stage by modifying the standard optimization update rule. Instead of treating all instances in a mini-batch with equal importance, each instance (xi, yi) is assigned a weight wi. The network parameters θ are then updated by a weighted sum of their individual gradients. Specifically, the update rule becomes θ ← θ − α * (1/k) * Σ(wi * ∇θf(yi, gθ(xi))), where α is the learning rate, k is the mini-batch size, f is the loss function (cross-entropy), and gθ(xi) is the model's prediction for instance xi. This dynamic weighting allows the model to put more training effort on instances that either contain more target knowledge or help preserve shared knowledge, based on their relationship with the current classifier. The weights wi are calculated and normalized at each mini-batch step.": 1035,
    "The MotionTransformer framework addresses this by employing a Recurrent Neural Network (RNN) encoder, denoted as `fenc`, to extract domain-invariant hidden representations `z` from raw sensory sequences `x`. This encoder is shared across all domains. To account for domain-specific features, a specific domain vector `θ` is concatenated with the inputs at every time step. While `θ` varies for different domains, the parameters of `fenc` remain shared, encouraging the encoder to capture features common across diverse sensor placements and motion dynamics. The output `z` is aligned to each time step `i` of the input `xi`. This design allows the encoder to learn a latent space where the underlying motion characteristics are preserved regardless of the sensor's attachment or orientation.": 1036,
    "The paper addresses this by reformulating the traditional matrix completion problem within a deep learning framework. An auto-encoder style architecture is employed, consisting of an encoder (W) and a decoder (V), both implemented as multi-layer fully-connected neural networks. Given a sparse augmented feature input `x` (where `x` is `[xS,0]` for source or `[0,xT]` for target), the encoder `W` maps `x` to a dense latent feature representation `h = W(x)`. The decoder `V` then reconstructs the missing entries in `x` to obtain a dense reconstructed `xr = V(W(x))`. The objective function for this deep matrix completion component, denoted as `Lmc(W,V)`, minimizes the Frobenius norm of the difference between the observed augmented matrix `X` and its reconstruction `V(W(X))`, masked by an indicator matrix `P` (which is 1 for observed entries and 0 for missing ones). Additionally, a low-rank regularization is imposed on the reconstructed matrix `V(W(X))` using the nuclear norm (`||.||*`), motivated by the assumption that augmented features may be linearly dependent and high-dimensional data is often controlled by a few latent factors. This non-linear factorization allows for exploring low-dimensional representative information that benefits subsequent distribution matching and classifier training. The gradients are computed using the chain rule, and for the non-differentiable nuclear norm, subgradients are used, involving Singular Value Decomposition (SVD) of the reconstructed matrix.": 1037,
    "The paper models the discrepancy between the target domain hashing projection (Wt) and the source domain hashing projection (Ws) as an error matrix E = Wt - Ws. Instead of assuming a Gaussian distribution for these errors (which would lead to a standard l2 loss and is unsuitable for cross-domain scenarios), the method adopts a Maximum Likelihood Estimation (MLE) view. It aims to minimize the negative log-likelihood function, which is expressed as the sum of a penalty function, rho_theta(en), for each element 'en' in the vectorized error matrix. The key innovation is the design of an iteratively weighted l2 loss. This is achieved by approximating the sum of rho_theta(en) using a first-order Taylor expansion, which results in an objective function of the form 0.5 * ||M^(1/2) (Wt - Ws)||^2, where M is a diagonal weight matrix. Each element Mij in M is defined by a weight coefficient, omega_theta(tilde_Eij), derived from a sigmoid function: omega_theta(Lambda_n) = exp(mu*delta - mu*Lambda_n^2) / (1 + exp(mu*delta - mu*Lambda_n^2)). This sigmoid function ensures that larger error values (Eij) are assigned lower weight coefficients, effectively punishing high errors less and making the model more adaptive to cross-domain data distributions. The weight matrix M is updated iteratively using the errors from the previous optimization step.": 1038,
    "The model addresses this by employing an auto-encoder architecture, G = (Enc, Swap, Dec), which first projects input images into a disentangled latent space. The encoder (Enc) takes an input image, x, and transforms it into an embedding, e. This embedding e is not a monolithic vector but is explicitly designed to be composed of k+1 distinct latent units: (a1, a2, ..., ak, za). Each ai represents the information pertaining to one of the k specific attributes (e.g., hair color, smiling), while za encapsulates attribute-irrelevant information, such as the image background or other general style elements. These latent units are implemented as grouped feature channels within a Convolutional Neural Network's feature map. This structured decomposition of the embedding ensures that different attributes, as well as non-attribute-specific content, are represented in separate, interpretable parts of the latent space. This design allows for the independent manipulation of individual attributes by operating on their corresponding latent units, while other units, including za, can remain unchanged, thereby preserving attribute-irrelevant details.": 1039,
    "The paper addresses the discrete and non-differentiable nature of Mixed Integer Programs (MIPs) by algorithmically generating a continuous surrogate optimization problem that can be differentiated. This is achieved through a pure cutting plane approach. The process begins by considering the linear programming (LP) relaxation of the original MIP, where integrality constraints are relaxed. If the solution to the current LP relaxation is fractional (i.e., not integral), a cut (e.g., a Gomory cut) is generated. This cut is a linear constraint that removes the discovered fractional solution while preserving all feasible integral solutions. This process iteratively tightens the feasible region of the LP relaxation. The cutting plane approach continues adding cuts until an integral solution is found or no more viable cuts can be generated. The resulting LP, augmented with these generated cuts (Sx ≤ t), forms a continuous and convex optimization problem that serves as a differentiable surrogate for the original MIP. Once this continuous surrogate LP is formed, its optimal solution can be differentiated with respect to the input parameters (objective coefficients `c`) by differentiating through its Karush-Kuhn-Tucker (KKT) conditions. To ensure strong convexity and facilitate differentiation, a small quadratic regularization term (γ||x||^2 / 2) is added to the objective function of the LP. This transforms the LP into a Quadratic Program (QP), whose KKT conditions are natively differentiable. The gradients of the solution `x` with respect to the parameters `θ` are then found by solving a system of equations derived from the KKT conditions.": 1040,
    "The paper introduces the 1/3-Envy-Free Algorithm for Arbitrary Valuations, which operates under the Robertson-Webb model. The algorithm iteratively allocates pieces of cake to agents, ensuring that each piece has a value of at least 1/3 for the receiving agent. Agents are processed in a sequence where each agent selects a piece of value 1/3 from the remaining cake. If the end of the cake is reached, any remaining agent receives the last piece. The algorithm ensures that the envy of any agent towards another is at most 1/3, and it runs in polynomial time.": 1041,
    "The AlignFlow framework addresses this by modeling data from each domain (e.g., A and B) using an invertible generative model based on normalizing flows. Specifically, it defines two invertible mappings: GZ→A, which maps from a shared latent space Z to domain A, and GZ→B, which maps from the shared latent space Z to domain B. The latent variables Z represent a common feature space between the observed variables in A and B. A prior density, such as an isotropic Gaussian, is assumed over Z. Since these mappings are invertible, their inverses, GA→Z = G−1 Z→A and GB→Z = G−1 Z→B, also exist. This design inherently provides a mechanism for cross-domain translation: to translate a data point from domain A to domain B, the data point is first mapped from A to the shared latent space Z using GA→Z, and then from Z to domain B using GZ→B, resulting in the composite mapping GA→B = GZ→B ◦ GA→Z. Similarly, translation from B to A is achieved via GB→A = GZ→A ◦ GB→Z. This shared latent space and invertible mapping structure allow for both modeling the marginal distributions of individual domains (by mapping from Z) and performing explicit, invertible translations between them.": 1042,
    "The framework first estimates the score function, which provides information about the distance of the test sample from the data manifold. If the score function exceeds a predetermined threshold, the sample is classified as a fringe example. For these fringe examples, the framework applies Langevin dynamics to move the samples towards high-density regions of the data distribution, thereby enhancing the mapping to the target domain. This process involves using the Metropolis-adjusted Langevin Algorithm (MALA) with a controlled step size and perturbation variance to ensure that the samples remain imperceptibly close to the original input.": 1043,
    "The paper proposes Discriminative Adversarial Domain Adaptation (DADA), which utilizes an integrated category and domain classifier, denoted as F(·), built on top of a feature extractor G(·). This integrated classifier is formed by augmenting the last fully connected (FC) layer of a standard K-category classifier with one additional neuron, resulting in K+1 outputs. The K+1th output, pK+1(x), represents the domain prediction (e.g., probability of being from the source domain), while the first K outputs, pk(x) for k=1,...,K, represent category probabilities. To establish a direct, mutually inhibitory interaction between category and domain predictions, DADA introduces a novel source discriminative adversarial loss, Ls(G,F), for labeled source instances xs. This loss is formulated as:\nLs(G,F) = - (1/ns) * sum_{i=1 to ns} [ (1 - pK+1(xs_i)) * log(pys(xs_i)) + pK+1(xs_i) * log(1 - pys(xs_i)) ]\nwhere pys(xs_i) is the predicted probability for the true category ys of source instance xs_i, and pK+1(xs_i) is the domain prediction for xs_i.\nIntuitively, this loss encourages pys(xs) and pK+1(xs) to be mutually inhibitory. When minimizing Ls over F(·), if pys(xs) is high (correct category prediction), pK+1(xs) is driven low (instance is from the source domain). Conversely, if pK+1(xs) is high (instance is predicted as source), pys(xs) is driven low. This means F(·) is trained to discriminate task categories while distinguishing the source domain. When maximizing Ls over G(·), the feature extractor is trained to make pys(xs) decrease and pK+1(xs) increase, effectively aligning the source domain to the target while retaining discriminability. This explicit interaction, particularly the condition that pys(xs) > 0.5 for source instances, is maintained by pre-training DADA on labeled source data with a K-way cross-entropy loss and continuing this supervision during adversarial training.": 1044,
    "The paper addresses this by employing Structured Prediction (SP) for pseudo-labeling. First, K-means clustering is applied to the projected feature vectors of all target samples (zt) to generate a number of clusters equal to the number of classes (|Y|). The cluster centers are initialized using class prototypes derived from source data. Next, a one-to-one matching is established between each target cluster and a source class. This matching is formulated as an optimization problem that minimizes the sum of distances between matched target cluster centers and source class prototypes. This problem is efficiently solved using linear programming. Once the clusters are matched to classes, target samples are collectively pseudo-labeled based on the class assigned to their respective cluster. This approach ensures that the pseudo-labeling considers the inherent grouping and structure of the target data, rather than labeling each sample independently. The conditional probability for a target sample belonging to a class is then calculated based on its distance to the matched target cluster center.": 1045,
    "The paper proposes Joint Adversarial Learning (JAL) to enhance adversarial training. This is achieved by introducing an auxiliary low-level domain discriminator, denoted as D1. Unlike the high-level domain discriminator, D1 is exclusively trained using low-level segmentation maps. The core mechanism of JAL is to add an auxiliary adversarial loss, Ladv jal(Xt), which compels the segmentation model to deceive this low-level domain discriminator (D1). Specifically, this loss is applied to the output of the high-level decoder, G(Xt), which represents Dec2(Enc2(Enc1(Xt))). By forcing the high-level target segmentation map to be misclassified as source by the low-level discriminator, JAL aims to make the target segmentation map more similar to source ones, thereby boosting the overall adversarial learning process.": 1046,
    "The paper addresses this by utilizing a Discriminator (D) as a probability distribution estimator. In the Weighting Scheme based Unsupervised Domain Adaptation (WS-UDA) framework, the discriminator D is trained adversarially alongside a shared feature extractor (Es) and private feature extractors (Epj). The objective of D is to discriminate which domain features are coming from, whether they are shared features from Es or private features from Epj. After the adversarial training converges, D's ability to discriminate between domains is leveraged to measure the instance-to-domain relations. Specifically, for an unlabeled target instance, D outputs probabilities indicating how likely that instance's features (either shared or private) belong to each source domain. These probabilities are then normalized to serve as instance-level weights (ˆwj) for each source classifier. These weights determine the confidence given to each source classifier's prediction (ˆcj) when combining them to derive the final pseudo label (ˆyi) for the target instance using a weighted sum: ˆyi = Σ(wj * ˆcj). This mechanism implicitly introduces data-dependent priors of source domains.": 1047,
    "The paper introduces a novel component called the Label Prober (P) within each of the two learning groups. Each label prober is designed to explicitly explore and leverage the document sentiment polarity present in the unlabeled target domain. Specifically, the prober in one group (e.g., P1) receives sentiment predictions generated by the sentiment classifier from the *peer* group (e.g., C2) for target domain documents. It then uses these predictions to guide the learning of the feature extractor within its *own* group (e.g., FE1) through gradient back-propagation. This is achieved by optimizing a mutual learning based loss function (LML) which measures the Kullback-Leibler (KL) divergence between the peer classifier's output (e.g., C2(di)) and its own prober's output (e.g., P1(di)) for target domain documents. This mechanism allows sentiment information from the target domain, albeit inferred by a peer model, to be effectively incorporated into the feature extraction process, acting as a bridge between the peer classifier and the local feature extractor.": 1048,
    "The paper proposes a Domain Adaptive Attention Module (DAAM) to achieve this separation. Given a feature map Fx from a backbone network (ResNet-50), the DAAM learns an attention map A(Fx) that ranges between 0 and 1. This attention map is then used to calculate the domain-shared (DSH) feature map Fsh x as the element-wise product of A(Fx) and Fx (Fsh x = A(Fx) ⊗ Fx). Concurrently, the domain-specific (DSP) feature map Fsp x is derived using a residual mechanism as Fsp x = (1 - A(Fx)) ⊗ Fx. This residual mechanism ensures that the DSH and DSP feature maps are separable and complementary. The DAAM itself learns spatial attention and channel attention sequentially. Spatial attention is learned using a depthwise separable convolution layer with c/2 3x3 kernels and a resizing bilinear layer. Channel attention is learned using two 1x1 convolution layers (first with c/16 kernels, second with c kernels) to exploit inter-channel relationships. Batch normalization and ReLU activation functions follow each convolution layer. The DSH feature map is designed to capture transferable, discriminative cues for the Re-ID task, while the DSP feature map models noisy, domain-specific information (like background) to prevent it from corrupting the learning of the DSH features.": 1049,
    "The paper designs a multimodal image-to-image translation network where the image-translation generators (Gx, Gy) are conditioned on a low-dimensional latent vector (z) drawn from a standard Gaussian distribution N(0,1). For instance, the generator Gx learns a deterministic mapping Gx(x,z) → y, where 'x' is an input image from the source domain X, and 'y' is the translated image in the target domain Y. This latent vector 'z' encapsulates ambiguous aspects of the output mode, allowing the generation of diverse variations of the translated image (e.g., a vehicle at nighttime with different ambient light levels or rear lamp conditions, while maintaining its type, color, and location). The network employs Conditional Instance Normalization (CIN) within the generator architecture to inject this random noise, which is noted to achieve more high-level variations in the generated output compared to simple concatenation. The adversarial loss function, LGAN1(Ex,Gx,Dx,X,Y), guides the encoder (Ex) and generator (Gx) to produce transformed images Gx(Ex(x,z),z) that appear similar to real images from the target domain Y, while the discriminator (Dx) aims to distinguish between translated and real samples. This conditioning on the latent vector 'z' during both the encoding and generation phases enables the stochastic sampling of diverse outputs from a single source image.": 1050,
    "The Unimodal Context Network addresses this by employing separate Long Short-Term Memory (LSTM) networks for each modality (text, visual, acoustic) within the context. For every context sentence `n` of a given modality `m`, the corresponding `LSTMm` processes the sequential input (`Cm,n`) and encodes its information into a single vector `hm,n`. This `hm,n` vector is the last output of the `LSTMm` for that specific context sentence. The recurrence step for each LSTM is based on the utterance of each word, leveraging word-level alignment across modalities. The overall output of this network is a set `H = {hm,n; m ∈ M, 1 ≤ n < NC}`, representing the encoded unimodal context for all modalities and sentences.": 1051,
    "The Proxy A-distance (PAD) is adapted to PAD* by applying the domain discrimination task on the intermediate representations of the task classifier rather than its raw input. The task classifier C is conceptualized as two functions: Gf, which projects the input X to a hidden representation of size m (Rm), and Gy, a linear layer that uses this representation to predict class labels. The domain classifier, denoted G*d, is designed to take the hidden representations Gf(x) as its input, instead of the original input. Initially, the learnable parameters θf and θy of the task classifier are trained by minimizing the task's loss function on the source domain. After this training, θf is frozen, ensuring that the intermediate representation Gf(x) is stable. Subsequently, the parameters θ*d of G*d are learned by minimizing the negative log-likelihood loss for the domain discrimination task, which involves distinguishing between samples from the source domain (Ds) and the target domain (Dt) based on their Gf(x) representations. The PAD* metric is then calculated as 1 minus twice the error rate of this domain classifier, E(G*d(Gf(x))).": 1052,
    "The paper addresses this by experimenting with several multitask learning (MTL) architectures: hard parameter sharing, cross-stitch networks, and gated networks.\nIn the hard parameter sharing setup, the word embeddings and lower Bi-LSTM (Bidirectional Long Short-Term Memory) layers are shared between the two tasks. On top of these shared components, each task has its own separate Bi-LSTM layer, followed by a task-specific output layer. For sentence scoring, the attention function used to construct sentence representations is also learned individually for each task. This allows the model to learn shared feature detectors in the lower layers while enabling top layers to learn task-specific features. A BERT (Bidirectional Encoder Representations from Transformers) variant of hard parameter sharing is also explored, where all of BERT's Transformer layers are shared except for the last layer, and output and attention layers are task-specific.\nThe cross-stitch network employs a soft sharing mechanism where parallel models have dedicated parameters for each task but are connected through cross-stitch units. These units contain alpha-parameters (α-parameters) that regulate information flow in each direction and are optimized during training. Cross-stitch sharing is applied after each recurrent layer, computing updated hidden states (hA, hB) for tasks A and B using the equations: (cid:101)hA = αAAhA + αBAhB and (cid:101)hB = αBBhB + αABhA. The α-parameters are specific to each layer and control the scaling of information transfer between networks. They are initialized with a bias towards favoring information within the same network (e.g., αAA = αBB = 0.9 and αAB = αBA = 0.1) and are optimized during training.\nThe gated network replaces the static α-parameters of the cross-stitch network with dynamic gates that calculate values for each input sentence, allowing for greater flexibility. Each pair of parallel layers has two gates: one modulates information flow from the main to the auxiliary task, and the other controls flow in the opposite direction. For jointly learned sentence-level tasks, two additional gates are placed before the classification layer, operating on sentence representations. The gating mechanisms are detailed by equations involving sigmoid activation functions (σ), weight matrices (WA, WB), and bias vectors (bA, bB), which dynamically determine the combination of hidden states (hA, hB) to produce updated states ((cid:101)hA, (cid:101)hB). The bias parameters of the gates are initialized with a bias towards one task.": 1053,
    "The paper addresses this by incorporating a Domain Classifier into the framework. The MRC module's encoder, parameterized by θe, processes input passage and question pairs, generating a feature vector. This feature vector consists of a passage representation (Mp) and a self-attended question representation (Mq). A self-attention layer is applied to Mp to reduce its size to Mp', which is then concatenated with Mq. This combined feature vector is then fed as input to the Domain Classifier, parameterized by θc. The Domain Classifier uses a two-layer Multi-Layer Perceptron (MLP) followed by a sigmoid function to predict the domain label (source or target) of the input. During the joint training process, the encoder (θe) is optimized to maximize the domain classification loss, effectively attempting to \"\"fool\"\" the Domain Classifier by producing representations that are indistinguishable across domains. Simultaneously, the Domain Classifier (θc) is trained to minimize this loss, aiming to accurately classify the domain. This adversarial interplay forces the encoder to learn feature representations that are robustly domain-invariant, meaning they retain discriminative information for the MRC task while being agnostic to the specific domain.": 1054,
    "To address the challenge of leveraging massive source domain data with unknown style labels for content preservation, the paper proposes the Domain Adaptive Style Transfer with generic Content preservation (DAST-C) model. This model utilizes a shared encoder-decoder framework (E,D) across both the source and target domains. For the source domain, where style labels are unknown, a special \"\"unknown-style\"\" label (lu) is assigned to all source data. The encoder E processes source sentences (x'i) to extract semantic representations (c'i = E(x'i)). The decoder D then reconstructs the source sentences (x'i) conditioned on these semantic representations (c'i) and the unknown-style label (lu). This process is governed by a source domain auto-encoding reconstruction objective (LS_ae = - E x'i~S [log pD(x'i|c'i,lu)]). By jointly training this auto-encoder on massive source domain data alongside the target domain data (which includes its own auto-encoding and style classification objectives), the model is encouraged to learn generic content information. This generic content information, primarily captured through the shared encoder-decoder's exposure to a large volume of source data, enables the model to yield better content preservation when performing style transfer on the target domain, even when target data is limited. The unknown-style label implicitly encourages the model to learn domain-specific features for the source domain, contributing to the overall adaptive learning.": 1055,
    "The paper designs a word sense discriminator, denoted as Dφ, which is an extension of existing word sense disambiguation (WSD) models. This discriminator is built upon a bi-directional Long Short-Term Memory (LSTM) network that processes an input sentence `x` to produce a context vector `c`. This context vector is then fed into a softmax layer with a word-specific parameter `Uw` and bias `b'` to produce a probability distribution over `k + 1` classes. The first `k` classes correspond to the `k` possible real word senses of the target pun word `w` in sentence `x`, meaning Dφ(y = i|x) denotes the probability that the sentence belongs to the `i`-th word sense. The `k + 1`-th class is a special \"\"generated\"\" class, where Dφ(y = k + 1|x) denotes the probability that the sentence `x` was produced by the pun generator. The discriminator is trained to minimize a multi-part objective function J(φ). This objective includes: 1) a negative log-likelihood term for sense-labeled real sentences (Ex,y∼pdata(x,y) logpφ(y|x)), encouraging it to correctly classify real sentences to their specific word sense labels; 2) a negative log-likelihood term for unlabeled real sentences (Ex∼pdata(x) logpφ(y < k + 1|x)), encouraging it to classify these as belonging to any of the real sense classes (not the generated class); and 3) a negative log-likelihood term for generated sentences (Ex∼Gθ logpφ(y = k + 1|x)), encouraging it to classify these as belonging to the \"\"generated\"\" class.": 1056,
    "The paper addresses the modeling of multi-level domain relevance by introducing schemes for both element-level and sample-level relevance.\nFor element-level relevance, the method aims to acquire a domain representation, denoted as `q`, which is a vector in R2dh (where `dh` is the dimension of the Bi-LSTM hidden state). Two methods are proposed for obtaining `q`:\n1.  Domain-q: `q` is a trainable domain-specific vector, shared by every element within a domain.\n2.  Sample-q: `q` is a domain-relevant feature extracted from each sample, shared by every element within that sample. The paper uses a Capsule network (`Capsule`) to capture these domain-relevant features within a sample, where `q = Capsule(h)`, with `h` being the hidden state matrix of a sample.\nOnce `q` is obtained, the similarity between the element representation and the domain representation is calculated using a matrix dot product: `welem_j = q^T B h_j`, where `h_j` is the hidden state of the j-th element, `welem_j` is its relevance weight, and `B` is a trainable matrix.\nFor sample-level relevance, the method first normalizes the element-level relevance weights across the sample length using a softmax function: `ˆwelem_j = exp(welem_j) / sum_k(exp(welem_k))`. Then, a sample representation `r` is obtained by the weighted sum of hidden states: `r = sum_j(ˆwelem_j * h_j)`, where `L` is the sample length. Finally, a Multi-Layer Perceptron (MLP) and softmax function are used for sample classification (two classes: source domain or target domain) to derive the sample relevance weight `wsamp`: `[wsamp, 1 - wsamp] = [softmax(MLP(r))]^T`.": 1057,
    "The paper proposes a novel tensor embedding method to capture contextual patterns of words and measure lexical similarity in short humor texts. For a given corpus of sentences, a vocabulary is first built. For each sentence, a word-word co-occurrence frequency matrix (Ws) is constructed. This matrix Ws(i,j) indicates how frequently word wi and word wj co-occur within a small predefined window H in that specific sentence. All these individual sentence-level matrices (Ws) are then stacked together to form a three-dimensional tensor W of size V x V x D, where V is the vocabulary size and D is the number of sentences. The objective is to approximate this tensor W by decomposing it into a low-rank representation. This decomposition is performed using the CANDECOMP/PARAFAC (CP) tensor decomposition method, which approximates W as a sum of R outer products of vectors (vr ⊗ vr ⊗ dr). The resulting component vectors dr for each sentence form the low-rank embedding C = [d1, d2, ..., dR], where the s-th row of C is the embedding vector for sentence s. This process effectively captures the similarity of contextual patterns between sentences.": 1058,
    "The paper addresses this by proposing a Joint Neural Structural Correspondence Learning (JNSCL) model that jointly optimizes two objectives: the primary supervised task in the source domain and a pivot feature reconstruction task. The model's architecture takes an input `x`, which consists of all extracted features (e.g., unigrams and bigrams). It then generates a shared hidden representation `h(x)` using a ReLU activation function applied to a linear transformation (`h(x) = ReLU(Whx)`), where `Wh` is the weight matrix for the hidden layer. This shared representation `h(x)` subsequently feeds into two distinct output layers: one for the task prediction `ftask(x)` and another for the pivot prediction `fpivot(x)`. Both output layers employ a Sigmoid activation function on a linear transformation of `h(x)` (`ftask(x) = Sigmoid(Wth(x))` and `fpivot(x) = Sigmoid(Wph(x))`), where `Wt` and `Wp` are weight matrices for the task and pivot prediction layers, respectively.\nThe training process utilizes a joint loss function `L(D;θ)`, which is a weighted sum of two Binary Cross-Entropy (BCE) losses: `BCE(ftask(x),y)` for the primary supervised task and `BCE(fpivot(x),pivots(x))` for the pivot feature reconstruction task. The `pivots(x)` function specifically selects the indices corresponding to the pivot features from the input instance `x`. A hyperparameter `λ` controls the weight of the pivot prediction loss, and `ρ` controls a regularization term `R(θ)`. The model is trained by alternating between passing labeled source data and unlabeled data from both source and target domains. For labeled data, both task and pivot prediction losses contribute to the error term. For unlabeled data, only the pivot prediction loss is computed, as task labels are unavailable. Training proceeds for 30 epochs with a mini-batch size of 50 instances, using the Adam optimizer with a learning rate of 0.001. The specific loss function weights used were `ρ = 0.1` and `λ = 100`.": 1059,
    "The paper addresses the first research question by training gradient boosting classifiers on representations of an utterance and its surrounding discourse. These classifiers are trained using features extracted from the text through various document embedding methods, which capture not only the immediate linguistic context but also the broader discourse elements. This training approach allows the classifiers to learn from both the local and global textual features, enhancing their ability to identify metaphors more accurately.": 1060,
    "The paper addresses the automatic extraction of metaphorical pairs using an unsupervised method based on two hypotheses: H1, that a metaphorical word deviates from its normal meaning to represent another concept, and H2, that metaphorical senses occur with relatively lower frequency than literal senses. The process, detailed in Algorithm 1, involves several steps. First, for a given target word (verb) in a sentence, its context is identified. A candidate word set is constructed by including the target word itself, its synonyms, and its direct hypernyms from WordNet (Miller, 1998). This set is augmented with inflections of these words. Word embeddings, obtained using a Continuous Bag-of-Words Model (CBOW) (Mikolov et al., 2013), are used to represent words. The \"\"fit word\"\" is then determined as the word within the candidate set that has the highest cosine similarity with the given context. This similarity is measured using OUT-IN vectors, where OUT vectors are output vectors and IN vectors are input vectors of the trained CBOW model. Finally, the similarity between the target verb and its identified fit word is calculated using IN-IN vectors. If this similarity is less than or equal to a predefined threshold (ε = 0.63), the target word and its fit word are extracted as a metaphorical pair, indicating a semantic violation. This process leverages the dissimilarity between neural embeddings as an indicator of metaphoricity.": 1061,
    "The local-global surprisal principle is operationalized for pun generation through a retrieve-and-edit framework. This framework aims to create the necessary contrast by first establishing a strong association between the alternative word (`wa`) and the local context, and then introducing a strong association between the pun word (`wp`) and the distant (global) context. The process begins by retrieving seed sentences from a large, unhumorous text corpus that contain the alternative word (`wa`). To create local surprisal, the retrieved `wa` is directly replaced with the pun word (`wp`). This step, called RETRIEVE+SWAP, places the pun word in a context where the alternative word would be expected, making the pun word locally unexpected but often grammatically acceptable due to homophony. Subsequently, to establish the global association for the pun word, a topic word related to `wp` is inserted at the beginning of the sentence. This two-step modification ensures that the pun word is surprising in its immediate vicinity but supported by the broader context, thereby instantiating the local-global surprisal principle.": 1062,
    "The paper proposes a novel tagging scheme called BPA, consisting of three tags: B, P, and A. The 'B' tag indicates that the current word appears before the pun in the given context. The 'P' tag highlights that the current word is the pun itself. The 'A' tag indicates that the current word appears after the pun. This scheme is designed to enforce the structural constraint that each context contains a maximum of one pun. In a sentence containing a pun, the specific pun word is assigned the 'P' tag, while all words preceding it are tagged 'B', and all words following it are tagged 'A'. For sentences that do not contain any puns, all words are assigned the 'B' tag. This deterministic procedure ensures that the model's output tag sequence will contain at most one 'P' tag, thereby properly capturing the single-pun constraint.": 1063,
    "The model addresses this by employing two distinct polarity classifiers, F1 and F2, which are trained to provide diverse views on the target domain data. Initially, the feature encoder G, along with F1 and F2, is trained to achieve global marginal alignment between source and target domains. However, this process can lead to ambiguous features near the decision boundaries. To explicitly identify these ambiguous features, the model then fixes the feature encoder G and trains the two classifiers, F1 and F2, to maximize the discrepancy between their probabilistic outputs, p1(y|x) and p2(y|x), on the unlabeled target domain data. This discrepancy, denoted as Ldis, is calculated as the average absolute difference across all K classes: d(p1(y|x),p2(y|x)) = (1/K) * Σ |p1i(y|x) - p2i(y|x)|. By maximizing this discrepancy (minimized as -λ2Ldis in the loss function L2 = Lcls - λ2Ldis), F1 and F2 are forced to disagree on samples that are difficult to classify or lie close to the decision boundary, thereby effectively locating these inconsistent polarity prediction points.": 1064,
    "The paper optimizes word embeddings for Chinese Word Segmentation (CWS) by modifying the Skip-gram model with several innovative techniques. First, a CWS-oriented word embedding model is used, which is a modified Skip-gram model. This model incorporates Context Negative Sampling, where for a target word and its context, any substring from the left or right context that exists in the dictionary but not in the current context is generated as a negative sample. This teaches the model what constitutes incorrect segmentations. Second, In-Word Negative Sampling is introduced: for a multi-character target word, its substrings are combined as negative samples (e.g., for \"\"c1c2c3\"\", (c1,c2), (c1,c3), (c2,c3), (c1c2,c3), (c1,c2c3) are negative samples). This encourages the model not to split multi-character words. Third, a modified Subsampling Multi-Character Words method is applied. Unlike standard subsampling that might discard frequent multi-character words, this method retains multi-character words if their subsampling probability is below a threshold relative to the sum of subsampling probabilities of their frequent substrings. This ensures the model pays more attention to segmenting such words. Finally, Dot Product Normalization is applied by using the magnitude of the dot product of word embeddings as input to the sigmoid layer in the Skip-gram objective function, making it consistent with the cosine similarity metric used in the subsequent segmentation step. Additionally, Smoothing Class Weights are used to balance the influence of positive and negative samples during training.": 1065,
    "The paper addresses this by extending the neural network baseline with a feature function that incorporates manually-defined linguistic features. These features, similar to those used in non-neural Conditional Random Field (CRF) models, include information about hashtags, @-mentions, and orthographic characteristics (e.g., whether a word starts with a digit or an uppercase letter). For each feature type, a dedicated embedding layer is set up within the neural network. This allows the model to learn vector representations for different expressions of these features. All the feature embeddings are then summed together to form a single feature embedding vector. This combined feature embedding vector is subsequently concatenated with other input representations, such as the character-level encoder output, FastText vectors, and Word2Vec vectors, before being fed into the bidirectional Long Short-Term Memory (LSTM) layers of the neural network. This approach combines the advantages of classical feature engineering with neural networks, providing explicit inductive bias to guide the learning task, especially beneficial for smaller datasets.": 1066,
    "To automatically adapt a general-domain sentiment dictionary, specifically H4N, for predicting financial outcomes, the method involves a two-stage process: domain-specific word embedding learning and subsequent word reclassification using a Support Vector Machine (SVM). First, Continuous Bag-of-Words (CBOW) word2vec embeddings are trained on a large corpus of 10-K financial reports. This training process generates dense vector representations for words, capturing their contextual meanings within the financial domain. Only words from the original H4N dictionary that appear in the 10-K corpus are considered for embedding, and words with frequencies below the word2vec threshold are excluded. Second, an SVM is trained using the labels from the original H4N dictionary, specifically to classify words as either negative or non-negative. Each word is represented by its newly learned financial-domain embedding. The SVM's raw scores are then converted into probabilities through logistic regression. A confidence threshold, denoted as θ (set to 0.8 in experiments), is applied; only words whose converted SVM scores exceed this threshold are included in the resulting automatically adapted dictionary, referred to as H4NRE.": 1067,
    "The paper addresses this by implementing the literal meaning function, denoted as Lθ, using a Long Short-Term Memory (LSTM) neural network. This LSTM takes an input utterance and an object (either a color or a color-grid) and produces an output value in the interval (0,1), representing the degree to which the utterance applies to the object. The object representation is crucial: for colors, it's a single continuous vector mapped into the LSTM's initial hidden state via a dense linear layer. For color-grids, an average-pooled, convolutional layer processes the grid representation (with weights shared across grid-cell representations) to produce a continuous vector, which then initializes the LSTM's hidden state. After initialization, the LSTM processes embeddings of the utterance's tokens sequentially. The final hidden state of the LSTM is then passed through an affine layer and squashed by a sigmoid activation function to yield the final Lθ output. This entire neural network structure contains all the learnable semantic parameters (θ) of the listener models.": 1068,
    "The ChID dataset is constructed by replacing idioms in passages with blank symbols, and for each blank, a list of candidate idioms is provided. To systematically control the difficulty level, the candidate choices are designed in a specific manner. First, the golden answer (the correct idiom) is included. Then, three \"\"similar\"\" idioms are chosen from the top 10 idioms whose embedding similarity score to the golden answer is between 0.65 and 0.80, indicating a high probability of being near-synonyms. Idioms with similarity scores higher than 0.7 to the golden answer are excluded to prevent including direct synonyms. Finally, three \"\"random\"\" idioms are sampled from the remaining idioms that are not among the top 10 similar idioms. This results in a list of seven candidate choices per blank: one golden answer, three similar idioms, and three random idioms, as illustrated in Figure 1. This design allows for the creation of test sets with varying difficulty, such as \"\"Sim\"\" (where all candidates are from top 10 similar idioms) and \"\"Ran\"\" (where all candidates are not similar to the golden answer), enabling analysis of how candidate choice design impacts performance.": 1069,
    "The paper addresses this by designing a Selection Distribution Generator (SDG), which is a multi-layer perceptron (MLP) model. The SDG learns the selection policy by being optimized through reinforcement learning (RL). At each step, the SDG receives a collection of representations (ΦBj) for a batch of source data, referred to as a \"\"data bag\"\" (Bj), from the feature extractor. It then maps these representations into a distribution vector (DBj), where each element (pj l) represents the probability of selecting the corresponding instance. Data instances are selected from the data bag according to these probabilities. The SDG is updated based on a reward signal. This reward (r(s,a,s')) is computed by assessing the distance between the representations of the selected source data (ΦˆBj) and the guidance set (Φt) from the target domain, comparing the current state (s') with the previous state (s). Specifically, the reward is calculated as d(ΦsˆBj−1, Φst) - γd(Φs'ˆBj, Φs't), where d(·,·) is a distribution discrepancy measurement (e.g., Jensen-Shannon divergence, Maximum Mean Discrepancy, R´enyi divergence, or LOSS) and γ is a discounting constant. A higher reward indicates that the selected data better fits the target domain's distribution. The SDG's parameters (W) are updated using a policy gradient method, optimizing an objective function J(W) that maximizes the expected cumulative reward over selection steps. This RL-driven process allows the SDG to automatically determine which instances to select without requiring a predefined threshold for the number of instances.": 1070,
    "The paper addresses this by first formulating initial knowledge graphs (KGs) as a list of tuples (head entity, relation, tail entity), where each entity is associated with its one-hop connected neighbors and a context description sentence. Entity and relation vector representations are randomly initialized. A Graph Structure Encoder captures the importance of each neighbor's feature to an entity by performing self-attention, computing a weight distribution over neighbors using a LeakyReLU activation function on concatenated linear transformations of entity and neighbor representations. This results in a structure-based context representation, further refined by multi-head attention to capture various relation types. Concurrently, a Contextual Text Encoder processes the entity's context sentence using a bi-directional Long Short-Term Memory (LSTM) network to obtain hidden states, followed by a bilinear attention mechanism to derive a local context-based representation. These two representations (graph-based and text-based) are then combined using a Gated Combination function, which employs an entity-dependent gate to balance their contributions. The model is trained using a marginal loss function based on the TransE model, which assumes relations are translations between entity representations. Negative tuples are generated by corrupting positive tuples. After training, the system computes a score for indirectly connected entities and relation types to indicate the probability of a link holding, thereby enriching the knowledge graph with predicted links.": 1071,
    "The paper addresses the challenge of explicitly controlling rhetorical modes by introducing a Manual Control CVAE (MCCVAE) model. This model extends the traditional Sequence-to-Sequence (Seq2Seq) framework by incorporating a Conditional Variational Autoencoder (CVAE). The CVAE is trained to maximize a variational lower bound on the conditional likelihood of the target sentence (Y) given a conditional variable (c). In the MCCVAE, this conditional variable `c` is composed of two parts: `[hX; e(r)]`, where `hX` is the encoding of the current poem sentences (X) obtained from an encoder LSTM, and `e(r)` is the embedding of the user-provided rhetorical variable `r` (e.g., metaphor, personification, or other). The model includes a prior network `pP(z|c)` to approximate the true prior distribution of the latent variable `z` (which encodes semantics and rhetoric), a recognition network `qR(z|Y,c)` to approximate the true posterior `p(z|Y,c)`, and a decoder `pD(Y|z,c)` to approximate the generation probability. Both the prior and recognition networks are modeled as multivariate Gaussian distributions, with their means and log variances estimated via multilayer perceptrons (MLP) conditioned on `c` (for prior) or `LSTM(Y)` and `c` (for recognition). The training objective involves minimizing the variational lower bound, which includes a KL divergence term between the recognition and prior networks and a cross-entropy term for the decoder's generation probability.": 1072,
    "The paper proposes the use of \"\"artiﬁcial titles\"\" to capture the grammatical style and vocabulary of the unlabeled target domain. Since titles are typically short, target texts between 4 and 10 words in length are selected to synthesize these artificial titles. A common baseline for summaries, such as the first few sentences of an article, is leveraged. Specifically, the first text segment meeting the length requirement is selected 90% of the time, and the second such segment is selected otherwise. This strategy of sometimes using the second text segment is crucial as it discourages the model from simply learning to copy the first sentence, thereby forcing both the encoder and decoder to learn from the target domain text more broadly. This process enables the use of an expanded, joint vocabulary trained on both source and target domains, allowing the model to learn the target domain's style and vocabulary. For Stack Exchange, the text is a sentence from a post, while for news articles, where titles are often phrases, a clause is selected.": 1073,
    "The paper addresses this question with the RNN HG (Recurrent Neural Network Hidden-GloVe) model, which is built upon the Metaphor Identification Procedure (MIP). This model classifies a word as metaphoric or literal by comparing its contextual and literal meanings. To represent the contextual meaning, the model uses BiLSTM (Bidirectional Long Short-Term Memory) hidden states, which encode a word based on its forward and backward contexts. For the literal meaning representation, the model employs pre-trained GloVe (Global Vectors for Word Representation) embeddings, assuming that these embeddings primarily capture the most common, and thus likely literal, senses of words. The input features for the BiLSTM are a concatenation of GloVe and ELMo (Embeddings from Language Models) representations. The core mechanism for comparison involves concatenating the BiLSTM hidden state (h) for a target word at position 't' with its corresponding GloVe embedding (g). This concatenated representation, [ht; gt], is then fed into a softmax function with learned weights (w) and bias (b) to predict the probability of a label (p(ˆyt|ht,gt) = σ(wᵀ[ht;gt] + b)). This concatenation highlights the contrast between the two meaning representations, allowing the classifier to learn whether they represent similar (literal) or different (metaphoric) meanings.": 1074,
    "The paper addresses this by introducing a \"\"dream phase\"\" (Algorithm 3) where the active learning (AL) policy is improved using simulations based on the target problem's data. Crucially, instead of querying a human annotator for policy improvement, the *current student learner* (mφ) acts as an imperfect annotator. In this phase, the student learner first generates pseudo-labels for the unlabelled data pool (Dunl) to create a pseudo-labelled dataset (Dpool). Then, AL tasks are synthesized by randomly partitioning the collected pseudo-labelled data into training (Dtrn) and evaluation (Devl) sets. An imperfect algorithmic expert is used to generate simulated AL trajectories (M) by running the querying policy for a fixed number of time steps (Td). For each time step, the expert either selects a query based on the policy's recommendation or selects the best query by a one-step roll-out, using the imperfect pseudo-labels. This roll-out involves retraining the underlying model with the pseudo-labelled candidate and calculating the loss on the evaluation set. The policy network (π) is then re-trained using a behavioural cloning algorithm, specifically DAGGER (Dataset Aggregation), on the collected state-action transitions (M) from these simulated trajectories. The objective is to maximize the probability of the imperfect expert's actions. An experience replay memory (Mnih et al., 2015) is also used to store historic state-action transitions, from which mini-batches are randomly sampled to retrain the policy network, ensuring sample efficiency and stability.": 1075,
    "The paper enriches a slot's semantic representation by combining its textual description with a small set of example values. First, the slot description tokens are mean-pooled to obtain a single encoding, `ds`. Second, for each of the K example values provided for a slot, their tokens are also mean-pooled to get individual example encodings, `ex_k`. To integrate these example encodings with the user utterance, an attention mechanism is applied. For each utterance token `h_i`, it serves as an attention context to compute attention weights `αx_ik` over all K slot examples using general cosine similarity. These attention weights are then used to create an attention-weighted encoding `ea_i` of all K slot examples for each utterance token. This `ea_i` dynamically captures the relevance of the example values to the specific part of the utterance. The final input to the tagger for each utterance token `h_i` is a concatenation of the utterance token encoding `h_i`, the slot description encoding `ds`, and the attention-weighted slot example encoding `ea_i`.": 1076,
    "The paper formulates idiom recommendation as a context-to-idiom machine translation problem by employing an encoder-decoder framework. This approach conceptualizes idioms as being written in a \"\"pseudo target language\"\" due to their ancient Chinese origins and limited vocabularies. The process unfolds in three main stages: First, an attention-based neural network is utilized to encode the input context sequence, which serves as the source language. Second, the resulting encoded context attention vector is decoded into an intermediate sequence, representing the idiom characters in the pseudo target language. Third, the final recommended idioms are selected by mapping this intermediate character sequence to a standard set of idioms.": 1077,
    "A Gradient Reversal Layer (GRL) is integrated into the adversarial domain adaptation framework to learn domain-invariant feature representations. The GRL is placed within a shared domain discriminator, specifically after the Bi-LSTM layer, to ensure that subsequent layers are trained on domain-invariant features. In the forward pass, the GRL acts as an identity transform, allowing activations to pass through unchanged. However, during the backward pass, the GRL multiplies the incoming gradient by a negative factor, -λ, effectively reversing its direction. This reversal creates a minimax game dynamic: the domain classification layer attempts to minimize the domain classification loss (becoming better at distinguishing domains), while the feature extraction layers (those preceding the GRL) act as adversaries. By having their gradients reversed, these feature extraction layers are trained to maximize the domain classification loss, making it harder for the domain discriminator to classify the domain. This adversarial training forces the feature extraction layers to generate shared feature representations that are nearly indistinguishable by the domain classification layer, thereby achieving domain invariance. The shared embedding and Bi-LSTM layers are specifically trained using this mechanism to generate these domain-invariant features.": 1078,
    "The paper applies Gaussian Process Preference Learning (GPPL), a Bayesian approach, to quantify humorousness and metaphor novelty. GPPL is based on a random utility model, specifically the Thurstone-Mosteller model, which assumes that when presented with a pair of texts, an annotator chooses one with a probability that is a function of the underlying utility (e.g., humorousness or novelty) of each text. This allows the model to infer latent utilities (scores) for each instance from pairwise comparisons. Relationships between instances are modeled by a Gaussian Process (GP), which computes the covariance between instance utilities as a function of their features. This means that texts with similar features are expected to have similar utilities. The Bayesian nature of GPPL allows it to cope better with sparse and noisy data, including disagreements between multiple annotators, by treating such inconsistencies as noise rather than errors, as no label is assigned a probability of one of being selected. For scalability with large datasets, the GPPL implementation uses stochastic variational inference, which limits computational complexity by substituting instances for a fixed number of inducing points during inference, making it feasible for arbitrarily large numbers of instances and pairs.": 1079,
    "For the second research question, the method introduces an Inter-Branch Fusion module that applies branch attention to fuse features between branches. After extracting features from each view, the method shifts the views to enlarge the receptive field and constructs cost volumes. The branch attention mechanism assigns higher weights to branches with less occlusions and clearer correspondences, ensuring that the most informative features contribute more to the final cost volume. This is achieved through multiple 3D convolutional layers and a sigmoid layer to generate attention maps, which are then multiplied with the cost volumes for effective feature integration.": 1080,
    "To leverage identity-wise similarities and ensure visually similar individuals are correctly associated across domains, the paper proposes an Identity-wise Similarity Enhancement (LSE) module. This module maintains an \"\"ID pool\"\" that accumulates the learned knowledge of all identities. Specifically, a running mean representation is computed for each identity (ID) in an iterative fashion, and these mean representations are further accumulated over training epochs to form a final effective representation for each ID. For each incoming image instance, the system searches for its top-k visually similar IDs within the ID pool. If the instance is from a peripheral domain, the search is conducted among IDs from *other* domains. If the instance is from the central domain, the search is conducted among IDs from the *same* domain to stabilize its distribution. To enforce similarity, the features of the incoming instance and the features of its top-k similar IDs from the ID pool are normalized using a softmax function with a temperature parameter (τ). The symmetric KL-divergence between these normalized feature distributions is then minimized (LSE loss). This process effectively reduces identity-wise domain-shifts by making visually similar samples, even from different domains, closer in the feature space than less similar ones, thereby forcing the network to learn more discriminative and domain-invariant features that align with real-world scenarios.": 1081,
    "To learn discriminative features for target domain retrieval without direct target text annotations, the paper proposes a novel mutually-exclusive pseudo-text selection mechanism. This mechanism refines the joint video-text embedding space, which is initially trained using a Source Ranking Loss (LS) on source domain paired data. The core idea is to assign \"\"pseudo text\"\" (pT) to unlabeled target videos (vT) from a pool of unbiased source text embeddings (g_text(tS)). A naive selection (picking the text embedding with the highest similarity score) is problematic because a single pseudo-text might become the nearest neighbor for many target videos, leading to the \"\"hubness problem.\"\" To mitigate this, the mutually-exclusive selection algorithm operates on a collection of source text embeddings (NQ) and a minibatch of target video features (NB). It first computes a similarity matrix (S) between target video features and source text embeddings. To enhance discriminability and enforce mutual exclusivity, it applies the softmax function along the text dimension of S to get S_text (amplifying text discriminative capability) and along the video dimension of S to get S_video (amplifying video discriminability). These two directional softmax outputs are then combined (S' = S_text * S_video) to refine the similarity matrix. The selection assignment for each target video (g_vid(vT_j)) then follows from this refined similarity matrix S' by choosing the unbiased source text embedding that yields the highest similarity score. This process ensures that the selected pseudo-text for one target video is less likely to be the \"\"best\"\" match for other different target videos, thereby diversifying pseudo-label assignments and robustly picking distinct source text descriptions for target visual queries. Finally, the joint space is refined by minimizing a second ranking loss (LT) between the target video embedding (g_vid(F(vT))) and its selected pseudo query embedding (pT), similar in form to the source ranking loss.": 1082,
    "The paper addresses the challenge of monitoring and improving reconstruction reliability by developing a reconstruction erroneousness modeling mechanism based on estimation-reconstruction consistency. This mechanism posits that if both the estimated density map (Di) and the reconstructed density map (Di^0) for a given frame Ii are accurate, they should be consistent (i.e., their difference should be negligible). Conversely, significant differences indicate erroneousness. To quantify this, a reconstruction erroneousness matrix (Ei) is introduced, where larger values of Ei(u,v) correspond to greater inconsistency between Di(u,v) and Di^0(u,v). This erroneousness is then integrated into the error-aware density isomorphism reconstruction objective (Liso). Liso minimizes the L2-norm difference between Di and Di^0, but crucially, it divides this difference by the erroneousness matrices (Ei-d and Ei+d), effectively suppressing the impact of unreliable density reconstructions during training. To prevent trivial solutions where erroneousness matrices become extraordinarily large, a regularization term (Lmod) is introduced. Lmod is defined as the logarithmic sum of the erroneousness matrices, which constrains their values. The overall objective function (L) for the EDIREC-Net is then formulated as the sum of Liso and Lmod, jointly encouraging accurate reconstruction while modeling and mitigating the influence of unreliable regions.": 1083,
    "To ensure meaningful and distinct sub-distributions, the paper proposes two loss functions for the Cross-Domain Grouping Network (C): Semantic Consistency Loss (Lco) and Orthogonality Loss (Lorth).\nThe Semantic Consistency Loss (Lco) encourages the category distribution of each group to be consistent between the source and target domains. For each group k, the class distribution Qk_l is estimated by applying an average pooling layer on the group-specific feature Fk_l. The loss minimizes the L2-norm difference between the source group's class distribution (Qk_S) and the target group's class distribution (Qk_T) across all K groups. This provides supervisory signals for aligning group-level features.\nThe Orthogonality Loss (Lorth) ensures that the class distributions of different groups within the same domain are distinct and non-overlapping. It achieves this by minimizing the cosine similarity between any two distinct group class distributions (Qj1_l and Qj2_l) within the same domain l. Since Qk_l values are non-negative, minimizing cosine similarity effectively pushes them towards orthogonality, allowing the grouping module to divide a multi-modal complex distribution into K simple, differentiated class distributions.": 1084,
    "The residual weighted self-training pipeline assigns different weights to pseudo-boxes based on their representation residuals. This ensures that higher quality pseudo-boxes contribute more to the training loss, reducing the impact of low-quality labels. The process involves calculating the representation residual for each candidate box, determining the training weights using a predefined function, and applying these weights during the self-training stages. This mechanism alleviates the effect of noisy pseudo-labels and trains a more robust cross-domain detector.": 1085,
    "The paper addresses this by generating a labeled pseudo-dataset, denoted as DP = (ZP, YP), from the estimated prototypical distribution. This process involves drawing random samples (z_p_i) from the estimated Gaussian Mixture Model (GMM), which serves as the surrogate for the source domain's abstract knowledge. For each drawn sample z_p_i, a corresponding pseudo-label y_p_i is assigned. This pseudo-labeling is performed using the pre-trained classifier subnetwork (h_hat_w0) of the model. To ensure the quality and reliability of the generated pseudo-dataset, a confidence threshold (delta) is applied: only samples for which the classifier's prediction confidence (h_hat_w0(z_p_i)) exceeds this threshold are included in the final pseudo-dataset. This selective inclusion helps to filter out potentially misclassified or ambiguous samples, thereby improving the accuracy of the pseudo-labels and the overall quality of the synthetic data used for adaptation. The resulting pseudo-dataset effectively provides \"\"labeled\"\" data that mimics the structure of the source domain, enabling supervised-like training steps for model adaptation without requiring access to the original source images or their annotations.": 1086,
    "Catastrophic forgetting on old target domains is addressed through the Target Memorization Constraint (TMC). This constraint ensures that the classification loss for each domain-episodic memory (Mi) does not increase when adapting to a new target domain Dt. Formally, Lce(θt, Mi) <= Lce(θt-1, Mi) for all i < t. To make this computationally efficient, especially as the number of target domains increases, an approximation is used: Lce(θt, M1:t-1) <= Lce(θt-1, M1:t-1), where M1:t-1 represents the aggregated domain-episodic memories from all previous target domains. Similar to the SDC, this is translated into an inner product constraint: hw, gdmi >= 0, where gdm is the gradient of the classification loss on the aggregated old target domain memories (∂Lce(θt, M1:t-1)/∂θt). The pseudo-labels required for computing the classification loss on old target domains are generated by clustering, and only high-confidence samples are retained. This constraint is then incorporated into the overall quadratic programming problem that calculates the model's parameter update vector.": 1087,
    "The paper proposes a composite unsupervised adaptor loss (LAdap) to optimize the domain adaptor using only unlabeled target domain data. This loss function consists of three main components: a reconstruction loss, an entropy minimization loss, and an orthogonality regularization loss. Firstly, the reconstruction loss, denoted as LAE(target), minimizes the reconstruction error of the target domain features using the autoencoder (R) that was trained on source domain data. This encourages the feature distribution of the target domain, after adaptation by the adaptor, to be similar to that of the source domain, ensuring the discriminative classifier (trained on source data) remains effective. Secondly, an entropy minimization loss, LEnt(target), is applied to the predicted probabilities on the target domain. This component encourages the model to produce more confident predictions on the unlabeled target data, thereby leveraging the discriminative information present in the target domain itself. Thirdly, an orthogonality regularization loss, LOrth(target), is imposed on the adaptor's parameters using the Spectral Restricted Isometry Property Regularization. This term prevents feature mode collapse, ensuring that the adaptor learns diverse and meaningful transformations without collapsing the feature space. The final unsupervised adaptor loss (LAdap) is a weighted sum of these three components, balancing their contributions to guide the unsupervised optimization of the adaptor.": 1088,
    "The paper introduces and utilizes the \"\"attention level\"\" as the learning label, which is a measure between 0 and 1 estimating the likelihood of numerical errors during an LP or MIP solve. This approach explicitly avoids using solution time as a label, recognizing that faster termination can sometimes be a misleading indicator if it results from numerical errors leading to incorrect solutions. The attention level is computed by sampling the condition numbers (`kappa`) of basis matrices (`kappa(ABi)`) at each search node where an optimal LP relaxation is found. These condition numbers are then categorized into four buckets: \"\"stable\"\" (`kappa(ABi) < 10^7`), \"\"suspicious\"\" (`kappa(ABi) < 10^10`), \"\"unstable\"\" (`kappa(ABi) < 10^14`), and \"\"ill-posed\"\" (`kappa(ABi) >= 10^14`). The relative frequencies of these categories are denoted as `p_stab`, `p_susp`, `p_unst`, and `p_ill`, respectively, such that their sum is 1. The attention level (`alpha`) is then defined as a weighted sum of the critical categories: `alpha = 0.01 * p_susp + 0.3 * p_unst + p_ill`. This formulation ensures that higher attention levels correspond to a greater likelihood of numerical issues, with ill-posed bases contributing most significantly. The learning label for the regression model is then defined as the difference in the square roots of the attention levels obtained with Curtis-Reid scaling (`alpha_cr`) and Standard scaling (`alpha_st`), specifically `delta = sqrt(alpha_cr) - sqrt(alpha_st)`. The square root transformation is applied to artificially inflate notable differences in attention level that are close to zero, as empirical observations showed most values were near zero.": 1089,
    "To exploit correlations and complementarities among multiple modalities within a heterogeneous target domain, the paper proposes a Condition Attention Module (CAM). After obtaining the stylized images from the Condition-Guided Style Transfer (CGST) module, denoted as ^XS = GST(XS;c), and using real target images XT, both are fed into a shared encoder (Enc). The encoder output is then processed by two individual decoders, DecC1 and DecC2, which capture condition-specific features fC1(x) and fC2(x) and probability predictions pC1(x) and pC2(x). Instead of simply using separate heads for each condition, CAM learns attention weights W from the concatenated condition-specific feature maps (fC1(x) and fC2(x)) using a convolutional block. These attention weights are then used to combine fC1(x) and fC2(x) into an enhanced feature 'f' (f = WC1 ⊙ fC1(x) + WC2 ⊙ fC2(x)). Finally, fC1(x), fC2(x), and 'f' are concatenated and fed into an additional parsing head (CA) to produce the final enhanced semantic feature map fCA(x) and probability predictions pCA(x). This mechanism allows the model to dynamically weigh and combine information from different condition-specific pathways, effectively leveraging their complementarities.": 1090,
    "The paper addresses the discovery of sparse associative structures among variables, while considering time lags, through a Sparse Associative Structure Reconstruction via Inter-variables Attention Mechanism. After obtaining the weighted segment representations `Z_i` for each variable `i` (as described in Solution 1), the goal is to model the association between variable `i` and variable `j`. Unlike a simple dot product that ignores time lags, the method calculates the degree of association between variable `i` and variable `j` by considering the segment representations `h_j_delta` of variable `j` at different time lags `delta`.\nSpecifically, the degree of association `e_ij_delta` between variable `i` and variable `j` at a specific time lag `delta` is computed as the normalized dot product between the weighted segment representation of variable `i` (`Z_i`) and the segment representation of variable `j` at that lag (`h_j_delta`). This is formulated as `e_ij_delta = (Z_i * h_j_delta) / (||Z_i|| * ||h_j_delta||)`. This calculation is performed for all relevant time lags `delta`. These degrees of association `e_ij_delta` are then normalized across all variables `j` and time lags `delta` using `sparsemax` to obtain the associative strength `beta_ij_delta`. The `beta_ij_delta` represents the associative strength between variable `i` and variable `j` with regard to a specific segment duration `delta`. This mechanism allows the model to capture the influence of one variable on another at different temporal offsets, thereby explicitly accounting for time lags in the discovered associative structure.": 1091,
    "A reinforced selector is adopted to provide an assessment, `phi'(z)` (a value between 0 and 1), for each augmented sample `z`, indicating its helpfulness for knowledge distillation. This selector is a reinforcement learning policy network that learns to select useful augmented data. The state of the model for the selector is defined by the outputs of both the teacher and student models. The action is a binary decision (0 or 1) on each input sample, effectively deciding whether to use the augmented sample. The reward `rt` at time step `t` is defined as the difference in performance before and after the student model updates: `rt = L(yi;fs(xi)) - L0(yi;fs(xi))`, where `L` is the evaluation metric (e.g., accuracy for classification, correlation coefficient for regression) of the updated model, and `L0` is the previous evaluation result. To improve training efficiency, an epoch is treated as an episode, and each batch as a step. The accumulated reward `r(sigma)` for a sequence of actions `sigma` is calculated as a discounted sum of future rewards: `r(sigma) = sum_{k=0}^{T-t} (gamma^k * rt+k)`, where `gamma` is a discount factor. The selector's policy `phi'` is then optimized to maximize this expected discounted sum of rewards over trajectories, `phi* = argmax_phi' E_{sigma~phi'(sigma)}[r(sigma)]`, effectively learning to select augmented data that leads to performance improvements in the student model. This feedback mechanism allows the augmentation strategy to be dynamically refined.": 1092,
    "An Ancillary Classifier Module (ACM) is introduced to focus on pixel-level details by generating attention-aware results. ACM adaptively assigns different weights to pixels based on their domain offsets, thereby reducing local domain gaps. This module uses an attention mechanism to identify and emphasize regions with significant domain shifts, ensuring that the model can capture fine-grained details crucial for accurate segmentation.": 1093,
    "To mitigate undesirable disagreements and instability among multiple target hypotheses during unsupervised adaptation, the paper introduces a Hypothesis Disparity (HD) regularization. This regularization term, `R(H)`, is integrated into the mutual information (MI) maximization objective function. The HD is formally defined as a dissimilarity measure, `d(hi(x);hj(x))`, between the predicted label probability distributions of any two distinct hypotheses, `hi` and `hj`, over the input space `X`. The paper specifically employs cross-entropy as the divergence measure for `d()`. The overall objective function for the proposed approach, named Hypothesis Disparity regularized Mutual Information maximization (HDMI), is formulated as maximizing the expectation of `(-I(XT;h(XT)))` plus a penalty term `λ * E[HD(hi;hj)]`, where `λ` is a weighting hyperparameter and the expectation is computed over pairs of target hypotheses `hi` and `hj`. For practical computational efficiency, the HD regularization is calculated by selecting an \"\"anchor hypothesis\"\" (randomly chosen from the target hypotheses) and computing the average disparity between this anchor and the remaining `M-1` hypotheses. By minimizing this HD, the regularization effectively compels the target hypotheses to maximally agree with each other, leading to a more stable optimization process and facilitating their coordinated learning of better representations through the shared feature extractor. This mechanism ensures that the uncertainty expressed through different source hypotheses is considered, while undesirable disagreements are marginalized out.": 1094,
    "Knowledge is distilled from a deeper neural network to a shallower target model by training a deeper network, denoted as `φ_T`, on the meta data `M` using a supervised task, specifically a classification loss (e.g., cross-entropy loss). This deeper network serves as a teacher model. A latent feature representation `Tm_i` for a sample `xm_i` is obtained from the feature extraction module of this deeper network `φ_T`. To transfer this knowledge to the shallower target network, a new distilled loss, `L_m`, is proposed (Equation 4). This loss minimizes the distance between the latent representations `~Z_i` and `~Z_j` (where `~Z_i` is the i-th column in `~Z`, representing the latent representation of sample `xm_i` from the shallower network) if their corresponding feature vectors `Tm_i` and `Tm_j` from the deeper network are close. The weight `Wm_ij` (Equation 5) quantifies this relationship, being an exponential function of the negative squared Euclidean distance between `Tm_i` and `Tm_j`, scaled by a hyperparameter `t`. A smaller distance between `Tm_i` and `Tm_j` results in a larger `Wm_ij`, incurring a heavier penalty if `~Z_i` and `~Z_j` are far apart. This distilled loss `L_m` is then integrated into the meta-learning objective function (Equation 6), allowing for simultaneous knowledge transfer from both task-level and model-level aspects.": 1095,
    "The Bi-Classiﬁer Determinacy Maximization (BCDM) algorithm employs a three-step adversarial training strategy to effectively utilize the Classiﬁer Determinacy Disparity (CDD) metric.\n1.  Source Supervision: In the initial step, the entire network, which includes a feature generator (G) and two distinct classifiers (C1 and C2), is trained by minimizing the standard supervised cross-entropy loss on the labeled source domain data. This ensures that the model learns to correctly classify source samples and establishes a baseline for feature discriminability.\n2.  Maximizing CDD for Classifiers: Subsequently, to encourage prediction diversity and identify target samples that are ambiguous or near the decision boundary, the two classifiers (C1 and C2) are trained to *maximize* the CDD loss on the unlabeled target domain data. This objective forces the classifiers to produce inconsistent predictions across categories for target samples, thereby exploring a wider range of output possibilities and highlighting areas of uncertainty.\n3.  Minimizing CDD for Generator: Finally, to generate highly discriminative features and enforce classifier determinacy on the target domain, the feature generator (G) is trained to *minimize* the CDD loss on the target data, while the parameters of the classifiers (C1 and C2) are kept fixed. Since the CDD metric is designed to be minimal only when predictions are consistent and fully determined, this step compels the generator to produce representations that lead to confident and unambiguous classifications for target samples, effectively pushing them away from the decision boundary and enhancing their discriminability.\nThis adversarial min-max game, where classifiers maximize CDD to promote diversity and the generator minimizes CDD to enforce determinacy, allows BCDM to achieve both objectives simultaneously, leading to improved generalization on the target domain.": 1096,
    "To alleviate the effects of false negatives, which are prevalent in the generated noisy pseudo-labels (especially for small and obscured objects), the paper introduces a strategy called False Negatives Simulation. Instead of attempting to mine these difficult-to-detect false negatives, the method simulates them through data augmentation. Specifically, Mosaic augmentation is employed. Mosaic combines four training images into a single image, applying random scaling and random cutting. This process effectively generates new training samples that mimic small-scale and blocked objects by transforming existing true positives. By training the model on these synthetically generated hard examples, the framework improves its robustness and ability to detect instances that would otherwise be missed, thereby suppressing the negative impact of actual false negatives.": 1097,
    "To apply pseudo-labels effectively and mitigate negative transfer in the Universal Domain Adaptation (UniDA) setting, the paper introduces a Sample Selective Pseudo-labeling approach. This method uses pseudo-labels only on high-confidence target samples that are likely to be part of the shared label set Y. The confidence measure for this selection is the previously defined sample transfer score `s(x)`. Pseudo-labels are generated and used for training only if `s(x)` for a given target sample exceeds a certain dynamic threshold, `s_tilde(t)`. This dynamic threshold adjusts during training according to the formula: `s_tilde(t) = (s0 / 2) + (s0 / 2) * (t / T)`, where `t` is the current training step, `T` is the total number of training steps, and `s0` is a fixed hyperparameter (set to 1.0 in experiments). This dynamic adjustment starts the threshold at a lower value (midpoint between 0 and `s0`) and gradually increases it towards `s0` as training progresses. This strategy aims to avoid negative transfer by initially being more permissive with pseudo-labels when the network is less certain, and becoming stricter as the network's classification of shared vs. private samples improves. The pseudo-label classification loss, `LC`, includes a term `1_{s(x)>s_tilde(t)} * LCE(argmax y(x); y(x))`, where `1` is the 0-1 indicator function, `LCE` is the cross-entropy loss, and `argmax y(x)` is the pseudo-label.": 1098,
    "The paper employs a systematic experimental methodology to analyze the scalability limits of Transformers in compositional tasks, specifically under conditions of fixed computational cost (fixed model size and fixed training steps). This methodology involves three main types of controlled experiments:\n1.  Experiments on Effect of Rule Scope (Section 4): This investigates how increasing the scope of natural language (by adding more rules) affects error rates. Transformers are evaluated on random splits of datasets from the *-CFQ rule set lattice (e.g., B-CFQ, L-CFQ, N-CFQ, X-CFQ). The model size and number of training steps are held constant across experiments. Error rates are plotted against training set size on a double log scale to observe scalability curves. This setup allows for analyzing how different types of rule additions (leaf vs. non-leaf) impact performance and whether increased training data can offset higher error rates due to broader scope.\n2.  Experiments on Effect of Compound Divergence (Section 5): This examines how the need for compositional generalization affects error rates across varying training sizes. The U-CFQ dataset is used with Maximum Compound Divergence (MCD) splits, which maximize compound divergence between train and test sets while ensuring small atom divergence. Experiments are conducted across a wide range of training sizes (from 10k to nearly 900k examples) and different compound divergence levels (e.g., 0, 0.2, 0.4, 0.6, 0.7). This setup allows for observing the relationship between training size and error rates in settings requiring compositional generalization, and how this differs from i.i.d. (compound divergence 0) settings.\n3.  Experiments on Effect of Data from Related Domain (Section 6): This investigates the degree to which blending supplementary training data from a related domain improves or harms performance on a fixed target domain test set. A fixed test set, WTEST, is randomly sampled from a limited-scope target domain (B-CFQ). The training set consists of a limited set of in-domain examples, WTRAIN (nin examples), and a potentially larger supplementary set, VTRAIN (nsup examples), sampled from a related domain (e.g., half-L-CFQ, L-CFQ, half-X-CFQ, X-CFQ, with B-CFQ generatable examples filtered out). Two blending strategies are explored:\n    *   Equal Weighting: WTRAIN and VTRAIN examples are weighted equally, allowing observation of dynamics similar to scaling to increasing language scopes where the train set is diluted.\n    *   Over-weighting of Target Domain Examples: The sampling weight of WTRAIN examples is increased relative to VTRAIN to ensure at least half of all training observations are in-domain. This simulates a scenario where control over sample weighting is possible.\n    These experiments analyze how the \"\"distance\"\" of the related domain impacts the benefits of additional data and whether the skew in train vs. test distribution outweighs the benefits of increased data.\nFor all experiments, a Transformer architecture with consistent hyperparameters is used, and results are averaged over 5 replicas for each split to ensure robustness.": 1099,
    "The Continuous Style-induced Image Generator (CSIIG) is central to enabling high-dimensional, non-adversarial perturbations for consistency regularization. The CSIIG is based on the AdaIN image-to-image style transfer generator, which combines a content image and a style image using a continuous content-style trade-off parameter (alpha). This parameter allows for the generation of images with a continuous blend of content and style features. For unsupervised domain adaptation, specifically in the source-guided unsupervised learning branch, the CSIIG takes a perturbed target domain image as content and a set of randomly sampled source domain images as styles. By transferring the target input image into a set of images, each matching one of the source image styles, the CSIIG creates a series of images that share identical semantic content (from the original target image) but possess different high-dimensional stylistic perturbations (derived from various source styles). This process generates diverse, high-dimensional variations of the target image in a non-adversarial manner. These perturbed images are then fed into the teacher segmentation network to acquire stable predictions, which are averaged and sharpened to form a pseudo-label for the original target image. The student network's prediction on a brightness/contrast-perturbed version of the original target image is then regularized against this pseudo-label, enforcing consistency across these high-dimensional perturbations. This mechanism allows the model to learn robust features from the unlabeled target data by ensuring consistent predictions despite significant stylistic variations, without relying on adversarial training.": 1100,
    "To optimize the feature representations for path coherence and endpoint reconstruction, the DERWENT model employs two specific loss functions:\n1.  Similarity Loss (li,1): For each sampled sequence Si = (x̂i,1, ..., x̂i,nsi), this loss enforces similarity between adjacent data points. It is defined as the negative logarithm of a scaled sigmoid function applied to the cosine similarity between x̂i,j and x̂i,j+1, combined with a negative example x̂i,j. This encourages the feature extraction network (φ(·)) to learn representations where neighboring points in a valid transfer path are close in the feature space, facilitating smooth transitions.\n2.  Sequence Loss (li,2): If a sequence Si successfully reaches a target data point (for Type 1 random walk) or a source data point (for Type 2 random walk), this loss ensures that the ending data point (x̂i,nsi) can be accurately reconstructed from the preceding data points in the sequence (x̂i,1, ..., x̂i,nsi-1). An LSTM (Long Short-Term Memory) network processes the sequence up to the second-to-last element, and a neural decoder (fd(·)) then attempts to generate an approximation of the final data point. The sequence loss is the L2 norm of the difference between the actual ending data point and its reconstruction. This forces the model to capture the sequential dependencies and information flow along the transfer path, ensuring that the path effectively \"\"leads\"\" to the endpoint.": 1101,
    "The paper proposes Cycle-Consistent Cooperative Networks (CycleCoopNets) to achieve unsupervised, bidirectional cross-domain translation. This framework involves simultaneously learning a pair of cooperative networks: one for translation from domain Y to X (GY!X(y;φX) and p(x;θX)) and another for translation from domain X to Y (GX!Y(x;φY) and p(y;θY)).\nThe training is performed using an Alternating MCMC Teaching algorithm. In each iteration, the process alternates between training the Y-to-X network and the X-to-Y network.\nFor the Y-to-X translation:\n1.  Sample examples (yi) from the data distribution of domain Y (pdata(y)).\n2.  Translate these examples using the current LVM GY!X to get initial examples (^xi = GY!X(yi;φX)).\n3.  Refine these initial examples by running a finite number of Langevin dynamics steps, initialized from ^xi, towards the EBM p(x;θX) to obtain revised examples (~xi).\n4.  Update the EBM p(x;θX) parameters (θX) using the revised examples (~xi) according to the maximum likelihood objective (Eq. 4).\n5.  Update the LVM GY!X parameters (φX) by minimizing the Lteach objective (Eq. 8), which makes GY!X(yi;φX) close to the revised examples (~xi).\nA similar, symmetric process is applied for the X-to-Y translation, involving sampling from pdata(x), translating with GX!Y, refining with p(y;θY), and updating θY and φY.\nTo enforce mutual invertibility between GX!Y and GY!X, a cycle consistency loss (Lcyc) is added as a constraint during the update of φX and φY. This loss (Eq. 9) penalizes deviations when an example is translated from one domain to the other and then back to the original domain. Specifically, it measures the difference between an input x and GY!X(GX!Y(x;φY);φX), and between an input y and GX!Y(GY!X(y;φX);φY). This cycle consistency loss is combined with the MCMC teaching loss (Lteach) for the overall optimization of the LVM parameters. The algorithm iterates for a specified number of steps, allowing both cooperative networks to learn and refine their translations while maintaining the inverse relationship between them.": 1102,
    "To disentangle domain-general and domain-specific features and construct the Latent Unified State Representation (LUSR), the paper employs a Cycle-Consistent VAE (Cycle-Consistent Variational Autoencoder). This model consists of an encoder, denoted as q(phi), and a decoder, denoted as p(theta). The encoder's function is to map a raw observation state (so) to a latent state (sz), which is then explicitly split into its domain-general component (sz) and domain-specific component (bsz). The training process for this mechanism involves collecting a large number of random observation states from a predefined set of domains. The disentanglement is enforced through a combined objective function, Lcyclic, which minimizes the sum of a modified variational upper-bound (Lforward) and a cycle consistency loss (Lreverse). The forward cycle ensures that decoding an encoded observation (Dec(Enc(so))) reconstructs the original observation (so) closely. The reverse cycle, Enc(Dec(bsz;sz)), ensures that encoding a decoded latent state reconstructs the original latent state components. A critical aspect of this mechanism is that swapping domain-specific embeddings (bsz) from different observations within the same domain should not significantly impact the reconstruction loss, thereby forcing bsz to capture only domain-specific information. Conversely, if two reconstructed images are generated using the same domain-general embedding (sz) but different domain-specific embeddings (bsz), their corresponding domain-general latent embeddings should remain identical, reinforcing the consistency of sz across variations. Once the Cycle-Consistent VAE is trained, its encoder (specifically, the part that outputs only the domain-general embedding) serves as the mapping function F: So -> Sz, which transforms raw observations into the desired LUSR.": 1103,
    "To further minimize correlations between the learned domain-agnostic (`z`) and domain-specific (`zd`) features, the paper introduces a novel Independent Excitation mechanism. This mechanism is implemented through an additional loss function, `LIE(z;zd;yd_i;d;wd;wy)`. The core idea is to actively force the disentangling classifiers to perform poorly when given the \"\"wrong\"\" type of feature. Specifically, the `LIE` loss encourages the activity classifier `DCy` to minimize its accuracy when it receives the domain-specific features `zd` as input. Simultaneously, it encourages the domain classifier `DCd` to minimize its accuracy when it receives the domain-agnostic features `z` as input. This is achieved by negating the standard cross-entropy loss for these \"\"mis-matched\"\" inputs: `L_IE = - [L(yd_i, DCy(zd;wy)) + L(d, DCd(z;wd))]`. By minimizing this `LIE` term, the model is compelled to make `z` irrelevant to domain labels and `zd` non-informative of activity labels, thereby promoting a stronger disentanglement and reducing redundancy between the two latent spaces beyond what the `LDC` alone achieves. The total loss `L` is a weighted sum of `Lelbo`, `LDC`, and `LIE`, with trade-off parameters `α` and `β`.": 1104,
    "A Bimodal Cross Attention Layer (BCAL) is introduced to learn the joint representation of two distinct modality groups: the HCF-enriched language embedding (Ul;h) and the non-verbal embedding (Ua;v). This layer is a modification of the original Transformer encoder layer. Within the BCAL, two sets of queries (Qm1, Qm2), keys (Km1, Km2), and values (Vm1, Vm2) matrices are generated from the input modality groups (Um1, Um2). Two Multihead Cross Attention sub-layers are employed, where each sub-layer uses its own query matrix to attend to the key and value matrices of the *other* modality group. Specifically, one sub-layer computes attention using Qm1, Km2, and Vm2, while the other uses Qm2, Km1, and Vm1, facilitating cross-alignment and information exchange between the two groups. The outputs of these two cross-attention sub-layers (~Um1, ~Um2) are then concatenated. This concatenated representation is subsequently fed into a Multihead Self Attention layer, which updates the representation by considering information from all elements within the combined input. Following this, a Feed Forward sub-layer processes the output to produce the final joint representation (Zm1;m2). Layer normalization and residual connections are applied after each sub-layer to stabilize training and improve performance.": 1105,
    "To dynamically discover and utilize sparse, domain-specific sub-networks, the PRUNE-TUNE method leverages the parameters that were not frozen as part of the general informative sub-network. These \"\"unfixed\"\" parameters provide the capacity for domain-specific adaptation. For a new target domain, the process begins with a warm-up training phase, where these free parameters are fine-tuned on the target domain data for a few steps. Following this warm-up, an iterative pruning method (similar to the one used for the general domain) is applied again, but this time specifically to these unfixed parameters. This iterative pruning process automatically uncovers a sparse structure tailored to the specific target domain, which the paper terms the lottery sub-network. Once this domain-specific lottery sub-network is generated, its structure is then fixed. The final step involves fine-tuning this newly identified and fixed lottery sub-network on the target domain data. This approach ensures that only a small, relevant subset of parameters is adapted for the new domain, making the process parameter-efficient and adaptive to the specific requirements of the target domain.": 1106,
    "The paper addresses the optimal combination of features from multiple Transformer layers within a task-specific tower by employing a Weighted Sum Pooling (WSP) strategy. After the last block in a task-specific tower (which consists of `n+1` blocks, from Block 0 to Block `n`), the hidden states from all Transformer layers within that tower are pooled. Specifically, for a task `τj`, the pooled features `H_pool(τj)` are calculated as a weighted sum of the Transformer hidden states `H_j(i)` from each block `i` (from `i=0` to `i=n`). The formula is `H_pool(τj) = sum(α_j(i) * H_j(i) for i=0 to n)`, where `α_j(i)` are learned parameters. These `α_j(i)` parameters are real numbers that allow the model to dynamically assign different importance to the features learned at different depths (layers) of the Transformer tower. This approach ensures that the model can leverage the most relevant features from each layer, as different Transformer layers are known to encode different utilities for semantic and syntactic downstream tasks. This dynamic weighting contrasts with simpler pooling methods like average pooling, which assign fixed weights.": 1107,
    "The paper proposes a Source-Side Mixup Loss (Lmix) to directly construct counterfactual interpolations. Unlike traditional Mixup that interpolates between two observed samples, this method interpolates between representations derived from source-domain texts (xs) paired with a source-domain tag (ts) and the same source-domain texts (xs) paired with a target-domain tag (tt). Specifically, it defines two data points: x1 = E(ts;xs) with label y1 = ys (the observed source-domain translation) and x2 = E(tt;xs) with an unobserved ideal target-domain translation y2 = y't. Although y't is unknown, the Mixup formulation allows training without explicit target labels for the interpolated samples. The loss Lmix minimizes the cross-entropy loss of the decoder (D) on interpolated representations: `(alpha * E(ts;xs) + (1 - alpha) * E(tt;xs))`, where `alpha` is drawn from a Beta distribution `Beta(alpha_param + 1, alpha_param)`. This linear interpolation in the representation space incentivizes the network to learn a smooth transition and interpolate effectively between the source and target domains, thereby bridging the generalization gap and augmenting the training data with constructed counterfactual representations.": 1108,
    "Once the insertion position is identified, the decoder generates a simile using a novel insertion bias mechanism. This mechanism injects positional information into the decoder, guiding it to produce similes that are contextually relevant. Specifically, the decoder receives an insertion bias vector \\( k \\) computed from the predicted insertion position, which biases the generation process. The probability of generating each word \\( y_t \\) is modeled as \\( P(y_t|y_{0:t-1}, i_{Ins}, X) \\). This ensures that the generated simile aligns with the surrounding context, enhancing the overall fluency and expressiveness of the text.": 1109,
    "The adaptive mean teacher modifies the original Mean Teacher model by incorporating automatic confidence thresholds and similarity-weighted loss. Automatic thresholds are computed separately for aspect terms and non-aspect terms to filter out unreliable pseudo labels. Additionally, similarity weights are assigned based on the domain discriminator's output, giving higher weights to words from sentences more similar to the source data. This adaptation ensures that the framework can effectively leverage high-quality pseudo labels while minimizing the influence of noisy labels.": 1110,
    "To achieve fine-grained alignment of entity distributions across domains, the paper integrates the generated entity-aware features into an Adversarial Training framework. After the Entity-Aware Attention Layer produces the entity-aware adversarial feature (hEA), this feature becomes the input to a domain discriminator. The primary role of the domain discriminator is to distinguish whether the input feature (hEA) originates from the source domain or the target domain. The adversarial training process is formulated as a min-max game, represented by the loss function LAdv. In this game, the parameters of the feature extractor (θf), which includes the LSTMs and the attention mechanism responsible for generating hEA, are optimized to minimize the discriminator's ability to correctly classify the domain. Simultaneously, the parameters of the domain discriminator (θd) are optimized to maximize its ability to correctly classify the domain. This adversarial dynamic forces the feature extractor to learn domain-invariant entity-aware features (hEA) that are indistinguishable by the discriminator, thereby effectively reducing the distribution gap of these specific entity features between the source and target domains. This targeted alignment, guided by the entity-aware attention, directly addresses the problem of \"\"false alignment\"\" observed in standard adversarial training by ensuring that the alignment focuses on the critical entity-level information.": 1111,
    "The disentangled representation involves encoding the domain attributes independently within the model's architecture. This separation ensures that the attributes do not interfere with each other, thereby reducing the risk of mode collapse and improving the robustness of the translation process. This method allows for more precise control over the attributes that are modified during the translation, leading to higher diversity and quality in the generated images.": 1112,
    "The issue of label noise in confidently predicted pseudo-labeled samples is effectively mitigated through a novel strong-weak self-training paradigm. This paradigm involves two models: a \"\"strong\"\" model (Gsr, Fsr1, Fsr2 with parameter θsr), which is the initial target model fine-tuned on the target domain, and a \"\"weak\"\" model (Gwe, Fwe1, Fwe2 with parameter θwe), which is trained from scratch (e.g., an ImageNet pre-trained model not trained on source or target). The motivation is based on the observation that deep networks tend to memorize correctly labeled samples first before memorizing noisy ones. During training, the weak model is used to help the strong model filter out high-confidence but incorrect predictions. This is achieved by gradually fusing the parameters of the weak model into the strong model at the end of each epoch using a smooth parameter movement strategy: θsr = αθsr + (1 - α)θwe. This fusion increases the confidence scores of easy and correct samples, encouraging them into the high-confidence set, while reducing the confidence of high-confidence but incorrect samples, causing them to be filtered out. The parameter α is set to 1 when the weak model's cross-entropy loss for noise labels falls below a certain threshold (e.g., 0.5), indicating its increasing strength and ability to identify correct labels. As the weak model strengthens due to reduced noisy labels, its predictive accuracy for high-confidence samples can eventually surpass the original strong model, leading to the strong model abandoning its original parameters in favor of the weak model's.": 1113,
    "The paper addresses the negative effects of noisy pseudo labels through an uncertainty-guided progressive label refinery framework, formulated as a joint optimization problem (Eq. 5). The objective is to minimize a function `E(w,v;β)` that balances reducing the overall uncertainty of selected samples (`viui`) with selecting as many samples as possible (`β`). Here, `w` represents the model parameters, `v` is a binary indicator vector (`vi ∈ {0,1}`) for sample selection, and `β` is an age parameter controlling the learning pace. This joint optimization is solved using Alternative Convex Search, where `w` and `v` are optimized iteratively while fixing the other.\n1.  Sample Selection (Optimizing v with fixed w*): With the model parameters `w*` fixed, the optimal sample selection `v*` is determined by minimizing `∑ viui - β∑ vi` (Eq. 6). This combinatorial optimization problem has a closed-form solution: `v*i = 1` if the sample's uncertainty `ui` is less than or equal to `β`, and `0` otherwise (Eq. 8). This means samples with uncertainty below a certain threshold are selected.\n2.  Model Fine-tuning (Optimizing w with fixed v*): With the selected samples `v*` fixed, the model parameters `w*` are optimized by minimizing `∑ v*i ui` (Eq. 7), which is achieved through gradient descent-based network training using the selected, more reliable samples.\nThis two-step process is iteratively conducted. The `β` parameter is dynamically updated to ensure a predefined proportion of samples are included at each iteration. A sequence `{N · pt}` (where `N` is total samples and `pt` is the proportion at time `t`) determines the number of selected examples. `β` is set to the `(N·pt)`-th smallest uncertainty value from the sorted uncertainties of all samples (Eq. 9). The proportion `pt` itself is progressively increased over training steps `t` using a rescaled logarithm-exponential function (Eq. 10): `pt = p0 + (eh·(1-p0) - 1) / (1 + ln(t/T) / (1/h))` where `p0` is the initial proportion and `h` controls the increasing rate. This progressive strategy allows the model to first learn from highly credible \"\"easy\"\" samples and then gradually incorporate \"\"harder\"\" samples as the model becomes more reliable, preventing overfitting to noisy labels and promoting stable training.": 1114,
    "During local clustering, temporal continuity is utilized to reduce ID switch errors. By incorporating the temporal distance between sample timestamps into the overall distance metric, the method distinguishes between look-alike individuals who appear at different times. This ensures that inherently similar but different individuals are not mistakenly clustered together, thereby reducing ID switch errors.": 1115,
    "AST for Cross-Domain Simulation (AST-Sim) leverages the disentangled AST-latent to simulate cross-domain feature stylization. This is achieved by replacing the AST-latent of a source feature with that of a target feature, specifically, `ˆhl,si✮j = AST(hl,si,zl,tj)`, where `hl,si` is the l-th layer feature of source instance `si`, and `zl,tj` is the l-th layer AST-latent of target instance `tj`. To avoid the need for unsupervised clustering, a novel \"\"Cross-Domain Pairing\"\" strategy is employed. For each target instance `tj`, a source instance `si` is identified by mining the source dataset for the instance `si` that is maximally separated from `tj` in the AST-latent space of the l-th CNN layer. This is formally expressed as `(si,tj) : ∀tj,i = argmax i′ ζ(zl,si′,zl,tj)`, where `ζ` is an L2 distance function. This hard negative mining approach ensures that the simulation occurs between instances exhibiting the most distinct domain styles, thereby promoting more effective stylization. Due to AST's phase preservation, the simulated features `ˆhl,si✮j` maintain the same task-related content as their original counterparts `hl,si`.": 1116,
    "This challenge is tackled through two interconnected mechanisms: Semantic Conditioned Manifestation and the Conditional Graph (CG). First, for Semantic Conditioned Manifestation, the well-modeled implicit semantics from the unbiased semantic paradigm (P) are manifested into dynamic, class-specific activation maps. This is achieved by sending P to a lightweight manifestation module, which is designed with a Recurrent Neural Network (RNN) based unit to capture cross-iteration semantic relationships and model inherent semantic evolution. This RNN unit, followed by a convolution layer, learns the parameters of conditional kernels (Wcon = Conv(tanh(RNN(P)))). These conditional kernels are then applied via convolution on source features (Fs) to obtain class-specific semantic activations (Ss = softmax(Wcon ∗ (Fs))). Focal Loss (Eq. 5) is adopted to train this dynamic branch, ensuring it accurately manifests implicit semantics. Second, in the target domain, the learned conditional kernels (Wcon) are crucial for establishing a Conditional Graph (Gcon) to model semantics without direct category annotations. Given target image features (Ft), conditional convolution with Wcon is performed to obtain explicit semantic activation maps (St). Class-specific feature representations (˜Ft,c) are generated by broadcasting the Hadamard product of Ft and St,c. To obtain a noiseless sampling prior, the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to St, yielding clean semantic maps (˜St). Based on this prior, an adaptive node sampling strategy is used to perform fine-grained node sampling on Ft, obtaining foreground and background node samples. These nodes form a pixel-level sub-graph (Gpix), which undergoes graph reasoning (similar to Eq. 1) to obtain enhanced node representations. Furthermore, Gpix is extended to a category-level sub-graph (Gcat), where nodes represent class embeddings and edges represent quadratic relationships between categories, providing a comprehensive semantic model in the target domain.": 1117,
    "To address the challenge of noisy pseudo-labels, the paper introduces a reliability estimation method based on cluster consistency. During each iteration, features from different branches of the running model and the mean feature from the temporally averaged model are extracted for target domain samples. These features are then separately clustered using an algorithm like DBSCAN, resulting in K+1 distinct cluster results for each sample (one for each of the K branches and one for the ensemble feature). The reliability of a sample is then measured by calculating the Intersection-over-Union (IOU) score of its cluster sets across these different clustering results. A higher IOU score indicates that the sample has more consistent neighbors across different cluster results, signifying higher reliability. Conversely, a low IOU score suggests inconsistency and unreliability. Based on this calculated reliability score (R), a reliability threshold (β) is set. Samples with an R value above this threshold are considered reliable and selected for training, while those below are treated as outliers. Furthermore, this estimated reliability (R) is incorporated into the Re-ID losses as a reliability-weighted classification loss, effectively re-weighting the contribution of each sample during training. This mechanism ensures that more reliable samples have a greater impact on the model's learning, thereby alleviating the negative influence of noisy pseudo-labels.": 1118,
    "To dynamically adapt the model's utilization of shape priors at test time, the paper devises a dual-consistency regularization mechanism. This mechanism is applied when deploying the model to new, unseen domains. Given an inference data sample `xt`, two different perturbations, `η` and `η'`, are applied to it (e.g., adding Gaussian noise to input or applying dropout to network parameters). The core idea is to regularize the model to generate consistent predictions for these two perturbed versions of the same input. Crucially, this consistency is enforced on two levels: the final segmentation predictions and the shape coefficient predictions from the regression branch. The segmentation consistency regularization, `Lseg_cons(xt) = ||f(xt,η') - f(xt,η)||^2_2`, uses a mean square error loss to ensure that the network's output `f(·)` remains consistent despite perturbations. Simultaneously, the shape coefficient consistency regularization, `Lcoef_cons(xt) = 1 - (α_hat_t(η') · α_hat_t(η)) / max(||α_hat_t(η')||_2 · ||α_hat_t(η)||_2,ϵ)`, uses a cosine distance loss to ensure that the predicted shape coefficients `α_hat_t(·)` remain consistent. The total dual-consistency regularization objective is `Ldual_cons(xt) = Lseg_cons(xt) + Lcoef_cons(xt)`. At test time, the model performs only one step of gradient descent by optimizing this `Ldual_cons` objective to adapt its parameters for each inference data. To ensure stability and avoid affecting the model's original discriminability, only the feature normalization parameters (i.e., parameters of the batch normalization layers) in the source model are adapted. This adaptation is performed in an online manner, meaning that when a new test sample `xt_j` arrives, the model state is initialized with parameters updated from the previous sample `xt_j-1`, allowing it to continuously leverage distribution information from all preceding samples.": 1119,
    "To address the challenge of adaptively weighting source pixels and relieving negative adaptation, the paper introduces Patchwise Prototypical Matching (PPM). This module aims to softly adjust the weight of each source pixel during training based on its similarity to the sole target sample. The process begins by taking the target image feature (fT4) obtained from layer4 of the segmentor and reshaping it into non-overlapping patches (pT). For each patch, a prototype (Protoi) is computed by averaging the features within that patch. Next, a confidence map (Confi) is generated by calculating the cosine similarity between each prototype (Protoi) and the source features (fS4) from layer4. A maximum operation is then performed across all prototypes to obtain a single confidence map (Conf) for the entire source sample, representing the maximum cosine similarity for each pixel. To further refine this confidence, it is rectified using the entropy of the source prediction (pxS). The entropy map (E) is calculated for the source prediction, where high entropy indicates low confidence. The final rectified confidence map (`\\Conf`) is then computed as `\\Conf` = Conf * (1 - E). This rectification ensures that only regions where the source prediction is already confident contribute significantly to the adaptation. Finally, this `\\Conf` map is used to weight the pixel-level cross-entropy loss (Lpce) during training. The overall objective function combines a standard cross-entropy loss (Lce) with this pixel-weighted cross-entropy loss: L = λLce + Lpce, where λ is a balancing factor. This mechanism effectively emphasizes source pixels that are more similar or relevant to the target domain, thereby reducing the impact of potentially \"\"negative\"\" source samples or pixels on the adaptation process.": 1120,
    "A self-labeling mechanism is designed to simultaneously classify seen categories and discover multiple distinct novel categories by optimizing pseudo-labels through mutual information maximization. An extended classifier, E, outputs a probability vector pe(y|xi_t) for each target sample xi_t, where the first |Cs| dimensions represent seen categories and the remaining |fCt| dimensions represent novel categories. The mutual information I(Y;Xt) between the input Xt and output Y is maximized, which is equivalent to maximizing the entropy of the expected network prediction H(Ext[pe(y|xt)]) and minimizing the entropy of the network output Ext[H(pe(y|xt))]. The minimization of Ext[H(pe(y|xt))] is achieved by minimizing the entropy loss Le(Dt), which is the negative sum of pe(k|xi_t)logpe(k|xi_t) for each sample xi_t across all classes k. The maximization of H(Ext[pe(y|xt)]) is achieved by minimizing the negative entropy of the mean probability distribution Lp(Dt), calculated as the negative sum of pe(k|Dt)logpe(k|Dt), where pe(k|Dt) is the mean probability for each mini-batch Dt and ^pe(k|Dt) is its moving average. The overall mutual information maximization loss, LMIM(Dt), is defined as -Lp(Dt) + Le(Dt). This combined objective encourages the network to produce high-confidence predictions for individual samples while ensuring that the overall distribution of predictions across the dataset is diverse, thereby enabling the clustering and discovery of multiple novel categories. The extended classifier E uses a cosine classifier with weight vectors W, where weights for seen categories wj are directly set to the source prototypes uj_s (Eq. 10), ensuring alignment with known classes.": 1121,
    "To address the sparse reward problem where a binary reward (1 for finding a backdoor, 0 otherwise) would make focused search impossible, the paper proposes using \"\"tree weight\"\" as a continuous reward function. The tree weight is a scoring function that maps a branch-and-bound tree (generated by evaluating a candidate backdoor) to a value in the range [0,1]. For a mixed-binary integer program, the tree weight `tree-weight(Tk)` at iteration `k` (after `k` nodes have been expanded) is defined as the sum `Σ v∈Fk 2^-d(v)`, where `Fk` is the subset of \"\"final\"\" or fathomed nodes (due to infeasibility, integer-feasibility, or being fathomed by bound), and `d(v)` is the depth of node `v`. At the start of branch-and-bound, with no fathomed nodes, the tree weight is 0. This function strictly increases as more nodes are fathomed, providing a meaningful score to candidate backdoors that are not true strong backdoors but successfully eliminate many binary assignments. This continuous score allows the MCTS to guide its search towards candidates that are \"\"closer\"\" to being a strong backdoor, even if they don't fully satisfy the conditions.": 1122,
    "The paper determines the target threshold, $\\delta_T^m$, by formulating an optimization problem. The core idea is to find a $\\delta_T^m$ that makes the distribution of the resulting \"\"normal\"\" target examples (i.e., those with scores below $\\delta_T^m$) as similar as possible to the previously derived source $\\delta_S^{cut}$ distribution. This similarity is quantified using the Kullback-Leibler (KL) divergence. The optimization problem is defined as finding the $\\delta_T^m$ that minimizes the KL divergence between the source $\\delta_S^{cut}$ distribution and the target $\\delta_T^{cut}$ distribution. The target threshold $\\delta_T^m$ is constrained to be within a practical interval, specifically $[\\epsilon, 1]$, where $\\epsilon > 0$ is a small value to prevent all examples from being classified as anomalous (as a contamination factor of 0 would imply $\\delta_T^m = 0$). The KL divergence, KL(S || T), is calculated as the integral from 0 to 1 of s(x) * log(s(x)/t(x)) dx, where s and t are the continuous distributions of S and T, respectively. This minimization ensures that the shape of the normal score distribution in the target domain closely matches that of the source domain, allowing for the transfer of the threshold.": 1123,
    "The paper integrates the domain-aware BERT, obtained from the specialized post-training procedure, into an adversarial training framework to derive enhanced domain-invariant features. This process involves a sentiment classifier and a domain discriminator operating on the hidden state `h[CLS]` of the `[CLS]` token from the post-trained BERT.\n*   Sentiment Classifier: This component is a fully-connected layer followed by a softmax layer, outputting sentiment probabilities `ys = softmax(Ws h[CLS] + bs)`. It is trained using the labeled data from the source domain, minimizing the cross-entropy loss `Lsen`.\n*   Domain Discriminator: This component aims to predict whether a given sample comes from the source or target domain. Its input `h[CLS]` first passes through a Gradient Reversal Layer (GRL).\n    *   Gradient Reversal Layer (GRL): The GRL acts as an identity function during forward propagation (`Qλ(x) = x`), but during backpropagation, it reverses the gradient by multiplying it by a negative scalar `λ` (`∂Qλ(x)/∂x = -λI`). The adaptation rate `λ` is dynamically increased during training based on the training progress `p = t/T` using the formula `λ = 2 / (1 + exp(-10p)) - 1`.\n    *   Discriminator Output: After passing through the GRL, the hidden state `ˆh[CLS]` is fed into the domain discriminator: `d = softmax(Wd ˆh[CLS] + bd)`. The discriminator's objective is to minimize the cross-entropy loss `Ldom` for distinguishing source from target domains.\n*   Joint Learning: The sentiment classifier and the domain discriminator are trained jointly. The total loss `Ltotal = Lsen + Ldom` is minimized.\n    *   Enhancement Mechanism: Due to the GRL, the parameters of the domain discriminator are optimized to improve its ability to predict domain labels. Crucially, the parameters of BERT (`θBERT`) are optimized to *fool* the domain discriminator. Because BERT has already been made domain-aware and capable of distilling domain-specific features through the preceding post-training, it can now identify and \"\"abandon\"\" these specific features more effectively during adversarial training. This leads to the generation of more robust and truly domain-invariant features, as the post-training has prepared BERT to better understand and separate domain characteristics, thereby enhancing the adversarial process's ability to minimize domain discrepancy and lower the upper bound of the target error.": 1124,
    "The dynamic fusion layer uses a Mixture-of-Experts (MoE) mechanism to dynamically incorporate all domain-specific knowledge for the current input. Given domain-specific features from all domains, an expert gate outputs a softmax score representing the correlation between each domain and the input token. The final domain-specific feature vector is a weighted mixture of all domain outputs, guided by the expert gate weights. This layer enhances the model's ability to transfer knowledge from different source domains to the target domain.": 1125,
    "The paper addresses this through the Adversarial Training (AT) module, which processes both the manually annotated source domain dataset (S) and the distantly annotated target domain dataset (T). The AT module employs three GCNN-based encoders and a discriminator:\n1.  Domain-specific encoders (Esrc and Etgt): Esrc extracts domain-specific features (Hs) from source domain sentences, and Etgt extracts domain-specific features (Ht) from target domain sentences. These encoders are trained to capture characteristics unique to their respective domains.\n2.  Sharing encoder (Eshr): This encoder is designed to extract domain-agnostic features (H∗s for source, H∗t for target) that are common across both domains. Eshr is trained adversarially against a discriminator.\n3.  Discriminator (Gd): A binary classifier (text CNN network) that attempts to distinguish whether a sentence's representation (produced by Eshr) comes from the source or target domain.\nThe training procedure involves an adversarial game:\n*   The discriminator's loss (Ld) is defined to maximize its ability to correctly predict the domain of sentences based on Eshr's output.\n*   The sharing encoder's loss (Lc) is a flipped version of Ld, meaning Eshr is trained to minimize the discriminator's ability to correctly predict the domain, thereby forcing Eshr to learn domain-agnostic features that \"\"fool\"\" the discriminator.\nThe overall training loss combines the standard CWS losses for both source (Lsrc) and target (Ltgt) domains (calculated after concatenating domain-specific and domain-agnostic features, e.g., [Hs ⊕ H∗s] for source) with the adversarial losses (Ld or Lc). The training alternates between updating the model parameters to minimize (Lsrc + Ltgt + Ld) and minimizing (Lsrc + Ltgt + Lc), ensuring that the model learns to segment words while simultaneously extracting both domain-specific and domain-agnostic features and reducing noise from the distantly annotated data.": 1126,
    "To jointly model contextualized word semantics and phonological word representations, the method first obtains a `dC`-dimensional contextualized word embedding `TCi` for each word `wi` using a pre-trained model like BERT. This `TCi` captures the word's meaning within its surrounding context. Simultaneously, a `dP`-dimensional pronunciation embedding `TPi` is derived for each word, representing its phonological characteristics as described in Solution 1. These two distinct embeddings, `TCi` and `TPi`, are then concatenated to form a `dJ`-dimensional joint embedding `TJi` for each word `wi`, where `dJ = dC + dP`. This concatenation effectively combines both semantic and phonological information into a single representation. To further enhance the representation and capture the overall semantics across the entire sentence, a self-attention mechanism is applied to the sequence of these joint embeddings `TJ = {TJ1, TJ2, ..., TJN}`. This self-attention mechanism computes an importance vector `αSi` for each `TJi` by transforming `TJi` with a function `FS(·)` and applying a softmax over all words in the sentence. The final self-attentive embedding `TJ[ATT]` for the entire input text is then computed as a weighted sum of all `TJi` vectors, using `αSi` as weights. Additionally, for pun location, a self-attentive joint embedding `TJi,[ATT]` is derived for each individual word `wi` by scaling its `TJi` with its corresponding `αSi`.": 1127,
    "The paper addresses this by employing a Lexically Constrained Rewriting Model (LCR) that builds upon existing constrained decoding techniques. The core idea is to train an end-to-end generator that rewrites a source sentence (x) into a target sentence (˜y) while satisfying specified lexical constraints. To overcome the limitation of traditional beam search in guaranteeing constraint adherence, the LCR model incorporates a specialized mechanism for constraint placement.\nThis mechanism organizes constraints that have not yet been generated into a trie data structure. For \"\"positive constraints\"\" (words or phrases that *must* appear), a counter is associated with each constraint, which is decremented each time the constraint is generated. For \"\"negative constraints\"\" (words or phrases that *must not* appear), the trie does not require counters. At each time step during the generation process, the model actively blocks the generation of any word IDs marked in the current node of the trie that correspond to an active negative constraint by setting their generation costs to infinity. This effectively prevents the model from producing forbidden words. The model also specifically addresses the challenge of constraints sharing a prefix, ensuring that the trie structure correctly manages these overlapping constraints without repetition. Hypotheses during beam search are ranked not only by their sequence scores but also by the sentence number and, crucially, the number of unmet constraints, prioritizing hypotheses that are closer to satisfying all required conditions. This approach ensures that both positive and negative lexical constraints are satisfied without redundant generation.": 1128,
    "The paper proposes two strategies for weighting back-translated data based on quality: measuring current quality and measuring quality improvements.\nFor Measuring the Current Quality:\n1.  Encoder Representation Similarities (Enc): For a source sentence `x` and its back-translated target sentence `y`, their representations are obtained by feeding them into the encoders of the respective Machine Translation (MT) models (MFE for `x` and MEF for `y`). The hidden states at the final layer are averaged to get `encFE(x)` and `encEF(y)`. The cosine similarity between `encFE(x)` and `encEF(y)` is used as the quality metric. A higher similarity implies better alignment and thus higher quality.\n2.  Agreement Between Forward and Backward Models (Agree): Inspired by Junczys-Dowmunt (2018), this method uses the conditional probabilities from both forward and backward translation models. For a sentence pair `(x, y)`, the agreement is calculated as `exp(-(|HFE(y|x) - HEF(x|y)|))`, where `HFE(y|x)` is the conditional probability of `y` given `x` from the source-to-target model (MFE) and `HEF(x|y)` is the conditional probability of `x` given `y` from the target-to-source model (MEF). A smaller absolute difference between these probabilities indicates higher agreement and thus higher quality.\nFor Measuring Quality Improvements:\nThis metric, `Imp(s)`, is designed to encourage the inclusion of in-domain sentences whose translation quality has improved over time. Every time a quality score (from Enc or Agree) is obtained for a sentence `s`, it is stored. The next time the same sentence is encountered, its new quality score is compared to its previous one using the formula: `Imp(s) = clip(current quality(s) / previous quality(s), wlow, whigh)`. The `clip` function limits the weight to a reasonable range, with `(wlow, whigh)` set to `(0.5, 2)`. This means if the quality has doubled, the weight is 2; if it halved, the weight is 0.5. This `Imp(s)` can then be combined with the current quality metric to assign the final weight to the back-translated sample.": 1129,
    "The paper addresses the generation of similes by fine-tuning a pre-trained sequence-to-sequence model, BART (Bidirectional and Auto-Regressive Transformers), on the automatically constructed parallel corpus of literal-simile pairs. BART is chosen due to its strong performance in various text generation tasks, functioning as a denoising autoencoder with a bidirectional encoder and a left-to-right autoregressive decoder. For the fine-tuning process, the literal input sentence serves as the encoder source, and the corresponding simile acts as the decoder target. This setup trains BART to map literal expressions to their figurative simile counterparts. During inference, after fine-tuning, the model takes a literal input sentence and generates a novel simile. To control the generation process and introduce diversity, a top-k sampling strategy (with k=5) is employed, where the model randomly samples from the top five most likely next words at each timestep, combined with a softmax temperature of 0.7. This approach allows the model to produce varied and novel similes while maintaining contextual relevance to the input.": 1130,
    "Statistical analysis of the HYPO-cn dataset explores various aspects of hyperbole, such as sentence length and lexical diversity. Manual analysis identifies common strategies used by humans to create hyperboles, including quantity concepts, extreme cases, common sayings, rhetorical devices, comparisons, supernatural concepts, descriptions of life, body states, nature, fictitious scenes, and impossible ordering. These strategies are categorized and quantified to provide insights into the linguistic mechanisms underlying hyperbole creation.": 1131,
    "To leverage class information and enhance feature discriminativeness, the paper introduces an intra-class loss component within the Class-aware Feature Self-distillation (CFd) framework. This component addresses the limitation of pure feature distillation, which ignores class-specific clustering. For each class `c`, a center feature `zc` is calculated by averaging all features (`zj`) belonging to that class from both the source labeled set (Sc) and the pseudo-labeled target set (T'c). The intra-class loss, `L_intra_class`, is then defined as the sum of squared Euclidean distances between each feature `zi` and its corresponding class center `zc` across all classes. This loss `sum(norm(zi - zc)^2)` encourages features from the same class to cluster more cohesively, making different classes more separable. The class centers `zc` are calculated and fixed before each training epoch and updated after each epoch. By minimizing this intra-class loss, the method aims to create more compact clusters for each class, which in turn helps to reduce the domain discrepancy (A-distance) by aligning features from source and target domains within the same class. This mechanism is crucial for improving robustness against noisy pseudo-labels by enforcing a stronger class structure on the learned features.": 1132,
    "The paper investigates the performance of state-of-the-art (SOTA) grammatical error correction (GEC) systems in low error density domains through a series of analyses. First, it benchmarks two SOTA GEC systems, GEC-PSEUDODATA (a transformer-based seq2seq model) and PIE (Parallel Iterative Edit, a BERT-based architecture), on various GEC benchmarks, including the newly introduced CWEB dataset. Performance is evaluated using the F0.5 metric calculated by ERRANT, with scores averaged across individual annotators to ensure fair comparison across datasets with varying numbers of annotators. The systems are pre-trained on synthetic errors and fine-tuned on learner data.\nTo analyze the Impact of Error Density, the proportion of erroneous sentences in each dataset is varied by either removing correct sentences or adding correct sentences of the same domain. This allows for observing how system precision is affected by covariate shift across domains while fixing error frequency. Precision is plotted as a function of the proportion of erroneous sentences for selected datasets (FCE, GMEG Wiki, CWEB-G) for both GEC-PSEUDODATA and PIE systems.\nAn Analysis of Gold Edits is performed to understand how errors and corrections differ across domains. This analysis is limited to sentences containing exactly one edit. Two factors are examined:\n1.  Semantic Change: Measured by calculating the cosine similarity between the original sentence and its corrected counterpart using sentence embeddings generated by Sentence-BERT.\n2.  Sentence Improvement: Calculated as the ratio of the perplexity of GPT-2 on a sentence after and before it has been edited. A lower perplexity ratio indicates improvement. This analysis is performed at both the corpus level (average semantic similarity and perplexity ratio per dataset) and the error type level (comparing FCE and CWEB-S for specific error types like PUNCT, VERB, OTHER, DET, NOUN, PREP, SPELL).\nFinally, the Language Model Importance is investigated by evaluating the performance of a purely language model based GEC system. This system, building on previous work, uses GPT-2 to generate alternative versions of an input sentence and decides on corrections based on language model probabilities. Hyperparameters (specifically a threshold, τ, for probability improvement) are tuned for each dataset. The performance (Precision, Recall, F0.5) of this language model based system is then compared across different datasets, particularly noting its lower scores on low error density domains like CWEB and AESW, and analyzing false positive examples where perplexity alone cannot differentiate correct rare sequences from erroneous ones.": 1133,
    "The paper ensures stable training for minimizing domain discrepancy by employing an alternating optimization strategy for the auxiliary classifier (f') and the feature extractor (ψ). In the overall minimax objective, `min f,ψ [LS + max f' [DT - γDS]]`, the optimization proceeds in two main phases. First, the inner maximization problem, `max f' [DT(f',f) - γDS(f',f)]`, is solved to update the parameters of the auxiliary classifier, `θf'`. This involves calculating the discrepancy measures `DS(f',f)` (disparity on the source domain) and `DT(f',f)` (disparity on the target domain), and then updating `θf'` to maximize the difference between `DT` and `γDS`. Second, for the outer minimization, the parameters of the feature extractor, `θψ`, are updated by minimizing the same `DT - γDS` term. Crucially, during this step, the auxiliary classifier `f'` is frozen, meaning its parameters are not updated. This implies that the gradients for `ψ` are backpropagated through the fixed `f'`. This alternating update mechanism, where `f'` and `ψ` are optimized sequentially rather than simultaneously through a gradient reversal layer (a common technique in other adversarial domain adaptation methods), is explicitly stated as contributing to more stable training in practice.": 1134,
    "The paper addresses the challenge of efficiently learning optimal data augmentation policies by proposing BayesAugment, an alternative to the computationally expensive AutoAugment.\n1.  AutoAugment (Baseline for comparison): This method uses a recurrent neural network (RNN)-based controller. The controller has `n` output blocks, each generating distributions for three components of a sub-policy: negative perturbation type, positive perturbation type, and transformation probability. An adversarial policy is generated by sampling from these distributions. This policy is then applied to the input dataset to create adversarial samples, which are added to the original dataset to form an augmented dataset. A downstream task model (e.g., DistilRoBERTaBASE for faster training) is trained on this augmented dataset, and its F1 score on a validation set is fed back as a reward to train the RNN controller using REINFORCE (Sutton et al., 1999; Williams, 1992). The search space for a single sub-policy is defined as 5 (negative perturbations + none) * 3 (positive perturbations + none) * 6 (discretized probability bins) = 90, leading to a total search space of 90^n for `n` sub-policies.\n2.  BayesAugment (Proposed Efficient Method): This method frames the adversarial policy search as a hyperparameter optimization problem, leveraging Bayesian methods to perform the search. Instead of an RNN controller, BayesAugment uses a Gaussian Process (GP) (Rasmussen, 2003) as a surrogate function to approximate the objective function (F1 score on the validation set). An Upper Confidence Bound (UCB) (Srinivas et al., 2010) acquisition function is used to sample points (i.e., combinations of transformation probabilities) from areas where improvement is most likely. The prior belief about the objective function is updated with noisy observations (validation F1 scores) to get a better estimate of the posterior.\n    *   Simplified Policy Formulation: Unlike AutoAugment's discrete sub-policies, BayesAugment eliminates the explicit sampling of positive and negative adversaries. Instead, it optimizes the transformation probabilities for all possible combinations of adversaries over a continuous range [0,1]. For each input QA sample, one of these combinations is randomly sampled to generate adversaries.\n    *   Efficiency: By using a surrogate model and an acquisition function, BayesAugment drastically reduces the number of training iterations required to find optimal policies. It requires significantly fewer training loops (e.g., 100 iterations with 20 restarts) compared to AutoAugment (thousands of steps, 15,000+ samples), allowing the use of larger pretrained models like RoBERTaBASE directly in the training loop, and consuming only about 10% of the GPU resources.": 1135,
    "To address the distinct domain shifts inherent in individual modalities, the paper introduces a Multi-Modal Embedding Alignment loss, Lmm. This loss explicitly minimizes the Maximum Mean Discrepancy (MMD) between the feature embeddings of corresponding individual modalities (visual and textual) across the source and target domains. Specifically, Lmm is a weighted sum of the squared MMD between source and target visual features (fa_s and fa_t) and the squared MMD between source and target textual features (fb_s and fb_t). Trade-off parameters, γa and γb, are used to balance the contribution of visual and textual modality alignment to the overall loss. This separate alignment ensures that the feature extractors for each modality independently reduce their respective domain differences, which is crucial given that visual and textual domain shifts often differ in complexity.": 1136,
    "The paper addresses this by integrating the Contextual Modulator's output with a linguistic pipeline that processes the full sentence. The linguistic pipeline first embeds the tokenized sentence S (words w1, ..., wn) into vector representations using GloVe embeddings. These word embeddings are then fed into an LSTM sequence model, which acts as a sentence encoder, producing a sequence of hidden states h1, ..., hn. Each hi represents the contextualized features of the i-th word in the sentence. The core integration happens in the AfﬁneTrans layer. This layer takes the hidden states hi from the LSTM encoder and applies the γ(cvn) (scaling) and β(cvn) (shifting) parameters generated by the Contextual Modulator. The feature-wise linear modulation is performed as f(hi, cvn) = γ(cvn) ⊙ hi + β(cvn), where ⊙ denotes element-wise multiplication. This operation dynamically scales and shifts each word's encoded representation based on the contextualized features of the targeted expression. This mechanism explicitly models the interaction between the metaphor components (captured by cvn) and the wider sentence context (captured by hi), allowing the model to adaptively highlight relevant features for discerning metaphoricity. Optionally, an attention layer can be applied to the output of the AfﬁneTrans layer to further focus on important elements before the final prediction.": 1137,
    "To project target-domain word embeddings onto the embedding space of a pre-trained NMT model while maintaining semantic consistency and addressing topological differences, the paper employs Locally Linear Mapping (LLM) (Sakuma and Yoshinaga, 2019). LLM is designed to learn a projection that preserves the local topology, or positional relationships, of the original embeddings after mapping, while being robust to global topological differences that may exist between domains and tasks. This property is crucial because local semantic relationships are expected to be consistent across domains, even if their overall semantic spaces differ significantly. The LLM process involves several steps:\n1.  For each word `wi` in the target-domain vocabulary (denoted as `V_T`), its `k`-nearest neighbors, `N(wi)`, are identified within the set of words shared across both domains (`V_shared = V_T ∩ V_S`) in the target-domain embedding space (`T_LM`). Cosine similarity is used as the metric for determining nearest neighbors.\n2.  The local topology around `wi` in `T_LM` is then learned by reconstructing `T_LM_wi` from the embeddings of its `k`-nearest neighbors as a weighted average. This is achieved by minimizing the objective function: `||T_LM_wi - Σ_wj∈N(wi) α_ij * T_LM_wj||^2`, subject to the constraint that the sum of weights `Σ_j α_ij` equals 1. The optimal weights `α_ij` are derived analytically.\n3.  Finally, the projected embedding `T_NMT_wi` in the source-domain embedding space (`S_NMT`) is computed. This is done by applying the previously learned weights `α_ij` to the corresponding nearest neighbors in the `S_NMT` space. The optimization problem for `T_NMT_wi` is given by `||T_NMT_wi - Σ_wj∈N(wi) α_ij * S_NMT_wj||^2`, which has a trivial analytical solution: `T_NMT_wi = Σ_wj∈N(wi) α_ij * S_NMT_wj`.\nThis methodology allows for shared subwords across domains to have distinct embeddings after projection (`T_NMT_w ≠ S_NMT_w`), thereby effectively capturing and adapting to semantic shifts that occur across different domains.": 1138,
    "The privacy attacker component is designed to infer user private attributes from the transferred representations. During training, the attacker is optimized to accurately predict these attributes using a multi-task loss function. Simultaneously, the recommender is trained to minimize this loss, thereby making it difficult for the attacker to infer private information. This adversarial training process involves adjusting the recommender's parameters to maximize the attacker's loss while minimizing the recommendation error. By incorporating this simulation, the model prepares for unseen attacks on user privacy in the future.": 1139,
    "After supervised pre-training, the DS and US are fine-tuned using reinforcement learning (RL) through self-play interactions. The paper investigates two reward designs:\n1.  Dialogue-Level Reward: This common approach assigns a positive reward (1.0) only at the end of a dialogue if the task is successful. A small penalty is applied at each turn to discourage overly lengthy dialogues. This reward is shared between both agents when updating the US jointly with the DS.\n2.  Turn-Level Reward: Recognizing the limitations of dialogue-level rewards in complex multi-domain dialogues (difficulty in attributing rewards to specific actions), a novel fine-grained turn-level reward function is proposed. This reward operates at the individual turn level and is designed separately for the DS and US based on their characteristics:\n    *   DS Reward: A positive reward is assigned if the DS: 1) requests slots it has not requested before (to refine search); 2) successfully provides an entity; or 3) correctly answers all additional attributes requested by the user. Otherwise, a negative reward (penalty) is given.\n    *   US Reward: A positive reward is assigned if the US: 1) provides new information about slots (to avoid repetition); 2) asks new attributes about a certain entity; or 3) replies correctly to a request from the DS. Otherwise, a penalty is given.\nOptimization is performed using the Policy Gradient Theorem (Sutton et al., 2000). For each dialogue turn, a reward (r*t, where * denotes ds or us) is assigned to the agents at the final step of their generated act sequence. The return for an action at the i-th step is R*i, calculated with a discounting factor γ. The policy gradient (∇θ*J*(θ*)) is computed as the sum of R*i multiplied by the gradient of the log probability of the act token at each step. Both agents are updated using this policy gradient at each turn within the simulated dialogue, allowing them to explore new, potentially better policies beyond strategies learned from the fixed corpus.": 1140,
    "Multimodal fusion methods are employed to integrate text and image features for enhanced metaphor detection. The framework uses various fusion techniques, including concatenation, element-wise multiply, and element-wise add. Specifically, the concatenation method merges text and image features into a single vector, which is then fed into a fully connected layer. This approach maximizes the complementarity between different modal data, eliminating noise and improving the detection effect. The fusion of text and image features is crucial for accurately identifying metaphors, especially those that require the interplay of both modalities.": 1141,
    "The paper identifies three crucial types of contexts for verb metaphor detection:\n1.  Global context: The entire sentence, which provides overall meaning and topic. Its contextual representation (cCLS) is obtained from the final hidden state of the `[CLS]` token output by the BERT (Bidirectional Encoder Representations from Transformers) model.\n2.  Local context: Words grammatically related to the target verb, specifically its nominal subject (csubj) and direct object (cobj). These are extracted using a Stanford dependency parser (Chen and Manning, 2014) to identify verb-subject and verb-direct object relations. To explicitly mark these components within the input sequence for BERT, special marker tokens like `[subj]`, `[/subj]`, `[verb]`, `[/verb]`, `[obj]`, and `[/obj]` are introduced around the respective words. The contextual representations for these local contexts are derived from the hidden states of the final transformer layer of BERT. Specifically, either the hidden state of the start marker (e.g., `[subj]`) or the averaged embedding between the start and end markers (e.g., `[subj]` and `[/subj]`) is used as the component representation.\n3.  Distant context: The basic meaning of the target verb itself, motivated by the Metaphor Identification Procedure (MIP) theory which compares contextual usage to basic meaning. This context-independent verb representation (vbsc) is obtained from the output of the BERT tokenizer. If the verb is split into word pieces by the tokenizer, their averaged embedding is used to form the basic meaning representation.\nThese distinct context components are then used to form contextual relations with the target verb.": 1142,
    "To match the joint distributions between a trained source model and unlabeled target data without direct source data access, the paper introduces Joint Kernelized Stein Discrepancy (JKSD). JKSD measures the discrepancy between a known distribution P (representing the source model's output) and an empirical distribution Q (derived from the target data). Unlike traditional methods like JMMD that require source data, JKSD leverages the Stein operator's property that its expectation over the true distribution P is zero, allowing discrepancy computation without P's samples. Specifically, for target samples (x,y) from Q, the JKSD is empirically estimated using a kernel trick. This estimation involves terms like ∇²K, Υ, and Ω, which are matrices derived from kernel functions k(xi,xj) and l(yi,yj), and crucially, the gradient of the log-probability of the source distribution ∇x logP(x,y). The JKSD is evaluated on latent representations (z,y) where z = Tt(x) and y = Gt(z) are the feature and pseudo-label outputs of the target model. The joint probability P(Y=y, Z=z) is approximated as p(y|z)p(z), where p(y|z) = yᵀGs(z) (using the source classifier Gs) and p(z) is approximated by the cosine similarity between the target representation z and the source model's representation Ts(x) for the corresponding input x. This allows the computation of ∇z logP(z,y), which is essential for the JKSD calculation, integrating activation-based and gradient-based features from the source model.": 1143,
    "The paper introduces two novel explicit self-supervised strategies: Type-Specific Masked Term Discrimination (TSMTD) and Pairwise Relations Discrimination (PRD).\nFor Type-Specific Masked Term Discrimination (TSMTD), the model is trained to identify the type of a masked term. In this task, aspects, opinions, or non-aspect/non-opinion terms are uniformly replaced with a special `[MASK]` token in the input sequence. The masked sequence, `Xtsmtd`, is fed into the pre-trained language model. The output representation of the `[CLS]` token from the language model is then used by a linear projection layer (`W3h[CLS] + b3`) to classify the masked term's type as \"\"Aspect,\"\" \"\"Opinion,\"\" or \"\"O\"\" (other). This forces the model to explicitly learn and discriminate between different term types based on their context.\nFor Pairwise Relations Discrimination (PRD), the model learns to determine if two masked tokens represent a true aspect-opinion pair. In this task, both an aspect term and an opinion term in a sentence are uniformly replaced with a special `[REL]` token. The modified input sequence, `Xprd`, is processed by the pre-trained language model. Similar to TSMTD, the `[CLS]` token's output representation is used by a linear projection layer (`W4h[CLS] + b4`) to classify whether the replaced tokens have a pairwise relation (True or False). Negative sampling is employed to generate false pairs, ensuring the model learns to distinguish between genuine and non-genuine aspect-opinion relationships. This strategy explicitly exploits the relations between aspect and opinion terms at a sentence level.": 1144,
    "To explicitly optimize the learning of the transformation matrix M and ensure the transformed query representations align well with actual quotations, the paper introduces an auxiliary objective called the mapping loss (`L_map`). This loss function is designed to minimize the Euclidean distance (L2 norm) between the transformed query turn representation (`M r_nc`) and the representation of its corresponding ground-truth quotation (`r_q_qc`). Specifically, `L_map` is calculated as the sum of `||M r_nc - r_q_qc||^2` over all conversations `c` in the training corpus. This `L_map` is then combined with the primary recommendation loss (`L_rec`), which is a standard cross-entropy loss calculated over the whole training corpus, aiming to maximize the probability of recommending the correct quotation. The final objective function `L` for training the model is a weighted sum of these two losses: `L = L_rec + λ · L_map`, where `λ` is a hyperparameter that controls the contribution of the mapping loss. By incorporating `L_map`, the model is explicitly guided to learn a transformation M that effectively bridges the semantic gap, ensuring that transformed queries are semantically close to their relevant quotations.": 1145,
    "To investigate the influence of experiential factors, the paper collects detailed background information from annotators via a post-survey. This survey gathers data on three key identifier categories: Political Ideology (self-identified as left, center, or right, and affiliation with U.S. political parties), News Access (frequency of accessing news on the 2020 U.S. presidential election, categorized as news or non-news), and Reddit Familiarity (level of participation on Reddit in the past year, categorized as redditor or non-redditor, with redditors further subdivided into political and non-political based on familiarity with political subreddits). The paper then calculates Krippendorff’s α (Krippendorff, 2004) to evaluate annotator agreement within these defined groups. Additionally, McNemar’s chi-squared test is used for pairwise comparisons between annotator groups to examine significant differences in their judgments, grounding the comparison against semi-supervised labels. This allows for an analysis of how consistency and judgment patterns vary across annotators with different backgrounds, revealing that factors like Reddit familiarity and political alignment significantly impact agreement and judgment differences.": 1146,
    "To address the challenge of explicitly learning category presence without gold-standard labels, the paper introduces the Category Proxy Prediction (CPP) task, a novel pre-training task. This task operates on the principle of distant supervision. Similar to CMLM, CPP leverages the cosine similarity between input words and aspect category names. For a given input text, the model computes the cosine similarity between each word in the text and the static word embedding of every aspect category name (from both source and target domains). Instead of masking words, for each aspect category, the highest similarity score observed across all words in the input text is identified. A binary label vector is then constructed for the input text: for each coordinate corresponding to an aspect category, its value is set to 1 if its highest similarity score exceeds a predefined threshold hyper-parameter (β), and 0 otherwise. This binary vector serves as the proxy label for the input text, indicating the presence or absence of each category. The BERT model, after being fine-tuned with CMLM, is then further fine-tuned on this CPP task using a linear layer on top of the [CLS] token, followed by a sigmoid layer, and optimized with a cross-entropy loss. This process encourages the model to learn a representation that is explicitly informed by the presence of aspect categories within the text, even in the absence of direct supervision.": 1147,
    "The paper addresses this by introducing a Position-aware Global Memory Network. This network aims to capture long-range dependencies among occurrences of the same word within a discourse. Instead of recalculating hidden states, it records the hidden states `hi` produced by the baseline BERT model for each word `xi`. All hidden states of `xi`'s occurrences in the discourse are grouped into `G = {hm_1, hm_2, ..., hm_V}`, where `V` is the number of occurrences and `hm_j` is the hidden state of the `j`-th occurrence. For a target word `xi` at the `t`-th position in its group `G`, its own hidden state `hm_t` is removed from `G` to form `MPt`. To account for the relative position of occurrences, position embeddings (`pos_j`) are added to each `hm_j` in `MPt`, creating `hp_j`. A dot-product attention mechanism is then applied between the target word's hidden state `ei` (which is `hi` plus its own position embedding `pos_t`) and each `hp_j` in `MPt` to compute attention weights `αj`. These weights are used to compute a weighted sum of `hm_j` (the original hidden states of other occurrences) to form `ri`, the response of the global memory network. Finally, `hi` is used to update `G` by replacing `hm_t`, ensuring the memory is dynamic. This mechanism allows the model to leverage information from other instances of the same word across the entire discourse.": 1148,
    "The paper proposes a novel Domain-Mixed Model to effectively learn and integrate both domain-aware and domain-invariant features, leveraging the multiple types of training data (source domain and fine-grained target subdomains) obtained from the fine-grained domain adaptation. The model consists of two main components: a (Sub)Domain-Aware component and a (Sub)Domain-Invariant component, both built upon an ADBERT-BiLSTM-CRF architecture and sharing a common BiLSTM-CRF for feature extraction and label inference.\n1.  (Sub)Domain-Aware Component: This component aims to learn features specific to each (sub)domain type. It uses a Parameter Generator Network (PGN) integrated into the adapter layers of ADBERT (referred to as PGN-ADBERT). The PGN takes the input sentence (X) and its corresponding (sub)domain type (dt) as input. It generates domain-aware parameters (V) for the adapters based on a learnable parameter (Θ) and the (sub)domain type embedding (edt). This allows the ADBERT to produce character representations (edm) that are specifically tailored to the input domain. These domain-aware representations are then fed into the shared BiLSTM-CRF for the joint CWS and POS tagging task.\n2.  (Sub)Domain-Invariant Component: This component focuses on learning generalizable features that are robust across different domains. It processes the character inputs (X) through a separate ADBERT to derive domain-invariant features (eiv). To ensure these features are truly domain-invariant, an adversarial learning approach is employed: sentence-level representations (v) are derived from eiv (via averaged pooling) and fed into a linear classifier that attempts to predict the (sub)domain type. The training objective for this component includes an adversarial loss (Ladv) that aims to \"\"cheat\"\" this classifier, making the domain-invariant features (eiv) indistinguishable across domains. Furthermore, this component attempts to reconstruct the domain-aware features (¯edm) from the domain-invariant features (eiv) and the (sub)domain type (dt) using a variational module with reparameterization. Specifically, BiAffine operations are used to generate a Gaussian distribution (defined by mean µi and variance σ2i) from eiv and edt, from which ¯edmi are sampled. These reconstructed domain-aware features (¯edm) are then also fed into the shared BiLSTM-CRF.\nThe model is trained using four optimization objectives:\n*   Lmajor: The loss for the (sub)domain-aware component's joint CWS and POS tagging predictions.\n*   Laux: The loss for the (sub)domain-invariant component's joint CWS and POS tagging predictions.\n*   Ladv: The adversarial loss for the (sub)domain type classification, encouraging domain-invariant features.\n*   Lmse: A Mean Squared Error loss that minimizes the distance between the domain-aware features (Edm) from the (sub)domain-aware component and the reconstructed domain-aware features (¯Edm) from the (sub)domain-invariant component, aligning the representations.\nThese four objectives are summed together with hyperparameters λ1 and λ2 (which are auto-adjusted during training) to form the final objective function. During inference, the (sub)domain-aware component is used, with the (sub)domain types of the source domain and the last fine-grained subdomain type (STn) used for decoding their respective domains.": 1149,
    "To address the scarcity of labeled data, the paper proposes a simple heuristic called Target-based Generating Strategy (TGS). This strategy operates on the principle that if a word serves as a detection target in a sentence, all other sentences containing this same word within a specified general corpus can serve as potential candidate instances. The process involves using the target words from the existing labeled dataset as \"\"heuristic seeds.\"\" For each such target word, the TGS retrieves all sentences containing that word from a large, general-purpose corpus, such as Wikipedia. These retrieved sentences form a large-scale candidate set, `U`. To manage data size and potential noise, sentences longer than 150 words are filtered out, and deduplication is performed. This strategy allows for the automatic collection of a diverse and large volume of unlabeled data without requiring manual annotation or complex predefined linguistic rules, thereby providing a substantial pool of instances for semi-supervised learning.": 1150,
    "To identify a broad range of non-humorous content beyond just posts with extremely low humor scores (which tend to be overly sad or extreme), the paper introduces a Non-Humor Score (NS). The NS is defined for posts whose reaction distributions show the lowest divergence from a \"\"standard Facebook post distribution.\"\" This standard distribution is first established by averaging the reaction distributions across the entire dataset of 785K cleaned posts, based on the assumption that the vast majority of Facebook posts are non-humorous. For any given post, its NS is then calculated as the negative\nlog of the mean-squared error between its observed reaction distribution and this averaged standard distribution. A higher NS indicates a lower divergence, meaning the post's reaction profile is closer to the typical non-humorous Facebook post. Similar to the HS, a `tanh` popularity multiplier is also applied to the NS to account for the reliability of reaction counts. The formula for NS is `NS = –log(tanh(t/50) * Σr∈R (S(r) − O(r))^2 / |R|)`. Here, `t` is the total number of reactions, `R` is the set of Facebook reactions, `S` maps a reaction to its percentage in the standard distribution, and `O` does the same for the observed post.": 1151,
    "The paper addresses the lack of detailed visual elements in input captions by augmenting them with external commonsense knowledge. This is achieved by first extracting relevant commonsense concepts for each frame. Nouns and verb words from all captions in a story are used to find ConceptNet triples that contain at least one of these words in their subject or object phrases. Pretrained GloVe embeddings are then used to identify a broader pool of related words, which helps in finding additional relevant triples. These collected triples are combined into a sub-graph for each frame, forming the commonsense knowledge input. This sub-graph is then encoded using a Graph Transformer, specifically the Transformer-based graph encoder from Graph Writer. The input graphs are converted into unlabeled connected bipartite graphs consisting of entities and relations, with an adjacency matrix describing directed edges. These entities and relations are projected into a dense, continuous embedding space and processed by L stacked Transformer blocks, each comprising a multi-headed self-attention layer, normalization, and a two-layer feed-forward network. The resulting graph contextualized vertex encodings, specifically the entity encodings, are then appended to the output from the MARTT encoder and used in the alignment module for image generation.": 1152,
    "To improve the quality of synthetic data generated during unsupervised domain adaptation, the paper proposes consistency filters. These filters generalize the notion of filtering low-confidence predictions. For both Question Generation (QG) and Passage Retrieval (IR) tasks, a \"\"generator\"\" (G) produces synthetic training data, while a \"\"critic\"\" (C) filters low-confidence predictions.\nTwo types of consistency filtering are defined:\n1.  Self-consistency: In this approach, the generator and the critic are the same model. This means the model filters out its own low-confidence predictions from the synthetic data it generates. For example, if the QG model (θG) generates a question, it also evaluates its own confidence in that generation, and only high-confidence pairs are retained.\n2.  Cross-consistency: Here, the generator and the critic are different models. For instance, the Passage Retrieval model (θR) can act as a critic to filter synthetic data generated by the Question Generation model (θG), and vice-versa. This means that after θG generates a synthetic (passage, question) pair, θR evaluates the quality of that pair (e.g., how well the passage and question align according to θR's understanding), and only pairs deemed high-quality by θR are kept.\nThe confidence scores used for filtering depend on the critic model:\n*   For a QG model (θG) acting as a critic, the conditional log-likelihood, logPr(q|p;θG), is used as the confidence score. Higher log-likelihood indicates higher confidence.\n*   For an IR model (θR) acting as a critic, the dot product similarity between the encoded representations of the passage and question, EP(p)·EQ(q), is used as the confidence score. Higher similarity indicates higher confidence.\nThese filters are applied before training, and thresholds are set to accept a specified percentage (e.g., 75%) of the synthetic data, ensuring that only the most reliable pseudo-labeled examples are used for fine-tuning.": 1153,
    "To transform detected figurative language into literal forms, the paper utilizes a Dictionary Replacement method based on Wiktionary (Zesch et al., 2008). Wiktionary, a collaboratively created online dictionary, provides entries for phrases with \"\"idiomatic\"\" usages, which encompass conventionalized metaphors, idioms, euphemisms, and common similes. Each entry in Wiktionary lists the surface form of the figurative construct paired with a gloss. These glosses are primarily literal interpretations of the figurative construct. The paper processes these glosses by removing extraneous details like dialect or etymology using simple regex-based rules, allowing the cleaned gloss to be directly used as a literal interpretation in context. Furthermore, the system expands entries whose surface forms contain uninflected verb forms or unrealized pronouns (e.g., \"\"someone,\"\" \"\"one's\"\") by spawning new entries for each pronoun-inflection combination using the pyinflect Python library. This process yields a dictionary containing 17,743 tuples of the form {figi, Lit(figi)}, where figi is the figurative form and Lit(figi) is its literal equivalent. For each utterance (u) identified in Dfig auto, every matched occurrence of a figurative form (figi) within that utterance is replaced by its corresponding literal translation (Lit(figi)). This lightweight technique does not necessitate retraining of the dialog models, as it acts as a preprocessing step.": 1154,
    "The paper addresses the challenge of achieving a superior quality trade-off and an improved Pareto frontier by combining Elastic Weight Consolidation (EWC) with the data mixing strategy. EWC, a method for preserving model performance during sequential learning, adds a regularization term to the training objective. This term, `λ/2 * sum(Fi(θi - θ*A,i)^2)`, selectively slows down learning on weights (`θi`) that are important for the original generic task (`A`), represented by `θ*A,i` and the diagonal of the Fisher information matrix (`Fi`). The strength of this regularization is controlled by `λ`. The paper theoretically justifies the combination by showing that regularization in data space (data mixing) and parameter space (EWC) are complementary within a Bayesian formulation of continued learning. Specifically, the combined objective becomes `L' = LB(θ) + LA1(θ) + λ/2 * sum(Fi(θi - θ*A,i)^2)`, where `LB(θ)` is the loss for the new domain data (`B`), and `LA1(θ)` is the loss for a sampled subset (`A1`) of the original generic training data. This combined loss effectively integrates the benefits of both approaches: data mixing directly influences the gradients by providing samples from the generic domain, while EWC regularizes the parameter updates based on their importance to the original task. The hyperparameter `λ` from EWC then serves as a \"\"knob\"\" to regulate the performance trade-off, allowing for fine-grained control over the balance between generic and adapted domain performance, which was not reliably achieved by data mixing ratio alone. This synergistic combination is shown to outperform EWC alone and improve the Pareto frontier for multi-domain adaptation.": 1155,
    "The paper integrates multimodal knowledge by constructing a unified, time-dependent hidden representation (h(t)n) for each noun (n). This is achieved by combining three distinct types of knowledge:\n1.  Perceptual knowledge: Captured from the ImageNet database. For each noun, 64 images are sampled and encoded using the VGG-19 convolutional neural network. The output vector from the last fully connected layer is extracted, and these encoded images are averaged to form a mean vector (xp(n) ∈ R1000) representing the noun's perceptual features. This 1000-dimensional vector is then linearly transformed to 300 dimensions to match other modalities.\n2.  Conceptual knowledge: Extracted from the ConceptNet knowledge graph. A diachronic slice of the ConceptNet graph is prepared for each time (t) by filtering words based on their frequency in a historical text corpus. Embeddings for the remaining concepts are computed by performing Singular Value Decomposition (SVD) on the positive pointwise mutual information matrix of the ConceptNet graph. The top 300 dimensions of the term and context matrix are combined symmetrically, with each row serving as the conceptual embedding (xc(n)(t) ∈ R300) for its corresponding noun.\n3.  Linguistic knowledge: Obtained from HistWords diachronic word embeddings (x(t)l ∈ R300), which are pre-trained on the Google N-Grams English corpora, representing the linguistic meaning of each noun at decade t.\nThese three unimodal representations (perceptual, conceptual, linguistic) are then combined into a joint vector (xn ∈ R300) by taking their mean. Finally, an integration function (g: R300 → RM), parameterized by a three-layer feedforward neural network, is applied to this joint vector to produce the final multimodal word representation (h(t)n). This integration network has an output dimension of M=100. For models using fewer than three modalities, missing embeddings are replaced with zero vectors before computing the mean.": 1156,
    "The paper establishes explicit label dependency between source and target domain slots through a Label Confusion (LC) strategy. This strategy is applied during the training process of the source domain. Instead of using a strict one-hot encoding for labels, the original one-hot label is \"\"confused\"\" into a probability distribution that incorporates information from both the source and target domains. Given a slot value `r` and its corresponding source domain label `y`, the process unfolds as follows: First, the cosine similarities are calculated between the prototype of the source domain label `y` (denoted as `zy`) and the prototypes of all slot labels `j` belonging to the target domain `T` (denoted as `zj`). This yields a set of similarities `sj = cosine_similarity(zy, zj)`. Second, these similarities `sj` are then normalized using an L1 norm to create `Dtgt = norm({sj}j∈T)`. `Dtgt` represents a soft probability distribution over the slot labels in the target domain, indicating how similar the source label `y` is to each target domain label. Third, the `Dtgt` (target domain soft distribution) is then concatenated with the one-hot distribution of the source domain label `y` (denoted as `Dsrc = one_hot(y)`). This concatenation is weighted by a label confusion factor `λ`. The resulting smoothed label distribution is `Dsmooth = concat(λ · Dsrc, (1 − λ) · Dtgt)`. This `Dsmooth` effectively blends the precise knowledge of the source label with the inferred relationships to target domain labels. Finally, a KL-Divergence loss, `Lkl`, is computed between the predicted distribution `Dpre` (obtained by `log_softmax(Mproto · rk)`, where `Mproto` is the representation matrix of slot prototypes and `rk` is the slot entity representation) and the `Dsmooth` distribution. This `Lkl` loss encourages the model's predictions to align not just with the exact source label, but also with its related target domain labels, thereby establishing the desired cross-domain dependency. The final loss function for the model is a combination of the Prototypical Contrastive Learning loss (`Lpc`) and the Label Confusion loss (`Lkl`), specifically `L = Lpc + αLkl`, where `α` is a hyperparameter controlling their balance.": 1157,
    "The paper addresses this by proposing the Contrastive Domain Adaptation for Question Answering (CAQA) framework, which integrates a question generation (QG) model, a Question Answering (QA) model, and a novel contrastive adaptation loss. The framework operates in three main steps. First, a QG model, specifically QAGen-T5, is utilized to generate synthetic QA data from the unlabeled target domain contexts (X't). QAGen-T5 is built upon a text-to-text transfer transformer (T5) encoder-decoder and performs a two-step generation: generating a question (xq) from a context (xc), and then generating a corresponding answer (xa) conditioned on both the context and the generated question. Input formatting for QAGen-T5 involves prepending \"\"generate question:\"\" for question generation and using \"\"question:\"\" and \"\"context:\"\" tokens for answer generation. Both T5 models within QAGen-T5 are fine-tuned separately on SQuAD by minimizing negative log likelihood. Quality control is applied via LM-filtering to select the best k QA pairs per context (e.g., k=5). Second, the QA model, which is BERT-QA (consisting of a BERT-encoder and an answer classifier), is trained using both the labeled source domain data (Xs) and the newly generated synthetic data from the target domain (Xt). Third, during the training of the QA model, the proposed contrastive adaptation loss (Lcon) is incorporated into the overall optimization objective. This loss, as detailed in Solution 1, helps the BERT-QA model learn domain-invariant features by reducing intra-class discrepancy and increasing inter-class discrepancy. By applying this loss during training on a mix of source and synthetic target data, the model is encouraged to generalize better to the target domain, even with limited synthetic data. The overall objective function for the QA model is Lqa(X) = Lce(X) + βLcon(X).": 1158,
    "The paper addresses this by designing a Multi-task Meta-learner Training (MMT) process that focuses on capturing transferable knowledge and maintaining task-agnosticism. The meta-learner's architecture resembles MT-DNN, featuring a shared PLM-based encoder and task-specific output layers for each task. The parameters of the underlying PLM encoder are initialized from pre-trained models. A meta controller manages the training process by ensuring that each data instance xi,j is selected with a probability p(xi,j) = 1 / (K * |Di|), which means each task Ti is selected with an equal probability p(Ti) = 1/K. This uniform sampling ensures that each task has an equal opportunity to be learned.\nThe training objective incorporates two main components. First, a weighted cross-entropy loss, LCE(xi,j), is used: LCE(xi,j) = - sum(1(ci,j=c) * mi,j * log(τc(xi,j))) over c in Ci. Here, 1(·) is an indicator function, and τc(xi,j) is the predicted probability of xi,j for class c. This loss ensures that each sample xi,j is weighted by its meta-knowledge score mi,j, giving larger weights to more transferable instances during training.\nSecond, to counteract potential bias towards highly similar tasks (e.g., T1 and T2 being more similar than T3, leading the meta-learner to be biased towards T1 and T2), Weighted Maximum Entropy Regularizers (WMERs) are proposed as an auxiliary loss, LME(xi,j): LME(xi,j) = - sum((mi,j / |Ci|) * log(τc(xi,j))) over c in Ci. This regularizer penalizes the meta-learner for overfitting to specific tasks by comparing the predicted probability distribution of xi,j against a uniform distribution (1/|Ci|, ..., 1/|Ci|), weighted by mi,j. This mechanism helps to generate a more task-agnostic and unbiased model.\nFinally, the total sample-wise loss L(xi,j) is derived by combining these two losses: L(xi,j) = - sum(mi,j * (1(ci,j=c) + λ / |Ci|) * log(τc(xi,j))) over c in Ci, where λ is a pre-defined balancing factor between the two loss components. This combined loss ensures that transferable instances are prioritized while simultaneously promoting task-agnosticism. The training process avoids second-order update steps typical of MAML due to computational expense with large-scale PLMs and the absence of meta-testing steps, making the algorithm efficient for multi-task NLP. After MMT, the meta-learner M is obtained, which is then fine-tuned in the Task-specific Model Fine-tuning (TMF) stage for each individual task. During TMF, the meta-knowledge and WMERs are removed from the loss function, and the model is fine-tuned using the standard cross-entropy loss on the task-specific training sets.": 1159,
    "The reinforced data selector (fθ) is guided to enhance instance selection through a combination of improved state representations and a novel dual-reward mechanism. The selector operates as an agent within a Markov Decision Process, making binary decisions (drop or keep) for each source instance in a batch. Its policy (πθ) is informed by state representations (Se_b) derived from two sources: semantic features generated by the shared encoder of the Transfer Learning (TL) module and prediction results from the target classifier. The improvement in these state representations stems from the TL module's ability to learn more stable and domain-invariant features due to its adversarial training with the Wasserstein discriminator. The selector's objective is to maximize an expected total reward (J(θ)), which is a weighted sum of two distinct reward signals. The first is a novel immediate reward (Rimm), which is Wasserstein-based and provided by the discriminator. This reward directly quantifies the domain proximity of selected source instances to the target domain, encouraging the selector to choose instances that are semantically \"\"closer.\"\" Specifically, `Rimm` is calculated as the difference between the averaged distance of all source and target data (`¯Lwd(xe*,xt*)`) and the distance of a specific selected source instance to its nearest target counterpart (`Lwd(xe_i,xt*)`). The second is a delayed reward (Rdelay), which reflects the performance improvement of the TL module after its updates. This is computed as the difference between the evaluation metric (e.g., accuracy or correlation) of the updated TL model and its previous performance. The total reward (`Rtot = αRimm + Rdelay`) balances these two signals, with `α` controlling their relative contributions. The selector's parameters are then updated using a policy gradient algorithm, leveraging these comprehensive reward signals to learn an optimal data selection policy that is both efficient and effective.": 1160,
    "Gloss-based interpretations are integrated into a joint model to enhance metaphor detection by combining the contextual representation of a word with an integrated representation of its glosses. First, a Context Encoder, which is a fine-tuned BERT model, processes the input sentence to generate a contextual representation (hi) for the target word (wi). This is done by feeding the token sequence (\"\"[CLS] w0 w1 ... wi ... wn-1 [SEP]\"\") into BERT and taking the corresponding vector for wi. Simultaneously, the Gloss Encoder (as described in Solution 1) generates representations for all candidate glosses (pj) for wi, and an attention mechanism computes weights (αj) for each gloss. An integrated gloss representation (p*i) is then computed as the weighted sum of all gloss representations (pj), where the weights are the attention probabilities (αj). This integrated gloss representation (p*i) is then concatenated with the contextual representation (hi) of the target word to form a joint representation (qi = [hi; p*i]). This joint representation is then passed through a fully-connected layer followed by a softmax classifier to predict whether the target word (wi) is metaphorical or literal, yielding a probability distribution (li). The entire model, named MDGI-Joint (Metaphor Detection and Gloss-based Interpretation Joint model), is trained by minimizing a combined loss function. This combined loss is the sum of the binary cross-entropy loss for the metaphor detection task (lossMD) and a cross-entropy loss for the metaphor interpretation task (lossMI), where the interpretation loss is only computed for words in a predefined candidate set (C) that require interpretation. This joint training allows the interpretation task to inform and improve the detection task.": 1161,
    "This question is addressed in the \"\"Additional Pre-training of PTLMs\"\" stage. The entire Pre-trained Language Model (PTLM) is trained using the training data of the downstream task, but with a modified loss function. The loss function combines the standard language modeling loss, LLM(X), with an l2-norm regularization term: LLM(X) + (1/|R(X)|) * sum(||f(Exi) - Fxi||^2). The first term, LLM(X), helps alleviate catastrophic forgetting by maintaining the PTLM's general linguistic knowledge. The second term, the regularization, forces the PTLM's static word embeddings (Exi) to become \"\"close\"\" to the target domain-specific fastText embeddings (Fxi) obtained in the previous stage. A trainable linear mapping function, f(z) = LN(Wfz + bf), is introduced to transform the PTLM's dLM-dimensional embeddings to the dFT-dimensional space of fastText embeddings, where LN denotes Layer Normalization. The regularization is applied only to a subset of tokens R(X), which excludes stop words and subwords shorter than a minimum length configured in fastText, further refining the adaptation process.": 1162,
    "The Domain Generalization step aims to convert each source-domain review into a domain-independent representation. This process involves two main sub-steps:\n1.  Domain-Specific Attribute Extraction: The method first identifies domain-specific attributes, specifically aspect and opinion terms, for both the source and target domains. For the unlabeled target domain (DU), an unsupervised dependency relation-based method called Double Propagation (Qiu et al., 2011) is employed. This method iteratively extracts terms: it begins by using a sentiment lexicon to identify initial opinion terms, then expands this list using 'conj' (conjunction) relations. These expanded opinion terms serve as seed words to extract aspect terms by identifying words holding 'amod' (adjectival modifier) and 'nsubj' (nominal subject) relations to any seed word. The aspect term list is further expanded using 'nn' (noun compound modifier) relations. This iterative process continues until no new aspect or opinion terms are found. For the labeled source domain (DS), aspect terms are directly obtained from the existing annotations, and Double Propagation is also utilized to expand its opinion term list. After obtaining comprehensive aspect and opinion term lists for both domains, all terms that are common to both (domain-independent terms) are removed. This yields distinct source-specific aspect (As) and opinion (Os) term lists, and target-specific aspect (At) and opinion (Ot) term lists.\n2.  Masking Source-Specific Attributes: For each source-domain review, denoted as `xs`, the method checks if any sub-sequence within it corresponds to a term found in either the source-specific aspect list (As) or the source-specific opinion list (Os). If a match is found, each word in that identified sub-sequence is replaced with a special token, [MASK]. This masking process transforms the original source-domain review into a domain-independent review, denoted as `xm`. This `xm` serves as the generalized representation, ready for the next step of domain specification.": 1163,
    "The training strategy for the few-shot slot tagging MRC model is remarkably simple, involving fine-tuning an off-the-shelf BERT-based MRC model. Unlike typical few-shot learning methods that often employ two-phase training or meta-learning, this approach trains the model in \"\"one-go.\"\" The training data is constructed by merging data from resource-rich source domains with the few-shot support set from the target domain. The converted <question, answer, context> triples from different samples are shuffled into batches and fed into the MRC model. The model is trained using a standard cross-entropy loss between its span predictions and the ground truth start and end positions. This direct fine-tuning on a mixture of source and target domain data, without any meta-learning or additional engineered components, forces the model to generalize to a semantic space compatible with both domains, effectively leveraging the linguistic and world knowledge embedded in the pre-trained language model.": 1164,
    "The paper designs two primary tasks to evaluate language models: Idiomaticity Detection (Task 1) and Idiomaticity Representation (Task 2).\nTask 1, Idiomaticity Detection, assesses a model's ability to identify idiom usage and consists of two subtasks. Subtask A is a coarse-grained classification where examples are classified as either \"\"Idiomatic\"\" (label 0, including \"\"Idiomatic\"\" and \"\"Meta Usage\"\") or \"\"Non-Idiomatic\"\" (label 1, including \"\"Literal\"\" and \"\"Proper Noun\"\"). Subtask B is a fine-grained classification where the model must identify the specific meaning of the MWE. Since each MWE can have multiple meanings, this is converted into a binary classification problem: an input pair (example sentence + paraphrase) is labeled 1 if the paraphrase represents the correct meaning and 0 otherwise. Both subtasks are evaluated in zero-shot, one-shot, and few-shot settings to test generalization and sample efficiency, using Macro F1-score as the evaluation metric due to data imbalance.\nTask 2, Idiomaticity Representation, aims to measure how consistently a model captures semantic similarity between sentences containing MWEs and their compositional counterparts. For each example (E), the model generates similarity scores such that `sim(E, E→c) = 1` (where `E→c` is the example with the MWE replaced by its correct meaning's paraphrase) and `sim(E, E→i) = sim(E→c, E→i)` (where `E→i` is the example with the MWE replaced by an incorrect meaning's paraphrase). To prevent trivial solutions, development and test data from standard Semantic Text Similarity (STS) datasets (STS benchmark for English, ASSIN2 for Portuguese) are spliced in. This task is divided into Subtask A (using only pre-training) and Subtask B (allowing fine-tuning). Pre-training is defined as training on any task other than idiomatic STS, while fine-tuning includes training on STS data with potentially idiomatic MWEs. Spearman correlation coefficient is used as the evaluation metric.": 1165,
    "Once the sparse structure of the lottery subnetwork is identified from the source domain model, the adaptation to the target domain is performed by selectively fine-tuning only the parameters within this subnetwork. The process involves using the original source domain model parameters as the starting point. During adaptation, only the parameters corresponding to the identified lottery subnetwork are updated through training on the limited annotated target domain data (e.g., 1k examples at most). The remaining parameters, which were pruned or not part of the selected subnetwork, are frozen; they are not updated during the fine-tuning process. However, these frozen parameters still contribute to the model's predictions by participating in the forward computation. This approach allows the model to leverage the knowledge encoded in the full source domain model while efficiently adapting to the target domain by focusing parameter updates on a small, deliberately selected subset, thereby mitigating the challenges posed by limited target data and a large model size.": 1166,
    "The paper proposes the Unsupervised Domain Adaptation through Language Modeling (UDALM) method, which employs a multi-task learning approach during the final fine-tuning step. After the initial Domain Pretraining (DPT) phase, the model is fine-tuned using a mixed loss function that combines a classification loss and a Masked Language Model (MLM) loss. The classification loss, denoted as LCLF, is a cross-entropy loss calculated on labeled examples from the source domain (DS). This component drives the model to learn the specific downstream task (e.g., sentiment classification) using the available labeled data. Simultaneously, the auxiliary MLM loss, denoted as LMLM, is used to predict masked tokens for unlabeled examples from the target domain (DT). This MLM objective ensures that the model continues to adapt to the language of the target domain, preventing it from overfitting to the source domain's linguistic patterns. The mixed loss is defined as `L(s,t) = λLCLF(s) + (1 − λ)LMLM(t)`, where `s` represents labeled source samples and `t` represents unlabeled target samples. The weighting factor `λ` is dynamically determined by the ratio of labeled source data samples (`n`) to the sum of labeled source and unlabeled target data samples (`m`) within a batch, specifically `λ = n / (n + m)`. During training, source and target data are interleaved and fed to the BERT encoder in mixed batches. Features extracted from the source data's [CLS] token representation are used for classification via a single feed-forward layer, while target features are used for the MLM task. This simultaneous optimization allows the model to learn the task from the source domain while continuously adapting its language understanding to the target domain.": 1167,
    "The proposed unsupervised domain adaptation method involves transferring knowledge from a source domain, where labeled data is available, to a target domain, where only unlabeled data exists. This is achieved by aligning the contextualized embeddings from both domains, allowing the model to generalize better to new, unseen data. The adaptation process is fine-tuned to ensure that the domain-specific features captured by the embeddings are effectively utilized in the sentiment analysis task, leading to improved performance across various Arabic dialects and domains.": 1168,
    "The paper addresses this by employing multiple language models (LMs) as discriminators, each independently fine-tuned on a specific target style. For `k`-dimensional style transfer with target styles `s = {s1, s2, ..., sk}`, `k` separate LMs are fine-tuned. Each LM is initialized with the same pre-trained Transformer model used for the encoder-decoder. These LMs are fine-tuned using the Causal Language Modeling (CLM) objective, which trains the model to predict the next token given previous tokens. The training loss for an LM for target style `si` with corresponding corpus `Ti` is `E_x~Ti[Σ_t=1^n -log PLM(xt|x1,...,xt-1)]`. This fine-tuning process transforms the language distribution of each LM to reflect its respective style `si`. These fine-tuned LMs then serve as \"\"soft\"\" discriminators. The core idea is that if a transferred sentence `x'` does not fit well into a target style `si`, the LM fine-tuned on `si` will assign a high perplexity to it, and conversely, a low perplexity if it fits well. This implicitly provides a measure of style adherence without requiring adversarial training. To integrate this feedback into the encoder-decoder model, a policy gradient reinforcement learning approach, specifically the REINFORCE algorithm (Sutton et al., 1999), is used. The reward `r(x)` for an input sequence `x` to a style discriminator `LMi` is calculated as the sum of log probabilities of its tokens: `r(x) = Σ_t=1^n log PLMi(xt|x1,..,xt-1)`. The objective for each target style `si` is to minimize `Lsi = E_x~T,x'~PθG(x)[-logPθG(x'|˜x)] * (r(x') - r(x))`, where `r(x')` is the reward for the transferred sentence `x'` and `r(x)` is a baseline reward from greedily sampling the input sequence `x`. This mechanism allows the discriminators to guide the generation towards specific styles by rewarding style adherence in the transferred sentences.": 1169,
    "The paper investigates this by employing Domain-Adaptive Pre-Training (DAPT). This method leverages a large, unlabeled corpus that is specifically related to the target domain. The pre-training process continues with the BART model, using its original pre-training objective function. This objective involves corrupting input documents and then optimizing a reconstruction loss, which is the cross-entropy between the decoder's output and the original, uncorrupted document. The core idea behind DAPT is to introduce domain-specific knowledge into the pre-trained language model, thereby enabling its rapid adaptation to the target domains for summarization. The unlabeled corpora for DAPT are collected from various sources corresponding to the six target domains (Dialog, Email, Movie Review, Debate, Social Media, Science).": 1170,
    "The generalization of metaphorical knowledge across languages is evaluated using a cross-lingual setup with the XLM-R pre-trained language model. The LCC dataset, which provides annotations in English, Farsi, Spanish, and Russian, is utilized for this experiment. To ensure fair comparison, the training sets for all languages are subsampled to have the same number of examples (12,238, the size of the Russian training set). For each language pair (source language for training, target language for testing), two experiments are run: one with the pre-trained XLM-R and another with a randomly initialized version of XLM-R, serving as a baseline. The edge probing architecture, as described in the general probing setup, is applied. By comparing the performance of the pre-trained model against the random baseline, and by observing the transferability from a source language to a target language (e.g., training on Spanish and testing on English), the extent of cross-lingual generalization of metaphorical knowledge encoded in XLM-R is assessed. The analysis also considers factors like target domain distribution similarities and the amount of pre-training data for each language.": 1171,
    "To learn domain-invariant representations of the [MASK] token, the paper introduces a novel domain adversarial training strategy. This strategy aims to reduce the distributional differences of the [MASK] token's features across domains, thereby encouraging the Masked Language Model (MLM) head classifier to extract features that are invariant to domain shifts. For each source domain `Sl` (where `l` ranges from 1 to `m`) and the target domain `T`, a dedicated domain discriminative function `gl` is employed. These `m` domain discriminators, collectively denoted as `g`, are tasked with distinguishing whether a given input example originates from a source domain or the target domain, based on the hidden representation `h[MASK]` produced by the Pre-trained Language Model (PLM). The domain discriminators are trained to minimize the domain discrimination loss `Ldomain`, which is a cross-entropy loss. Conversely, the parameters of the PLM and the soft prompt vectors are trained to maximize this same `Ldomain`. This creates a two-player minimax game: the discriminators try to identify the domain, while the PLM and soft prompts try to generate domain-agnostic features that fool the discriminators, thereby learning domain-invariant knowledge specifically for the [MASK] token's position.": 1172,
    "The paper investigates Transformer's internal mechanisms by analyzing attention patterns and hidden states. For encoder self-attention, it measures attention weights in three ways: \"\"PIE-to-PIE\"\" (incoming weights to a PIE noun from other PIE tokens), \"\"PIE-to-context\"\" (attention from PIE tokens to surrounding context nouns), and \"\"Context-to-PIE\"\" (attention from context to PIE nouns). These patterns are compared between figurative PIEs that are paraphrased ('fig-par') and literal PIEs translated word-for-word ('lit-wfw'), as well as all figurative ('fig') versus all literal ('lit') instances. For cross-attention, translations are decoded and aligned with source tokens using the eflomal toolkit. Attention weights are then measured from target-side tokens aligned to a PIE noun to that source PIE noun, to other source PIE tokens, and to the </s> token. For hidden representations, Canonical Correlation Analysis (CCA) is applied to compare representations from adjacent layers for PIE and non-PIE nouns, assessing how much they change across layers. Additionally, the influence of tokens is measured by masking a token in the attention mechanism and computing the CCA similarity of the hidden representations of its neighboring tokens, comparing the original and masked setups. This is done by masking PIE tokens to observe their impact on other PIE tokens and context tokens, and by masking context tokens to observe their impact on PIE tokens and other non-PIE tokens.": 1173,
    "To evaluate proverb recommendation and alignment prediction, the paper fine-tunes various large language models (LLMs) such as BERT, RoBERTa, DistilBERT, ALBERT, Sentence-BERT, BART, and T5. For proverb prediction, models are trained to retrieve the correct proverb for a given narrative from a set of 250 proverbs. This is tested in two settings: \"\"seen proverbs,\"\" where proverbs in the train and test sets are the same (narratives are split 6:4), and \"\"unseen proverbs,\"\" where proverbs in the train and test sets are disjoint (150 for training, 100 for testing). Performance is measured by accuracy and Mean Reciprocal Rank (MRR). For alignment prediction, the task is to predict an aligned span from the narrative given the narrative, the proverb, and a span from the proverb. This is framed as a span prediction problem, similar to answer span prediction in SQuAD, where the language model outputs two probability distributions corresponding to the start and end positions of a span over the narrative tokens. Token-level precision, recall, and F1 scores are used for evaluation. The paper also explores predicting proverbs and alignment jointly through multi-task learning, where the language model's parameters are shared across both tasks. A pipelined baseline is also considered, which first predicts the proverb and then attempts span prediction only if the correct proverb was identified.": 1174,
    "The contingency measure captures the unexpected frequency with which a set of words co-occurs in a phrase. This is achieved using a generalization of pointwise mutual information (PMI), which extends to sets of more than two events. Conditional probabilities from XLNet, an auto-regressive language model, are used to estimate the joint probability of words in the phrase. Marginal probabilities are obtained by masking out individual words, allowing the calculation of the PMI score. This method evaluates the strength of association between words in idiomatic phrases.": 1175,
    "High-quality, manually crafted gold standard pairs are created by expert annotators. For idioms, figurative sentences are collected from the MAGPIE dataset, and for metaphors, from the VUA Metaphor Corpus, Mohammad et al. (2016) dataset, and annotated instances from the Gutenberg poetry corpus. Annotators are given a figurative sentence and the focus of the figurative expression (the IE for idioms, or the focus word for metaphors). They are instructed to rewrite the sentence literally by removing or rephrasing the figurative component, which yields gold standard paraphrases for entailment pairs. To create non-entailed hypotheses, annotators are asked to write sentences that keep as much of the original utterance as possible while removing the main figurative element. For idioms, this often involves adding or adjusting words to force a literal reading of the idiom (e.g., \"\"kicked the bucket\"\" to \"\"kicked the bucket on the right\"\"). For metaphors, it typically involves adapting the sentence to have a different, non-metaphoric meaning while retaining similar phrasing (e.g., \"\"The gun kicked back into my shoulder\"\" to \"\"The mule kicked back into my shoulder\"\"). Additionally, for idioms, the technique of replacing keywords in the manually elicited definitions with their \"\"antonyms\"\" is replicated to yield non-entailing pairs that negate the original figurative meaning. These manual annotations are not restricted to individual words or phrases, allowing for more diverse and creative pairs, and are written by experts to ensure higher quality compared to automatic annotations.": 1176,
    "To obtain high-quality and robust results from predictions, the paper employs a Pattern Ensemble (PE) approach. This method addresses the observation that words predicted by the Masked Language Model (MLM) can have preferences for different patterns, making a single pattern insufficient. The paper designs four classes of patterns to capture various relationships within a simile triple (tenor, attribute, vehicle):\n1.  Class I: Models the relationship between all three elements (T, A, V), e.g., \"\"The {tenor} is as {attribute} as {vehicle}.\"\"\n2.  Class II: Focuses on the relationship between the vehicle and attribute (V, A), e.g., \"\"The {attribute} {vehicle}.\"\" This class is designed to find attributes salient to the vehicle.\n3.  Class III: Focuses on the relationship between the tenor and attribute (T, A), e.g., \"\"The {attribute} {tenor}.\"\" This class is designed to find attributes salient to the tenor.\n4.  Class IV: Handles cases where the attribute is omitted, modeling the relationship between tenor and vehicle (T, V), e.g., \"\"{tenor} is similar to {vehicle}.\"\"\nFor each class, multiple patterns are designed (e.g., three patterns per class). The output distribution of the Pattern Ensemble, denoted as QPE, is formulated by averaging the logarithmic probabilities of the candidate words across all relevant patterns for a specific task. Specifically, QPE(w|P) = (1/|P|) * Σ_{p(τ)∈P} log(Q(w|p(τ))), where P is the set of patterns p(τ) for the specific task τ (Simile Interpretation or Simile Generation), and Q(w|p(τ)) is the probability distribution of word 'w' given pattern p(τ) from the MLM. Note that Class IV patterns are not used for Simile Interpretation (SI) as the attribute is missing, and Class III patterns are not used for Simile Generation (SG) due to the lack of a vehicle.": 1177,
    "The paper introduces the Global Information Fusion (GIF) block to selectively combine information from the acoustic (HA) and visual (HV) modalities, which are outputs from the MCA2 mechanism. The GIF block designs two distinct gates: an acoustic gate (ga) and a visual gate (gv). These gates are learnable parameters that control the amount of information transmitted by each modality. Specifically, ga and gv are computed by applying a linear transformation to the concatenation of the original intermediate textual representation (H) and its respective modality-infused representation (HA or HV). The final multimodal information fused representation (ˆH) is then obtained by adding the original textual representation (H) to the element-wise product of each gate (ga, gv) and its corresponding modality-infused representation (HA, HV). This gating mechanism allows for the selective inclusion of information relevant to the satirical language and actively prohibits any multimodal noise from seeping into the model.": 1178,
    "The proposed Substructure Distribution Projection (SUBDP) explicitly addresses the limitations of prior work that primarily relied on one-to-one alignments and hard tree projections.\n1.  Handling Many-to-One Alignments: Unlike traditional methods that discard information from many-to-one alignments, SUBDP's projection formulation naturally incorporates them. The word alignment matrix ˜A, generated by SimAlign, can inherently capture many-to-one alignments (e.g., multiple source tokens aligning to one target token, or vice versa). By applying the `add-dummy-position` operator (∆) and `row normalization` operator (NR) to ˜A and its transpose (˜Aᵀ), the method creates right-stochastic alignment matrices (As→t and At→s) that distribute the alignment probability across multiple linked words. This means if a source word aligns to multiple target words, its probability is distributed among them, and vice versa. When a source parse is a hard tree, SUBDP projects arcs involved in many-to-one alignments into *soft arc distributions* in the target language, allowing a target language word to have multiple heads with associated probabilities that sum to one.\n2.  Processing Soft Tree Inputs: While the primary evaluation uses hard trees from the English treebank, SUBDP is designed to be more general. It can take dependency arc or label *distributions* (i.e., soft trees) as input from the source language(s), rather than being restricted to hard trees. This flexibility allows for potential future applications where source parsers might output probabilistic parses, or where ensemble methods could generate soft source trees. The mathematical formulation of the projection (Equation 4 for arcs and the analogous one for labels) inherently works with probabilities, making it compatible with soft tree inputs.": 1179,
    "The paper proposes a knowledge-enhanced training objective to improve PLMs' ability to infer simile properties, building on the insight that topic and vehicle are crucial components. The core idea is to treat the property (p) as a relation between the topic (t) and vehicle (v), following conventional knowledge embedding (KE) methods.\nThe training procedure involves jointly optimizing two loss functions:\n1.  Knowledge Embedding (KE) Loss (LKE): This loss function is designed to incorporate simile knowledge (t, p, v) triplets into PLMs. Inspired by relational facts represented as triplets, the distance between the topic and vehicle in the embedding space is used to represent the plausibility of the property. The paper adopts the scoring function from TransE (Translating Embeddings for Modeling Multi-relational Data) to define the KE loss as a Mean Squared Error (MSE) loss: LKE = MSE(Et + Ep, Ev), where Et, Ep, and Ev are the representations of topic, property, and vehicle encoded by the PLMs. More advanced KE methods like TransH and TransD were also explored but TransE yielded the best results.\n2.  Masked Language Model (MLM) Loss (LMLM): This is the standard MLM objective inherited from PLMs, where the model is trained to recover masked tokens in a sentence. For this task, the property of each simile is replaced with the special token [MASK], and the model is asked to recover the original property.\nThe overall knowledge-enhanced objective (LOurs) is a joint optimization of these two losses: LOurs = αLKE + LMLM, where α is a hyperparameter used to balance the contribution of the two objective functions. This joint optimization integrates simile knowledge and language understanding into the PLMs, aiming to bridge the performance gap with humans in simile interpretation. The training data for fine-tuning with this objective is collected from the Standardized Project Gutenberg Corpus (SPGC), where similes are extracted via syntactic pattern matching, and simile components are automatically annotated using dependency parsing.": 1180,
    "The Question Value Estimator (QVE) is trained using a reinforcement learning (RL) approach, specifically the REINFORCE algorithm, to overcome the absence of direct supervision for question usefulness. The training objective is to optimize the QVE to assign higher values to synthetic questions that demonstrably improve target-domain Question Answering (QA) performance. The learning process is structured into outer and inner training iterations. In each outer iteration, a batch of synthetic QA examples, `D = {(cl, ql, al)}`, is sampled. The QVE (`eγ`) then estimates a value `vl` for each example. Based on these estimated values, a selection vector `S = (s1, s2, ..., sBo)` is generated, where `sl ∈ {0, 1}` indicates selection. This selection is modeled as sampling from a Bernoulli distribution with probability `vl` for each example, which encourages policy exploration. The selected subset of examples is then used to fine-tune a pretrained QA model (`fθ`). This fine-tuning occurs over multiple inner iterations, where smaller batches of selected samples update the QA model's parameters (`θ`) by minimizing a cross-entropy loss (`Lqa`). Following the QA model fine-tuning, a reward (`rqve`) is computed. This reward is defined as the Exact Match (EM) gain in QA performance on a small set of available target-domain human annotations (`Dt`), comparing the QA model's performance before (`fθ0`) and after (`fθ`) fine-tuning with the selected synthetic data. The QVE's parameters (`γ`) are subsequently updated using the REINFORCE algorithm to maximize this reward. The loss function `Lγ` is the negative expected reward: `Lγ = -E_S~πγ(·|D)[rqve]`, and its gradient `∇γLγ` is computed as `-E_S~πγ[rqve ∇γ log πγ(S|D)]`. To enhance training stability, the QA model is reset to its pretrained checkpoint (`θ0`) at the conclusion of each outer iteration, ensuring a consistent starting point for QA model updates. The pretrained Question Generation (QG) model remains fixed throughout this process. This direct, performance-driven feedback mechanism enables the QVE to learn to identify synthetic questions that are most beneficial for improving downstream QA performance.": 1181,
    "The paper proposes a Keyword Aggregation Algorithm (Algorithm 1) to generate a comprehensive and concise list of keywords per sample from multiple annotators. The process involves:\n1.  Reliability Score Calculation: For each annotator `wj`, a reliability score `Sj` is computed as the average of `(#keywordphrases - #averagetokensineachkeyword)` across all samples. A higher score indicates more comprehensive and concise keywords.\n2.  Anchor Selection: The annotator with the highest reliability score is chosen as the initial anchor worker `wa`.\n3.  Fuzzy Matching and Filtering: For each keyword phrase `Kz` from the anchor's annotation, fuzzy matching scores are calculated against all keyword phrases `Kp` from other annotators' annotations for the same sample. For each other annotator, the keyword phrase with the highest fuzzy matching score (above a minimum threshold of 60) is selected and added to a `Kfilter` list. This step filters keywords that are similar to the anchor's.\n4.  Average Fuzzy Matching Score (AVGa): The average fuzzy matching score between the anchor's keyword phrase `Kz` and all elements in the `Kfilter` list is calculated.\n5.  Second Anchor and Comparison: The annotator with the second-highest reliability score is chosen as a second anchor worker `wb`. Steps 3 and 4 are repeated to calculate `AVGb` for this second anchor.\n6.  Final Aggregation: If `AVGa` is greater than or equal to `AVGb`, the keyword phrases from the first anchor `wa` are appended to the `aggregated_keywords`. Otherwise, the keyword phrases from the second anchor `wb` are appended. If only one worker has keyword annotations, their annotations are used. Finally, duplications are removed from the `aggregated_keywords` list.": 1182,
    "The paper establishes a baseline model for context-situated pun generation, which also functions as a unified framework for both homographic and heterographic puns. The core approach involves a two-stage strategy using a T5-base model. The first stage is Pretraining on Non-Pun Text (T5PT). To enable the model to learn to incorporate words and their senses into generated sentences, a T5-base model is pretrained on BookCorpus. For each word `w` (from a heterographic pun pair to avoid polysemic ambiguity at this stage), 200 sentences containing `w` are mined from BookCorpus. Keywords are extracted from these sentences using RAKE to form a context `C`, excluding `w`. The input prompt for pretraining is \"\"generate a sentence that situates in {C}, using the word {w}, {w} means {Sw} and {w} means {Sw}\"\", with the target output being the original retrieved sentence. This stage aims to teach general word and sense incorporation. The second stage is Finetuning on Pun Data (T5FT). After pretraining, the T5 model is finetuned on the SemEval 2017 Task 7 dataset, which contains puns annotated with pun word pairs (`pw`, `aw`) and their sense information (`Spw`, `Saw`). Context `C` for finetuning is constructed by extracting keywords from the pun text. The input prompt for finetuning is \"\"generate a pun that situates in {C}, using the word {pw}, {pw} means {Spw} and {aw} means {Saw}\"\". This stage teaches the model pun structure and how to incorporate both word senses into the generated pun within the given context, using a unified prompt format for both homographic and heterographic puns.": 1183,
    "The paper evaluates transfer learning for spatial language understanding by focusing on Spatial Question Answering (SQA) and Spatial Role Labeling (SPRL). For SQA, a Pretrained Language Model (PLM) with classification layers is used as a baseline. This model is designed to answer challenging multi-hop spatial reasoning questions, supporting both Yes/No (YN) and Find Relations (FR) question types. The input to the PLM is the concatenation of the question and story, with the [CLS] token's output fed into a classification layer. Binary classifiers are employed for multi-label classification (e.g., FR questions with multiple valid answers), and a multi-class classifier for single-answer questions (e.g., YN questions). Model training utilizes the summation of cross-entropy losses, with inconsistent answers post-processed during inference. For SPRL, a pipeline model is implemented, comprising two distinct, separately trained modules: a Spatial Role Extraction (SRol) module and a Spatial Relation Extraction (SRel) module. The SRol module uses a PLM (BERT) to generate token representations, followed by a BIO (Begin, Inside, Outside) tagging layer to identify spatial entities (Trajector, Landmark) and spatial indicators. A Softmax layer selects these roles based on the highest probability, trained with Cross-Entropy loss. The SRel module also employs BERT and a classification layer. For each (Trajector, Spatial Indicator, Landmark) combination extracted by SRol, an input is constructed and passed to BERT, utilizing segment embeddings to denote spatial role positions. The [CLS] output is then fed to a one-layer Multi-Layer Perceptron (MLP) to predict the triplet's probability. An auxiliary task predicts the spatial type for each triplet via another multi-class classification layer on the same [CLS] token, trained with a joint loss function. The core transfer learning strategy involves further pre-training the BERT module within these SQA and SPRL models using the SPARTUN synthetic dataset, followed by fine-tuning on various target domain benchmarks. This approach aims to leverage SPARTUN's broad spatial coverage to improve performance, especially when target domain training data is limited.": 1184,
    "To maximize the quality and utility of the automatically generated datasets, the framework employs two key mechanisms: normalization and self-training. First, in the Dictionary Matching step, the retrieved phrases forming the pseudo-dictionary (˜V) undergo Normalization. This involves applying a set of predefined rules (e.g., removing punctuation, splitting composite mentions, excluding short or all-lowercase phrases, handling stop words, and refining entity boundaries using tools like AutoPhrase) to refine their spans and improve their quality. These rules are designed to align with annotation guidelines and inherent characteristics of different entity types. After normalization, every occurrence of a phrase from the refined pseudo-dictionary within the retrieved unlabeled sentences (˜Xtrain) is annotated to generate weak labels (˜Ytrain). If a phrase is associated with multiple entity types, label ambiguity is resolved probabilistically based on the phrase's observed frequency with each type. This results in a weakly labeled training dataset (˜Dtrain = {˜Xtrain, ˜Ytrain}). Second, to mitigate the noise and incompleteness inherent in these weak labels, a Self-training method (Liang et al., 2020) is utilized. A teacher model and a student model, both initialized with pre-trained language model weights (e.g., RoBERTa or BioBERT), are used. The teacher model is initially trained on the generated ˜Dtrain. It then re-annotates ˜Xtrain, and the student model is trained on this re-annotated corpus. The teacher model is periodically updated with the student model's weights (every `Tupdate` steps) in an iterative process until a maximum epoch is reached. The student model with the best validation F1 score is selected as the final NER model, effectively refining the noisy labels and improving model robustness.": 1185,
    "To evaluate machine learning models' generalization across distribution shifts, the paper simulates shifts in individual modalities (image or text) separately. For text-based distribution shifts, text extracted from memes is used to create topic clusters. First, Sentence-BERT (Reimers and Gurevych, 2019) is employed to compute feature vectors from the extracted text, leveraging its ability to provide good sentence embeddings. Then, a hierarchical clustering algorithm is applied to identify distinct topic clusters. In-distribution (ID) and out-of-distribution (OOD) evaluation scenarios are defined: in ID, train/validation/test data are from the same text topic cluster, while in OOD, train/validation data are from a different cluster than the test data. This results in one ID and two OOD evaluation sets. For visual categories, memes are manually labeled and grouped into four categories: Artistic (including clips-arts, illustrations, anime, hand drawings), Real (including manipulated photos, realistic CGI), Infographic, and Mixed. Two evaluation schemes are proposed: training on Real category memes and evaluating on Real (ID) and Artistic, Infographic, Mixed (OOD sets); and training on Artistic category memes and evaluating on Artistic (ID) and Real, Infographic, Mixed (OOD sets). The Infographic and Mixed categories are used only as test sets due to their small data size. This methodology is applied to FigMemes, Memotion2 (subtask B), and MAMI (subtask B) datasets to understand the effects of distribution shifts across different meme classification tasks.": 1186,
    "To address whether pretraining on automatically selected smaller subsets can achieve competitive performance, the paper sets up an experimental pipeline. First, OntoNotes 5.0 is designated as the target corpus, and a small sample from the training corpus of the CoNLL 2012 Shared Task is used as the representative corpus for the Cynical Data Selection (CynDS) process. The Pile dataset, a large 1250GB text corpus, serves as the general data source from which documents are selected. The extended document-level CynDS method is applied to the Pile to extract top-scoring documents at various percentages (0.5%, 1%, 2%, 5%). A RoBERTa-like encoder, based on a BERT-base model and using RoBERTa's pretraining objectives and settings, is then pretrained on these cynically selected subsets. For comparison, encoders are also pretrained on randomly sampled subsets of the Pile of the same percentages, and a manual baseline using 30GB from Wikipedia and BookCorpus. The performance of these pretrained encoders is evaluated using perplexity on the target domain (OntoNotes) and a held-out set of the Pile, as well as on a suite of 8 Edge Probing tasks, some of which are derived from OntoNotes. This comprehensive evaluation allows for direct comparison of performance, data efficiency, and computational cost.": 1187,
    "The LAPA framework addresses the challenge of parameter efficiency and overfitting in low-resource settings by strategically employing adapter modules and parameter freezing. Specifically, after the initial pre-training of the backbone PLM (BART), adapter layers are inserted into each transformer layer. An adapter layer is designed as a small, bottlenecked feed-forward network, formulated as `Adapter(zl) = WluReLU(Wldzl) + zl`, where `zl` is the input, `Wld` is the down-project matrix, and `Wlu` is the up-project matrix. This design ensures that the adapter layers themselves have a very small number of parameters compared to the full PLM.\nCrucially, during both the meta-training and fine-tuning stages, the vast majority of the pre-trained backbone PLM's parameters (θpre) are frozen and remain untrainable. Only the parameters of the newly inserted adapter layers (Φ) and the normalization layers are allowed to be updated. This \"\"parameter freezing\"\" mechanism, combined with the residual connection within the adapter layer, serves two key purposes: it retains the extensive prior knowledge embedded in the PLM, preventing \"\"negative transfer\"\" where new training might degrade previously learned capabilities, and it drastically reduces the number of trainable parameters. The paper states that LAPA achieves competitive performance with only 2% of the trainable parameters of the full PLM and 1% of the labeled data of the target task. This minimal parameter footprint directly mitigates the overfitting problem commonly encountered when fine-tuning large models on scarce data, making the approach highly efficient and effective for low-resource scenarios.": 1188,
    "The Prompt-based Secret Key method introduces an explicit secret key to restore access to the target domain. A randomly chosen discrete prompt `P = {P1,...,Pm}`, consisting of `m` tokens, is assigned as the secret key. When an authorized user wishes to access the target domain, this prompt `P` is concatenated as a prefix to the input sentence `x`, forming an authorized input `[P, x]`. The BERT-based feature extractor (`ψ`) then processes this concatenated input, and the hidden representation of the `[CLS]` token is used as the feature representation for classification by the feed-forward network (FFN).\nTo enable this recovery, the training objective is modified to `Lprompt = LCE + β·LDC([P,S],T) + λ·L'MMD(P,S,T)`. The `LCE` term remains the same, ensuring source domain performance. The `L'MMD` (modified Maximum Mean Discrepancy) loss is defined as `α·dP,S−min(c,dS,T)`. This term has two objectives: it aims to *close* the distance (`dP,S`) between the target+prompt domain (`P`) and the source domain (`S`), effectively transferring knowledge from the source to the target+prompt domain. Simultaneously, it continues to *enlarge* the distance (`dS,T`) between the source domain (`S`) and the original target domain (`T`) (without the key), maintaining the non-transferable property for unauthorized access. The `α` is a scaling hyperparameter. The `LDC` (Domain Classification) loss is also modified to `LDC([P,S],T)`, where `[P,S]` denotes the combined data distribution of the source and target+prompt domains. This trains the domain classifier to differentiate between this combined authorized domain and the unauthorized target domain. This setup ensures that without the prompt, the model performs poorly on target data, but with the prompt, the target+prompt domain behaves like the source domain, allowing for good performance.": 1189,
    "The paper addresses the challenge of generating harder and more efficient negative examples by proposing two novel in-domain negative sampling strategies: Random In-Domain (Random-ID) and Hard In-Domain (Hard-ID). These strategies are motivated by the hypothesis that negatives which are lexically similar to the golden entity, yet semantically different and close in the representation space, lead to better optimization. Unlike previous cross-domain sampling methods (Random-CD, Hard-CD) that disregard the golden entity's domain, the proposed methods explicitly leverage this information.\nIn Random In-Domain (Random-ID) sampling, negative entities for a given mention are randomly selected exclusively from the set of entities belonging to the *same domain* as the golden entity. This approach ensures that the sampled negatives share a contextual similarity with the golden entity, making them potentially harder to discriminate than randomly sampled cross-domain negatives.\nIn Hard In-Domain (Hard-ID) sampling, the process is more sophisticated. During each training epoch, all entities *within the domain* of the golden entity are first ranked based on their relevance score computed by the current trained model. From this ranked list, the top-k entities are then chosen as hard negative examples. This method aims to select negatives that are not only from the same domain but are also challenging for the current model to distinguish from the golden entity, thereby providing stronger gradient signals for optimization.\nThe key advantage of these in-domain sampling strategies is efficiency. By restricting the sampling pool to entities within the golden entity's domain, the size of the candidate set for negative sampling is significantly reduced compared to sampling across all domains. This makes the process of generating hard negatives more scalable and computationally less expensive, especially when dealing with large knowledge bases that may contain millions of entities.": 1190,
    "The graph matching module aligns the label graphs of the source and target domains. By treating the cross-domain NER problem as a graph matching task, the model learns to find correspondences between the source and target label graphs. This alignment helps in transferring the knowledge from the source domain to the target domain more effectively, even when the label sets do not overlap directly.": 1191,
    "The optimization problem aims to minimize the divergence between the initial and coarsened distributions of candidate keywords. The algorithm selects a subset of keywords that minimizes the Kullback-Leibler (KL) divergence between the original and coarsened distributions. Mutual information (MI) estimates the probability of translating one keyword into another. The greedy algorithm guarantees near-optimal solutions by iteratively adding keywords that provide the highest gain.": 1192,
    "The paper leverages the interdependence between simile component extraction decisions through the use of \"\"decoding features\"\" and knowledge distillation. Instead of extracting tenor and vehicle components in parallel, the model employs two extra models that extract components sequentially: one extracts the tenor before the vehicle (modelt), and the other extracts the vehicle before the tenor (modelv). These sequential models consume the encoder states of the first extracted component as extra decoding features for recognizing the second component. To fully leverage all possible decoding features, the ensemble of these two sequential models (modelt and modelv) along with a parallel extraction model (modelp) serves as a teacher model. During training, this teacher model simultaneously guides each individual model (modelt, modelv, and modelp) via distillation. The distillation objective minimizes the Kullback-Leibler divergence between the output probabilities of an individual student model and the ensemble teacher model. The final objective for each student model combines the standard training objective (simile sentence classification and component extraction losses) with the distillation objective, controlled by a hyper-parameter lambda that linearly increases during training. During inference, only one of the trained models (the best performing one on the development set) is used to avoid computational overhead.": 1193,
    "A BERT (Bidirectional Encoder Representations from Transformers) sequence classification model, referred to as Bertn, is fine-tuned to predict the type of the next token in a pun sentence. The training data for Bertn is curated through a data-efficient procedure that boosts from a weak, unsupervised method to a stronger, weakly supervised method. Initially, an unsupervised approach based on word semantic similarity is used: the cosine similarity between GloVe embeddings of the pun word (pw), alternative word (aw), and each word in the sentence (tw) is computed. A word is labeled D1 (distinct to pun word) or D2 (distinct to alternative word) if the difference `|cos(tw,pw) - cos(tw,aw)|` exceeds a threshold T, otherwise it's labeled A (ambiguous). To improve label reliability, a BERT-base model, Bertc, is fine-tuned as a sequence classifier on 8,000 automatically labeled samples. For these samples, words are considered distinct if the difference is >1.5T and ambiguous if <T, discarding samples in the [T, 1.5T] range to ensure cleaner labels. Bertc takes the incomplete pun sentence, current word, and pun pair as input. Finally, Bertn is trained on an additional 430 human-labeled data points where the unsupervised method and Bertc disagreed, combined with 8,000 automatic labels where both models agreed. This multi-stage data curation allows Bertn to learn the word-level structure of puns (A, D1, D2) despite limited ground truth. At inference time, Bertn predicts the type of the next word, and if its confidence (c) is above a threshold (T), it steers the generation process.": 1194,
    "The OPTIMA framework simplifies the optimization of soft prompts by decoupling the process of achieving domain invariance from the primary task loss optimization. Unlike methods like Domain-adversarial Neural Network (DANN) where the prompt itself is optimized to learn domain-invariant features (which can lead to optimization difficulties due to competing losses), OPTIMA introduces input disturbance vectors, $\\delta^*$, that are optimized for domain similarity. This means the perturbation, $\\delta^*$, is responsible for making the source data resemble target data (as described in Solution 1). Consequently, the soft prompt, $p$, only needs to optimize for the task-specific cross-entropy classification loss, $\\ell_{xe}(x_s, y_s, p)$, and the smoothness regularization term, $\\ell_{KL}(\\delta^*, p, x_s)$, which is derived from the already optimized $\\delta^*$. The overall loss function for prompt optimization, $L_R$, is $E_{(x_s, y_s) \\in D_s}[\\ell_{xe}(x_s, y_s, p) + \\ell_{KL}(\\delta^*, p, x_s)]$. This separation of concerns allows the prompt to focus solely on improving task performance and smoothness, while the adversarial perturbation mechanism handles the domain alignment, leading to more stable and effective optimization for low-capacity prompts.": 1195,
    "To generate synthetic dialogues for data augmentation, the paper employs a self-chat mechanism involving two agents: the pre-trained and fine-tuned user simulator (described in Solution 1) and a vanilla system agent. The system agent is also a T5 model, pre-trained on the same collection of public dialogue datasets as the user simulator (excluding the \"\"WoW\"\" dataset, which requires knowledge-grounded responses). The key difference is that the system agent does not take the user goal as input, focusing solely on generating a response based on the task description and dialogue context. Both agents are fine-tuned on the limited target domain data. During self-chat, the user simulator initiates a dialogue based on a sampled user goal, and then the two agents interact turn-by-turn, generating utterances. This process allows for the creation of an unlimited number of synthetic dialogues. Crucially, the system agent's response generation is not directly grounded in a database, avoiding the need for domain-specific database interaction, which typically prevents simulators from using large-scale dialogue corpora for domain-adaptive pre-training. To maintain consistency and facilitate downstream tasks, values within utterances are marked with special tokens during training, allowing for delexicalization or replacement for specific tasks later.": 1196,
    "The paper proposes an autoregressive framework that models the probability of the label sequence `Y` given the input sequence `X` as `p(Y|X) = Product(p(yi|y1,...,yi-1,X))`. This means that the prediction of the current label `yi` explicitly depends on the input sequence `X` and all previously predicted labels `y1,...,yi-1`. This autoregressive nature allows the model to build internal context information and capture label dependence, which is crucial for cross-domain transfer.\nTo effectively leverage previous label information and transfer knowledge across domains, the model employs a two-stage training strategy: pre-training and fine-tuning.\n1.  Pre-training Stage: The model is initially trained on the source domain dataset (`Dsrc`). During this stage, the label lookup table (U), which contains embedding vectors for each label, is learned. This process allows the model to acquire valuable label embeddings, especially for shared Named Entity (NE) labels that exist in both source and target domains. The input sequence encoder (e.g., BERT) is also pre-trained or fine-tuned on domain-related corpora (Domain-Adaptive Pre-Training, DAPT) to narrow the difference between source and target domains in terms of background and text distribution.\n2.  Fine-tuning Stage: After pre-training, the model is fine-tuned on the target domain dataset (`Dtgt`). Since the Bi-LSTM (Label Encoder) utilizes the shared label embeddings learned in the first stage, the model can further learn the relationships between these shared NE labels and target domain-specific NE labels (those only present in the target domain). This also helps in modeling intrinsic label dependency information. By leveraging the knowledge of shared labels from the source domain, the model can better understand and predict unseen or less-frequent labels in the target domain, facilitating robust domain adaptation. The autoregressive prediction mechanism ensures that the model continuously uses the context of previously predicted labels to inform subsequent predictions, enhancing the transfer of label-level knowledge.": 1197,
    "To effectively train a dense retriever using the fine-grained pseudo-labels, the Generative Pseudo Labeling (GPL) method utilizes the \"\"MarginMSE-Loss\"\" (Hofstätter et al., 2020). This loss function teaches the dense retriever, referred to as the student model, to mimic the score margin (δi) predicted by the powerful cross-encoder. Formally, the MarginMSE-Loss is defined as LMarginMSE(θ) = -1/M * Σ|ˆδi - δi|^2, where M is the batch size, ˆδi is the corresponding score margin of the student dense retriever (calculated as the dot-product of the query and passage embeddings: ˆδi = fθ(Qi)Tfθ(Pi) - fθ(Qi)Tfθ(P-i)), and δi is the pseudo-labeled margin from the cross-encoder. This approach addresses two critical issues: first, if a generated query is of low quality or not well-matched to its passage, the cross-encoder will assign a low score, resulting in a small target margin (δi). The MarginMSE-Loss will then not force the dense retriever to place the query and passage close in the vector space, effectively \"\"denoising\"\" the impact of bad queries. Second, if a \"\"hard negative\"\" passage is actually relevant (a false negative), the cross-encoder will assign a high score to it, leading to a smaller margin (δi). The MarginMSE-Loss will then not force the dense retriever to assign a large distance between the query and this false negative, preventing detrimental training signals. This mechanism ensures robustness against imperfections in query generation and hard negative mining.": 1198,
    "The Emotional Variance Adaptor (EVA) module conditions the variance adaptor on additional emotion variables of valence and arousal. It generates suitable intermediate features of pitch and energy at the frame level and duration at the phoneme level. These features are consumed by the decoder along with the encoder output to generate Mel frames. By conditioning on emotion variables, the EVA module allows for better control over emotional prosody in rendered speech. The EVA module predicts these features using linear weighted combinations of valence and arousal vectors learned from data during training.": 1199,
    "To address the issues of slot type ambiguity and example ambiguity, the paper introduces a novel query construction template that synthesizes the query by incorporating abundant information. This query template is formulated as: \"\"in domain [domain description], find the [slot description], like [example entity] in [context for example], in sentence: [input sentence x].\"\" The four blank placeholders are filled with specific information: the first with a domain description, the second with a slot description, the third with an example entity, and the fourth with the context for that example. For instance, for the \"\"object_name\"\" slot in the \"\"RateBook\"\" domain, the query might be \"\"in domain rate book, find the object name, like lessons from madame chic in i rate lessons from madame chic 10 stars, in sentence: i am rating this book titled a history of warfare under the war series 1 out of 6 stars\"\". This comprehensive inclusion of domain descriptions helps to distinguish slot types shared across different domains, thereby alleviating slot type ambiguity. Similarly, providing examples with their specific contexts helps the model understand the precise meaning and scope of the example, mitigating example ambiguity. To ensure the integrity of the evaluation, examples and contexts are manually constructed such that the example entities do not appear in the dataset, preventing data leakage. The specific mappings between domains/slots and their corresponding descriptions, contexts, and examples are predefined and detailed in the appendices.": 1200,
    "The paper adapts a pre-trained sequence-to-sequence model, BART (Bidirectional and Auto-Regressive Transformers), for this purpose, implementing several modifications for reconstruction and label generation.\nFor the encoder, in addition to standard word embedding (`Ex`) and position embedding layers, a label embedding layer (`El`) is established. The input to the BART encoder is the sum of the token embedding of the masked sentence `˜X` and the label embedding of its corresponding label sequence `L`. This allows the encoder to process both the masked text and its fine-grained labels simultaneously, producing a hidden state `H`.\nFor the decoder, to inform BART about the target domain for generation, a domain label tuple, either `([source], O)` or `([target], O)`, is inserted at the beginning. This tuple acts as a \"\"domain prompt.\"\" At each time step `t`, the decoder takes the previous decoder predictions (previous token `x<t` and previous label `l<t`) and the encoder output `H` as inputs. It then uses two separate linear layers to predict the possibility of the next token `xt` and the next token label `lt`. The hidden layer vector `zt` for time step `t` is derived from the BART decoder, which takes the sum of the token embedding of `xt-1` and the label embedding of `lt-1` as input. The model is trained to minimize a combined negative log-likelihood loss for both tokens (`Lx`) and the label sequence (`Ll`). During inference for cross-domain data generation, given a source-domain labeled sentence, its domain-specific features are masked. The masked tuple is fed to the BART encoder, but the decoder is specifically provided with `([target], O)` as the domain prompt to generate a target-domain sentence and jointly predict its token-level labels auto-regressively.": 1201,
    "The paper comprehensively evaluates language models by testing both auto-regressive Language Models (LMs) and masked LMs in various settings. For auto-regressive LMs (GPT-2, GPT-neo, GPT-3), zero-shot performance is assessed by directly computing the probability of an interpretation given a figurative phrase, using length-normalized probabilities to minimize length effects. Fine-tuning is also performed on the training data, with specific hyperparameters and early stopping for GPT-2 and GPT-neo, and default API parameters for GPT-3. For masked LMs (BERT, RoBERTa), direct zero-shot probability output is not possible, so transfer performance is tested by fine-tuning them on related binary choice tasks like WinoGrande or Natural Language Inference (NLI) datasets (SNLI, MNLI, FEVER-NLI, ANLI). Additionally, a contrastive fine-tuning strategy is used where both correct and incorrect answer choices are fed as input. The evaluation includes both \"\"forward\"\" probabilities (predicting interpretation from metaphor) and \"\"backward\"\" probabilities (predicting metaphor from interpretation), and also employs a more stringent \"\"paired evaluation\"\" (group scoring) where both sentences in a pair must be correctly interpreted. Human performance is benchmarked on the test set with 10 volunteers in a zero-shot setting, providing a crucial comparison point for model capabilities. This multi-faceted evaluation strategy allows for a thorough assessment of LM performance across different model architectures and learning paradigms.": 1202,
    "The paper proposes a Hyperbolic Span Masking technique to identify and mask specific text spans that contribute to the hyperbolic effect. This is based on the observation that hyperbole is often localized to a single word or phrase. The method leverages two features: POS n-grams and unexpectedness. First, POS n-gram patterns of hyperbole are extracted from the training set of the HYPO dataset, yielding 262 distinct patterns. Second, an unexpectedness score is defined for text spans. For a given n-gram, its unexpectedness is calculated as the average cosine distance between word embedding vectors of tokens inside the n-gram and those outside it, indicating how distant the span is from its literal context. During training, for a hyperbolic sentence from HYPO-XL, all text spans matching one of the hyperbolic POS n-grams are identified. These identified spans are then ranked by their unexpectedness scores, and the top-3 items are selected as the masked spans. During inference, for a literal input sentence, only the POS n-gram matching is applied to identify potential hyperbolic spans, as the unexpectedness score is not applicable to a literal context.": 1203,
    "To derive context words that specifically link to each distinct sense of a pun word, the system explores three different computational methods:\n1.  Method 1: Extractive (TF-IDF): For each related word obtained from the previous step, the system retrieves sentences containing that word from the One Billion Word dataset (Chelba et al., 2013). Keywords are then extracted from these retrieved sentences using RAKE (Rose et al., 2010), a keyword extraction algorithm. Based on their TF-IDF (Term Frequency-Inverse Document Frequency) value, the top 10 context words that are most likely to co-occur with the related words (and by extension, the pun word) are selected.\n2.  Method 2: Similarity (Word2Vec): This method obtains context words based on semantic similarity, inspired by the distributional hypothesis. A Word2Vec model is trained on Wikipedia, a corpus known for its comprehensive coverage of diverse topics. This training ensures that the generated context words capture relevant semantic relationships.\n3.  Method 3: Generative (Few-shot GPT3): For a generative approach, the powerful language model GPT3 (Brown et al., 2020) is employed in a few-shot learning setting. The system provides GPT3 with two examples of prompt-completion pairs, where a prompt asks for keywords for a given concept (e.g., \"\"generate seven keywords for laptop:\"\"), and the completion provides a list of relevant keywords (e.g., \"\"battery, macbook, internet, technology, keyboard, technology, portable\"\"). Following these examples, GPT3 is then prompted to generate seven keywords for each related word, effectively producing context words.": 1204,
    "The path selection algorithm operates during inference to evaluate the performance of a held-out domain. By leveraging the GMM clusters and the hierarchical structure, the algorithm assigns probabilities to data sequences from the held-out domain. The best path leads to the cluster that assigns the highest probability to the largest fraction of sequences, while the second-best path leads to the cluster with the second-highest probability. Using these paths, the algorithm averages the outputs from the corresponding adapter sets, thereby improving generalization for unseen domains without additional training.": 1205,
    "To address the inconsistency between source domain knowledge (stored in memory) and target domain tasks, MANNER leverages Optimal Transport (OT) to retrieve and process information from the memory. After retrieving the most similar entity type `k*` from memory (as described in Solution 1), the optimal transport plan `T*k` between the token representations in the support set (`Hk`) and the retrieved memory content (`Mk*`) is obtained using the Sinkhorn algorithm. This `T*k` plan is then used to adapt the retrieved information from the source domain to the target domain via a barycentric mapping. Specifically, the adapted representation `ˆHk` is calculated as a weighted average of `Hk`, where the weights are derived from the optimal transport plan `T*k`. This process, represented by `ˆHk = diag(T*k 1nk)^-1 T*k Hk`, explicitly transforms the source domain representations to align with the target domain's distribution, thereby alleviating the inconsistency problem and improving performance in the cross-domain setting.": 1206,
    "The paper's Prefix Generation mechanism takes the slot-specific prompt (S) generated in the previous step and transforms it into per-layer key (Ki) and value (Vi) prefixes for each layer (i) of the Transformer. This transformation is achieved using a sequence of down and up projections separated by a Rectified Linear Unit (ReLU) activation function. Specifically, for each layer i, dedicated prefix generators produce Ki = RELU(S Wkdowni) Wkupi and Vi = RELU(S Wvdowni) Wvupi. Here, Wkdowni and Wvdowni are down-projectors of size d x r, and Wkupi and Wvupi are up-projectors of size r x d, where r is a bottleneck dimension set to d/4. This architecture is chosen to balance parameter efficiency and performance, avoiding the limitations of simpler MLPs or more complex full Transformer blocks.\nOnce Ki and Vi are generated for each layer i, they are split into Nh head vectors (Kji and Vji, each of size N x dh, where dh = d/Nh) for each head j of the multi-head self-attention mechanism. These dynamically generated key and value prefixes are then concatenated directly into the self-attention mechanism at each layer of the Transformer encoder. The attention calculation for the j-th head at layer i becomes headji = (hi Wjqi [Kji, hi Wjki]⊤) [Vji, hi Wjvi], where hi is the input to the i-th layer, and Wjqi, Wjki, Wjvi are the query, key, and value weight matrices for the j-th head at layer i. This integration ensures that the dynamically generated prefixes influence the attention mechanism at every layer, providing continuous exposure to the slot-specific information and enabling the model to adapt to new domains in a zero-shot manner without requiring supervised fine-tuning of the prefixes.": 1207,
    "A controllable simile generation (CSG) model, named Similor, is designed as an encoder-decoder architecture to effectively incorporate multiple pre-specified constraints. The process begins by taking a given prefix, referred to as the topic, which contains a tenor. This topic is then concatenated with any optional sequential constraints, such as the vehicle, comparator, and context, using a special separator signal `[SEP]`. For instance, if a vehicle is pre-specified, it is included in this concatenated input sequence. This combined input is then fed into the encoder of the Similor model. The model, which is instantiated with a large Chinese text generation model (e.g., ChineseBART) that has been pre-trained on a large corpus of student compositions and subsequently fine-tuned on the GraCe dataset, then auto-regressively generates the simile. This design allows Similor to condition its generation on the provided constraints, ensuring that the output simile adheres to the user's specific requirements. The fine-tuning on the GraCe dataset enables the model to adapt to the specific domain of simile generation and leverage the rich, fine-grained annotations for better control.": 1208,
    "To leverage the informativeness of different source tasks, the proposed MetaAdapt method introduces a similarity-based gradient rescaling mechanism. After the inner-level optimization updates the initial parameters (θ) to task-specific parameters (ϕi) for each source task, and the metaloss `L(ϕi, X't)` is computed, the method calculates a similarity score (si) between the task gradients (`ϕi - θ`) and the meta-gradients (`dϕi/dθ ∇ϕiL(ϕi, X't)`). This similarity is computed using cosine similarity: `si = CosSim(ϕi - θ, dϕi/dθ ∇ϕiL(ϕi, X't))`. A high similarity score indicates that the source task's optimization path is more \"\"helpful\"\" for improving the metatask performance (i.e., target domain performance). These similarity scores from multiple source tasks are then transformed into a probability distribution using a tempered softmax function: `s = softmax([s1/τ, s2/τ, ..., sn/τ])`, where `τ` is a temperature hyperparameter. Finally, the original parameters (θ) are updated by rescaling the meta-gradients with these normalized similarity scores: `θ - β Σ(si * dϕi/dθ ∇ϕiL(ϕi, X't))`, where `β` is the meta learning rate. This adaptive reweighting ensures that the model learns more effectively from source tasks that are more aligned with the target domain adaptation objective.": 1209,
    "The paper proposes a dynamic difficulty measurement and scheduling strategy for curriculum learning. The difficulty of a training example Y_i is assessed based on the model's contrastive objective, specifically how well its embedding (x_i) is separated from its negative counterpart (x_i-) relative to its positive counterpart (x_i+). The difficulty measure, d_M(Y_i), is defined as the ratio of the distance function f(x_i, x_i+) to the sum of f(x_i, x_i+) and f(x_i, x_i-). A smaller value indicates an easier example, as the anchor is closer to its positive example and further from its negative example. This metric intuitively reflects the separability of figurative and literal embeddings in the embedding space; if they are well-separated, classification is easier.\nThe scheduling strategy is dynamic, meaning the order of training examples is not fixed. Since the difficulty levels are derived from the contrastive objective, they naturally update as the model (M) fine-tunes and its ability to distinguish between senses improves. After each training epoch, the difficulty score d_M_n(Y_i) for every example Y_i is re-calculated using the current state of the model (M_n). Only examples whose difficulty scores have changed from the previous epoch are considered for re-arrangement. These updated examples are then sorted based on their new difficulty scores, and this re-arranged set (P*_n) is used for the subsequent training epoch. This easy-to-hard learning strategy progressively optimizes the model's performance by adapting the training sequence to the model's current learning state.": 1210,
    "The paper proposes a novel two-stage self-training algorithm, named S2ynRE, which iteratively and alternately learns from synthetic and golden data. Unlike classical self-training that merges pseudo-labeled data with labeled data, S2ynRE employs a sequential training procedure. In each iteration, a teacher model (η), initialized from an auto-encoding language model (e.g., BERT) and trained on the golden training dataset (Dtr), first annotates the unlabeled synthetic data (Dsyn) to produce soft pseudo-labels (bYsyn). To further eliminate fluctuations in pseudo-labels, multiple teachers trained with different random seeds are used, and their pseudo-labels are combined. Then, a new student model (θ) is re-initialized. In stage-one training, the student model (θ) is trained on the synthetic data (Dsyn) using these soft pseudo-labels through knowledge distillation, minimizing the Kullback-Leibler divergence (LKD) between the student's predictions and the teachers' pseudo-labels. This stage transfers knowledge from the teacher to the student based on synthetic data. Subsequently, in stage-two training, the student model (θ0t) from stage-one is further trained on the golden training dataset (Dtr) using standard cross-entropy loss (LCE). The resulting model (θ00t) then becomes the teacher model (η) for the next iteration, re-annotating Dsyn. This process is repeated for a set number of iterations (T), incrementally sampling more synthetic data from Dsyn in each iteration until all synthetic data is used. This sequential training procedure favors golden data by introducing it in the latter stage of the training curriculum, ensuring its importance.": 1211,
    "To understand the characteristic error patterns, the paper conducts a detailed manual evaluation of the model's predictions on the test sets, particularly focusing on English. Two annotators (the authors) independently evaluate the correctness of the generated source domains for English, resolving disagreements through discussion. One annotator evaluates the Spanish answers.\nThe correctness of a predicted source domain is determined if it either matches the gold standard or is deemed semantically correct by the annotators, even if phrased differently (e.g., \"\"musical harmony\"\" vs. \"\"music\"\"). In ambiguous cases, annotators apply the Metaphor Identification Procedure (MIP) to analyze words for their more basic, physical meaning and assess concordance with the predicted domain.\nAll incorrect predictions are then systematically classified into a predefined typology of error types to gain insights into the model's misinterpretations:\n*   Wrong with trigger: The model predicts an incorrect source domain influenced by specific words in the input sentence that are literally related to that domain, but not metaphorically relevant to the target domain (e.g., \"\"arms race\"\" leading to \"\"war\"\").\n*   Wrong without trigger: The model predicts an incorrect source domain with no apparent related words or indicators present in the sentence (e.g., \"\"Sally gave the idea to Sam\"\" leading to \"\"ideas are children\"\").\n*   Too literal: The model identifies a literal relationship instead of the intended metaphorical mapping (e.g., \"\"I’m down to my bottom dollar\"\" interpreted as \"\"money is investment\"\").\n*   Should be non-metaphoric: The model incorrectly predicts a metaphorical source domain for a sentence that is actually non-metaphoric.\n*   Should be metaphoric: The model incorrectly labels a metaphorical sentence as non-metaphoric.\n*   Too specific: The predicted metaphorical source domain is overly precise compared to the broader implication of the sentence (e.g., \"\"He finally caught up to schedule\"\" predicted as \"\"schedule is people\"\").\n*   Too general: The predicted source domain is excessively vague or unspecific (e.g., \"\"The idea slipped through my fingers\"\" predicted as \"\"mind is space\"\").\n*   Wrong subelement mapping: The model correctly identifies the general source domain but fails to pinpoint the precise element within that domain that corresponds to the metaphor (e.g., for \"\"China is a fertile ground for revolt,\"\" gold standard \"\"plants\"\" but model predicts \"\"land\"\").": 1212,
    "The paper addresses the challenge of noisy pseudo-labels by introducing a label-to-text direction, which is a reverse process of the text-to-label stage. Instead of directly using the noisy `ˆyT` predictions for self-training, the model is trained on the labeled source dataset (DS) with reversed input and output: the input becomes the sentiment tuples `y`, and the output is the original sentence `x`. For instance, in ASTE, the input would be `<pos> aspect <opinion> opinion` and the output the original sentence. This label-to-text model is trained by minimizing the standard maximum likelihood loss. After training, it takes the noisy sentiment tuples `ˆyT` (extracted from the target domain's unlabeled data `xT` by the text-to-label model) as input and generates a coherent natural language text `ˆxT` that plausibly matches `ˆyT`. Even if `ˆyT` is inaccurate with respect to the original `xT`, the generated `ˆxT` is designed to be consistent with `ˆyT`. This process creates a generated dataset (DG) of `(ˆxT, ˆyT)` pairs. This DG acts as an intermediary, connecting source and target domains through generated sentences that incorporate target-domain sentiment elements while being contextually coherent. To ensure the quality of DG, post-processing filters out pairs with invalid `ˆyT` formats, words not present in `ˆxT`, or where `ˆyT` differs from the text-to-label model's prediction on `ˆxT`. This effectively minimizes the negative impact of noisy initial predictions and yields more accurate pseudo-training samples.": 1213,
    "The creativity of a generated simile is automatically quantified based on the frequency of its vehicle components within large-scale linguistic corpora or knowledge bases. The intuition is that less common or more novel vehicles contribute to higher creativity. The metric calculates the creativity score (C) for a simile by considering the frequency of each extracted vehicle (v) from the simile. Specifically, it uses the inverse of the vehicle's frequency (Nv) in a large-scale simile knowledge base like MAPS-KB, where Nv represents the sum of frequencies of all (topic, property, vehicle) triplets containing that vehicle. To mitigate the influence of extreme frequency values and better align with human perception of creativity, a negative logarithmic transformation is applied to the inverse frequency. The formula used is C = -log(1 / (mv * Nv + 1)), where mv is the number of vehicles extracted from the simile. This approach allows for an automatic, data-driven assessment of creativity by penalizing highly conventional or frequently used simile vehicles.": 1214,
    "The paper proposes a few-shot technique called \"\"Data-Gen\"\" to capture the target distribution without extensive supervision. This method prompts a large language model (LLM), specifically Chowdhery et al. (2022)'s LLM, to generate a sentence given a passage. The process uses eight seed examples from the target domain to generate additional training data, which helps bootstrap adaptation in the target domain.\nThe prompting format is tailored to the domain: for PubMed articles, the prompt is \"\"After reading the article, «context» the doctors said «sentence»\"\". For other target corpora, \"\"doctor\"\" is replaced with domain-appropriate terms like \"\"engineer\"\" (StackOverflow), \"\"journalist\"\" (DailyMail), or \"\"poster\"\" (Reddit).\nTo ensure the quality of the generated sentences and filter out invalid ones, three criteria are applied:\n1.  The generated sentence must not include a number.\n2.  The generated sentence must not repeat part of the passage verbatim.\n3.  The generated sentence must have at least 75% word set overlap with the passage (after removing stopwords).\nThis approach leverages the LLM's ability to condition on a single variable (context) and compress multiple facts from a passage into a single sentence, which is found to be easier than conditioning on both context and an answer span. The generated sentences are then used to train a Dense Passage Retriever (DPR) model with source domain data and approximately 8,000 examples containing pairs of original passages and generated sentences for each target dataset.": 1215,
    "The proposed Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling (DA2LM) framework addresses this question through three key stages.\nStage 1: Domain-Adaptive Pseudo Labeling (DAPL). The goal is to assign high-quality pseudo-labels to unlabeled target-domain data. To mitigate the domain discrepancy between source and target data, an aspect-aware domain adaptation model (Cb) is trained. This model minimizes a combined loss function: `L = Lcrf + αLmmd`. `Lcrf` is the Conditional Random Field (CRF) loss for the ABSA or AE task, calculated on the labeled source-domain data. `Lmmd` is an aspect-level Maximum Mean Discrepancy (MMD) loss, which explicitly minimizes the distance between aspect term representations from the source and target domains. Aspect terms from the source domain are obtained via gold labels, while those from the target domain are extracted using a rule-based algorithm called Double Propagation. A pre-trained BERT model is used to obtain sentence and aspect term representations. After training, this base model Cb is used to predict pseudo-labels for the unlabeled target-domain data, resulting in a set of pseudo-labeled target data (DPT).\nStage 2: Domain-Adaptive Language Modeling (DALM). The labeled source data (DS) and the pseudo-labeled target data (DPT) are combined to train the novel Domain-Adaptive Language Model (DALM), as detailed in Solution 1. This DALM is designed to learn the shared distribution of words and labels across domains by simultaneously generating the next token and predicting the label of the current token at each timestep. This unified training process enables the model to capture transferable context and annotation patterns.\nStage 3: Target-Domain Data Generation. After the DALM is trained, it is employed to generate target-domain data with fine-grained annotations in an autoregressive manner. The generation process starts by fixing the `⟨BOS⟩` token and the target-specific `[target]` token as the first two input tokens, and `⟨BOL⟩` and 'O' as the first two input labels. A probability-based generation strategy is then used: at each timestep, the DALM predicts probabilities for the next token, and the top-k tokens are selected as candidates. A token `wt+1` is sampled from this candidate set, introducing diversity. For label generation, the label with the highest probability (computed by the DALM) is directly selected as the label for the current token `yt`, ensuring quality. This process continues until the `⟨EOS⟩` token is predicted or a maximum number of generated data (Ng) is reached. To further ensure quality, a filtering step is applied to the generated data (Dg): 1) data with illogical labels violating BIO tagging schema are deleted; 2) repetitive data are removed; 3) data whose label sequences predicted by the base model Cb differ from the generated labels are deleted. Finally, a standard BERT-CRF model is trained on this filtered, generated target-domain data (Dg) to predict labels for test data in the target domain.": 1216,
    "The paper develops a multi-task pretraining strategy using self-supervision on Wikipedia to enable the Chain-of-Skills (COS) model to perform superior zero-shot retrieval. This pretraining focuses on bi-encoder skills, namely single retrieval, expanded query retrieval, and entity linking, where abundant self-supervision is available. Unlike prior work that often focuses on single-type pretraining, COS considers a multi-task setting by leveraging individual Wikipedia pages and their hyperlink relations. For single retrieval, a positive example is constructed from a pair of randomly cropped views of a passage, similar to existing methods. For entity linking, a short text snippet containing a hyperlinked entity (referred to as the entity mention context) serves as the query, and the first paragraph of its linked Wikipedia page is treated as the target entity description. For expanded query retrieval, a pseudo-query is constructed using a randomly sampled short text snippet along with the first paragraph from the same Wikipedia page, and the first paragraph from a linked page is used as the target. This multi-task self-supervision allows the pretrained COS model to be directly used for retrieval without requiring dataset-specific supervision, thereby enhancing its zero-shot performance and generalization capabilities across different ODQA tasks.": 1217,
    "This component of the solution focuses on the comparison between the basic and contextual meanings of words. Once the basic meaning is modeled, it is compared with the contextual meaning derived from the target sentence. The disparity between these two meanings, if significant, suggests a metaphorical usage. This method relies on precise modeling and effective comparison algorithms to ensure accurate metaphor detection.": 1218,
    "The paper addresses this by employing a RoBERTa-based Contextual Encoder that integrates the information from the target-oriented parse tree. After the RoBERTa network produces hidden state representations `H = (h_cls, h_0, ..., h_n)` for an input sentence `S = (w_0, ..., w_n)`, the model computes the sentence embedding `vS` in a novel way. Instead of using the common practice of directly employing the CLS token embedding or averaging all word embeddings, RoPPT computes `vS` by performing average pooling only on the hidden states (`h_i`) of the words (`w_i`) that are identified as relevant context within the `n`-neighbor range (`C_n`) by the Target-oriented Parse Tree Pruning Module. This selective averaging effectively denoises the context by ignoring irrelevant words.\nFurthermore, the encoder models two key metaphor identification theories: Metaphor Identification Procedure (MIP) and Selectional Preference Violation (SPV).\n*   The contextual target word embedding (`vS,t`) is obtained directly from the hidden state of the target word (`h_t`).\n*   The literal target word embedding (`v_t`) is obtained by feeding only the single target word (`w_t`) to the RoBERTa network.\n*   SPV is modeled by concatenating the denoised sentence embedding (`vS`) and the contextual target embedding (`vS,t`), followed by a multi-layer perceptron (MLP) `f1`.\n*   MIP is modeled by concatenating the contextual target embedding (`vS,t`) and the literal target embedding (`v_t`), followed by an MLP `f2`.\nFinally, these two hidden vectors (`hMIP` and `hSPV`) are concatenated and passed through a final linear layer with a sigmoid activation to compute the prediction score `y_hat`, which is then used with a binary cross-entropy loss for training. This entire process ensures that the semantic representations are derived from a syntactically pruned, relevant context, leading to more robust metaphor detection.": 1219,
    "The paper integrates the learned FrameNet embeddings by concatenating them with the representations derived from the Sentence Encoder's Metaphor Identification Procedure (MIP) and Selectional Preference Violation (SPV) modules. The Sentence Encoder first generates a contextualized encoding for the target word (vS,t), an isolated encoding for the target word (vt), and a sentence representation (vS). The original MIP representation (hMIP) is formed by concatenating the isolated and contextualized target word embeddings (vt ⊕ vS,t). The original SPV representation (hSPV) is formed by concatenating the contextualized target word embedding and the sentence representation (vS,t ⊕ vS). To integrate the FrameNet embeddings, the pre-trained Conceptual Encoder provides three types of frame embeddings: contextualized target word frame embedding (hS,t), isolated target word frame embedding (ht), and CLS frame embedding (hcls). These are then appended to the respective MIP and SPV vectors. The enhanced MIP vector becomes hMIP = vt ⊕ vS,t ⊕ ht ⊕ hS,t, and the enhanced SPV vector becomes hSPV = vS ⊕ vS,t ⊕ hcls ⊕ hS,t. Finally, these two enriched hidden vectors (hMIP and hSPV) are concatenated (hMIP ⊕ hSPV) and passed through a linear layer with a sigmoid activation function to compute the final metaphor prediction score (ˆy). The entire framework is then trained end-to-end using binary cross-entropy loss for metaphor prediction.": 1220,
    "To enable controlled experiments and targeted evaluation, the paper developed a phrase-matching tool for collecting and annotating idiom data. This tool is built on top of Spacy’s (Honnibal and Montani, 2017) rule-based matching engine, which offers flexibility beyond regular expressions by allowing pattern matching over linguistic units such as parts-of-speech (POS) or dependency relations. This capability allows the tool to capture complex variations of a given phrase.\nThe process involves:\n1.  Input Phrase List: The tool takes as input a list of idioms (in this work, a manually collected list of 225 English idioms).\n2.  Automatic Pattern Creation: For each phrase in the input list, the tool automatically creates a pattern based on simpler rules and assumptions. This allows it to identify different variations of an idiom, such as inflected verb forms, optional particles, or variations in placeholder words (e.g., \"\"pull the wool over *someone's* eyes\"\" can match different words for \"\"someone\"\").\n3.  Sentence Extraction and Annotation: The tool then searches large parallel corpora (Europarl v7 and WMT newstranslation task data, TED talk transcripts) for sentences containing these idioms. When a match is found, the translation pair is extracted, and the specific span within the source sentence where the idiom occurs is automatically annotated. This annotation is critical for enabling targeted evaluation metrics like LitTER and APT-Eval.\n4.  Data Splitting: The collected idiom data is then divided into idiom-train and idiom-test sets. To ensure a balanced distribution and prevent overfitting to high-frequency idioms, half of the sentence pairs for each distinct idiom are allocated to the idiom-train set and the other half to the idiom-test set. Sentences containing idioms that occur only once are discarded. This systematic data collection and splitting enable controlled experimental conditions, including zero-shot, joint, and upsampling scenarios.": 1221,
    "The paper tackles this by proposing the Noise-aware Semantic Adjustment (NSA) module. NSA consists of two main components: Bi-directional Cross-Attention (BCA) and Noise-Guided Modulation (NGM). BCA involves Noise-to-Speech (N-S) and Speech-to-Noise (S-N) cross-attention layers that compute noise-aware speech embeddings (`g'ns`) and speech-aware noise embeddings (`g'n`). These attention mechanisms allow the noisy speech and noise embeddings to interact and refine each other. The outputs of these attentions (`hn→s` and `hs→n`) are fused back with residual connections. Subsequently, NGM incorporates the noise information into the speech embeddings. It takes the fused speech-aware noise embedding `g'n` and applies a fully connected layer to form dynamic filters `Θ`. Each filter `θi` represents noise information for a specific timestep and modulates the speech embeddings. Channel-wise attention is then used to modulate the noise-aware speech feature `g'ns`, resulting in `gs|n = Θ ◦ g'ns`. Finally, `gs|n` is fused back with `g'ns` via a residual connection to derive the final, cleaner speech embedding `gs`, which is more robust to noise.": 1222,
    "The paper enables large language models to simultaneously learn target domain distribution and discriminative task signals by combining two distinct objectives within a unified training framework, given the concatenated text sequences [xS; xT]. The first objective is In-context Task Learning, which is a supervised task to predict the task label (y) for the source input, augmented by the retrieved target contexts. This objective aims to learn task discrimination with the help of cross-domain context. The second objective is In-context Language Modeling, a token prediction task designed to predict tokens from the target context (xT). This objective encourages the model to learn the target distribution. By mixing the source input with the target context and optimizing both objectives, the model is encouraged to fuse distributions from two different domains, thereby bridging the domain gap. The overall goal is to learn task-aware knowledge that is indistinguishable across domains. For encoder-only models, this involves minimizing a joint negative log-likelihood of the ground truth task label and masked tokens in the target context, where Masked Language Modeling (MLM) is applied to 15% of tokens in the target context. For decoder-only models, the two objectives are intrinsically merged by computing the Causal Language Modeling (CLM) loss on every token within the crafted example [prompt; xT; xS; y]. When a token belongs to xT, the loss corresponds to in-context language modeling; when it belongs to y, it relates to in-context task learning.": 1223,
    "To create more challenging and reliable metaphor identification datasets from naturally distributed corpora, the paper proposes a simple sampling procedure, referred to as VUAC_BO, specifically applied to the VU Amsterdam Corpus (VUAC). The VUAC is a collection of documents where each word is annotated as metaphoric or not. The core of the procedure involves framing the VUAC for binary classification by sampling literal instances of potentially metaphorical expressions (PMEs). To avoid sampling biases, literal instances are preferentially sampled from the set of expressions that also occur as metaphors. If the exact same word sequence is not found, the procedure relies on identical lemma sequences. As a final fallback, if identical lemma sequences are not found, it relies on identical Part-of-Speech (PoS) sequences. This careful sampling ensures that literal examples are structurally similar to metaphorical ones, making the task more challenging and less susceptible to superficial cues.": 1224,
    "The analogical chaining method trains language models to capture and generalize relational similarity between word meta-sense prototypes. This approach is inspired by parallelogram models of human and machine analogical inference. First, a \"\"word meta-sense prototype,\"\" denoted as `h(w,m)`, is defined as the mean contextualized embedding of all mentions of word `w` exhibiting meta-sense `m` in a reference corpus. An \"\"offset embedding,\"\" `z(w,m,m') = h(w,m) - h(w,m')`, is then calculated to represent the relational representation between the two meta-senses of word `w`.\nThe core hypothesis of analogical chaining is that for any two lexical instantiations `(w1, w2)` within the same systematic meta-alternation `(m,m')`, their respective offset embeddings `z(w1,m,m')` and `z(w2,m,m')` should be similar. To enforce this, the language model is trained to minimize a loss function, `Lanalog`, which quantifies the dissimilarity between these offset embeddings. Specifically, at each training trial, a systematic alternation `(m,m')` and a pair of its lexical instantiations `(w1, w2)` are sampled. The loss function is defined as `Lanalog = - Σ d(w1, w2, m, m')`, where `d(w1, w2, m, m') = ||z(w1,m,m') - z(w2,m,m')||2`. By minimizing this loss, the model learns to align the relational representations across different words that undergo the same systematic meta-sense alternation, thereby enabling it to generalize this learned relational regularity to unseen lexical items within the same meta-alternation category.": 1225,
    "To address the problem of excessive alignment in statistical domain adaptation, the paper introduces an Instance Re-weighting Scheme into the Maximum Mean Discrepancy (MMD) calculation. Initially, MMD is used to quantify the statistical divergence between the source and target domain feature distributions. The key innovation lies in dynamically assigning weights to target instances based on their relevance to source instances. This relevance is approximated by the prediction confidence of a classifier, g(·), which has been trained on the source domain. Specifically, for each target instance xtj (with its embedding ztj), the classifier predicts the probability that it is vulnerable, denoted as Prob(ytj=1|ztj). A weight, f(ztj), is then calculated for this instance using the formula: f(ztj) = 2 * |Prob(ytj=1|ztj) - 0.5|. This function assigns higher weights to instances where the classifier is more confident in its prediction (i.e., Prob(ytj=1|ztj) is closer to 0 or 1) and lower weights to instances where the classifier is uncertain (i.e., Prob(ytj=1|ztj) is closer to 0.5). These calculated weights f(ztj) are then incorporated into the MMD loss function, specifically by multiplying the cross-domain kernel terms k(zsi, ztj) with f(ztj). This re-weighted MMD loss (LMMD-IR) effectively reduces the contribution of irrelevant target instances (those with low confidence and thus low f(ztj)) to the overall distribution alignment, allowing the model to focus on aligning more relevant and confidently predicted instances, thereby alleviating the issue of excessive alignment.": 1226,
    "The paper proposes two distinct approaches for estimating the importance weights, which are crucial for computing the transported response ˆµ(PT): TEXT-TRANSPORTclf and TEXT-TRANSPORTLM.\nTEXT-TRANSPORTclf (classification approach) estimates the density ratio dPT/dPR(X) by leveraging the notion that it can be rewritten using conditional probabilities: dPT/dPR(X) = [P(C=T|X) / P(C=R|X)] * [P(C=R) / P(C=T)]. Here, C denotes the distribution (corpus) from which a text X is drawn (T for target, R for source). P(C=T|X) and P(C=R|X) are estimated by training a binary classifier, Mθ, to predict whether a text X came from the target (T) or source (R) distribution. P(C=R) and P(C=T) are determined by the sample proportions of the combined corpora. For implementation, 10% of the source data (DR) and 10% of the target data (DT) are used as a training set (Dtrain) for the classifier. The classifier, which uses embeddings from a pre-trained MPNet model as input to a logistic regression, then predicts the probabilities for the remaining 90% of DR. The sample proportions |DR| and |DT| are used for P(C=R) and P(C=T) respectively.\nTEXT-TRANSPORTLM (language model approach) takes an alternative route that does not directly estimate the density ratio but rather the individual probabilities PR(X) and PT(X). This approach capitalizes on the ability of language models to learn text distributions. Pre-trained large language models (LLMs) are particularly useful. The method involves prompting an LLM (e.g., GPT-3) in a way that induces it to focus on either the source (PR) or target (PT) distribution. For example, prompts like \"\"You are writing a positive statement\"\" for PR or \"\"You are writing a negative statement\"\" for PT are used. After prompting, sentence probabilities from the LLM are computed as reflections of PR(X) or PT(X). If a text has multiple sentences, the average of their probabilities is taken as the overall text probability. Finally, the ratio ˆPT(X) / ˆPR(X) is computed as the importance weight. This approach can be implemented without additional model training or direct access to target data, making it resource-efficient. Both approaches utilize stabilized (normalized) versions of the importance weights, as per the Hájek estimator, to maintain stability.": 1227,
    "To assess the capabilities of existing models, the paper evaluated the performance of several standard summarization models: T5, Pegasus, BART, and mT5. T5 is a versatile transformer-based model that utilizes a unified text-to-text transfer learning framework, applicable to diverse tasks including summarization. Pegasus is a transformer-based model specifically designed for text generation tasks like summarization, employing pre-training with extractive supervision. BART is a sequence-to-sequence model built on the transformer architecture, pre-trained using document denoising. mT5 is a multilingual extension of T5, with the specific variant used in this work having been trained for multilingual news summarization on the XL-Sum Dataset. These pre-trained models were fine-tuned on the newly introduced PoemSum dataset. The fine-tuning process involved a learning rate of 0.0001, an AdamW optimizer, and a batch size of 10. The PoemSum dataset was partitioned into train, validation, and test sets following an 80:10:10 ratio for model training and evaluation. The performance of the models was assessed by comparing their generated summaries against the gold reference summaries using several metrics. These included ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL), which measure the overlap between summaries based on unigrams, bigrams, and the longest common subsequence, respectively. Additionally, BERTScore (BS) was used, which analyzes token similarity through contextual embeddings, offering a more nuanced evaluation than exact match metrics. This evaluation aimed to critically analyze the models' ability to interpret creative language and identify their limitations in handling figurative patterns.": 1228,
    "The Prompting stage addresses the classification of filtered candidate spans by leveraging contextual and relational information while mitigating prompt interference. This stage employs an adversarial prompt-based span classification method.\nFirst, soft prompts are constructed around each candidate span. Inspired by common patterns in pre-training corpora (e.g., Wikipedia's full name-abbreviation pattern), a span `si` is wrapped into a template `xp = {xpart1, [p1], si, [p2], [MASK], [p3], xpart2}`, where `[pi]` denotes a soft prompt. These soft prompts are directly inserted before and after the span to capture contextual position information. A pre-trained language model (PLM), specifically BERT, then predicts the probability of each label `y` being filled in the `[MASK]` position, effectively classifying the span.\nTo alleviate the interference caused by the soft prompts and preserve the potential connections between nested entities, contrastive learning is introduced. Unlike traditional contrastive learning that uses negative pairs, this approach focuses solely on constructing positive pairs. Positive pairs `(xp1, xp2)` are defined as different wrapped spans obtained from the filtered candidate span set. All spans within this set are paired. The contrastive learning objective is to shorten the distance between sentence representations of these positive pairs. This is achieved by calculating a cosine embedding loss between the `[CLS]` token representations of `xp1` and `xp2` obtained from BERT. This mechanism encourages the model to learn more robust and consistent representations for related spans, effectively preserving the relationship information between nested entities despite the prompt-induced modifications.": 1229,
    "The paper addresses this by implementing an Unmasking step, which is the third phase of the domain obfuscation process. This step aims to reduce the number of edits to the original text and restore context by selectively unmasking tokens that do not provide strong domain-specific cues. After the initial Base Masking and OTT Masking steps produce a masked text `˜XKD` (where `K` is the set of masked token positions), the method sequentially restores tokens. For each masked token at position `i`, it calculates `mu(wki) := fDd(˜XK-{ki}D) - fDd(˜XD)`, where `fDd(·)` returns the probability score of the original domain `D` from the domain classifier `fd`. This `mu` score measures the change in domain-label probability upon unmasking a token. Tokens are sequentially restored (unmasked) in ascending order of their `mu(wki)` scores, meaning tokens that cause the least change in domain confidence (i.e., are less domain-specific) are unmasked first. The unmasking iteration stops when the condition `fDd(˜XK-UD) < τ3` is violated, where `U` is the set of unmasked token positions and `τ3` is a threshold (set to 0.4). This greedy approach prioritizes unmasking tokens that are less correlated with domain specificity, thereby improving the precision of the masked output while retaining essential domain-dependent masks.": 1230,
    "The paper proposes a novel method for calculating weights to average source prefixes. First, for a small set of `m` unlabelled documents from the target domain (`DTm,sample = {x1, x2, ..., xm}`), summaries are generated using each of the `n` source prefixes (`PSj = {hjK, hjV}`). For each target document `xi` and source prefix `PSj`, a summary `yji = M(xi; PSj)` is generated, where `M` is the frozen pre-trained language model. Next, an encoder `f` (SentenceBERT) is used to generate sentence representations for the generated summary (`rji = f(yji)`) and the original document (`ti = f(xi)`). The averaged document-summary cosine similarity score for each source prefix `PSj` is then computed as `sj = (1/m) * Sum(i=1 to m) [cosine-similarity(rji, ti)]`. Finally, these scores are converted into weights `wj` by applying a softmax function: `wj = exp(sj) / Sum(i=1 to n) [exp(si)]`. A higher similarity score indicates greater relevance of that source prefix to the target domain, resulting in a higher weight.": 1231,
    "To statistically isolate the specific influence of embodiment, the study employs a multicollinearity test using the Variance Inflation Factor (VIF). This analysis is performed on the predictor variables: embodiment score, Age of Acquisition (AoA), word frequency, and word length. AoA scores are obtained from Kuperman et al. (2012), and word frequency data from Van Heuven et al. (2014). Word concreteness in context is also considered, using an open-source predictor based on distributional models and behavioral norms (Rotaru, 2020), with the word frequency behavioral norm excluded to avoid redundancy. The VIF values are calculated for all these features. A VIF value close to 1.0 indicates no multicollinearity, while values above 4 or between 5 and 10 suggest moderate to likely multicollinearity, respectively. By demonstrating VIF values close to 1.0 for all features, the study confirms that the embodiment score is not highly correlated with the other linguistic features, thereby allowing its unique effect to be assessed.": 1232,
    "The semi-supervised learning (SSL) component trains another classifier using labeled and unlabeled target data. This classifier benefits from the additional unlabeled data by generating pseudo-labels for the unlabeled samples. These pseudo-labels are iteratively added to the training set based on a confidence threshold, allowing the SSL component to refine the classifier's performance over multiple iterations. The SSL component ensures that the model can effectively utilize the limited labeled data and the abundant unlabeled data in the target domain.": 1233,
    "The paper defines and evaluates three simile-specific tasks using the Multilingual Simile Dialogue (MSD) dataset: recognition, interpretation, and generation.\nFor Simile Recognition, the task is framed as a binary classification problem where a model must distinguish whether an input multi-turn dialogue contains a simile (True) or is literal (False). Baselines include a fine-tuned BERT model and a large language model, ChatGLM, used in a zero-shot setting. BERT is fine-tuned on the MSD-En/Ch data (8:1:1 train/validation/test split), using the output vector of the first input token (`<cls>`) to calculate the classification score. ChatGLM is prompted with a definition of simile, a requirement to answer \"\"yes or no,\"\" and simile dialogue examples.\nFor Simile Interpretation and Simile Generation, these are defined as multi-choice tasks, specifically using the \"\"as...as\"\" mode in MSD-En data, where the shared property naturally exists within the comparator.\nIn the Interpretation task, a simile dialogue is presented with the shared property between the two \"\"as\"\" words removed and replaced with a blank. The model must select the correct property from four choices (one correct, three distractors). Distractors are constructed using ConceptNet (Speer et al., 2017): first, the antonym of the property is used as one distractor; then, related concepts to the tenor are found using nine specific ConceptNet relations (e.g., `Relation:Definition`, `Causes`, `Desires`, `DistinctFrom`, `SymbolOf`, `MannerOf`, `LocatedNear`, `CausesDesire`, `MadeOf`), and their properties (`HasProperty` relation) are used as the other two distractors. For tenors that are phrases or sentences not found in ConceptNet, keywords (e.g., subject, noun) are used.\nIn the Generation task, the vehicle in a simile dialogue is removed and replaced with a blank. The model must select the proper vehicle from four candidates (one correct, three distractors). Distractors are also constructed using ConceptNet by finding related concepts to the vehicle using the same nine relations. For vehicles that are phrases or sentences not found in ConceptNet, vehicles from other MSD dataset dialogues are used as distractors.\nBaselines for both interpretation and generation include BERT-large (predicting masked words), BERT-Probe (He et al., 2022), and BERT-ANT (Chen et al., 2022). Predicted words and candidates are encoded into dense vectors using a sentence-transformer (`sentence-transformers/all-MiniLM-L6-v2`), and cosine similarity is used to choose the answer.": 1234,
    "The paper designs a three-step Human-AI collaboration approach to generate and curate a high-quality visual metaphor dataset. First, human experts manually select linguistic metaphors that are \"\"visually grounded,\"\" meaning they can be rendered as visual metaphors, even if abstract concepts are represented through their usual visual representations (e.g., \"\"love\"\" as hearts). Second, Large Language Models (LLMs), specifically InstructGPT-3 (davinci-002) with Chain-of-Thought (CoT) prompting, generate initial visual elaborations for these metaphors. These elaborations include \"\"Objects to be Included\"\" and \"\"Implicit Meaning\"\" as intermediate steps. Crucially, human experts then review and perform minor edits on these LLM-generated visual elaborations to ensure they accurately represent the implicit meaning and involved objects. Third, diffusion-based models, such as DALL·E2, generate multiple images from these (potentially edited) visual elaborations. Post-generation, human experts conduct a quality check, examining each set of generated images to validate whether they accurately and fully represent the meaning of the original linguistic metaphor, contain relevant objects, and exhibit correct positioning and attribute binding. Images that do not meet these criteria are discarded, ensuring the high quality of the final dataset.": 1235,
    "To address the \"\"multiple prediction problem,\"\" where a model might predict multiple slot types for a single entity span, the paper introduces an Inverse Prompting strategy. This auxiliary task is designed to warm up the model parameters and strengthen the relationship between entities and slot types by forcing the model to learn explicit distinctions. Unlike the main task where a slot type question leads to an entity answer, the inverse prompting task inverts this relationship: it takes each slot value (entity token) as input and requires the model to generate its corresponding slot type as output. For example, if \"\"ilse delange\"\" is an \"\"Artist,\"\" the inverse prompt would be \"\"ilse delange\"\" as input, and \"\"Artist\"\" as output. This formulation enables the model to learn deep semantic relationships between slot types, thereby reducing the likelihood of ambiguous or repeated generations. Both the main task and the inverse task are trained in the same auto-regressive way. The paper found that pre-training with the inverse task before the main task yields the best performance. Additionally, a negative sampling strategy is employed for the inverse task, where randomly sampled spans in sentences are set to \"\"none\"\" as their corresponding answer, improving the model's anti-noise ability and robustness.": 1236,
    "To collect culturally relevant examples, the paper employs a crowd-sourcing protocol involving two or more native speakers for each of the seven selected languages. Workers are instructed to generate paired metaphors that begin with the same words but convey different, often opposite, meanings. Crucially, they are also required to provide the literal interpretations of both phrases. Annotators are encouraged to incorporate creativity but are given the caveat that any examples must be easily understood by native speakers of that language and culturally relevant (e.g., avoiding expressions like \"\"as classic as pancakes for breakfast\"\" if pancakes are not a traditional breakfast food in that culture). This protocol ensures that the collected expressions are grounded in the cultural experiences of the native speakers.": 1237,
    "The framework leverages cross-lingual and cross-figurative knowledge transfer by training a single multilingual pre-trained text-to-text transformer (mT5) model on a diverse dataset encompassing three figures of speech (hyperbole, idiom, metaphor) and seven languages (English, Chinese, German, Spanish, Italian, Farsi, Russian). For cross-figurative knowledge transfer, the hypothesis is that different figures of speech share common figurative features, and a multitask framework can benefit from jointly modeling these tasks. By training the model on data from multiple figures of speech simultaneously, it can learn shared representations and patterns that are beneficial across tasks, potentially leading to knowledge gain through transfer from one figure of speech to another. For cross-lingual knowledge transfer, multilingual PLMs like mT5 are pre-trained on massive textual data from multiple languages, enabling different languages to be represented in a shared semantic space where similar words and phrases across languages are positioned closely. This allows the model to generalize from knowledge learned in one language to another (e.g., `Mlm(x,pt)=y′` for `(x,y)∈Ttin` language `ln` based on `lm`). The unified model is expected to improve performance in multilingual modeling by exploiting these shared representations and transfer capabilities, particularly for figures of speech like hyperbole and metaphor that are less culturally specific than idioms.": 1238,
    "The study compares black-box TLMs (BERT, RoBERTa, XLNet) and white-box models (Logistic Regression, Random Forest, Decision Tree, Naive Bayes) on a figurative language classification task. Performance is evaluated using macro-F1 scores, and feature attention is analyzed through SHAP values. Black-box models outperform white-box models significantly, with BERT, RoBERTa, and XLNet achieving F1 scores of 0.95, 0.95, and 0.94 respectively, compared to Logistic Regression's 0.87. Feature analysis shows that black-box models better capture prominent cues, whereas white-box models focus more on function words.": 1239,
    "The paper introduces an explicit alignment loss function, specifically a contrastive loss (LCTR), to enforce the mapping of acoustic representations (`a`) and lexical representations (`l`) to a shared feature space. This is achieved by conducting average pooling at the utterance level to obtain pooled vectors (¯a_i, ¯l_j). For a batch of size N, paired vectors (¯a_i, ¯l_i) are treated as positive samples, while unpaired vectors (¯a_i, ¯l_j) where i ≠ j from the same mini-batch are considered negative samples. The training objective minimizes the contrastive loss, which uses a similarity metric (implemented as a dot product) to promote higher similarity between paired acoustic and lexical representations compared to unpaired ones. This contrastive learning is jointly trained with the Masked Language Model (MLM) objective (LMLM) using a weighted sum: L = LMLM + α · LCTR, where α is a hyperparameter.": 1240,
    "To reduce the training complexity for few-shot slot filling, the paper proposes an approach of reconstructing slot labels. This method transforms the standard BIO (Beginning, Inside, Outside) labeling format, typically used in sequence labeling tasks, into a simpler slot entity labeling format. This transformation effectively halves the number of distinct slot labels, thereby reducing the number of classifier parameters and the overall difficulty of model training. The process involves two main steps: First, the sequence labeling task in BIO format is converted into a predicting slot entity task, where the model predicts only the entity types for all tokens in the dialogue (e.g., `city_name` instead of `B-city_name` or `I-city_name`). Second, for evaluation purposes, these predicted entity types are reconstructed back into the BIO format using predefined rules, following the natural language order from left to right. Additionally, to address the challenge of distinguishing between semantically similar slot labels (e.g., \"\"from.city_name\"\" vs. \"\"to.city_name\"\") in low-resource scenarios, the paper introduces the focal loss function to replace the commonly used cross-entropy loss. The focal loss function, defined as `FL(pt) = -(1-pt)^γ * log(pt)`, where `pt` is the probability of the correct class and `γ` is a non-negative hyperparameter, reduces the relative loss contribution from easily-to-classify samples. This forces the model to focus more on hard-to-classify samples, which are often the semantically similar ones, by increasing their relative weight in the loss calculation.": 1241,
    "To align the overall data distributions of Metaphor Detection (MD) and Basic Sense Discrimination (BSD and learn task-invariant representations, the AdMul framework employs a Global Discriminator (Qgd) within an adversarial training setup. The shared feature extractor, denoted as Qf (implemented using DeBERTa), processes the input `x` and generates a sentence embedding `h` via average pooling, i.e., `h = Qf(x)`. This embedding `h` then passes through a Gradient Reversal Layer (GRL), denoted as Qλ. The GRL acts as an identity function during forward propagation but reverses the gradient by multiplying it with a negative scalar (`-λ`) during backpropagation.\nThe Global Discriminator (Qgd) takes the output of the GRL (which is `h`) and attempts to predict which task (`d`, where `d=0` for MD and `d=1` for BSD) the input feature `h` originated from. The objective of Qgd is to minimize its classification loss (`Lgd`). Simultaneously, the shared feature extractor Qf is trained to *maximize* this same loss. This adversarial dynamic forces Qf to generate features that are indistinguishable by Qgd, meaning Qf learns universal representations that align the data distributions of MD and BSD. As the model converges, Qf becomes adept at producing task-invariant features, effectively fooling the global discriminator and ensuring that the learned representations are useful for both tasks without being biased towards one.": 1242,
    "To reliably identify and classify different types of metaphors, especially those relevant as register markers, within a German corpus, the authors developed a differentiated annotation scheme and methodology. The annotation process was performed using the INCEpTION tool. Metaphors were annotated independently by two annotators, achieving a Krippendorff’s alpha inter-annotator agreement of 0.89. The core of the annotation scheme involved classifying metaphors based on their degree of conventionalisation and structural properties. Building on the definition by Steen et al. (2010), an expression's context-based sense qualifies as metaphorical if it differs from a more 'basic' sense (e.g., more concrete or related to bodily action), and both senses are similar but not subsumable under a common hypernym. To distinguish degrees of conventionalisation, the annotators cross-referenced suitable lexical resources, specifically the Duden dictionary and the Digitales Wörterbuch der deutschen Sprache. If the context-based metaphorical sense was listed alongside the basic sense in at least one of these dictionaries, the metaphor was classified as \"\"conventionalised.\"\" Otherwise, if only the basic sense was listed, it was considered \"\"non-conventionalised.\"\" Beyond conventionalisation, the scheme also identified \"\"extended metaphors\"\" (where several metaphors in a discourse are based on the same kind of similarity) and \"\"potential metaphors\"\" (where tokens of an expression combine basic and metaphorical senses in the same discourse). Additionally, \"\"metaphor flags\"\" (explicit linguistic markers like `wie` 'like' or `praktisch` 'in effect' that signal a metaphor) were noted. The annotation also included a layer of syntactic structure, derived using the Stanza package, to facilitate future analyses of the metaphorical potential of specific syntactic constellations. The guidelines for this annotation were created by starting from those of Steen et al. (2010) and Herrmann et al. (2019).": 1243,
    "The integration of metaphor detection as an auxiliary task within the multi-task learning (MTL) framework, leveraging a shared RoBERTa-BASE model, is evaluated by comparing its performance against single-task learning (STL) models across various propaganda identification tasks. For single-label propaganda detection, the MTL approach consistently improves results. Specifically, for \"\"name-calling,\"\" performance (F1 score) increased by 1.02 points in news articles (from 28.72 to 29.74) and by 1.24 points in memes (from 56.53 to 57.77), with these gains being statistically significant and accompanied by increased stability (lower standard deviation). For \"\"loaded language,\"\" smaller but consistent improvements were observed (0.22 points in news, 0.34 points in memes), also with reduced variability. In multi-label propaganda detection for news articles, the MTL model achieved the best overall performance with an F1 score of 24.32, outperforming the STL model's 23.78. The paper also provides qualitative analysis showing that MTL models are better at detecting figurative language, which contributes to identifying propaganda techniques that utilize such devices, like idioms in \"\"loaded language\"\" or metaphorical terms in \"\"name-calling.\"\" Furthermore, model predictions indicate a higher prevalence of metaphors in propagandistic text fragments, particularly for \"\"loaded language\"\" and \"\"name-calling,\"\" suggesting that these techniques more frequently resort to metaphor.": 1244,
    "The paper addresses the effective transfer of knowledge from the adapted rerank model to the dense retrieval model (bi-encoder) through knowledge distillation, with a specific innovation in the choice of the teacher model. This process utilizes a cross-architecture training approach, where the cross-encoder acts as the teacher model to label the margin (M) between pairs of (query, relevant passage) and (query, irrelevant passage). The margin M is defined as the difference between the cross-encoder's score for a relevant passage (CE(Q, P+)) and its score for an irrelevant passage (CE(Q, P-)). The student model, the bi-encoder, is then trained using the Margin Mean Squared Error (Margin-MSE) loss. This loss function minimizes the squared difference between the teacher's margin (M) and the bi-encoder's margin (BE(Q, P+) - BE(Q, P-)). The key innovation lies in the \"\"Mix\"\" teacher model. Instead of using only the adapted cross-encoder or the unadapted cross-encoder as the teacher, the \"\"Mix\"\" model calculates the margin by averaging the scores from *both* the unadapted cross-encoder and the adapted cross-encoder. This ensemble approach for margin estimation is found to yield more precise labels for the pseudo-queries and passages, as the unadapted model tends to assign a smaller absolute margin (maintaining a certain level of regularization), while the adapted model widens the gap between relevant and irrelevant passages. By combining both, their complementary strengths are leveraged to provide a more robust and accurate teaching signal for the bi-encoder's distillation.": 1245,
    "The identified type-related features (TRFs) are leveraged through a two-stage mechanism: automatic TRF selection and prompt incorporation.\nFirst, for automatic type-related feature selection, given an input sentence `x` and the extracted TRF set `R`, the selection of relevant TRFs is formulated as a cloze-style task for the pre-trained language model (PLM)-based basic model (`Mb`). A prompt template function `f(x) = \"\"x[SEP]type-related features:[MASK]...[MASK]\"\"` is used. By inputting `f(x)` into `Mb`, the hidden vector `h[MASK]` for each `[MASK]` token is computed. For each `[MASK]`, the probability `p([MASK]=r|f(x)))` for each token `r` in `R` is calculated using a softmax over dot products of `r`'s embedding and `h[MASK]`. The token with the highest probability is selected as the relevant TRF, discarding repeats. To train `Mb` for TRF selection, a loss function `Lgen` is defined, maximizing the log probability of the correct `[MASK]` tokens, where the ground truth `ϕ(x,i)` for the `i`-th `[MASK]` token is obtained by computing Euclidean distance between PLM embeddings of `r` in `R` and tokens in `x`, selecting the top-K features.\nSecond, for prompt incorporation, a unique prompt is generated for input `x` using the selected relevant TRFs `R'(x)`. This is achieved with the template function `f'(x) = \"\"x[SEP]t1:R'(x,t1)[SEP]...[SEP]t|T|:R'(x,t|T|)\"\"`, where `ti` is an entity type name and `R'(x,ti)` are selected TRFs related to type `ti`. If no TRFs are selected for a type, that type and its TRFs are excluded. This constructed prompt `f'(x)` is then input into `Mb` to recognize entities in the given sentence `x`.": 1246,
    "The paper proposes a dynamic scheduling method that re-arranges training examples based on their updated difficulty scores after each training epoch. Unlike traditional curriculum learning schemes that fix the order of examples, this dynamic approach acknowledges that the perceived difficulty of an example changes as the model learns.\nThe process, outlined in Algorithm 1: PPLCL (Perplexity-based Curriculum Learning), works as follows:\n1.  Initialization: At the beginning, the initial difficulty scores `D0` for all training examples `P` are calculated using the initial model `M`. The dataset `P` is then sorted based on these initial difficulty levels, resulting in `P0`.\n2.  Epoch-wise Training and Update: For each epoch `n` (from 1 to `N`):\n    *   The model `Mθn` is trained on the currently arranged dataset `Pn-1` (Algorithm 2: TRAIN is called here).\n    *   After training, a new set `Dn` and `P*n` are initialized.\n    *   For every training example `(X; Y)` in the original dataset `P`, its difficulty score `dn(Y)` is re-calculated using the model `Mθn` trained in the most recent epoch. This re-calculation uses the combined `dr(Y) + dp(Y)` metric.\n    *   If the newly calculated difficulty `dn(Y)` is different from the previous epoch's `dn-1(Y)`, the example and its new score are added to `P*n` and `Dn` respectively. This ensures that only examples whose perceived difficulty has changed are considered for re-arrangement.\n    *   Finally, `P*n` is sorted based on the updated `Dn` scores, resulting in the new arrangement `Pn` for the next epoch.\nThis dynamic re-arrangement ensures that the curriculum adapts to the model's learning progress, continuously presenting examples in an easy-to-hard order based on the model's current understanding.": 1247,
    "The paper addresses this by proposing a joint learning approach, depicted in Figure 1(b). This method employs a multitask training procedure based on the Sentence-BERT architecture, where the training phase is optimized in a round-robin fashion. At its core, it uses contrastive learning with the multiple-negatives ranking loss, similar to the continual learning approach. For a given metaphorical sentence, the loss is first computed for learning the correct source domain representation. Subsequently, this loss is backpropagated, and then the loss for learning the highlighted aspect representation is computed and backpropagated. This creates an alternating training cycle where information about both highlighted aspects and source domains is mutually exploited. A shared encoder, specifically a Sentence-BERT with DeBERTa, is used for both tasks, employing hard parameter sharing (Ruder, 2017) to facilitate this mutual learning. This concurrent optimization aims to allow the model to learn representations that benefit both prediction tasks simultaneously.": 1248,
    "The paper proposes the Cascading Domain Knowledge Integration (CDKI) benchmark to enhance the model's ability to detect metaphors by incorporating domain knowledge. CDKI addresses the disparity in how domain knowledge is present in text and images. For the text modality, a segmentation technique is employed to preserve multiple inherent domain-specific vocabularies by extracting nouns. These extracted nouns are then connected using the `[SEP]` token to form `Ktext`, which serves as the domain knowledge input for the text modality. For the image modality, a cascaded domain word set is constructed to introduce and enhance the dependency relationships of domain knowledge within the images. This cascaded domain word set for an image consists of three parts: the macro-level domain word set (`Setmacro`), the micro-level domain word set (`Setmicro`), and the entity-level domain word set (`Setentity`). These word sets are composed of vocabulary corresponding to the respective levels in the pre-constructed domain lexicon. To construct these sets, the CLIP model is utilized to obtain the probabilities of each vocabulary term from the domain dictionary appearing in the image. The top `n` (for macro), `p` (for micro), and `q` (for entity) vocabularies with the highest probabilities are retained as the respective word sets. Collectively, these three sets constitute the domain knowledge within the image, denoted as `Kimage` (`Kimage = Setmacro + Setmicro + Setentity`). Finally, the domain knowledge from both image and text modalities are combined to generate the final domain knowledge for the image-text pairs, denoted as `Kpair` (`Kpair = Kimage + Ktext`). This `Kpair` is then converted into vectors using BERT, serving as the domain feature input. These domain features are subsequently modeled jointly with image features (extracted by ResNet50) and textual features (extracted by BERT) using a cross-attention mechanism. The resulting outputs are concatenated and fed into a softmax function for classification.": 1249,
    "Counterfactuals S* and E* are generated and intervened on X to block the backdoor path and eliminate the causal effect of the original S and E on X. Meanwhile, the original S and E are preserved and restored to maintain semantic information. Counterfactuals X* and YX* are computed using specific formulas. This process ensures that the causal effects of variables are dynamically adjusted based on domain characteristics.": 1250,
    "The Multimodal Figurative Language Detection Task is designed to assess Vision and Language Pre-Trained Models (VL-PTMs) by requiring them to choose the image that best visualizes the meaning of a figurative expression. The task instances are constructed as \"\"mixed\"\" instances, each containing four candidate images: one correct answer (a figurative image) and several distractors, including partially literal images and random images. For idioms, 1-2 partially literal distractors are included. Simile instances feature two literal distractors: one related to the target concept without the compared property or with its antonym visualized, and another related to the source concept. Metaphor \"\"mixed\"\" instances contain between 1-3 partially literal distractors. Models are evaluated by encoding the figurative phrase and each candidate image, producing a matching score for each pair, and then selecting the image with the highest score as the best match. This setup specifically challenges models to prioritize figurative understanding over literal connections.": 1251,
    "The framework employs Gaussian-distributed embeddings to represent utterance-tokens, where each token becomes a density rather than a single point in the latent feature space. A Gaussian Transformation Network is used to generate Gaussian mean (µ) and diagonal covariance matrix (Σ) embeddings from the contextual representations (h_i) obtained from a pre-trained encoder (e.g., BERT). The mean µ_i is derived from fµ(h_i) and the variance Σ_i from ELU(fΣ(h_i)) + 1, ensuring non-negative variance. Unlike existing contrastive learners that optimize pairwise similarities between point embeddings, HiCL aims to optimize distributional divergence using Kullback-Leibler (KL) divergence between Gaussian distributions. For a pair of positive samples, the symmetric KL divergence is calculated as s(u,v) = 0.5 * (DKL[Nu||Nv] + DKL[Nv||Nu]). This distributional divergence is then used in an NT-Xent loss function, which encourages similar labels to be close in semantic space while pushing dissimilar ones apart. This approach allows the model to better capture semantic uncertainty and coverage variation of token-classes, facilitating the modeling of generalized slot-agnostic features.": 1252,
    "A token-level attention mechanism is designed to identify important tokens by leveraging the invariant features extracted by the mask layers. For binary classification, the filtering vector `mL` from the last layer is used as the query vector. Attention weights `ai` are computed by performing a matrix product between `mL` and each masked token embedding `eLi`. These attention weights are then used to aggregate the masked token embeddings into a sparse sequence representation `v = sum(ai * eLi)`. This representation `v` is then fed into a fully-connected layer to generate a predictive distribution over the label space. For multi-class classification, multiple mask layers, `mLy`, are used in the last layer, one for each label `y`, to capture class-specific features and tokens. Each label has its own attention weights `aLy` and its own representation `vLy`. Instead of a fully-connected layer, a learnable weight vector `pL` per class projects `vL` to a scalar `cL`. These scalars are concatenated into a vector `c`, and the predictive distribution is computed by `softmax(c)`. To ensure that each mask layer extracts label-specific features, a distance regularization term `Ldist` is added to the loss function, penalizing pairwise cosine similarities between the corresponding mask layers.": 1253,
    "To assess large language models' (LLMs) metaphor understanding, the paper designs two primary tasks: Paraphrase Judgement and Paraphrase Generation. The Paraphrase Judgement task is a multiple-choice setup where an LLM is given a sentence with a highlighted metaphorical word and two candidate replacement words (Option A and Option B), along with \"\"Both\"\" (Option C) and \"\"Neither\"\" (Option D) choices. The candidates include one apt paraphrase and one inapt paraphrase, or two inapt paraphrases, or two apt paraphrases. This task is presented in two scenarios: \"\"Word-judgement,\"\" where only the candidate words are provided, requiring the model to implicitly form the paraphrased sentences, and \"\"Sentence-judgement,\"\" where each candidate word is embedded in the full sentence. Furthermore, three conditions are tested for each scenario: \"\"Implicit\"\" (no mention of metaphor), \"\"Metaphor-Sent\"\" (revealing the sentence contains a metaphor), and \"\"Metaphor-Word\"\" (revealing the highlighted word is metaphorical). This design specifically tests the model's ability to distinguish correct contextual interpretations (target domain) from literal or source-domain-related alternatives (inapt paraphrases). The Paraphrase Generation task prompts the model to generate a single word to replace the highlighted metaphorical word, aiming to produce a semantically equivalent sentence. This task employs a two-step generation process: first, the model generates a single token to reveal its top-ranked answers, and then, for tokens matching human answers, the model completes them into full words. This allows for a detailed comparison of model-generated paraphrases against human annotations, providing insight into the types of errors made (e.g., nonsensical, lack of contextual understanding, ungrammatical, or preference differences).": 1254,
    "The framework includes two main components: data collection and annotation of the AcnEmpathize dataset, followed by the integration of figurative language features into empathy detection models. The dataset consists of 12,212 forum posts annotated for the presence of empathy, gathered from the “Emotional and Psychological Effects of Acne” forum on acne.org. Posts are annotated as containing empathy or not by three annotators following guidelines provided by Sharma et al. (2020). The integration of figurative language features is achieved by appending one-hot encoded labels for idioms, metaphors, and hyperboles to the text of each post. Feature-based models include SVM, Naive Bayes, and Logistic Regression trained using LIWC psycholinguistic features. Pre-trained language models (PLMs) used include RoBERTa-large-mnli, RoBERTa-twitter-sentiment, and T5, fine-tuned with the appended figurative language features. An ensemble approach combines all PLM conditions with dynamic weighting based on each model's confidence scores. Experiments show marked improvements in accuracy and F1 scores across all models with the addition of figurative language features.": 1255,
    "Large Language Models (LLMs) effectively generate syntactically correct and contextually relevant logical forms within the Generation Stage of FuSIC-KBQA. The process leverages LLM few-shot in-context learning. For a given test question, the LLM is provided with a prompt that includes: (i) the test question itself, (ii) the retrieved and re-ranked KB context for each aspect (data-paths, entity types, relations) from each supervised retriever, (iii) an instruction to generate the logical form, and (iv) few-shot exemplars from the target domain. In a zero-shot setting, these target exemplars are replaced with randomly selected exemplars from the source training set. To minimize syntactic errors, FuSIC-KBQA generates logical forms in SPARQL, a common language with which LLMs are expected to be more familiar than niche languages like s-expressions. For representational consistency, retrieved KB data paths are also translated into SPARQL before being included in the prompt. When multiple retrievers are used, the retrieval size (k) for individual retrievers is linearly decreased to fit within the LLM's context length constraints.": 1256,
    "After obtaining the additional modal information (mI, mT, mMix) generated by the Multi-modal Large Language Model (MLLM), a modality fusion architecture is designed to facilitate inter-modal integration. First, an image encoder (ViT-Encoder) and a text encoder (XLMR-Encoder) are used to obtain vectorized encodings of the original image (xI) and text (xT). The MLLM-generated information (mI, mT, mMix) is treated as extra textual data and is concatenated with the original text (xT) before being processed by the text encoder. To enable the text encoder to distinguish between texts from different modalities, a segment encoding mechanism is adopted, similar to BERT's segment encoding. Learnable parameter vectors are added for text from each modality, where `segment(xi)` is 1 for `mI`, 2 for `xT` and `mT`, and 3 for `mMix`.\nThe output of the image encoder (V) and text encoder (T) are then prepared for fusion. For the textual modality, the average of all word vectors `mean(T)` is computed as the sentence representation. For the visual modality, the vector of the CLS token `VCLS` is taken as the image representation. A linear layer with a GeLU activation function maps `VCLS` to the same feature space as the textual modality, resulting in `Vreshape`. These two vectors, `Vreshape` and `mean(T)`, are then concatenated to obtain the final fused vector representation, `EMix = [Vreshape, mean(T)]`. This `EMix` vector is used by a linear layer and a softmax classifier for the main metaphor classification (`ˆy`).\nAdditionally, two separate classifiers are employed for fine-grained metaphor detection, categorizing metaphors predominantly driven by the image modality or the text modality. This is achieved by forming `EI = [Vreshape, mean(TmI)]` for image-dominated metaphors (`ˆyI`) and `ET = mean([TxT, TmT])` for text-dominated metaphors (`ˆyT`), where `TmI`, `TxT`, and `TmT` represent specific parts of the text encoding vector describing the image and text respectively.": 1257,
    "Conceptual domain mining uses VerbNet to annotate arguments and their semantic roles. A domain concept list is constructed from Master Metaphor List and OpenCyc. The algorithm traverses hypernym paths in WordNet to identify candidate conceptual domains. Domain granularity metrics are calculated to find the best conceptual domains, balancing broad meanings without being overly abstract.": 1258,
    "The paper addresses this by introducing a novel task called Self-Disclosure Abstraction and developing a model for it. The objective is to rephrase a given self-disclosure span within a sentence to reduce sensitive and specific details while preserving its core meaning and utility, fitting seamlessly into the original sentence. To generate training data, the paper uses a recent subset of its Reddit corpus. Automatic generation of training data is performed using large language models (LLMs), specifically GPT-3.5 (for training and development sets) and GPT-4 (for the test set), through chain-of-thought prompting with few-shot demonstrations. This prompting strategy instructs the LLM to first generate a rationale explaining why the disclosure span needs abstraction, and then to generate three diverse abstractions to accommodate varied user preferences. The abstraction models are built by fine-tuning Llama-2-7B (an open foundation and fine-tuned chat model) using LoRA (Low-Rank Adaptation of Large Language Models). Three different generation methods are explored: sampling (generating one abstraction at a time and sampling three times), end-to-end training (generating three abstractions simultaneously as input → A,B,C), and iterative generation (breaking down the three abstractions into separate training instances: input → A, input+A → B, input+A+B → C). For each method, various input-output setups are considered, including formatting input with special tokens and calculating loss on abstraction, or formatting input with instructions and calculating loss on either abstraction or rationale plus abstraction.": 1259,
    "The method integrates the contextual information from simulated dialogues by feeding both the original question and the generated dialogue into the LLM for final reasoning. In the \"\"Dialogue-Enhanced Reasoning\"\" step, the LLM (M) receives the original task description (T), the specific question (Q), and the simulated dialogue scenario (S) as input. A final prompt P3 (e.g., \"\"Finally, according to the question and conversation, reason and give the final answer\"\") is appended to guide the LLM towards producing the ultimate response (R). This process is formally represented as R = M(T ⊕ Q ⊕ S ⊕ P3). By providing the LLM with the rich, nuanced context embedded within the simulated conversation, the method enables the model to better comprehend the subjective aspects of the task, such as metaphorical relationships or emotional responses, which are often missed by traditional objective reasoning pathways. This integration allows the LLM to leverage the \"\"knowledge behind dialogues\"\" to inform its final answer.": 1260,
    "To generate high-quality counterfactual reference dialog chains, the paper employs GPT-4. This process involves creating two distinct sets of dialog chains: one for the incorrect literal intention (`IL`) and one for the true intention (`IT`). For the literal intention chain, `UL1` (the speaker's alternative utterance conveying `IL`) is generated by providing GPT-4 *only* with the `Incorrect Literal Intention (IL)`, specifically to prevent any potential contextual inference from `C` influencing `UL1`. Subsequently, `UL2` (the listener's response to `UL1`) is generated. For the true intention chain, `UT1` (the speaker's alternative utterance conveying `IT`) is generated by providing GPT-4 with both the `Context (C)` and the `True Intention (IT)`, aiming to mimic an ideal direct and cooperative utterance generation setting. Following this, `UT2` (the listener's response to `UT1`) is generated. This meticulous process ensures that the `UL1`, `UL2`, `UT1`, and `UT2` reference responses accurately represent the intended literal misinterpretation and true pragmatic meaning, respectively, serving as robust ground truth for the generative evaluation.": 1261,
    "The paper leverages the LLM's unfunning capability to generate synthetic aligned datasets by taking existing humorous texts and transforming them into non-humorous versions. For the Unfun task, the original satirical headlines are paired with the LLM-generated unfunned counterparts. These synthetic pairs form datasets used to train binary humor classifiers. Two configurations of synthetic data are considered for training: `[Synthetic unfun; Original satire]` and `[Human unfun; Synthetic satire]`. The first configuration assesses the LLM's \"\"unfunning\"\" ability, while the second assesses its joke-writing capabilities. The quality of these synthetic datasets for training humor classifiers is evaluated by measuring the holdout accuracy of MISTRAL and ROBERTA classifiers trained on them, compared to classifiers trained on human-edited data or unrelated news headlines. Human evaluations further assess the \"\"realness,\"\" \"\"funniness,\"\" \"\"grammaticality,\"\" and \"\"coherence\"\" of the generated unfunned texts, providing qualitative insights into the dataset quality. The goal is to determine if these synthetically generated aligned pairs can effectively serve as training data for robust humor detection models.": 1262,
    "The paper addresses this by implementing a \"\"metaphorical attribute-object binding\"\" process through \"\"attentional registration\"\" during the diffusion process. This method, named GOME, internalizes vehicles in the final scenario by blending metaphorical attributes from vehicles (source domain) into tenor objects (target domain). First, \"\"cross-domain bindings,\"\" consisting of object nouns and attribute modifiers, are identified from the enhanced visual elaboration sentence (Sv). This sentence is formed by concatenating the original elaboration with the perceived natural grounding sentence. A spaCy transformer-based dependency parser (ParserDP) is used to analyze Sv, identifying all object-nouns (proper or common) that are not direct modifiers of other nouns and are intended for visual inclusion. All modifiers of these nouns are then recursively collected into a \"\"metaphor bindings set\"\" (SMB), denoted as SMB = ParserDP(Sv). This set includes various syntactic relations like adjectival modification, compounds, adjectival complements, and coordination. To enforce these spatial relations within the attention maps of the diffusion model, a specifically designed loss function, `L`, is employed. `L` combines two terms: `Lpos` and `Lneg`. `Lpos` aims to minimize the distance (maximize overlap) between the attention maps (`Ao`, `Aa`) of object-attribute pairs `(o, a)` within SMB, using a symmetric Kullback-Leibler divergence (`Mdis(Ai, Aj) = K(Ai||Aj) + K(Aj||Ai)`). `Lneg` is designed to encourage separation between the attention maps of grammatically unrelated words; it computes the distance between words within the SMB set and words in the set of unmatched words (`Uv`) obtained by excluding SMB words from the full prompt. This loss is then minimized during the inference phase of a pretrained diffusion model by performing a gradient step on the noised latents, thereby steering the generation of visual metaphors to align attention maps of related objects and attributes.": 1263,
    "The Disambiguation module utilizes visual cues to clarify which of the possible meanings of a pun is being used in a given context. This involves a comparative analysis between the textual ambiguity and the visual elements, determining alignment and enhancing clarity. The module uses machine learning techniques to assess and match the visual cues with the ambiguous terms, effectively resolving the ambiguity.": 1264,
    "The paper addresses this through two types of denoising mechanisms: Code-Based Denoising (CBD) and Prompt-Based Denoising (PBD). CBD involves defining a set of post-processing rules, R, to remove irrelevant outputs such as extra explanations and unsolicited proofs from the LLM's direct output, resulting in a cleaned formal statement d(s) = R(fϕ(s)). PBD, on the other hand, uses a prompt, pden, to guide the LLM itself in performing the denoising. This is formulated as d(s) = LLM(pden, {(si,ϕi)}s, fϕ(s)), where {(si,ϕi)}s are retrieved items from MS-RAG used to maintain semantic consistency during the denoising process. The prompt-based approach allows for more flexible control over the output behavior compared to rigid rule-based methods. The paper explores different PBD prompts, including those that only remove explanations (PBD1A), add instructions for stylistic alignment (PBD1B), include fixed formal statement examples for stylistic alignment (PBD1C), or use retrieved examples from MS-RAG for stylistic alignment (PBD1D).": 1265,
    "To establish robust and comprehensive ground truth, the paper employs a multi-protocol human annotation methodology. Initially, a standard fine-grained annotation protocol was used, where three Upwork annotators, fluent in English, assigned binary faithfulness labels to each sentence in a summary. If a sentence was marked unfaithful, a brief written justification was required. A summary was deemed unfaithful if two or more annotators marked the same sentence as such. This process yielded an almost perfect inter-annotator agreement (Fleiss-kappa of 0.85). However, recognizing that even high agreement might miss subtle errors, this initial set of \"\"annotator labels\"\" was compared against two additional human evaluation protocols: \"\"Expert\"\" and \"\"Hybrid.\"\" The \"\"Expert\"\" protocol involved three of the paper's authors, experienced in faithfulness research, reviewing and adjudicating their labels for each summary until full agreement was reached. The \"\"Hybrid\"\" protocol (detailed in Solution 3) used GPT-4 to generate potential inconsistencies, which were then reviewed by new Upwork workers. Since each method detected different, legitimate inconsistencies, a final \"\"expanded gold set\"\" of labels was created by taking the union of errors detected by the annotator and expert labels, and then manually reviewing and filtering legitimate errors from the hybrid method's suggestions. This manual review and merging process, including written error explanations, ensured broader coverage of inconsistencies, demonstrating that a single protocol is insufficient for comprehensive ground truth in challenging narrative summarization.": 1266,
    "Specialized expert models are effectively trained by leveraging the categorized multimodal datasets, where each subset corresponds to a specific interaction type. The method involves training three distinct expert models: fr for redundancy, fu for uniqueness, and fs for synergy. Each expert model is trained exclusively on the subset of data points that maximally exhibit its corresponding interaction type. For instance, the redundancy expert model (fr) is trained only on data points categorized as Redundancy, the uniqueness expert model (fu) on Uniqueness data, and the synergy expert model (fs) on Synergy data. While these individual expert models share the same input format (image and text data pairs), their learning outcomes are significantly differentiated due to the specific multimodal data distributions they are exposed to during training. To optimize training efficiency and performance, the expert training phase is initialized not from scratch, but from fine-tuned baseline models. This approach allows the expert models to build upon already learned features and maintain a general competency across the entire dataset, which is beneficial during inference if the fusion process misassigns data points. Additionally, data filtering and rebalancing techniques are applied to the categorized subsets to ensure high-quality and balanced training data for each expert, preventing overfitting to majority classes, especially for interaction types with fewer training examples like synergy.": 1267,
    "The paper proposes a systematic human evaluation framework for metaphor translation based on manual annotation, designed to provide a more accurate and insightful assessment of MT performance in handling metaphors. This framework evaluates MT outputs in terms of their metaphorical expressions across four key aspects: Quality, Metaphorical Equivalence, Emotion, and Authenticity. Annotators use a 5-point Likert scale for Quality and Authenticity, and specific categorical labels for Equivalence and Emotion.\nThe four criteria are defined as follows:\n1. Quality: This criterion assesses the overall translation quality, inspired by existing MT assessment methods. It comprises three primary aspects:\n    * Fluency: Evaluates how well-formed and grammatical the translation is, ensuring it sounds like it was originally written in the target language.\n    * Intelligibility: Assesses how easily the translation is understood and whether it sufficiently conveys metaphorical meanings, allowing readers to grasp the intended interpretation.\n    * Fidelity: Measures the extent to which the translation is faithful to the source sentence, minimizing distortion, twisting, or altering of meaning.\n    * Overall: A holistic assessment of the entire sentence's quality.\n2. Metaphorical Equivalence: This novel criterion ascertains how metaphors impact MT by describing how figurative expressions are translated. It considers two features: how the meanings of the source and target are conveyed, and whether the translation remains figurative. Annotators label the translation using five distinct tags:\n    * Full-Equivalence: Both literal and contextual meanings of the target word are the same between source and translation.\n    * Part-Equivalence: Only contextual meanings are similar; literal meanings differ, but both source and translation are metaphorical.\n    * Non-Equivalence: Only contextual meanings are similar; the translation is non-metaphorical, and literal meanings differ.\n    * Misunderstanding: Literal meanings are similar, but the translation fails to convey the contextual meaning of the target word in the source language.\n    * Error: The target word is mistranslated, with both contextual and literal meanings differing.\n    For non-metaphorical source instances, translations are simply classified as Literal, Metaphorical, or Error.\n3. Emotion: Inspired by prior work, this criterion investigates whether metaphorical expressions in translations convey additional emotional information. Annotators determine the extent to which the target word and its translation convey different amounts of emotion using four labels:\n    * Zero: If the target word in the source context conveys no emotion.\n    * Less: The translation conveys less emotion than the source.\n    * Same: The translation conveys a similar degree of emotion as the source.\n    * More: The translation conveys more emotion than the source.\n4. Authenticity: An extension of existing criteria, this evaluates whether the translated metaphor reads like standard, well-edited language, such that a native speaker of the target language would understand it idiomatically.\nThe annotators are asked to judge all these criteria on a 5-point Likert scale (for Quality and Authenticity) or by selecting specific categories (for Equivalence and Emotion). This fine-grained protocol allows for a comprehensive analysis of metaphor translation complexities.": 1268,
    "The paper develops MEIRa (Major Entity Identification via Ranking), a supervised model inspired by entity ranking formulations, which maintains an explicit representation for entities. The MEIRa model consists of three main steps: document encoding, candidate mention proposal, and an identification module. Document encoding is performed using a Longformer-Large, denoted as `ϕ`, which is fine-tuned for the task. Mentions (or spans) are encoded as `mi = ϕ(mi, d)` by concatenating the first, last, and an attention-weighted average of token representations within the mention span. Designative phrases `p(ej)` for major entities are also encoded using the same backbone, resulting in entity representations `ej = ϕ(p(ej), d)`. A mention proposal network, similar to prior work, predicts high-scoring candidate mentions, identifying all potential mentions (`Mall`), not just those corresponding to major entities. The identification module initializes a working memory `EW` as a list of `L` major entities based on their designative phrase representations. For a given mention `mi`, the identification module computes the most likely entity by scoring the likelihood of tagging `mi` with each entity `ej` using an MLP function `f()`. This function takes as input the mention encoding `mi`, the entity embedding `ej`, and metadata `χ(mi, ej)`. The metadata `χ(mi, ej)` includes a distance embedding representing the log distance between `mi` and the last tagged instance of `ej`. The mention `mi` is assigned to the top-scoring entity `e*i` if its score `s*i` is above a threshold `τ` (set to 0 in practice), or to a null entity `∅` otherwise. For parallelization, the MEIRa-Static (MEIRa-S) variant is introduced, where the working memory `EW` of entity representations remains constant and is not updated with new mention associations. This static memory allows mentions to be processed in parallel batches, significantly speeding up the inference process compared to models that require sequential updates.": 1269,
    "To improve the quality of LLMs' explanations, the paper proposes a combination of fine-grained punchline checks and coarse-grained pairwise comparisons. The fine-grained check involves verifying whether elements of the pun pair (pun word, alternative word, pun sense, and alternative sense) are correctly mentioned in the explanation. Annotators assess the quality of explanations for 100 homographic and 100 heterographic puns, ensuring high reliability. The coarse-grained comparison uses GPT-4 to choose the better explanation between human and model-generated ones, calculating win rates to evaluate overall quality. This dual approach ensures thorough and reliable assessment of pun explanations.": 1270,
    "To investigate the possibility of geographically attributing proverbs, the paper formulates the task as both a text classification problem (detecting the place of origin) and a text regression problem (estimating geographical coordinates). A balanced corpus is created using 500 proverbs per location from 23 locations with over 1,000 unique proverbs, totaling 11,500 proverbs, split into train (90%), dev (5%), and test (5%) subsets. For text classification, various methods are benchmarked: Authorship Analysis (AA), which trains a 3-gram character-based language model for each location and attributes a proverb to the model with the lowest perplexity; and supervised machine learning algorithms including Logistic Regression (LR), Random Forests (RF), K-Nearest Neighbours (KNN), and linear Support Vector Machines (SVM). These conventional models use character n-gram frequency and inverse-proverb frequency representations. Transfer learning is assessed using Gr-BERT (Greek BERT), a BERT model pre-trained on contemporary Greek text, which is fine-tuned for the classification task. Additionally, embeddings from large language models (Gemini, OpenAI, Mistral) are used with Logistic Regression. For text regression, the task is framed as a multi-output regression problem predicting latitude and longitude. Models benchmarked include Linear Regression, ElasticNet, K-Nearest Neighbours Regression, Random Forests, and Extremely Randomised Trees (Xtrees), all using TFIDF features.": 1271,
    "The paper addresses this through a \"\"Domain-Specific Knowledge-Guided Response Generation\"\" phase. While the base model (Mbase) can generate domain-specific responses conditioned on the previously generated instructions and contexts, the approach optionally allows for leveraging external domain-relevant knowledge. This is achieved via a retrieval component (Mret). For a given query (x), formed by concatenating the instruction (i) and context (c), Mret fetches the top-k relevant documents (d1:k). Each document (dj) is then independently paired with the query (x) to form a prompt for Mbase. The final domain-specific responses (y) are produced by marginalizing over the probabilities of these k-combinations at each generation step, using a relevance score (pret) from the retriever and the language model distribution (plm). This integration encourages generated responses to be more nuanced and domain-specific. Additionally, the framework supports \"\"Iterative Self-Specialization,\"\" where the better-aligned model (Maligned) from a previous iteration can be used as the base model for re-generating instructions and responses, iteratively refining the model's domain expertise.": 1272,
    "The paper introduces a vision-language agent whose primary mission is to bridge the gap between understanding visual context and language, making this information usable for the cerebral language agent's planning. Its core component is a vision-language model (VLM), which is a task-driven large model that translates visual input into language output required by specific tasks. In the context of LEGO assembly training, two distinct tasks are defined for this agent:\n1.  Object detection (T1): Given an image or sequence of images, the agent predicts the position of a requested object and outputs it in the format \"\"<Object><Xleft><Ytop><Xright><Ybottom>\"\". For example, if a trainee asks \"\"Is this the one?\"\" while pointing to an object, the agent recognizes the object.\n2.  Assembly state detection (T2): Given an image or sequence of images, the agent identifies if the current assembly state matches a reference state from the instruction manual. For instance, it assists with queries like \"\"Am I assembling them correctly?\"\".\nThe vision-language agent utilizes MiniGPT-v2 for generating inference output for object detection.": 1273,
    "To improve the unified model's performance, JointEDI incorporates two auxiliary tasks: Euphemism Detection (ED) and Euphemism Identification (EI). The ED task focuses on determining whether potential euphemism terms are used euphemistically, and its training objective is defined by a binary cross-entropy loss (LED). The EI task aims to identify the specific categories corresponding to potential euphemism terms, using a multi-class cross-entropy loss (LEI). The overall training objective for JointEDI is a weighted sum of three loss functions: the main task's cross-entropy loss (Lce), the ED auxiliary loss (LED), and the EI auxiliary loss (LEI). This combined loss function is formalized as `L = αLce + βLED + γLEI`, where `α`, `β`, and `γ` are hyperparameters representing the weights of each loss, summing to 1. This multi-task learning approach allows the model to benefit from explicit supervision on the sub-tasks, guiding it to learn more robust representations for both detection and identification.": 1274,
    "To address the challenge of aligning humor preferences while preventing catastrophic forgetting of structural preferences, the paper proposes an Improved Humor DPO algorithm in the second stage. This algorithm introduces a triplet preference learning method. After the first stage of structural optimization, candidate sentences are sampled from the structurally optimized LLM using the same prompts (x). Sentences that satisfy the pun structure, as determined by the structured discriminator, are retained and called \"\"rumination\"\" samples (y∗). Negative samples (y−h∗) are selected from the candidate sentences that lack humor (judged by a humor discriminator) and also do not satisfy the pun structures (judged by the structured discriminator). The positive samples (y+) remain the labeled puns from the dataset. These three types of samples (y+, y−h∗, y∗) form humor preference triplets. The humor preference of the LLM is then optimized using a new triplet loss function: `LI-humor-dpo(θ) = -logσ(r+ϕ(θ) - r∗(θ)) - logσ(r∗(θ) - r−h∗(θ))`. Here, `r+ϕ` and `r∗` are pseudo-rewards for positive and rumination samples, respectively, relative to the structurally optimized model (`πϕ`) from stage 1, and `r−h∗` is the pseudo-reward for the negative samples. This loss function aims to maximize the reward difference between positive and rumination sequences, and also between rumination and negative sequences, thereby guiding the LLM to maintain structural integrity while enhancing humor.": 1275,
    "To enhance diversity through label composition, the framework first performs lexical clustering on the labels (l') of the filtered target domain data (Dtf). This involves encoding each label (l'i) into a vector representation (hl'i) using MiniLM-L6. The K-means clustering algorithm is then applied to partition labels into K clusters, where K is determined by the silhouette coefficient method. Within each cluster, the semantic similarity between the text of each pair of data points (t'i, t'j) is measured using cosine similarity on their vector representations (ht'i, ht'j), also encoded by MiniLM-L6. Data points with the lowest semantic similarity are selected to maximize diversity. Finally, the labels (l'i and l'j) from these selected farthest points within the same cluster are concatenated (l'' = l'i ⊕ l'j) and fed into the generation model (Mg) to obtain a synthesized text (t'' = Mg(l'')). This process combines different labels, even those with potentially different sentiment polarities, to generate more complex and information-dense sentences, thereby increasing both information density and label diversity.": 1276,
    "The paper proposes a swap reconstruction mechanism based on the intuition that ideal modality-invariant and modality-specific representations should be capable of reconstructing their original unimodal representations when combined. This mechanism assumes that modality-invariant representations from various modalities encapsulate the same semantic information, while modality-specific representations hold unique modality-related information. To promote tighter cross-modal alignment and enhance representation quality, the process involves reconstructing the original unimodal speech representation (`ˆria`) by merging the text modality-invariant representation (`hit,inv`) with the speech modality-specific representation (`hia,spe`) using a linear layer `ˆEa`. Similarly, the original unimodal text representation (`ˆrit`) is reconstructed by combining the speech modality-invariant representation (`hia,inv`) with the text modality-specific representation (`hit,spe`) using a linear layer `ˆEt`. Finally, a reconstruction loss (`Lrecon`), calculated as the mean squared error between the original and reconstructed representations, is minimized to enforce this reconstructive capability.": 1277,
    "To evaluate the impact of figurative language (FL) features on Authorship Attribution (AA) tasks, the Multi-task Figurative Language Model (MFLM) is utilized. The classification layer of the MFLM is discarded, and the underlying Transformer model is used to generate sentence embeddings. These sentence embeddings are computed by mean-pooling all token embeddings, including the [CLS] token, from the last hidden layer. Document embeddings are then created by averaging the embeddings of individual sentences, resulting in a 768-dimensional vector for each document. These MFLM document embeddings are then used as input features for Multi-Layer Perceptron (MLP) classifiers. The MLP models consist of a single hidden layer with 1024 units. The performance of MFLM embeddings is evaluated against several baselines: classical Stylometric features (52 text metrics), SBERT (all-roberta-large-v1) sentence embeddings, and word and character n-gram TF-IDF vectors. Furthermore, the paper explores the impact of integrating FL features by concatenating MFLM embeddings with these baseline document vectors and subsequently training new MLP classifiers. The evaluation is performed on three publicly available AA datasets (IMDb-62, PAN-2006, PAN-2018) using weighted average F1-score.": 1278,
    "The method updates the pre-trained language model's loss function through a Loss Function Update Mechanism (LDADA). This involves two main steps: extraction of the model's term distribution and injection of this information into the loss function. For Extraction from model, the model's internal term distribution, referred to as Rmodel(d;M), is derived to bridge the gap between the model's embedding space and the vocabulary space. This is achieved by applying a Max-Sim-like operation, where the embedding matrix `E` generated by the model `M` for document `d` is combined with the vocabulary matrix `S` obtained from the intermediate output of the observation-level distribution (SPLADE). Specifically, `Rmodel(d;M)` is computed as `max_i∈[|E|](E_T_i · S)`. For Injection into model, the goal is to minimize the discrepancy between the data-driven `Runified(d)` (from Solution 1) and the model-extracted `Rmodel(d;M)`. Both `Runified(d)` and `Rmodel(d;M)` are normalized using softmax to represent probabilities. The `LDADA(d;M)` loss component is then defined as the Kullback-Leibler (KL) divergence between the softmax-normalized `Runified(d)` and `Rmodel(d;M)`. This `LDADA` component is added to the base GPL loss `LGPL(q,d;M)`, forming the overall training objective `L(d;M) = LGPL(q,d;M) + LDADA(d;M)`, thereby guiding the model to incorporate the multi-level distributional feedback.": 1279,
    "The paper proposes a cost-efficient method to repurpose traditional lexica, specifically the Dictionary of Standard Modern Greek (Triantafyllides, 1998), into structured datasets using ChatGPT (Brown et al., 2020) as a language-neutral parser. The pipeline begins by filtering the dictionary's SQL database for entries containing at least one example, resulting in 28,831 unique lemmata. A random sample of 100 lemmata is then manually converted into a succinct JSON format, specifying the lemma, senses, definitions, and examples, with extra effort to disentangle hierarchical senses and remove inconsistencies. This manually curated JSON serves as the training set for a quick one-shot tuning of ChatGPT4 (specifically, `gpt-3.5-turbo` via the fine-tuning API). The raw text of the remaining dictionary entries (stripped of HTML tags for token economy) is then passed through the fine-tuned model. The model's output is further filtered to retain senses with examples and entries with at least two senses. Finally, all resulting entries are manually checked to fix parsing errors, homogenize presentation, and correct JSON formatting, yielding a high-quality structured dataset from which various NLP tasks can be derived.": 1280,
    "To investigate the influence of euphemistic semantic categories on cross-lingual knowledge transfer, the paper conducts a follow-up experiment using specific euphemistic categories: physical/mental attributes (ATTR), bodily functions/parts (BODY), death (DEATH), and sexual activity (SEX). Test sets are created by isolating a single language and a single category. Two distinct training settings are compared: \"\"same-category, out-of-language\"\" (SC-OOL) and \"\"same-language, out-of-category\"\" (SL-OOC). In the SC-OOL setting, the model is trained only on examples from the same euphemistic category but from other languages (e.g., training on English, Spanish, and Yorùbá ATTR euphemisms to test on Chinese ATTR). In the SL-OOC setting, the model is trained only on examples from the same language but from other categories (e.g., training on Chinese euphemisms from BODY, DEATH, and SEX categories to test on Chinese ATTR). Since the number of SC-OOL examples was consistently fewer than SL-OOC examples, the maximum available SC-OOL examples were used, and SL-OOC examples were down-sampled to match. A random 90-10 split was used for creating training and validation sets within these specific category experiments. The fine-tuning parameters were similar to the main experiments, except the early stopping patience was increased to 10 due to smaller datasets, and only 10 runs were performed for each setting. The results are presented as the differences in average Macro-F1 scores between the SC-OOL and SL-OOC settings, indicating which training strategy performed better for each language-category scenario.": 1281,
    "The explicit multi-task model facilitates representational alignment by employing two distinct standard softmax classifiers that share a common Pre-trained Language Model (PLM) encoder. The representation of each token, `xPLM`, obtained from the PLM's last layer, is simultaneously forwarded to two separate classifiers. The first is a Trigger Detection (TD) softmax classifier, `softmax(WTtdxPLM + btd)`, which is responsible for predicting the IOB2 event trigger label for the token. The second is a Relation Extraction (RE) softmax classifier, `softmax(WTrexPLM + bre)`, which predicts the IOB2 relation label for the token. During training, the (multi-class) cross-entropy loss is calculated for each classifier independently on a mini-batch basis. The average of these calculated TD and RE losses is then used to update the parameters of both the shared PLM encoder and the two classifiers. This joint optimization process is where the interaction and knowledge transfer between the two tasks occur. Crucially, at inference time, the OIE relation labels are not directly used. The intuition behind this design is that if the notion of triggers is universal across domains and OIE relations are indeed domain-independent, then leveraging the in-domain trigger-relation connection during training should be sufficient to improve cross-domain performance without requiring OIE system output at inference.": 1282,
    "The Gradual Training + Stratified Clustering-based Sampling (G+SC) approach improves domain adaptation by iteratively retraining a pre-trained model using a dynamic mix of source and target domain data, where target domain samples are strategically chosen using the proposed clustering-based method. In this sequential training setup, the model undergoes 'I' iterations. At the beginning of each iteration, the model's weights are initialized with the trained weights from the preceding iteration, with the first iteration starting from the original pre-trained weights. Each iteration utilizes a total of 'k' few-shot samples, composed of varying concentrations of target domain data and source domain data. Specifically, the proportion of target domain samples is progressively increased (e.g., from 20% to 100% across iterations), while the number of source domain samples is concurrently decreased. For the target domain samples, the high-quality, representative samples are selected using the Stratified Clustering-based Sampling (SC) approach. Conversely, source domain samples are chosen via uniform random sampling. This iterative process, characterized by a gradual shift in data distribution from source to target and the inclusion of highly representative target samples from SC, aims to achieve a smoother and more robust domain adaptation.": 1283,
    "The paper introduces a Domain Knowledge Warmup strategy to align the representations between the domain-invariant and domain-variant knowledge. This is crucial because the domain-variant adapters remain static after training on their specific datasets, while the domain-invariant adapter continuously changes throughout the continual learning process. Direct combination of these adapters could lead to mismatches and performance degradation. To mitigate this, after the N-th domain has been trained, the model performs additional training. In this warmup phase, each domain-variant adapter (Bi, Ai) from the set of learned adapters (B1,A1),...,(BN,AN) is combined with the current domain-invariant adapter (BS,AS). During this process, only the domain-invariant adapter (BS,AS) is fine-tuned using replay data (DR,N), which comprises samples collected from all previously learned domains. The domain-variant adapters are kept frozen. This approach ensures that the domain-invariant adapter adapts and aligns its parameter distribution with the diverse set of domain-variant adapters, thereby facilitating effective subsequent combinations during inference.": 1284,
    "A comprehensive dataset for metaphor reasoning is constructed by leveraging ChatGPT3.5 to expand existing traditional metaphor datasets, namely VUAALL, MOH-X, and TroFi. The process involves designing a specific prompt, referred to as \"\"Prompt 1,\"\" which takes three inputs: the original sentence (si), the target word (wi), and its usage (metaphor or literal). ChatGPT3.5 is then guided by this prompt to generate a corresponding reason for the given usage. For example, if \"\"dominated\"\" is used metaphorically, ChatGPT generates a reason like \"\"Visually appealing, cost-effective, lack of feature films from studios.\"\" This method is chosen over manual annotation due to its cost-effectiveness. The generated reasons serve as the foundational reference data. The expanded datasets (VUAALL_R, TroFi_R, MOH-X_R) are then divided into training, testing, and validation sets with a ratio of 0.7, 0.15, and 0.15, respectively.": 1285,
    "The Average Concept Distance (ACD) metric evaluates the creativity of generated metaphor captions by computing the cosine distance between the primary and secondary concepts in the caption and grounding it with the BERTScore. This metric captures both the novelty of the comparison and the semantic similarity to the reference caption. The primary and secondary concepts denote the objects being compared, while the cosine similarity measures the distance between them. By weighting the cosine distance with the BERTScore, the ACD metric provides a balanced evaluation of the generated caption's creativity and fluency.": 1286,
    "To mitigate catastrophic forgetting and mode collapse during the multi-level adversarial domain adaptation, the proposed method integrates knowledge distillation (KD) alongside the generator loss. For each layer `i`, a knowledge distillation loss, `LKDi`, is computed. This loss is defined as the KL-divergence between the class probabilities predicted by the source-trained classifier `Ci` when applied to the source encoder's output `Esi(xs)`, and the class probabilities predicted by the same source-trained classifier `Ci` when applied to the target encoder's output `Eti(xs)`. Specifically, `LKDi = KL(Ci(Esi(xs)), Ci(Eti(xs)))`. By minimizing `LKDi`, the target encoder is encouraged to produce representations at each layer that, when processed by the fixed source classifiers, yield similar classification outputs to those of the source encoder. This mechanism ensures that as the target encoder adapts to become domain-invariant, it simultaneously preserves its original task-specific knowledge. The total generator loss for each layer `i` is then a combination of the adversarial objective and the knowledge distillation objective, expressed as `Li = Lgeni + LKDi`. This layer-wise distillation, facilitated by the presence of attached exit classifiers, helps maintain the integrity of the learned features and prevents the model from deviating significantly from its original classification task or collapsing into trivial solutions during the domain adaptation process.": 1287,
    "To mitigate hallucination and improve faithfulness, the proposed framework integrates two explicit reasoning steps: Question answerability classification (ac) and Answer sentences selection (ss). In the `ac` state, the current user query (qi) is classified as either answerable or unanswerable from the given document (d). This classification is performed by few-shot prompting an LLM, or optionally, a separate classifier fine-tuned on existing QA data. If the query is deemed unanswerable, the Agent utterance generation (au) state deterministically produces a predefined \"\"no answer\"\" text, preventing the generation of fabricated responses. In the `ss` state, relevant sentences from the document (d) pertaining to the user query (qi) are identified and selected. This step ensures that the subsequent Agent utterance generation (au) can focus solely or primarily on the identified grounding sentences, thereby increasing the faithfulness of the response to the document. The output of `ss` (relevant sentences) is provided as input to `au`, guiding the generation process to be strictly content-grounded.": 1288,
    "To enhance decision-making, the paper proposes a self-exploration strategy where a fine-tuned Intent Predictor (Mintent) generates multiple probable intents. Mintent is a smaller language model (Flan-T5-XL or Mistral-7B) trained on the intent-augmented demonstration dataset. Given the current observation, previous actions, and previous intents, Mintent predicts a set of top-k probable intents (ˆz1t,...,ˆzkt) using beam search. These top-k intents are then provided as a concatenated list within the input prompt to the LLM policy (π). The LLM policy is instructed to examine these suggested intents collectively. It is guided to select the first valid and sensible intent from the sorted list (most probable to least probable) that aligns with the current situation, task, and previous progress. This internal process of reviewing multiple plausible directions allows the LLM agent to implicitly explore different semantic hypotheses, leading to improved action prediction without explicit environmental interactions. If none of the suggested intents are valid, the LLM attempts action prediction without them.": 1289,
    "The paper explicitly models the dynamic change in a word's sensorimotor properties through a dedicated component called MIP_SM (Metaphor Identification Procedure - Sensorimotor). This module is designed to capture the difference in sensorimotor activation between a word's literal (basic) and contextual (potentially metaphorical) uses. First, the system obtains two types of word embeddings using roberta-base encoders: `ht`, representing the basic meaning of the target word (derived from inputting the word with special tokens into an encoder), and `hS,t`, representing the contextual meaning of the target word within a given sentence. These `ht` and `hS,t` embeddings are then fed into the previously trained sensorimotor regressors (as described in Solution 1) to generate `SMt` (basic sensorimotor embedding) and `SMS,t` (contextual sensorimotor embedding), respectively. To model the *change*, the `SMt` is concatenated with its corresponding basic meaning embedding `ht`, and similarly, `SMS,t` is concatenated with its contextual meaning embedding `hS,t`. These combined, enriched representations are then passed through a linear function within the MIP_SM module. This linear function is trained to learn and reflect the specific shifts or abstractions in embodiment-related dimensions that occur when a word is used metaphorically, providing a direct, cognitively motivated signal of sensorimotor change that is then utilized for metaphoricity classification.": 1290,
    "The paper addresses this by incorporating a meta-learning stage as part of its Meta-Knowledge Distillation (MKD) strategy. This stage employs a model-agnostic training approach with a two-step gradient update to learn general meta-knowledge among multiple source domains, aiming for better parameter initialization. Initially, the parameters (`theta`) of the hypernetwork-assisted model are randomly initialized. For each `k`-th source domain (`Sk`), `n` training instances are sampled to calculate the average loss `LSk(f_theta)`, where `f_theta` represents the model's output. In the first step, a temporary set of parameters (`theta'k`) is obtained by updating `theta` using gradient descent based on the loss from `Sk`: `theta'k = theta - alpha * nabla_theta LSk(f_theta)`. In the second step, the model's parameters (`theta`) are updated by minimizing a meta-learning objective function. This involves recalculating the loss using the temporary `theta'k` for each source domain and summing these losses across all source domains: `theta <- theta - beta * nabla_theta Sum_Sk LSk(f_theta'k)`. This optimization process continues until the validation accuracy on the source domains ceases to increase. The resulting meta-updated `theta` parameters are then used to initialize the teacher network for the subsequent knowledge distillation stage, effectively capturing domain-invariant knowledge and providing a robust starting point for fine-tuning.": 1291,
    "The paper proposes an interactive self-learning strategy for robust source-free domain adaptation in the target domain. This strategy involves an iterative cycle of fine-tuning three components: the Question Generation (QG) model (fgen), the Question Answering (QA) model (f), and the soft prompt (π).\n1.  Pseudo-Question Generation: For each unlabeled context (c') in the target domain, the QG model (fgen), which was initially trained on the source domain, generates a pseudo-question (q' = fgen(c')). These generated pseudo-questions are then subjected to Language Model (LM) filtering to select high-scoring, high-quality questions.\n2.  Pseudo-Answer Generation: The filtered pseudo-questions (q') are combined with their respective contexts (c') and fed into the QA model (f), which leverages the prompt (π) to elicit pseudo-answers (a' = f(π, c', q')). These pseudo-answers are also subjected to LM filtering to sieve out low-confidence responses.\n3.  Iterative Fine-tuning Cycle: The resulting pseudo-triplets (c', q', a') instigate a systematic self-learning cycle:\n    *   Prompt Fine-tuning: The prompt (π) is refined first using the pseudo-labeled data, while the QA model (f) is kept static. This step aligns the prompt with the subtleties of the target domain.\n    *   QA Model Fine-tuning: After the prompt is optimized, a fresh set of pseudo-answers (a'') is generated by the QA model using the newly refined prompt (a'' = f(π, c', q')). The QA model (f) is then fine-tuned using these (c', q', a'') pairs, with the prompt (π) remaining static.\n    *   QG Model Fine-tuning: Finally, the QG model (fgen) undergoes fine-tuning using the (c', q') pairs, refining its question-generation capabilities specifically for the target domain.\nThis cycle perpetuates in an alternating fashion, progressively honing the prompt, QA model, and QG model. Each phase utilizes the pseudo-labeled samples, with LM filtering ensuring the quality of the generated labels, thereby fostering a more robust and effective domain adaptation without needing source domain data.": 1292,
    "Both general linguistic capabilities and domain-specific knowledge are simultaneously preserved during pruning by modifying the original loss function of LLM training with a regularization term. This updated loss function, Lours, combines the standard next token prediction loss, Lnext, with a regularization term, Lregular. The regularization term is designed to constrain the change of important general weights identified in the first step. Specifically, Lregular is defined as λ * Σ(Gm * (Wm' - Wm)^2) over all M prunable weights, where Gm is the general weight importance, Wm' denotes the updated weight value, and λ is a hyperparameter. To make this computationally feasible, especially given the large number of parameters, the regularization term is reduced by substituting Wm' - Wm with -α * gmnext, where α is the learning rate and gmnext is the gradient of each parameter with respect to Lnext. This results in Lregular = λ * Σ(α^2 * Gm * (gmnext)^2). During the backward pass, instead of directly computing second-order derivatives for the Hessian matrix, the method approximates the diagonal of the Hessian using the empirical Fisher information matrix. The empirical Fisher is approximated by the average of the squared gradient of the model's prediction. The final gradient for the regularized loss, ∂Lours/∂Wm, is then computed as the sum of ∂Lnext/∂Wm and ∂Lregular/∂Wm, allowing the training process to identify weights contributing to both general and domain knowledge.": 1293,
    "During the finetuning stage on target domain data, the model aims to enhance its ability to distinguish target entities from entities that are likely to be mislabeled as target entities but actually belong to the source domain. This is achieved through a two-step process:\n1.  Pseudo-Labeling: For a given passage in the target training data that contains annotated target entities, the model (pretrained in the source domain as described in Solution 1) is used to automatically detect entities that might belong to the source domain. These detected entities are then assigned pseudo-labels, effectively identifying them as \"\"out-of-domain\"\" or \"\"source-like\"\" entities within the target corpus.\n2.  Contrastive Discrimination: To enable the model to discriminate between these true target entities and the pseudo-labeled source entities, the multi-similarity (MS) loss is employed again. In this context, the labels for the MS loss are defined such that target entities form one group, and pseudo-labeled entities form another. The MS loss then pushes the representations of these two groups into separate regions of the feature space. This explicitly reduces the similarity between the representations of target entities and the potentially mislabeled source entities, thereby minimizing the risk of false positives in the target domain. The final finetuning objective combines the standard cross-entropy loss for NER with this multi-similarity loss term.": 1294,
    "To investigate the relationship between memorization and reasoning, the paper employs two distinct evaluation tasks. For memorization, mLLMs are prompted to complete proverbs where the last word is masked out. For non-MLM models, if the greedily generated string starts with the missing token or the entire proverb is a substring of the generated string, it is counted as memorized. For MLM models, the last word is masked with '<mask>', and the model's prediction for this token is evaluated. Five different prompt patterns are used, and the union of memorized examples across these patterns determines memorization accuracy. For reasoning, an inference task is designed where models are given a proverb in a conversational context and two choices (one correct, one wrong) for its meaning. The model computes the logits for each answer candidate ('A' or 'B') using a specific prompt template (Table 9, Appendix B), and the choice with the larger logit is selected as the answer. This allows for a direct comparison between memorization rates and reasoning accuracy. Furthermore, to assess reasoning with figurative expressions, the results of the inference task are broken down based on whether the proverb is labeled as figurative or non-figurative in the MAPS dataset, allowing for an analysis of performance differences on these two categories.": 1295,
    "This question is addressed by the M-MIP (Multiple Metaphor Identification Procedure) module. Building upon the core idea of MIP, which identifies metaphoricity by detecting the difference between a word's contextual and literal meanings, M-MIP enhances this process to capture a broader spectrum of semantic nuances. While existing methods often rely solely on fully connected layers to learn these differences, which can increase complexity and noise, M-MIP introduces an additional component. It calculates a similarity vector (hsim) using the Manhattan distance between the contextual meaning vector (ht) and the literal meaning vector (hl) of the target word. This hsim explicitly represents basic semantic similarity. Simultaneously, it employs a fully connected layer (with weights WM-MIP and biases bM-MIP) to process the concatenated ht, hl, their absolute difference, and their element-wise product, thereby learning more complex feature interactions. The final hM-MIP representation is formed by concatenating the output of this fully connected layer with the hsim vector. This dual approach ensures that the model learns both fundamental and intricate differences between the contextual and literal meanings, leading to a more robust identification of metaphorical usage.": 1296,
    "The paper addresses this by conducting a numerical analysis of the dataset, focusing on the count and mean ratio of metaphorical tokens (all, single, and composite) relative to total tokens for both conservative and liberal readers across the three persuasive effects: challenging, no effect, and reinforcing. To determine significant variations among these values, the paper initially performs significance tests, specifically the Anova test if homogeneity and normality assumptions are met, or the Kruskal test otherwise. If the p-value from these initial tests is less than 0.05, a post-hoc analysis is then conducted. This post-hoc analysis employs an independent t-test when normality is satisfied, or a Mann-Whitney test otherwise, with Bonferroni correction applied for each pair of effects being compared (Challenging vs. Reinforcing, Challenging vs. No Effect, and Reinforcing vs. No Effect). Additionally, the effect size (r) is calculated to quantify the strength of any observed significant differences, providing a measure of the practical significance of the findings.": 1297,
    "To strategically increase a pre-trained language model's (PLM) sensitivity towards domain-specific terms (DS-terms) while retaining knowledge of generic terms, the paper introduces the concept of \"\"mask-specific losses.\"\" This approach aims to impose larger penalties on the model for inaccuracies in predicting corrupted (masked) DS-terms compared to corrupted generic terms. The computation of these mask-specific weights involves first obtaining the total number of Entity-level Masking (ELM) and Base-level Masking (BLM) masked tokens within the training dataset, denoted as NELM and NBLM respectively. A preliminary mask-specific weight `wx` for each mask type (ELM & BLM) is computed as `1 - (Nx / Sum(Nx for x in {BLM, ELM}))`, representing the difference between 1 and the corresponding mask type probability. To elevate sensitivity towards DS-terms while avoiding overfitting, a \"\"sensitivity threshold\"\" of 0.5 is introduced. This threshold forces a balance in their probability distribution, implying that BLM and ELM are equally likely to occur for a given input sequence. Specifically, the ELM-masked token related weight (`wELM`) is encouraged to be greater than or equal to 0.5, and the BLM-masked token related weight (`wBLM`) is suppressed to be less than or equal to 0.5. For instance, if the initial `wELM` is 0.4, it is calibrated to `max(0.5, 0.4) = 0.5`. Similarly, if initial `wBLM` is 0.6, it is calibrated to `min(0.5, 0.6) = 0.5`. The final mask-specific weight vector `w` is then obtained by applying a softmax function over these calibrated weights. This normalized weight vector `w` is subsequently used to compute the Mask-Specific Language Modeling loss (`LMSLM`) during the prediction of masked tokens `xi`, as given by the formula `LMSLM = −Sum(w(x)i * log P(xi|s))`, where `w(x)i` is the mask-specific weight for a masked token `xi`. This weighting scheme ensures higher loss costs for masked named entities compared to masked generic words during prediction, making the model more sensitive to DS-terms while maintaining awareness of their surrounding context.": 1298,
    "The paper addresses this by proposing an effective \"\"Synthetic Query Generation\"\" strategy. For each document selected through the \"\"Domain Document Selection\"\" process, a large language model (LLM), specifically Llama2-7B-Chat, is employed to generate a synthetic in-domain query. The key innovation here is the use of \"\"in-context examples\"\" within the few-shot prompting strategy. Instead of relying on generic or out-of-domain examples (like those from MS-MARCO used in prior work), the method creates a small set (e.g., 3) of human-generated query-document pairs specific to the target domain. These in-domain examples are then included in the prompt template provided to the LLM. This targeted few-shot prompting guides the LLM to generate queries that are not only high-quality but also highly representative of the specific domain and task, capturing the user's information need and domain-specific terminology. The LLM uses greedy decoding with a temperature of 0.0 for query generation.": 1299,
    "The paper addresses this by constructing a sequence of scaffolding questions (P=[p1,p2,...,pK]) that guide the large language model (LLM) through the metaphor reasoning process, using the Metaphor Knowledge Graph (KG) as an instructional reference. Each scaffolding question is designed to induce a reasoning process based on a specific fact (f) within the metaphor KG. Two types of reasoning processes are defined:\n1.  Forward Reasoning: For facts like (head entity, directed relation, tail entity), given an instantiated head entity (and its attributes), the LLM is prompted to determine the unknown instantiated tail entity. These questions are typically constructed using Wh-questions (e.g., \"\"What is the source domain...?\"\").\n2.  Relation Reasoning: For facts like (entity1, undirected relation, entity2), given two instantiated concept entities (and their attributes), the LLM is prompted to infer whether the undirected relation between them exists. These questions use a \"\"Decide whether...\"\" template (e.g., \"\"Decide whether the source domain is different from the target domain?\"\").\nThis sequence of questions, derived directly from the structured facts in the Metaphor KG, directs the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions.": 1300,
    "The paper proposes two strategies, or filters, to identify similar relations, which are crucial for alleviating confusion and informing the Label Prompt. The final set of similar relations is determined by taking the intersection of candidates from both filters.\nFirst, a Semantic Filter identifies relation types with semantically similar label descriptions, as these are prone to confusion. Label descriptions from training instances are processed by sentence-transformers to generate embeddings. For each relation, its cosine similarity with all other relations is calculated, and the top-k relations with the highest scores are selected as candidates.\nSecond, a Feature Filter identifies relations whose corresponding samples exhibit close overall feature distributions. For each relation type, multiple samples are randomly selected, and the average of their embeddings is computed to represent their overall feature space representation. Euclidean distances between these representations are then calculated for each relation, and the top-k closest relations are chosen as candidates.\nFor both filters, `k` is set to 5, and the intersection of the candidate sets from the semantic and feature filters forms the final set of similar relations (`Ra = {Sa1, Sa2,...,Saj...}` for a given relation type `a`). Relations not in this set are classified as non-similar.": 1301,
    "Sentences from the CC-100 corpus are classified by emotional polarity (positive, neutral, negative) and subjectivity (first person vs. third person) using Stanza’s sentiment analysis model. The metaphor identification model is applied to determine the presence of metaphors. The metaphor usage rate (MUR) is calculated for each category, and differences are analyzed using permutation tests.": 1302,
    "The paper addresses the reliable identification and separation of target private categories by introducing a bimodality hypothesis for target discriminative probabilities and employing a data-based statistical approach. After the initial contrastive feature alignment, the task of recognizing open-set domain adaptation (DA) is transformed into an out-of-distribution (OOD) detection problem. For target samples, the discriminative probability vectors `{p_i}` are obtained, and `q_i = max(p_i)` represents the maximum logit. The core hypothesis is that if the target domain contains private classes, the distribution of `{q_i}` will exhibit a bimodal structure, where a left peak with lower classification confidence corresponds to target private examples.\nTo validate this bimodality, the Hartigan’s dip test is applied to the distribution of `{q_i}`. This test measures multimodality by finding the maximum difference between the empirical distribution function and the unimodal distribution function that minimizes this difference. If the test p-value is small (e.g., < 0.05), it indicates a significant bimodal structure, confirming the presence of private categories in the target domain.\nOnce bimodality is confirmed, a two-component Gaussian mixture model (GMM), `πN(θ1, σ1^2) + (1 - π)N(θ2, σ2^2)` with `θ1 < θ2`, is used to fit the distribution of `{q_i}`. The parameters of this GMM (`θ1`, `σ1`, `θ2`, `σ2`, `π`) are estimated. Based on the three-sigma rule, an \"\"unknown\"\" threshold `δ` is set as `δ = θ1 + σ1`. This threshold is designed to contain approximately 80% of samples within the left peak, which corresponds to the lower confidence scores of private categories. Target samples with `q_i < δ` are then classified as belonging to target private categories and are assigned an \"\"unknown\"\" label, denoted as `Dp_t`. This data-driven statistical approach avoids manual threshold setting and reliance on potentially biased source classifiers.": 1303,
    "The paper proposes a novel domain alignment principle: semantically consistent samples should be geometrically adjacent to each other, whether within or across domains. Based on this criterion, a neighborhood consensus contrastive learning paradigm is developed to uncover the intrinsic manifold structure of both domains. This is achieved through a cross-domain multi-sample contrastive loss (L3) that leverages mutual nearest neighbors (MNN).\nFirst, intra- and inter-domain MNN pairs are constructed. For a sample 'i' in the source domain (Ds), its MNN set (Ms_k(i)) includes samples 'j' from Ds with the same category label (yi = yj) or samples 'j' from Ds that are mutual k-nearest neighbors with 'i' (i is a k-NN of j, and j is a k-NN of i). Similarly, for a sample 'i' in the target domain (Dt), its MNN set (Mt_k(i)) includes samples 'l' from Dt that are mutual k-nearest neighbors with 'i'. These MNN pairs are viewed as positive pairs, representing samples that should be close in the feature space.\nSecond, a multi-sample contrastive loss is designed to integrate this knowledge. The loss function (Li_c) aims to pull mutual nearest neighbor pairs across the source and target domains closer to each other, while pushing away non-geometrically close samples. Specifically, for a source sample 'i', the loss maximizes the similarity between 'i' and its MNNs (both source and target MNNs) relative to all other samples in the batch. A similar formulation applies to target samples. This is inspired by the InfoNCE loss, using a temperature parameter (τ) to control the distribution of similarities.\nTo manage the computational complexity for large datasets, a hybrid memory bank (¯Z) is employed. This memory bank maintains a running average of all source and target features, which are l2 normalized. During training, the memory bank values are updated by mixing the current feature (zi) with its stored running average (¯zi) using a mixing parameter (γ). This allows the contrastive loss to implicitly cover computations involving all embedded features without requiring direct computation of all pairwise similarities in every iteration. The overall training objective (L) is a weighted sum of the evidential deep learning loss (L1), the uncertainty calibration loss (L2), and the contrastive feature alignment loss (L3), with λ balancing the components.": 1304,
    "An explicit and simplified relationship between Softmax weights and class-wise means is established under Assumption 2, termed \"\"Near-optimal representations.\"\" This assumption specifies two main properties for representations as their dimension (p) grows large:\n1.  A1 (Between-class means orthogonality): The dot product of class means (m_i . m_j) asymptotically approaches a constant (beta_1) if i=j, and approaches zero (beta_2 = O(p^-epsilon)) if i is not equal to j. This ensures that between-class means are asymptotically orthogonal, maximizing between-class variance.\n2.  A2 (Within-class covariances isotropy and low variance): Class-wise covariance matrices (C_j) are asymptotically isotropic, meaning they approximate a scaled identity matrix (alpha_1,j * I_p) at the first order, with a small off-diagonal component (alpha_2,j = O(p^-epsilon)). Additionally, the variance (alpha_1,j) tends to zero. This models the fact that optimal representations should have independent features and minimized within-class variance.\nUnder these conditions, Proposition 3 demonstrates that for sufficiently large p and as a small perturbation parameter (epsilon) in the generalized labels approaches zero, the class-weight vectors (w_hat_l) become asymptotically proportional to the centered class-wise means (m_l - (1/k) * sum(m_j)). This explicit link is further supported by Corollary 2, which states that the cosine similarity between the learned weights (w_hat_l) and the centered class-wise means (m_hat_l) is asymptotically high, approaching 1 - O(p^-epsilon). This theoretical finding suggests that neural networks classify by aligning their Softmax weights with the class-wise means of their representations.": 1305,
    "To quantify instance uncertainty, the paper introduces the Min-versus-Second-Min (MvSM) strategy. For a given unlabeled target sample x, MvSM (U(x)) is calculated as the difference between the energy of the lowest energy output (E(x;y*)) and the energy of the second-lowest energy output (E(x;y0)), where y* is the label corresponding to the minimum energy and y0 is the label corresponding to the second minimum energy among all possible labels Y. Specifically, U(x) = E(x;y*) - E(x;y0). A smaller difference indicates higher uncertainty or confusion about the class membership, as the model is less confident in distinguishing between the top two most probable classes. This MvSM value is computed for all samples within the candidate set identified in the first step (those with high free energy). From this candidate set, a further top percentage (alpha2%) of samples with the highest MvSM values are selected as the active samples for annotation. This two-step process ensures that the queried samples are not only representative of the target domain (high free energy) but also highly uncertain under the current model (high MvSM), making them maximally informative for improving the model's performance.": 1306,
    "To facilitate knowledge transfer and noise reduction, GearNet introduces a consistency regularization mechanism using a symmetric Kullback-Leibler (KL) divergence loss, denoted as ℓguide or ˜ℓguide. This loss enforces the training model to mimic the predictions of its dual model for every sample from the *other* domain. Specifically, during the forward step, when training model fθ, the consistency regularization ℓguide is calculated based on the target domain data. It measures the symmetric KL divergence between the class label distributions output by fθ (pt1) and ˜f˜θ (pt2) for target instances. Similarly, during the backward step, when training model ˜f˜θ, the consistency regularization ˜ℓguide is calculated based on the source domain data. It measures the symmetric KL divergence between the class label distributions output by fθ (ps1) and ˜f˜θ (ps2) for source instances. Each probability metric (e.g., pc1(xs i)) is computed by applying the softmax function to the corresponding logits (zc1). By minimizing this symmetric KL divergence, the models are encouraged to align their classification predictions on the domain they are not primarily supervised on, thereby generalizing better to that domain and implicitly reducing the negative impact of label noise by leveraging the dual model's potentially different biases.": 1307,
    "A mapping matrix S ∈ {0,1}kt×ks is introduced to relate hidden target subdomains to source ones, where Si,j=1 if target subdomain Xt i maps to source subdomain Xs j, and 0 otherwise. To enhance interpretability, S is encouraged to be sparse, ideally with each target hidden subdomain mapping to only one source hidden subdomain (∑ksj=1 Si,j=1). In a weakly supervised setting, once subdomain separations X∗ are determined, S∗ is estimated by minimizing the prediction error over a small set of labeled target domain points: S∗ = argminS (1/nw) ∑(xt,yt)∈D l(ht(xt;K,X∗,S),yt), where l is a loss function. In an unsupervised setting, S∗ is estimated by minimizing the one-dimensional Wasserstein distance W(·,·) between the output distributions of the target classifier and the pre-trained source model: S∗ = argminS W(ht(Xt;K,X∗,S),hs(Xs)). Since these are discrete optimization problems, the constraints over S are relaxed using a Softmax function, ωi,j(˜S) = exp(˜Si,j) / ∑ksk=1 exp(˜Si,k), allowing optimization of ˜S (a real-valued matrix) via gradient descent. The final discrete S is obtained by setting Si,j=1 if ˜Si,j is the maximum in its row, and 0 otherwise.": 1308,
    "The MIP-GNN architecture represents a given BLP as a bipartite graph B(I) = (V(I);C(I);E(I)), where V(I) are variable nodes, C(I) are constraint nodes, and E(I) are edges connecting a variable node and a constraint node if the variable has a non-zero coefficient in that constraint. Edge features are the coefficients Aij, and node features include objective coefficients and node degree for variables, and right-hand side coefficients and node degree for constraints. The architecture consists of two interleaved passes: a variable-to-constraint (v-to-c) pass and a constraint-to-variable (c-to-v) pass. In the v-to-c pass, each variable node vi passes its current embedding v(t)i to its adjacent constraints cj. The constraint embedding c(t+1)j is updated by aggregating features from neighboring variable embeddings using a function fW1;C aggr and merging them with the current constraint embedding c(t)j using fW2;C merge. Crucially, to guide the model towards meaningful variable embeddings, an \"\"error signal\"\" is propagated in the c-to-v pass. For each constraint, after receiving variable embeddings, a neural network fWasg assigns a scalar value (cid:22)xi to each variable, forming a vector (cid:22)x. A normalized residual error e = softmax(A(cid:22)x - b) is computed, indicating how much each constraint is violated. This error signal e is then propagated back to adjacent variables. The variable embedding v(t+1)i is updated by aggregating features from neighboring constraint embeddings (including their Aji, bj, and the error signal ej) using fW1;V aggr and merging them with the current variable embedding v(t)i using fW2;V merge. The column-wise concatenation of variable embeddings over all layers is finally fed into a Multi-Layer Perceptron (MLP) to predict the variable biases.": 1309,
    "The paper proposes a novel approach within its Prior-Guided Transfer Learning (PGTL) framework. Instead of directly transferring knowledge from the source domain to the target domain by minimizing their global distribution discrepancy, PGTL first leverages instance relationships across domains to extract \"\"prior knowledge\"\" for target domain instances. For each target domain instance `xt_i`, a set of related source domain instances `Ri` is identified (e.g., items with the same ID or category). The combined information from these related source instances forms the prior feature `xp_i` for `xt_i`, typically by averaging their features. This creates an implicit \"\"prior domain\"\" (`Dp`) whose data distribution (`eQ`) is naturally much closer to the source domain (`Ds`) than the target domain (`Dt`). Knowledge transfer is then performed between the source domain and this implicit prior domain using domain adversarial learning. This involves two generators, `Gs_f` (for source features) and `Gp_f` (for prior features), and one discriminator `Gd`. The discriminator learns to distinguish between source and prior domain samples, while the generators learn to produce features that fool the discriminator, effectively aligning their distributions. A Gradient Reversal Layer (GRL) is used to facilitate this min-max optimization. This indirect transfer via a prior domain simplifies the alignment task and enhances knowledge transfer, particularly when the direct discrepancy between source and target is large.": 1310,
    "The RDA system incorporates a Junk Filtering module designed to detect and filter irrelevant and noisy content from the image stream. This module utilizes a deep learning model specifically trained to identify and classify irrelevant concepts. These concepts include, but are not limited to, cartoons, celebrities, banners, and advertisements. By applying this model to unique images (those that have passed through the deduplication stage), the system effectively removes non-pertinent visual data, significantly reducing the volume of content that would otherwise contribute to information overload for human decision-makers during time-critical disaster response efforts.": 1311,
    "To accurately handle feature shifting for consistency evaluation when disparities are very close, the paper proposes a novel phase shift-based warp function. This function addresses the limitation of conventional bilinear or bicubic interpolation, which can cause misjudgment between adjacent disparities due to very similar interpolated features when the disparity distance (δ) is small and the number of hypothesis disparities (K) is large. The core idea is to apply phase shift theory within a local window. Instead of performing a global Fourier transform, the method extracts a small 3x3 window (e.g., f2,5) around a pixel (e.g., (2,5)) in the feature map (Fu,v). This window is then transformed into the frequency domain using a 2D Fourier transform. In the frequency domain, a phase shift (∆i) corresponding to the required subpixel shift for warping is applied. After the phase shift, an Inverse Fourier transform is performed on the shifted frequency-domain window to bring it back to the spatial domain (e.g., f'2,5). The central pixel of this transformed spatial-domain window is then extracted to obtain the warped feature for that specific pixel. This process is repeated for each pixel (x,y) to construct the complete warped feature map (W_i_u,v). This localized application of phase shift theory allows for more accurate subpixel interpolation compared to traditional methods, ensuring that even very close disparities remain discriminative during cost volume construction.": 1312,
    "The paper proposes a Dynamic Sub-domain Division (DSDD) scheme to partition the single source dataset `S` into multiple sub-domains, simulating meta-train and meta-test sets for a meta-learning framework. This division is based on clustering the image latent features extracted from the network. Specifically, K-Means clustering is applied to the image features `z` (the output of the DS unit before DSCM re-encoding). The clustering is performed initially before training (using a pre-trained backbone) and then dynamically refined after every epoch during training. This dynamic refinement is crucial because image features are continuously optimized as network parameters update, improving their representativeness. To ensure consistency and avoid large shifts in sub-domain label assignments between iterations, the paper permutes current sub-domain label assignments across clusters to achieve major agreement with previous ones. This permutation problem is solved as a Maximum Bipartite Matching problem using the Kuhn-Munkres algorithm. The features `z` are chosen for clustering because they have promoted domain-specific information while still being extracted uniformly, allowing for prominent sub-domain division as the network updates.": 1313,
    "The paper leverages the domain-robustness of optical flow to construct reliable supervision signals through a novel Segmentation-based Flow Consistency (SFC) method. The core idea is to impose a consistency constraint between the segmentation-based flow (SF) generated by the proposed Segmentation-to-Flow Module (SFM) and the actual optical flow (F) estimated by a pre-trained, fixed FlowNet. This consistency is enforced using the endpoint error loss (EPE), a common metric in optical flow estimation. During the main training phase for domain adaptation, the VSS model G (which includes the SegNet) produces segmentation predictions on the target domain. These predictions, even if noisy due to domain shift, are fed into the pre-trained and fixed SFM to generate SF. Simultaneously, the fixed FlowNet estimates the optical flow F for the same target frames. The SFC loss (Lflow) then calculates the EPE between SF and F (Lflow = EPE(SF T, F T)). Crucially, while SFM and FlowNet are fixed, the gradient of this Lflow passes through SFM and directly acts on the VSS model G. This implicit supervision guides G to improve its segmentation predictions on the target domain. The rationale is that as G's segmentation predictions become more accurate, the SF generated by SFM will become more consistent with the domain-robust optical flow F, thereby optimizing G without relying on domain-sensitive pseudo-labels.": 1314,
    "To learn generalized representations from the content-dependent image (Ic), the paper introduces a Hierarchical Guidance Generalization Network (HGGN). The HGGN is composed of multiple Structure-Guided Enhancement (SGE) modules, designed to extract domain-invariant features and enhance spatial information learning. The Ic generated by the AFM serves as the primary input to the HGGN. While the filtering operation provides a content-dependent image, some internal structure information, which is beneficial for segmentation, might be removed. To address this, the HGGN also utilizes the texture-dependent image (It), as It is observed to retain some internal structural details.\nWithin each SGE module (four are used hierarchically in the HGGN), features extracted from Ic and It are fused. This fusion is performed via concatenation to ensure that the internal structure information from It is well-preserved. Following fusion, the combined features undergo Instance Normalization (IN) to extract style-normalized feature representations. Although IN is effective for style normalization, it can weaken spatial feature representation. To counteract this and highlight spatial features crucial for DG semantic segmentation, a Contour Detector (CD) is integrated into each SGE module. The CD generates a predicted contour map (y) through three convolution layers. This predicted contour map is then used to enhance the spatial feature representation of the extracted feature from It via element-wise multiplication. The supervised training of the CD uses a class-balanced cross-entropy loss (Lcontour) against a contour ground truth, which is derived from the semantic labels, explicitly guiding the model to learn domain-invariant shape and spatial information. The total contour loss is the sum of Lcontour from all four SGE modules. Finally, a classifier is responsible for generating semantic segmentation results, supervised by a standard cross-entropy loss (Lseg). The overall objective function (Ltotal) combines Lseg and Lcontour, weighted by a hyper-parameter (λ), which is empirically set to 2.5. This hierarchical guidance, fusing content and texture information with explicit contour supervision, enables the network to learn robust, domain-invariant features.": 1315,
    "The paper designs a new attack objective function, named Lefd+cos, to effectively corrupt the semantic information of benign images for cross-domain transfer-based attacks. This function is based on the feature-space attack paradigm and combines two main components. The first component is the feature disruption term (Lfd), which is derived from the standard FDA+fd attack (Inkawhich et al. 2020b). This term is formulated as the squared L2 norm of the difference between the intermediate feature representations of the adversarial example (hl(x + δ)) and the original benign image (hl(x)), normalized by the squared L2 norm of the benign image's features (∥hl(x)∥2). The objective of this term is to encourage the adversarial example to move far away from the original benign image in the feature space. To make full use of the deep neural network's parameters, the Lfd term is computed and summed across multiple selected intermediate feature layers (L = {l1,...,lk}). The second component is the cosine similarity loss (Lcos), which is added to help the images deviate from their original semantic features. This term is formulated as the negative cosine similarity between the final output vector of the feature extractor for the benign image (h(x)) and the adversarial example (h(x + δ)). The overall Lefd+cos objective function is the sum of the aggregated Lfd terms and the Lcos term, as expressed by Equation 4. This combined loss aims to corrupt both intermediate and final layer semantic information, leveraging the properties of contrastive learning where positive sample pairs are close and negative pairs are far, to push the adversarial example away from the benign image's semantic representation.": 1316,
    "The paper introduces the Adaptive ColorMLP (AdaCM) framework, which adaptively predicts the parameters for the ColorMLP based on input content and style images. This is handled by the Parameter Prediction Module (PPM). First, the input content image (Ic) and style image (Is) are down-sampled to lower resolutions (¯Ic and ¯Is) to reduce computational cost. The PPM then takes these low-resolution counterparts as input. Inside the PPM, a pre-trained VGG-19 (Simonyan and Zisserman 2015) network is used to extract multi-resolution feature maps from both the content and style images. These feature maps are then fused using multiple Style-based Splatting (Xia et al. 2020) blocks, which integrate content and style feature maps based on Adaptive Instance Normalization (AdaIN) (Huang and Belongie 2017). Finally, a series of convolution and fully-connected layers summarize the fused feature statistics and predict the complete set of parameters (Θ) for the ColorMLP. This process allows the ColorMLP to generate a color transformation specific to each unique content and style image pair, enabling universal style transfer.": 1317,
    "To address the weak correlation between text and image modalities, the model proposes a novel global-local cross-modal interaction model. This model utilizes an efficient mechanism called Cross-modal Attention Promotion (CAP), which leverages symmetric cross-modal attention to explore inherent correlations between input feature sequences (`hT` for text and `hI` for image). CAP takes `hT` and `hI` as inputs and generates mutually reinforcing information `hT→I` and `hI→T`. The computation for `hT→I` involves Layer Normalization (LN) followed by Multi-head Cross-Attention (MCA) between `hT` and `hI`, then Multi-head Self-Attention (MSA) on the result, and finally a Feedforward Neural Network (FN). This process is symmetric for `hI→T`. Instead of traditional local-local interactions, the model establishes a global multimodal context `g(i)` at each layer `i` by concatenating the representations of each modality (`hT(i)` and `hI(i)`). This global context then interacts with local unimodal features. Specifically, `hT(i+1)` is derived from `CAPT↔G(hT(i), g(i))` and `hI(i+1)` from `CAPI↔G(hI(i), g(i))`. Through multi-level stacking, the global multimodal context and local unimodal features mutually reinforce and progressively refine each other. This hierarchical learning approach integrates information from various modalities, allowing the model to acquire higher-order semantic features and effectively combine information in a deeper manner.": 1318,
    "The paper addresses this through a Spatial Domain Transfer (ST) module that employs a batch momentum update-based histogram matching strategy. For a given source domain image and a batch of target domain images, all images are first normalized to an integer intensity range. The normalized histogram of the source image is calculated. Simultaneously, an average image of the current batch of target domain images is computed, and its normalized histogram is calculated. To ensure a stable target histogram for matching, a batch momentum update is introduced, where the average target image for the current batch is updated cumulatively based on the previous batch's average and a momentum factor (set to 0.7). This momentum update helps smooth the target domain histogram and accounts for intensity variations. Histogram equalization transformations are then computed for both the source image histogram and the momentum-updated average target image histogram. Finally, the source image pixel values are mapped to the corresponding pixel values from the target domain based on these transformations, resulting in a histogram-matched, target-like image. This generated image is then used to train the second segmentation teacher network, also with a combined cross-entropy and Dice loss.": 1319,
    "To maintain spatial correlation without strict positional alignment, the paper proposes an Align-Free Spatial Correlation (AFSC) mechanism, quantified by the Lasc loss. This solution leverages the small domain gap established by the Progressive Adaptation. Given a latent variable zi, the method first identifies semantically similar regions between images generated by Gn-α(zi) and Gn(zi) using a dense optical flow estimation algorithm (Farnebäck optical flow). This process constructs a pixel-wise positional mapping table, Mzi n,α, which maps a position 'p' in Gn-α(zi) to its corresponding position in Gn(zi). Once these positional correspondences are identified, the method encourages relational consistency. For a set of sampled positions (pi,j n-α) from Gn-α(zi) and their aligned counterparts (qi,j n) in Gn(zi), local features are extracted using RoIAlign (Region of Interest Align) to handle non-grid-aligned positions. A spatial correlation (φ) is then computed for each generator (Gn-α and Gn) based on the cosine similarity of these extracted local features. Finally, the Lasc loss is formulated as the KL-Divergence between these spatial correlations (φl,i n-α and φl,i n), ensuring that the relational consistency is preserved across the progressively adapted models without requiring fixed spatial alignment.": 1320,
    "The paper addresses this by proposing a Learnable Style Memory (My) that is trained through backpropagation, a significant departure from previous key-value memory networks like MGUIT (Jeong et al. 2021) which relied on simple updating mechanisms. The My consists of N class memories, each storing U key-value pairs (ky, vy) to represent diverse styles within a class. During the \"\"read\"\" operation, the content feature (cx) from the source image (Ix) is separated by its semantic label (Lx) into class-specific content features (cx n). For each pixel's content feature (cx n,i), a similarity (wx n,i,j) is calculated with the keys (ky n,j) of the corresponding class memory using cosine similarity. The memory style (ˆsy n,i) is then obtained by calculating a weighted sum of the values (vy n,j) based on these similarities. Crucially, unlike prior methods where gradients were stopped at the memory, SHUNIT enables the memory to learn its parameters jointly with the rest of the network. This is achieved by introducing a Style Contrastive Loss (Lstyle) that directly supervises the memory style (ˆsx) for the translated target image. This loss encourages the memory to learn discriminative class-wise style representations by setting the source component style (sx i) as a positive sample for a given pixel's memory style (ˆsx i) and other features as negative samples, forcing the memory to reduce the overall loss effectively through backpropagation.": 1321,
    "The paper addresses the issue of lost image context by proposing a Neighborhood Feature Aggregation Module (NFAM). After initial feature extraction by a 2D backbone network, an image feature map (G2D) is obtained. When 3D points are projected onto the image, only N pixel features corresponding to these points are typically retained, leading to a loss of surrounding image information. The NFAM enhances these sampled pixel features. For each sampled pixel feature (gi 2D), its neighbor features (N(gi 2D)) within a defined region (e.g., 5x5) are identified. The sampled pixel feature (gi 2D) acts as a query token, while its neighbors (N(gi 2D)) form the key-value pairs. These are fed into a transformer block that implements a neighborhood attention mechanism. Query (Q), Key (K), and Value (V) matrices are computed using linear projections. Attention weights are calculated via scaled dot products between Q and K, then multiplied with V to produce an aggregated feature vector (hi 2D). This aggregated feature is then combined with the original pixel feature via a skip connection and processed by a feed-forward network to produce an updated feature representation (˜gi 2D). To further enlarge the receptive field, the module extends this concept to dilated neighborhood attention, where neighbors are sampled with a dilated rate (e.g., from a 9x9 region with rate 2), resulting in dilated neighbors (DN(gi 2D)). The process is repeated with dilated neighbors to produce another updated feature (ˆgi 2D). Finally, the original feature, the aggregated feature from immediate neighbors, and the aggregated feature from dilated neighbors are summed (fi 2D = gi 2D + ˜gi 2D + ˆgi 2D) to form a more representative 2D pixel feature (fi 2D). This aggregated feature is then used for cross-modal learning.": 1322,
    "To generate pseudo-labeled images, the framework utilizes the disentangled liveness (zL) and content (zC) features along with a pair of decoders, Gs for the source domain and Gt for the target domain. The core idea is to swap the domain-invariant liveness features between domains and concatenate them with their corresponding domain-specific content features. Specifically, to create a synthetic target image (ˆxt) that possesses source domain-invariant liveness features and target domain-specific content features, the target decoder (Gt) takes the liveness feature from the source domain (zL s) and the content feature from the target domain (zC t) as input, i.e., ˆxt = Gt(zL s, zC t). Similarly, a synthetic source image (ˆxs) can be generated. To ensure the authenticity of these synthetic images, a pair of discriminators (Ds and Dt) are introduced, which try to differentiate between real and generated images within their respective domains. The generators (Gs and Gt) are trained adversarially against these discriminators to produce realistic synthetic images. These generated pseudo-labeled images (ˆxt) are then used in a subsequent Stage 2 to train a robust classifier (M), which can be either LGSC or ResNet-18, under the supervision of the source domain labels.": 1323,
    "Knowledge transfer in P2FCDR is handled by the Embedding Transformation module. To bridge the potential data distribution variations across domains, an optimizable orthogonal mapping matrix (XA for Domain A, XB for Domain B) is introduced. For example, to transfer user embedding from Domain A to Domain B, the in-domain user embedding uA_i is transformed into uAB_i = uA_i * XA. The orthogonal property of these matrices is leveraged because it preserves similarities between user embeddings before and after transformation (due to inner product preservation) and simplifies the learning procedure as its inverse mapping matrix is equivalent to its transpose. Crucially, to ensure privacy protection during this transfer, Local Differential Privacy (LDP) is applied. Before the transformed user embedding (e.g., uAB_i) is exchanged with the other domain, Laplace noise La(0,λ) is added to it. This LDP mechanism prevents external attackers from inferring real user representations and also prevents the receiving business partner from inferring sensitive information from the transformed embedding, even with knowledge of the orthogonal mapping matrix, thus upholding commercial privacy policies. The strength of this Laplace noise is controlled by the hyperparameter λ. The orthogonal mapping matrices (XA, XB) are consistently synchronized and kept equal between the two domains during the Parameter Update and Synchronization phase.": 1324,
    "The paper tackles the stability issue by integrating a GAN Stabilizer (GS) into the Stable Privacy-Preserving Generator (SPPG). GANs are inherently difficult to converge due to their minmax optimization objective, a challenge exacerbated when Rényi Differential Privacy (RDP) is applied by adding noise to gradients. GS, parameterized by `τ` (a positive constant), formulates a closed-loop control system for the GAN's dynamics. This system modifies the discriminator's update rule by introducing an extra optimization objective, `LGS`. This `LGS` acts as a regularization term, specifically `−(τ/2) * (E[D(r,uid)^2] + E[D(˜r,uid)^2])`, where `r` is real data and `˜r` is fake data. Intuitively, this regularization penalizes abrupt outputs from the discriminator. By incorporating `LGS` into the discriminator's loss function (`LD = LD + LGS`), GS guides the discriminator to measure the distance between real and fake user preferences within a tighter latent representation space. This mechanism effectively stabilizes the GAN's training process, allowing it to better capture the distribution of private data in the source domain despite the noise introduced by RDP.": 1325,
    "Adaptive task selection is achieved by measuring task relevance and assigning sampling probabilities to candidate meta-tasks based on their refined representations. This process leverages the prompt data from the target domain to guide the selection.\nFirst, a target domain representation (pT) is computed by averaging the refined task representations of all prompt tasks: pT = (1/Mt) * Σ(j=1 to Mt) p˜τj. This representation aims to capture the overall characteristics of the target domain.\nNext, a relevance score (ri) is calculated for each candidate meta-task τi with respect to the target domain representation pT. This score is derived using a Multilayer Perceptron (MLPWc) applied to the element-wise multiplication (⊙) of the candidate meta-task's refined representation (pτi) and the target domain representation (pT): ri = MLPWc(pτi ⊙ pT). The MLPWc is activated by a Sigmoid function.\nIn addition to the relevance score, an additional score, or task signature (oi), is assigned to each candidate meta-task τi. This score is determined by applying a Sigmoid activation function (ρ) to a trainable parameter (wi) associated with each task: oi = ρ(wi). This task signature directly reflects the task's importance for generalization to the target domain.\nFinally, an overall score (ti) for each candidate meta-task τi is computed by summing its relevance score and task signature: ti = ri + oi. The probability (˜ti) of selecting meta-task τi is then calculated by normalizing its overall score across all candidate meta-tasks: ˜ti = ti / Σ(m=1 to Ms) tm. With these sampling probabilities, B meta-tasks are selected from the candidate pool. These selected meta-tasks are then used to meta-optimize the graph base learner. The graph base learner's parameters (θ) are updated using an optimization-based meta-learning approach, where θ(k+1) = θ(k) - α∇θ(k) (1/B) * Σ(i=1 to B) L(Ωqi;θ(k)i), where θ(k)i is the task-specific parameter obtained by an inner update on the support set Ωsi, α is the meta-update rate, and L is the cross-entropy loss on the query set Ωqi.": 1326,
    "The paper extracts simile components (topic `t` and vehicle `v`) by designing rules based on constituency parsing of simile sentences. This approach addresses two main challenges: component positioning with long-distance dependencies and component span identification. The method, detailed in Algorithm 1, first identifies leaf nodes corresponding to the word \"\"like\"\" as anchor nodes. The vehicle is typically found in the subtree rooted at the sibling node of the \"\"like\"\" anchor. The topic is located by iterating upwards from the \"\"like\"\" anchor's parent nodes until an S (sentence) or NP (noun phrase) labeled node is found, with the topic residing in its subtree.\nA crucial part of this process is the `GETCOMP` function, which is designed to find the optimal component span. `GETCOMP` performs a pre-order traversal over a given subtree. It returns the text of the first node whose child nodes are all labeled as NP or PP (prepositional phrase), and importantly, at least one child is an NP. This condition ensures that the extracted component is a meaningful noun phrase and not a complex clause or an improper component. For instance, in \"\"They were like kids in a candy store,\"\" `GETCOMP` aims to identify \"\"kids in the candy store\"\" as the vehicle rather than just \"\"kids,\"\" capturing richer semantics. After extraction, a final filtering step is applied based on the quality of the extracted (topic, vehicle) pairs to reduce noise from the simile detection process.": 1327,
    "The paper addresses this by introducing the Geometry-Aware Adaptation stage, which leverages more compact 3D geometric point cloud representations. After obtaining target pseudo depth maps from the previous stage, RGB-D images are formed and then transformed into 3D point clouds (V). Each point in the cloud consists of 3D spatial coordinates (x, y, z) and RGB color information (r, g, b). A key mechanism here is the disentanglement of 3D Geometric Coordinates and RGB Colors: the geometric coordinate (x, y, z) is treated as domain-invariant information and is explicitly separated from the RGB color. A 3D point-based encoder (denoted as `phi`) is employed, which takes local regions of points, specifically their geometric coordinates (H) and point features (V), as inputs. This encoder is designed such that all its operations are explicitly performed on the 3D spatial coordinates, ensuring that the extracted geometric features (Fgeo) maintain critical 3D geometric information to the largest extent. These geometry-aware features are then used to perform 3D geometric segmentation, which complements the 2D semantic segmentation. This explicit exploitation of 3D topology allows for better understanding of object positions and shapes, leading to improved domain adaptation and pseudo-label refinement.": 1328,
    "The paper addresses catastrophic forgetting in lifelong learning for Mixed Integer Programs (MIPs) by combining two main strategies: Knowledge Distillation (KD) and Elastic Weight Consolidation (EWC). This approach aims to update model parameters for new tasks while preserving knowledge from previous ones.\n1.  Mimicking Past Behavior through Knowledge Distillation (KD):\n    *   The model maintains a fixed-size memory buffer (M) that stores a small set of (state, logit) pairs from past tasks. The logits `z = fθj*(s)` represent the optimal behavior of the model `f` with parameters `θj*` (learned for task `Tj`) on a sample `s`.\n    *   When learning a new task `Ti`, a Knowledge Distillation loss (`LKL`) is introduced. This loss encourages the current model `fθ(s)` to generate logits similar to those stored in the buffer for samples `s` from past tasks. This is achieved by minimizing the Kullback-Leibler (KL) divergence between the stored past logits `z` and the current model's logits `fθ(s)`.\n    *   Reservoir sampling is used to populate the buffer, ensuring that samples from all past tasks are stored with equal probability, without prior knowledge of the total number of tasks.\n2.  Preservation of Model's Important Parameters through Elastic Weight Consolidation (EWC):\n    *   After training on a task `Tj` is complete, the importance of each model parameter `w` for that task (`Ωw j`) is computed. This importance is determined by the diagonal of the Fisher information metric, which quantifies how sensitive the loss function is to changes in that parameter for task `Tj`.\n    *   When a new task `Ti` arrives, an importance-based regularization term (`Limportance`) is added to the overall loss function. This term is a quadratic penalty that encourages the current parameters `θw i` to stay close to the optimal parameters `θw j*` learned for previous tasks `Tj` (where `j < i`). The penalty for each parameter `w` is weighted by its importance `Ωw j` for the respective past task. This prevents significant drift in parameters that were crucial for previously learned tasks.\nThe overall lifelong learning optimization objective combines the standard imitation loss (`L(θ)`) for the current task `Di` with the Knowledge Distillation loss (`LKL`) and the Elastic Weight Consolidation regularization (`Limportance`). The relative weights of these components are controlled by hyperparameters `α` (for KD) and `β` (for EWC). This combined objective allows the model to learn new tasks while consolidating past information and maintaining the stability of parameters important for previously learned tasks.": 1329,
    "The captured internal distribution, represented by the estimated GMM (pJ(z)), is utilized to adapt the pre-trained model to a new target domain in a sequential, source-free setting through a two-step process: pseudo-dataset generation and model adaptation. First, a labeled pseudo-dataset, denoted as ˆDP = (ZP, YP), is generated. This involves drawing random samples (zp_i) from the estimated GMM (ˆpJ(z)). For each sampled point, a label (yp_i) is ascribed according to the prediction of the pre-trained classifier subnetwork (hw0). To ensure a clean pseudo-dataset, a confidence threshold (τ) is applied, and only generated samples for which the classifier's prediction confidence is greater than τ are included. Second, the sequential model adaptation problem is formulated as an optimization problem (Eq. 3) that minimizes a combined loss function. This objective function has two terms: the first term is a classification loss (L) applied to the pseudo-dataset, ensuring the classifier continues to perform well on the internal distribution. The second term is a domain alignment matching loss, which measures the distributional discrepancy D(·,·) between the target domain's embedding distribution (ϕv(pT(XT))) and the empirical distribution of the generated pseudo-dataset (ˆpJ(ZP)). The Sliced Wasserstein Distance (SWD) is chosen as the distribution metric D(·,·) due to its suitability for deep learning and efficient computation. By minimizing this objective, the encoder sub-network (ϕv) is updated to match the target domain distribution with the internal source distribution in the embedding space, making the embedding domain-invariant and allowing the classifier (hw) to generalize well on the target domain.": 1330,
    "The paper defines the discordance-based distance, `DSDI-disc(Ps, Pt)`, as the largest distance between two domains (source `Ps` and target `Pt`) with respect to a hypothesis space `H`, using SDI as the underlying distance function. It is formally defined as:\n`DSDI-disc(Ps,Pt) = max_{h,h'∈H} E_{Ms={x1,...,xN~Ps}, Mt={x1,...,xN~Pt}} [|SDI(h,h';Ms) - SDI(h,h';Mt)|]`\nThis measure quantifies the discrepancy between distributions by comparing how two ranking functions `h` and `h'` perform on samples drawn from the source versus the target domain. `DSDI-disc` reaches its maximum when `h` and `h'` rank instances similarly in one domain but differently in the other.\nUsing this `DSDI-disc` and the metric properties of SDI, the paper derives an error generalization bound for the target survival domain. For any hypothesis `h ∈ H`, the SDI on the target domain `Dt` is bounded as:\n`SDI(rh,ft;Mt) ≤ ηD(fS,ft) + Σ_{i=1}^K wi · (SDI(rh,fsi;Msi) + DSDI-disc(Psi,Pt))`\nwhere `rh` is the risk function induced by `h`, `ft` is the true risk function for the target, `fsi` for source `i`, `wi` are weights for source domains (summing to 1), and `ηD(fS,ft)` is the minimum joint empirical SDI losses on sources and target achieved by an optimal hypothesis `h*`. This bound allows for quantifying the target domain loss based on weighted source domain losses and the discrepancy between source and target distributions.": 1331,
    "The paper employs a Column Generation (CG) algorithm to efficiently solve the path-based MIP formulation, which can have an excessively large number of candidate decision rules (columns). The CG procedure works by iteratively solving a Restricted Master Program (RMP) and a subproblem.\nInitially, the RMP considers only a small subset of paths, denoted as ˆL, and relaxes the integrality constraints on zj (0 ≤ zj ≤ 1). The RMP's objective is to minimize the sum of costs associated with selected rules and penalties for unassigned samples, subject to the set partitioning and cardinality constraints. The dual variables associated with the set partitioning constraints (λi) and the cardinality constraint (µ) are obtained from the RMP solution.\nThe core of the Column Generation approach lies in the subproblem, which is responsible for identifying new, potentially beneficial columns (decision rules) to add to the RMP. The subproblem seeks to find paths (rules) with the most negative reduced cost (rcj). The reduced cost for a path j is defined as `rcj = ξj - (sum_{i=1 to N} aij * λi + µ)`. A negative reduced cost indicates that adding this path to the RMP could improve the objective function. This subproblem is formulated as a K-shortest path problem (KSP) over the feature graph G, where the cost of each path is its reduced cost. The algorithm identifies up to K paths with the most negative reduced cost.\nThe high-level CG procedure is as follows:\n1.  Start with an empty set of candidate rules (ˆL = ∅) in the RMP, which initially sets slack variables si to 1.0 and yields an initial dual solution.\n2.  Use the current dual values (λi and µ) to solve the KSP subproblem, identifying up to K candidate rules with the most negative reduced cost.\n3.  Add these newly generated rules to ˆL.\n4.  Re-optimize the RMP to obtain a new primal-dual solution.\n5.  Repeat steps 2-4 until no violating path (i.e., no path with a negative reduced cost) exists, indicating dual feasibility and convergence to an optimal solution of the LP relaxation of the OMT.\nAfter the CG procedure converges, the binary restrictions are reimposed on the zj variables, and the resultant Master-MIP is solved using a standard optimization package (e.g., CPLEX) to obtain the final integer solution. This dynamic generation of columns allows the method to efficiently search through the vast space of possible rules without explicitly enumerating all of them.": 1332,
    "To dynamically coordinate and weight the transformed user interest embeddings, the paper proposes a Reinforcement Learning (RL) based Interest Selector (IS). This selector operates within an RL framework defined by states, actions, and rewards. The state `s` for an instance is a concatenated vector `Es` comprising the K transformed interest embeddings (`tpj_u`), the pre-trained user embedding (`ut`), and the item embedding (`it`) in the target domain, along with a prediction (logit) vector `L` for each transformed interest. The agent's action is to choose whether to select each interest embedding (represented by a binary action value, though sampling probabilities `Pj` are used as weights). A policy function `πθ(s,a)` parameterized by `θ` (a two-layer feed-forward network `g(Es;θ)`) determines these sampling probabilities `Pj`. The transformed user embedding `but` for prediction is then calculated as a weighted sum: `but = sum(Pj * tpj_u)`. The reward `r` for the RL agent is defined as the negative of the task-oriented loss (`LossMIT`) of the Multi-Interest Transfer (MIT) model, which is the mean squared error between predicted and true ratings. This reward is delayed until a batch of training instances is processed. The policy parameters `θ` are optimized using the standard policy gradient method REINFORCE, where `θ` is updated based on the gradient of the policy multiplied by the reward. This iterative process of sampling actions, training MIT, calculating rewards, and updating the policy allows the IS to learn how to dynamically assign appropriate weights to different transformed interests for various training instances.": 1333,
    "The paper enhances the robustness of feature learning through Adversarial Learning (ALE), which involves generating \"\"hard examples\"\" for both the discriminative and transfer modules and enforcing output consistency on these challenging inputs. This mechanism operates as follows:\n1.  Adversarial regularization for discriminability: To enhance the robustness of the discriminative module (G), ALE generates \"\"non-discriminating examples\"\" (xnon-disc). These examples are created by adding adversarial perturbations to original examples (x), specifically by finding `xnon-disc = argmax x';∥x−x'∥≤ϵ ℓdisc(x')`, where `ℓdisc` is the discriminability loss function. The discriminative module is then regularized by minimizing its consistency loss on these non-discriminating examples: `ℓALE disc(x) = Dist.[G(F(x))∥G(F(xnon-disc))]`. This forces G to maintain consistent predictions even when faced with inputs designed to be difficult to classify.\n2.  Adversarial regularization for transferability: To enhance the robustness of the transfer module (H), ALE generates \"\"non-transferring examples\"\" (xnon-tran). These are produced by adding adversarial perturbations to original examples, found by `xnon-tran = argmax x';∥x−x'∥≤ϵ ℓtran(x')`, where `ℓtran` is the transferability loss function. The transfer module is then regularized by minimizing its consistency loss on these non-transferring examples: `ℓALE tran(x) = Dist.[H(F(x))∥H(F(xnon-tran))]`. This ensures that H maintains consistent domain alignment even when presented with inputs designed to be difficult to align across domains.\nThe maximizers for generating xnon-disc and xnon-tran are approximated by moving the original example x towards its positive gradient of the respective loss function: `x' = x + ∇xℓ(x)/∥∇xℓ(x)∥2`. By regularizing the modules on these hard examples, ALE significantly enhances the robustness of the learned feature representations in terms of both discriminability and transferability.": 1334,
    "Domain discrepancy at the local sensor level for Multivariate Time-Series (MTS) data is reduced through the proposed endo-feature alignment, which comprises two key components: Sensor-correlation Alignment (SCA) and Sensor-feature Alignment (SFA). First, SCA aims to make the sensor correlations between domains identical. This is achieved by minimizing the expectation of the absolute discrepancy between each corresponding edge (`emn`) in the source and target domain graphs, denoted as `minLSCA = E(|est mn − et mn|)`. This ensures that the interactions between sensors exhibit similar trends across domains. Second, SFA aligns individual sensor features using a contrastive mechanism. The core idea is that features from the same sensor should have similar distributions across domains, while features from different sensors should remain distinct. This is formulated as minimizing `minLSFA = −E(log(eφ(ps m,pt m) / Σj∈P t eφ(ps m,pt j)))`, where `φ(ps m,pt m)` represents the similarity between the m-th sensor's feature in the source domain (`ps m`) and the m-th sensor's feature in the target domain (`pt m`), and the summation is over all target domain sensor features `pt j`. This contrastive loss pulls features of the same sensor closer while pushing features of different sensors apart. The overall endo-feature alignment loss, `minLEndo`, is a weighted sum of `LSCA` and `LSFA`, controlled by hyperparameters `λSCA` and `λSCA`, allowing for a balanced alignment of both sensor features and their correlations.": 1335,
    "A generic Graph Adaptive Network (GRADE) framework is proposed to leverage the Graph Subtree Discrepancy (GSD) for cross-network transfer learning. The core idea of GRADE is to minimize a combined objective function: `min θ C(Gs;θ) + λ · dGSD(Gs,Gt;θ)`. Here, `θ` represents all trainable parameters of the model. `C(Gs;θ)` is the task-specific loss function computed on the source graph, which can also include target graph loss if partial labels are available. `dGSD(Gs,Gt;θ)` is the Graph Subtree Discrepancy term, which aims to minimize the distribution shift between the source and target graphs. `λ ≥ 0` is a hyper-parameter that balances the importance of the task-specific loss and the distribution discrepancy minimization.\nThe framework is instantiated for two specific cross-network mining tasks:\n1.  Cross-Network Node Classification (GRADE-N): For this task, the objective function becomes `min θf,θh L(h(f(Gs;θf);θh),Ys) + λ · dGSD(f(Gs;θf),f(Gt;θf))`. Here, `f(·)` is a message-passing graph neural network (specifically, a Graph Convolutional Network (GCN)) parameterized by `θf`, used to extract subtree representations. `h(·)` is a classifier function (implemented as a Multi-Layer Perceptron (MLP)) parameterized by `θh`. `L(·,·)` is the cross-entropy loss function. The `dGSD` term is calculated over the subtree representations `f(Gs;θf)` and `f(Gt;θf)` using the finite iteration formula `(1/(M+1)) * Σ(m=0 to M) db(Gsm,Gtm)`.\n2.  Cross-Domain Recommendation (GRADE-R): For recommendation, the objective function is `min θf,θh' L(h'(f(Gs;θf);θh')) + L(h'(f(Gt;θf);θh')) + λ · dGSD(f(Gs;θf),f(Gt;θf))`. Similar to GRADE-N, `f(·)` is a GCN for extracting subtree representations. `h'(·)` is a link prediction function (implemented as an MLP) parameterized by `θh'`, which infers whether a link between two nodes exists. The loss `L(·,·)` is the binary cross-entropy, applied to predict `yuv` (link label) based on concatenated node features `[f(u)||f(v)]`. The `dGSD` term is computed identically to GRADE-N.\nBy minimizing this combined objective, GRADE learns a latent feature space where the distribution shift between source and target graphs is reduced, thereby enhancing knowledge transferability and improving prediction performance on the target domain.": 1336,
    "Multi-Level Contrastive Learning (MLCL) is proposed to learn discriminative and generalizable features from the data projected by SSDP. This scheme considers semantic relationships at three levels: pixel, instance, and class. Class prototypes are constructed by average pooling features within each class region and updated using a moving average strategy. Instance prototypes are similarly computed by average pooling features within each connected region of an instance.\nThe MLCL loss `Lmlcl` is a combination of three components:\n1.  Pixel-to-Pixel Contrastive Learning (`Lpp`): This novel loss aims to bring pixels with the same class label closer and push those with different labels apart in the feature space. For a given class, `Nk` features are sampled to manage memory. A similarity matrix `W` is calculated over sampled pixel-wise features, where `Wij = exp(zi · zj/τ)`. A ground truth label matrix `L` indicates semantic relationships (`Lij = 1` if `yzi = yzj` else `0`). Both `W` and `L` are normalized row-wise to form transition probability matrices `fW` and `eL`. The `Lpp` loss is then defined as the sum of Jensen-Shannon (JS) divergence between each row of `fW` and `eL`, effectively matching their distributions. This formulation includes the feature with itself as a positive pair along the diagonal.\n2.  Pixel-to-Class Contrastive Learning (`Lpc`): This loss enforces that pixel-level features should be closer to their corresponding class prototypes. It is formulated as a standard classification loss, ensuring each pixel can be correctly classified by the class prototype classifier.\n3.  Instance-to-Class Contrastive Learning (`Lic`): This loss constrains class prototypes to correctly classify instance prototypes. Instead of simply pulling all instance prototypes of a class closer to the class prototype, which might reduce diversity, a margin triplet loss is adopted. This loss encourages an instance prototype `pk_m` of class `k` to be closer to its class prototype `Pk` than to any instance prototype `p\\k_n` from other classes, with a specified margin `ξ`.\nThe total multi-level contrastive loss `Lmlcl = λLpp + Lpc + Lic` is used during training, where `λ` balances the contribution of the pixel-to-pixel loss.": 1337,
    "The paper leverages the transport plan, denoted as `pi_star`, obtained from solving the m-PPOT problem to distinguish \"\"known\"\" and \"\"unknown\"\" samples. The transport plan `pi_star` is a matrix measuring the matching between source prototypes and target features. The core idea is that samples (or prototypes) that are more easily transported (i.e., have higher mass in the transport plan) are more likely to belong to common classes (\"\"known\"\" samples). For target samples, the column sum of `pi_star` is used to derive a weight `wt_i` for each target sample `i`. Specifically, `wt_i = (n/epsilon) * sum_j(pi_star_ji)`, where `n` is the total number of target samples and `epsilon` is the total mass transported. This `wt_i` indicates the \"\"knownness\"\" of target sample `i`. For source prototypes (representing classes), the row sum of `pi_star` is used to compute weights `ws_j` for each source prototype `j`. These `ws_j` values represent the possibility that each source category belongs to a common class. These derived weights (`wt` for target, `ws` for source) are then directly incorporated into the subsequent loss functions to guide the model in distinguishing and handling \"\"known\"\" and \"\"unknown\"\" samples.": 1338,
    "This challenge is addressed by the Cross-domain Self-training (CS) module. The core idea is to iteratively identify and utilize \"\"confidence\"\" samples from the target domain's query set (QT) as new class prototypes, replacing or augmenting those from the source domain's support set (XS). The process begins by assuming that some \"\"confidence\"\" samples in QT can be correctly classified by the support set (XS) when domain distributions are aligned. These \"\"confidence\"\" samples are selected based on a similarity score threshold (empirically set to 1.7). Once selected, these target domain samples are regarded as new prototypes for their respective classes. This self-training process is\nconducted iteratively. To further improve the performance of the target domain classifier and ensure samples within the same class are closer, a class matching loss (Lclm) is designed. This loss aims to make the similarity pattern of a target sample with its most similar class (ppos_q) greater than its similarity pattern with the second most similar class (pneg_q) by a predefined margin (m, empirically set to 1.5). This iterative process, guided by the class matching loss, helps to correct misclassified samples in QT and lighten the impact of the domain gap by effectively transporting labels from the source support set to the target query set.": 1339,
    "The paper formulates the training process as a bi-level optimization problem solved through a novel meta-learning scheme, specifically an episodic training scheme similar to Model-Agnostic Meta-Learning (MAML). This approach aims to optimize the domain transformer `ψ` and a shared classifier initialization `θh` such that the augmented data `˜St+1` and the real target data `St+1` can share the same predictive model.\nThe training protocol, detailed in Algorithm 1, consists of two main steps:\n1.  Inner-loop Updates: In each episode, a mini-batch of data points is sampled from consecutive domains `t` and `t+1`. The inner loop focuses on learning a domain-specific classifier `θht+1`. This classifier is optimized using an `Linner` loss function on the *directional transform augmentations* `˜Zt+1` (generated by `ψ`). The `Linner` loss is a combination of cross-entropy loss (`Lcls`) and a Kullback-Leibler (KL) divergence distillation loss. The KL divergence term, applied to softened softmax outputs at a temperature `τtemp`, helps preserve instance semantics. The classifier `θht+1` is updated via a single gradient step: `θht+1 = θh - α∇θhLinner`, where `α` is the inner-loop learning rate and `θh` is the shared classifier initialization. This step ensures that the classifier quickly adapts to the augmented features.\n2.  Outer-loop Updates: After the inner-loop update, the adapted classifier `θht+1` is evaluated on the *real* features `Zt+1` from the next domain `t+1`. The `Louter` loss is calculated as the cross-entropy loss between the true labels `yt+1` and the predictions from `ht+1(zt+1|θht+1)`. This `Louter` loss then drives the update of all overall DDA parameters: `θ = {θh, θψ, θϕ}` (feature extractor `ϕ`, domain transformer `ψ`, and shared classifier `h`) using an outer-loop learning rate `β`. This outer loop ensures that the shared classifier initialization `θh` is generalizable and that the domain transformer `ψ` generates augmentations whose decision boundaries align with the true next domain. The bi-level structure allows the model to learn to generate augmentations that are useful for training a classifier that performs well on the actual unseen target domain.": 1340,
    "The paper introduces Spectral Adversarial Data Augmentation (SADA) to generate hard-to-learn samples that suppress model sensitivity. SADA operates by adversarially perturbing the amplitude spectrum of source images while preserving their phase spectrum, leveraging the observation that phase retains semantic structure while amplitude contains style information. For a given source image `x`, its amplitude `Aorg` and phase `Porg` are first computed using Fast Fourier Transform (FFT). The original amplitude spectrum `Aorg` is then initialized with a random perturbation `A0 = Aorg ⊙ (1 + Unif(−ϵ,ϵ))`, where `Unif(−ϵ,ϵ)` is a 2D matrix with entries sampled uniformly from `[-ϵ,ϵ]` and `⊙` is the Hadamard product. The amplitude spectrum `At+1` is iteratively optimized by adding a weighted sign gradient of the cross-entropy loss to the current amplitude `At`. Specifically, `At+1 = At · {1 + δ · sign[∂ℓCE(F(IFFT [At,Porg]),y) / ∂At] ⊙ MS}`, where `δ` is the perturbation step size, `ℓCE` is the cross-entropy loss, `F` is the model, `IFFT` is the Inverse Fast Fourier Transform, and `MS` is the enhanced model sensitivity map (from RQ1). The `MS` map guides the perturbation towards highly sensitive frequencies. In each iteration, the augmented image `xk,t` is reconstructed by clamping `IFFT [At,Porg]` to `[0,1]`. An early-stop mechanism is employed to accelerate the process if the model prediction changes. The final augmented image `˜xk` is the clamped `xk,t` after `T` steps. This targeted adversarial perturbation in the frequency space generates samples with significant appearance variation that directly address the model's spectral weaknesses.": 1341,
    "To learn robust domain-invariant features and alleviate the noise impact from pseudo-labels, the paper introduces a Cross-Attention Module (CAM). While the Rumor Representation Module (RRM) uses self-attention to learn individual rumor representations, the CAM specifically applies cross-attention to pairs of source and target rumors that share the same (pseudo) label.\nThe CAM operates as follows:\n1.  Input Pairing: It takes a pair of rumors, one from the source domain (`Xs`) and one from the target domain (`Xt`), both of which have been assigned the same label (ground-truth for source, pseudo-label for target).\n2.  Cross-Attention Mechanism: The query (`Qs`) is derived from the source rumor (`Xs`), while the keys (`Kt`) and values (`Vt`) are derived from the target rumor (`Xt`). The cross-attention score is calculated by `softmax(Qs * Kt^T / sqrt(dk)) * Vt`. This mechanism allows the model to identify and focus on the most semantically similar propagation paths between the source and target rumors, even if they come from different domains. The intuition is that rumors of the same category, regardless of domain, tend to express similar semantic patterns (e.g., supporting or denying attitudes).\n3.  Domain-Invariant Feature Learning: By forcing the model to attend across domains based on shared labels, the CAM explicitly encourages the learning of features that are invariant to domain-specific variations but indicative of the rumor's underlying category.\n4.  Noise Robustness: The cross-attention mechanism is inherently robust to noisy inputs. Given that the target pseudo-labels might contain errors, the CAM's ability to selectively attend to relevant parts of the target data based on the source query helps to filter out noise and focus on consistent semantic patterns, thereby mitigating the negative impact of incorrect pseudo-labels.\n5.  Joint Training with KL-Divergence: The output of the CAM (`^Os_crossi`) is fed into the Rumor Prediction Module (RPM) to generate predictions (`pcross_i`). The model then minimizes the KL-divergence (`LCA`) between these predictions from the CAM output and the predictions (`pi`) obtained directly from the source rumor's self-attention output (`^Os_i`). This ensures that the domain-invariant features learned by the CAM are consistent with the discriminative features learned from the labeled source data, further enhancing alignment and robustness.": 1342,
    "To accomplish the meta task, the paper proposes a Task Adaptation Network (TAN). This network operates in two phases. In the first phase, a Basic Slot Tagging Network (BSTN) is trained on the auxiliary set (large data from the basic domain) in a supervised learning manner. BSTN uses a CRF-based few-shot slot tagging framework to learn historical information, particularly for overlap slots. In the second phase, a Task Adaptation Meta Network (TAMN) is trained. TAMN employs a Meta Encoder (ME) to capture meta knowledge from the target support set. For each word, both the Basic Encoder (BE) (from BSTN) and ME encode it into feature vectors. A Task Adaptation module then fuses these two feature vectors using a weighted sum, where the weight (wb_i) for historical information is dynamically calculated by two fully-connected linear layers based on the concatenation of the BE and ME outputs. This adaptive fusion mechanism allows the model to determine the knowledge transfer degree from the basic domain according to the specific task, effectively combining historical information with novel meta knowledge.": 1343,
    "To investigate the ability of Transformer-based Language Models (TLMs) to distinguish between different senses of polysemous words, the paper conducts \"\"Semantic Similarity Trials\"\" (‡2). This involves creating a specialized dataset of polysemous domain terms that appear in both the vocabulary of the TLMs and the TechQA dataset. For each polysemous word, the paper notes that the TechQA corpus typically manifests only a single, dominant technical sense. To introduce auxiliary senses, contexts for these other meanings are scraped from vocabulary.com, with ten contexts collected per sense for a given word. The core of the method involves computing the average cosine similarity between the contextualized embeddings of the target polysemous word. This computation is performed for two scenarios: first, for embeddings derived from contexts belonging to the *same* sense group (intra-group similarity), and second, for embeddings derived from contexts belonging to *different* sense groups (inter-group similarity). The expectation is that models capable of effective sense disambiguation will exhibit high intra-group similarity and significantly lower inter-group similarity, or at least a substantial margin between the two, indicating their ability to segregate distinct word senses. This analysis is applied to BERT, RoBERTa, SciBERT, and SenseBERT to evaluate their semantic understanding capabilities in a closed-domain setting.": 1344,
    "Transformer-based models are finetuned for accurate classification of nuanced humor subtypes by leveraging pre-trained large language models and adapting them to the newly constructed dataset. The models chosen for finetuning include BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (A Robustly Optimized BERT Pretraining Approach), and DeBERTa (Decoding-enhanced BERT with Disentangled Attention). These models, with parameter counts of 110M, 125M, and 184M respectively, are selected for their strong performance in various natural language processing tasks. Additionally, the Longformer (The Long-Document Transformer), with 102M parameters, is evaluated to accommodate longer text contexts, as 8.96% of the dataset examples exceed the 512-token limit of the other models. The finetuning process involves training these pre-trained models on the novel jokes dataset, which contains Wholesome Jokes, Dark Jokes, Dirty Jokes, and News articles, for the multiclass classification task. The models learn to distinguish between these categories based on the textual content of the jokes. The performance of each finetuned model is measured using accuracy, precision, recall, and the micro-averaged F1 score to assess their ability to classify the humor subtypes.": 1345,
    "The Log-Attention module is proposed to address the loss of semantic information from parameters discarded during log parsing. This module modifies the structure of the original Transformer encoder. After the log parsing process, which typically extracts templates and identifies parameters, the Log-Attention module specifically processes the `P` parameters obtained for each log sequence. For every character `Pi` within these parameters, a feature extractor is employed to derive a character-level embedding, denoted as `PEi`. Subsequently, a Linear layer is utilized to encode the entire `PE` (the collection of character-level embeddings for all parameters) into a single output vector, `ϕp`, which represents the parameter encoding. This `ϕp` is then assigned a learnable scalar, which functions as a bias term. This bias term is directly incorporated into the self-attention mechanism of the Transformer. The Log-Attention computation is defined as `Softmax(QKT / sqrt(d/h) + ϕp)V`, where `Q`, `K`, and `V` are queries, keys, and values, `d` is the input dimension, and `h` is the number of heads. By adding `ϕp` as a bias to the attention scores, the module effectively supplements and integrates the information from the parameters, which would otherwise be ignored, directly into the attention mechanism, thereby enhancing the model's ability to detect anomalies.": 1346,
    "The Mutual Information Maximization (MIM) module is designed to align representations of truly positive video-query pairs in the unsupervised target domain. It captures cross-modal semantical relevance in bidirectional video-to-query and query-to-video ways. During learning, MIM adaptively aligns video-query pairs that are more likely to be relevant in the target domain, ensuring progressive increases in positive pairs and enhancing the discriminability of target features. This enables the model to learn domain-invariant and semantic-aligned cross-modal representations.": 1347,
    "The paper addresses the challenge of accurately identifying unknown-class samples by proposing a novel Cross-domain Manifold Mixup (CMM) scheme. This scheme explicitly simulates unknown-class samples in a smoother way across domains, preventing issues arising from distribution shifts. The process begins by randomly sampling an interpolation factor, λ, from a Beta distribution (Beta(α,α) with α=2.0). This continuous factor allows the network to leverage arbitrary intermediate states between source and target features.\nGiven a source sample xs_i and a target sample xt_j, a mixup feature vector zm_i,j,λ is generated in the manifold space using the feature extractor F: zm_i,j,λ = λF(xs_i) + (1 - λ)F(xt_j). This mixup feature is designed to represent a potential unknown-class sample. The rationale is that such a mixed feature, derived from different domains, will belong to a known class with low probability, making it a suitable simulation for an unknown class.\nTo supervise the open-set classifiers and enable them to identify these simulated unknown-class samples, a loss function called Lcmm is defined. Lcmm(xs_i, xt_j) is calculated as -log(1 - po(ys_i | zm_i,j,λ)), where po(ys_i | z) is the positive score from the open-set classifier for the source class ys_i given the feature z. By minimizing Lcmm, the positive scores for these simulated mixup samples are driven to be smaller, effectively training the open-set classifiers to recognize them as unknown, thereby improving unknown-class identification.": 1348,
    "The paper proposes a Relation-aware Temporal Consistency (RTC) regularization to model inter-class consistent relationships across frames. Instead of treating each class independently, RTC considers the class ranking as a random event. First, scores of pixel-class relations (si,j) between a pixel (pi,j) and the target-aware prototypes (PT) are derived using a softmax operation. These scores are then transformed into class-ranking probability distributions (P(π ∈ P|s)). This transformation involves calculating the probability of every permutation (π) of classes based on their scores, where P(π|s) is computed as a product of normalized scores for each class in the permutation. For computational efficiency, the method focuses on permutations of the top-4 classes in each prediction, as these are observed to contain almost all probabilities. To enforce consistency along the temporal dimension, an optical flow network is used to estimate optical flow (Ot→t−1) between adjacent frames. The feature map (Ft−1) from the previous frame is then warped to spatially align with the current frame (t). Finally, the Kullback-Leibler (KL) Divergence loss (LKL) is applied to measure the consistency between the class ranking probability distributions of the current frame (st,i,j) and the warped previous frame (st−1,i,j), thereby providing effective supervision signals that leverage inter-class relationships.": 1349,
    "To mitigate overfitting to spurious correlations and promote the learning of generalizable features, the paper proposes an Out-of-Domain (OoD)-aware objective function called Generalizable loss (G-loss), denoted as `Lg(θ,ω,α)`. This loss is designed to counteract the tendency of deep neural networks (DNNs) to prioritize easy-to-learn, non-causal features (e.g., background elements) that may be spuriously correlated with ground truth labels in the training data, leading to poor OoD generalization. The G-loss is mathematically formulated as `1/2 * ||ˆy1||^2 - 1/2 * ||ˆy2||^2`, where `ˆy1` represents the output of the classification head and `ˆy2` represents the output of the regression head. This specific formulation is derived from theoretical analysis based on the Neural Tangent Kernel (NTK) theory, which models infinite-width neural networks. The core principle behind `Lg` is to discourage the network from relying on a few dominant parameters or architectural candidates for predictions. By doing so, it forces both the DNN's parameters and the Differentiable NAS process to utilize a broader range of information in representation learning. The theoretical derivation suggests that the gradient descent for a variational parameter `Φ` (related to `Lg`) avoids interference between different elements, effectively making training data independent and preventing the emergence of dominant features, thereby encouraging the activation of more diverse features for prediction. The `Lg` term is incorporated into the overall training loss `Ltrain` as `Ltrain = Ldet + Lcls + Lreg + λg · Lg`, where `Ldet` is the detector loss, `Lcls` is the cross-entropy loss for classification, `Lreg` is the smooth L1 loss for bounding box regression, and `λg` is a hyper-parameter controlling the weight of `Lg`.": 1350,
    "To ensure the effectiveness and accuracy of the gaze frontalization process, the paper introduces a Consistency Loss `Lcon`, which comprises two terms: Gaze Term `LGcon` and Head Orientation Term `LHcon`. For the Gaze Term, the generated frontalized face image `xfro` is fed back into the Gaze Estimation Module (specifically, the feature extractor `F` and the gaze prediction layer `G`) to obtain its estimated gaze direction `ˆgfro`. The loss `LGcon` is then calculated as the L1 distance between `ˆgfro` and the target frontal gaze (0,0). For more stable training, (0,0) is replaced with `gref`, the gaze label of the reference image `xref`, making the loss `LG∗con = ∥ˆgfro − gref∥1`. Similarly, for the Head Orientation Term, `xfro` is fed back into the Gaze Estimation Module (feature extractor `F` and head orientation prediction layer `H`) to obtain its estimated head orientation `ˆhfro`. The loss `LHcon` is the L1 distance between `ˆhfro` and the original head orientation `h` of the input image `x`. For stability, `h` is replaced with `href`, the head orientation label of `xref`, resulting in `LH∗con = ∥ˆhfro − href∥1`. These consistency losses ensure that the eyeballs in `xfro` are indeed looking at (0,0) and that the head orientation remains consistent with the original input image `x`, thereby strengthening the auxiliary learning.": 1351,
    "The paper proposes the Frequency-Augmented Contrastive Learning (FACL) module, which operates on features extracted from the surrogate model. After the perturbation generator produces an adversarial image `x′s`, both the clean image `xs` (after FADR transformation) and the adversarial image `x′s` undergo spectral decomposition. This decomposition is performed using a band-pass filter `Mbp` and a band-reject filter `Mbr` to separate the inputs into mid-band and low/high-band frequency components, respectively. These components are then inversely transformed to the image domain by IDCT and passed through the `k`-th layer of the surrogate model `f` to extract band-specific features. The spectral decomposition operator `zband = fk(ϕ−1(ϕ(xinput) ⊙ Mband))` yields two pairs of frequency-augmented features: `(zm, z′m)` for mid-band (domain-agnostic) and `(zlh, z′lh)` for low/high-band (domain-specific). The FACL module defines a loss function, `LFACL = sim(zm, z′m) − sim(zlh, z′lh)`, where `sim` is the cosine similarity. The objective of `LFACL` is to repel the domain-agnostic mid-band feature pair (`zm, z′m`) and attract the domain-specific low- and high-band feature pair (`zlh, z′lh`). This push-pull action among band-specific feature pairs guides the perturbation generation towards a more robust regime, enhancing generalization. The final learning objective for the generator `Gθ(·)` minimizes `min θ (λorig · Lorig + λFACL · LFACL)`, where `Lorig` is the baseline loss for attacking the surrogate model and `λorig`, `λFACL` are loss coefficients.": 1352,
    "To preserve inter-sample similarity relationships from both source and target domains in the hash codes, the paper introduces the Relation Preservation Loss (RPL). This loss is composed of two main terms. The first term, Lrel1 (Eq. 7), directly guides the hash learning process using the labels available in the source domain. It minimizes the Frobenius norm of the difference between a scaled similarity matrix (ηSs), computed from source labels, and the cosine similarity of the relaxed hash codes (Hs) generated by the HashEncoder for source samples. The second term, Lrel2 (Eq. 9), transfers knowledge from the source to the target domain by enforcing that similar features in the domain-shared feature representation space of both domains should generate corresponding hash codes. Specifically, it minimizes the Frobenius norm of the difference between the cosine similarity of features (Fs, Ft) from both domains and the cosine similarity of their corresponding relaxed hash codes (Hs, Ht). The features are obtained from the MLP, and hash codes from the HashEncoder. Finally, these two terms are combined into the overall Relation Preservation Loss (LRPL) (Eq. 10) using a fusion factor gamma (γ) to balance their importance. This integrated loss ensures that the learned hash codes retain the semantic relationships present within the source domain and also maintain consistency across the source and target domains in the shared feature space.": 1353,
    "The paper addresses this dual challenge by implementing two distinct domain alignment strategies tailored to the previously divided target node groups.\nFor target nodes in the Certain Group (Groupc), which are highly probable to belong to known classes, an Adversarial Domain Alignment mechanism is utilized. This involves an embedding extractor `G(·)` and a domain discriminator `O(·)` engaged in a minimax game. The objective function, `Ladv`, is formulated as `min O max G Ladv = Exs∈Xs log(O(G(xs))) + Ext∈Groupc log(1 - O(G(xt)))`. Here, `Xs` represents the source nodes. The domain discriminator `O` attempts to distinguish between source nodes and target nodes from `Groupc` based on their learned embeddings, while the embedding extractor `G` aims to produce embeddings that are indistinguishable to `O`, thereby learning domain-invariant features for the known classes. This process effectively aligns the `Groupc` target nodes with the source domain.\nFor target nodes in the Uncertain Group (Groupu), where direct adversarial alignment could lead to negative transfer due to the presence of unknown classes, a novel Neighbor Center Clustering (NCC) approach is proposed. This method aims to better identify and separate unknown-class nodes from known-class nodes within `Groupu`. The procedure is as follows:\n1.  Center Matrix Formation: K-means clustering is applied to the embeddings of nodes in `Groupu` to obtain `K` cluster centers, denoted as `{µ1t, ..., µKt}`. Simultaneously, the weight vectors `Wf = [w1f, ..., w|Cs|f]` from the classifier `F` are used as source class centers. These two sets of centers are combined to form a comprehensive center matrix `M = [µ1t, ..., µKt, w1f, ..., w|Cs|f]`. This matrix `M` is primarily used to cluster unknown class data while also incorporating source class centers to avoid negative clustering for known classes that might be present in `Groupu`.\n2.  Probability Calculation: For each target node embedding `git` from `Groupu` and each center `mj` in `M`, the probability `pi,j` that `mj` is the neighbor center of `git` is calculated using a softmax function: `pi,j = exp(⟨git, mj⟩ / τ) / Σk=1 to K+|Cs| exp(⟨git, mk⟩ / τ)`. Here, `⟨·,·⟩` denotes the inner product for similarity measurement, and `τ` is a temperature parameter (empirically set to 0.05). Both `µi` and `wj` are L2-normalized.\n3.  Clustering Loss: The Neighbor Center Clustering loss `Lncc` is then formulated as `Lncc = - (1/Ngroupu) Σi=1 to Ngroupu Σj=1 to K+|Cs| pi,j log(pi,j)`. By minimizing `Lncc`, the model encourages target nodes in `Groupu` to cluster around these centers. This effectively pushes known-class nodes within `Groupu` towards the source class centers and unknown-class nodes towards the newly formed cluster centers, thereby making the learned target embedding space more discriminative for unknown class identification.\nThe overall objective function combines these components: `LSDA = Lce + Ladv + βLncc`, where `Lce` is the cross-entropy loss for the source graph, and `β` is a hyper-parameter balancing the NCC loss.": 1354,
    "To maintain contextual sensitivity, the paper employs a Disentangled Contrastive Matching (DCM) module, which operates after a Contextual Semantic Adaption (CSA) module. First, literal (`EL`) and metaphorical (`EM`) embeddings, which may have semantic and encoding gaps due to different inputs and initialized parameters, are transformed using linear layers to `FL` and `FM` respectively. These are then fed into a multi-head cross-domain attention (CDA) mechanism, where `FL` and `FM` interact to produce an enriched representation `RL`. The DCM module then aims to disentangle and match contextual semantics with literal representations contrastively. It assumes that each meme focuses on one specific kind of contextual semantics. To achieve this, the Straight-Through Gumbel-Softmax (STGS) function is applied over `FM` to reparameterize the process of maximizing the likelihood of present latent contextual semantics while minimizing absent ones. Gumbel noise (`δ`) is added to the pooled output of `FM` to obtain `NM`. A smooth approximation is applied by adopting a self-paced annealing schedule for the temperature (`τk`) of the Gumbel-Softmax distribution. This `τk` is adjusted based on the average total loss (`Lk-1`, `Lk-2`) of the last two epochs, allowing for curricular learning where the distribution smoothness (and gradient flow) adapts to the difficulty of training samples. The sampled discrete distribution (`DM`) from this process is then leveraged to provide supervision for the literal representations (`FL`) through a KL divergence loss (`LDe`), ensuring contextual compatibility.": 1355,
    "To improve content preservation, especially for complex motions where Dual Diffusion Implicit Bridges (DDIBs) might struggle, the paper introduces Keyframe Manifold Constraint Gradients (KMCGs). KMCGs incorporate additional corrective content during the inference phase without requiring retraining of the diffusion models. This is achieved by constructing a bias from source domain keyframes, which are selected based on the acceleration of body joints. These keyframes act as a content constraint (y = Hxi + ϵ). During the backward diffusion process (reverse diffusion), the KMCGs apply a corrective term to steer the generation closer to the context manifold defined by these keyframes. Specifically, the standard reverse diffusion equation is modified to include a gradient term (∂/∂xk ∥W(y − H ˆx0(xk))∥2 2), where ˆx0 is the estimated original motion, y is the measured content (keyframes), and W and H are weight matrices and measurement operators respectively. This gradient term incorporates the information from the source keyframes, ensuring that the generated motion in the target domain retains the postures and content of the source motion. This method allows for the imposition of source domain-related information across multiple intermediate diffusion steps, enhancing content coherence.": 1356,
    "The paper introduces a novel pseudo-labeling strategy called Clustering Consistency (CC) to accurately label low-confidence target samples and reduce noise. This strategy is motivated by the observation that target data often form clear patterns in the feature space, even if the source-dominated model struggles to recognize them due to domain discrepancy, leading to high model uncertainty. CC aims to detect these \"\"easy patterns\"\" for pseudo-labeling.\nInstead of relying solely on model confidence, CC leverages clustering algorithms to discover intrinsic patterns within the target data's embedding space. Specifically, the feature embeddings `fi = Fs(xt_i)` (extracted by the pretrained source model `Fs`) are used. Two different clustering algorithms are applied: Gaussian Mixture Model (GMM) clustering and K-means clustering. These algorithms produce two probabilistic matrices, `P1` and `P2`, respectively, where `p*ik` denotes the probability of the `i`-th sample belonging to the `k`-th class for each clustering view.\nTo generate pseudo-labels, CC utilizes the consistency between these two clustering views. For a sample `xt_i` to be pseudo-labeled as class `k`, it must satisfy two conditions: `xt_i` must be among the top `M2` samples with the largest prediction probability to class `k` according to `P1`, AND `xt_i` must also be among the top `M2` samples with the largest prediction probability to class `k` according to `P2`. This dual-condition approach filters out inconsistent samples, thereby decreasing the noise of pseudo-labels. The parameter `M2` is fixed to be the same as the `M2` used for AEN in NDNS, avoiding additional hyperparameters. The pseudo-labeled samples `ˆDTl` are then appended to the labeled dataset `DL` and removed from the unlabeled dataset `DTu`, effectively increasing the number of \"\"labeled\"\" target samples without manual effort.": 1357,
    "The relaxed variant, R-SLSD, incorporates a small subset (e.g., 1% of the training set) of the target data that provides protected attributes. During pre-training, this subset helps refine the cross-domain protected group estimator by providing ground truth labels for some target data points. This enhances the accuracy of group estimations on the target domain, leading to better fairness improvements in the downstream model.": 1358,
    "The paper proposes a Guided Diffusion-based Decoder (ϵ) to generate diverse skills from the disentangled domain-invariant (zρ) and domain-variant (zσ) embeddings. This decoder is based on the Denoising Diffusion Probabilistic Model (DDPM), which reconstructs an action (at) from a noisy input (xK) by iteratively denoising it. The key innovation is the redefinition of the decoder ϵ(xk,k,st,zρ,zσ) as a combination of two separate parts: a domain-invariant decoder ϵρ(xk,k,st,zρ) and a domain-variant decoder ϵσ(xk,k,st,zσ). Specifically, the combined decoder output is formulated as (1 - δ)ϵρ(xk,k,st,zσ) + δϵσ(xk,k,st,zρ), where δ > 0 is a guidance weight. This conditional generation mechanism allows the domain-invariant decoder to reconstruct actions that consistently execute the designated task across domain features, while the domain-variant decoder provides guidance that encapsulates domain-variant features. This joint training with the hierarchical domain encoder enables the generation of diverse action sequences by combining different domain-invariant and domain-variant features.": 1359,
    "To mitigate mode collapse, which results in the generator producing nearly identical synthetic samples, the paper adopts the MultiGAN (Multi-Generator Generative Adversarial Networks) approach. This method involves initializing a pool of multiple (e.g., 10, denoted as NG) identical conditional-generator models. During the training process, a random generator from this pre-initialized pool is selected and used in a round-robin fashion for each training step. The key insight is that the randomness introduced by the different initializations of these multiple generators, combined with their independent training dynamics, helps to increase the variance of the generated images. This increased variance directly contributes to producing a more diverse set of synthetic samples, thereby effectively mitigating the mode collapse problem that often plagues single-generator GANs.": 1360,
    "After obtaining unbiased domain outputs (˜y_i^t) for each source bottleneck feature (as described in Solution 1), the paper addresses how to learn inter-domain ensemble weights (β) to combine these into a final classification result (¨y^t). This is also part of the Bi-level ATtention ENsemble (Bi-ATEN) module. The method leverages an attention mechanism based on bottleneck features. First, key embeddings (ϕ̂_K^t) are formed by concatenating the linearly transformed bottleneck features (ϕ_i^t W_F) from all sources. For query embeddings (ϕ̂_Q^t), all bottleneck features are first concatenated and then linearly transformed (W_QF, Linear3 in Fig. 2). The inter-domain weights (β) are then computed by applying a softmax operation over the cosine similarity between these query and key embeddings. This process dynamically determines the importance of each unbiased domain output for the final ensemble. To train these weights, a dynamic-cluster-based strategy is employed to generate pseudo labels for the unlabeled target data. This strategy involves dynamically computing centroids for each class based on aggregated source model outputs and dynamically combining source bottleneck features for each target sample. The final objective for the inter-domain weights includes a cross-entropy loss (CE) with label smoothing against these pseudo labels and an Information Maximization (IM) loss (L_IM) to ensure confidence and diversity of the final ensemble output.": 1361,
    "To prevent over-adaptation in self-training and ensure robustness under continual domain shift, the paper introduces a tri-net architecture that incorporates an anchor network for regularization. The core self-training mechanism involves a Teacher network `ft(x;Θ)` and a Student network `fs(x;Θ)`, where the Teacher predicts pseudo-labels for the Student's training. The Student's BN layers are independently updated, while other weights are shared. The regularization is achieved by introducing an Anchor network. This Anchor network is a frozen copy of the source domain model, but its regular Batch Normalization layers are swapped out with the proposed Balanced BN layers, which are dynamically updated by testing samples. An anchored loss, `Lanc`, is then defined as the Mean Square Error (MSE) between the posterior predictions of the Teacher network (`pt`) and the Anchor network (`pa`). This loss is calculated only for confident pseudo-labels, identified by a threshold `H0` on the entropy of the Teacher's predictions. The final optimization objective for test-time adaptation is a combination of the self-training loss `Lst` and the anchored loss `Lanc`, weighted by a hyper-parameter `λanc` (L = Lst + λancLanc). By jointly optimizing these losses, the model is encouraged to adapt to the current domain (via self-training) while being regularized by the stable Anchor network, which prevents it from overly shifting its weights and thus maintains generalization across evolving domains.": 1362,
    "The paper derives an effective target classifier without traditional training by generalizing Maximal Correlation Regression (MCR). After the optimal source weights (α) are determined, the target feature extractor fT is established as the weighted sum of source feature extractors. The optimal target classifier, gT, is then directly calculated based on a theoretical form derived from MCR, specifically g*(y) = sum(αi * EPX|Y[fi(X)|Y = y]). This means the classifier is not learned through an iterative training process but is analytically determined. Notably, this gT(y) functions as a label encoder, mapping labels 'y' to a newly defined 'y-feature' in the feature space, rather than outputting class probabilities from extracted features. In the testing mode, the classification result for an input 'x' is obtained by maximizing the correlation between the extracted target feature fT(x) and the derived y-feature gT(y), i.e., ˆy(x) = argmaxy fT T(x)gT(y). This direct calculation method contributes to high efficiency in transfer.": 1363,
    "The framework aligns under-adapted samples by leveraging feature structure through a Graph Contrastive Learning (GCL) module. This module aims to align under-adapted samples not only to well-adapted samples of the same potential classes but also to their local neighbors and their own augmented views in the feature space.\nThe process involves:\n1.  Model Architecture and Augmentations: For each target sample x, a weakly-augmented view α(x) is generated and fed to the target model hT for classification logits. A projection network g(·), sharing the same encoder as hT, embeds α(x) into a normalized low-dimensional feature vector q. Additionally, a strongly-augmented view A(x) is generated, and its feature vector q′ is obtained using g′(·), which is a momentum-updated version of g(·). SimAugment is used for weak augmentation and RandAugment for strong augmentation.\n2.  Locality Label Graph (W^L): For a batch of samples, a B × B locality label graph W^L is constructed. The edge weight W^L_ij between samples xi and xj depends on their potential class labels (ˆy) and neighborhood status:\n    *   If both xi and xj are in the well-adapted subdomain (DW), W^L_ij = I(ˆyi = ˆyj), indicating a strong connection if they share the same potential class.\n    *   If xi is in DW and xj is in the under-adapted subdomain (DU), W^L_ij = λI(ˆyi = ˆyj), where λ is a hyper-parameter (between 0 and 1) that assigns a lower weight to edges involving under-adapted samples, allowing for a more error-tolerant alignment.\n    *   If xi is in DU, W^L_ij = λI(ˆyi = ˆyj ∨ qj ∈ N^k_i), meaning under-adapted samples are aligned to samples with the same potential class OR to their k-nearest neighbors (N^k_i) in the feature space.\n    The potential class ˆy is determined by the source-predicted label (argmax(z)) for samples in DWe, and by the self-predicted label (argmax(hT(α(x)))) for other samples. A value of 1 is filled on the diagonal of W^L to encourage strong self-alignment. To enlarge the neighborhood, a feature queue storing current momentum features is maintained, expanding the graph size.\n3.  Feature Graph (W^F): A B × B feature graph W^F is constructed based on the similarities of features generated by the two augmented views. The edge weight W^F_ij is calculated as exp(q_i^T · q_j' / t), where t is a temperature parameter.\n4.  Graph Contrastive Learning Objective (L_cont): The objective is formalized as the cross-entropy loss between the row-wise normalized versions of the feature graph (ˆW^F) and the locality label graph (ˆW^L): L_cont = (1/B) * Σ_i=1^B H(ˆW^L_i, ˆW^F_i). This objective pulls features of samples that should be related (according to W^L) closer in the feature space, while pushing unrelated ones apart, thereby aligning under-adapted samples to their local context and well-adapted anchors.": 1364,
    "The large-scale mixed-integer program (MIP) formulation for FairWASP is reformulated into a computationally tractable linear program (LP) and its dual through a three-step process.\nFirst, the original problem (4) is reformulated as a MIP (8). The fairness constraints J(pZ;θ(y|d), pY(y)) ≤ ϵ are converted into 2|Y||D| linear constraints on the weights θ (Aθ ≥ 0) by inverting a fractional linear transformation. The Wasserstein distance objective Wc(pZ;θ, pZ;e) is equivalently formulated as a linear program with n^2 variables: min ⟨C,P⟩ subject to Pe = e, P⊤e = θ, and P ≥ 0n×n, where P is the transportation plan matrix and C is the cost matrix. Combining these yields the MIP (8).\nSecond, the LP relaxation (9) of this MIP is considered by relaxing the integer constraint on θ. This LP is further simplified by noting that θ = P⊤e is implicitly constrained to sum to n, allowing the problem to be rewritten as (P): minP∈Rn×n ⟨C,P⟩ subject to Pe = e, P ≥ 0n×n, and AP⊤e ≥ 0. The dual problem (D) of this LP is then derived from its Lagrangian L(P,λ) = ⟨C,P⟩ - λ⊤AP⊤e. This dual problem is expressed as maxλ≥0 -F(λ), where F(λ) = maxP∈Sn ⟨¯C,P⟩, with Sn being the feasible set for P and ¯C = Σj=1^m λjea⊤j - C. Lemma 1 demonstrates that F(λ) is a convex function of λ, and its function values and subgradients can be efficiently computed. Specifically, F(λ) equals the sum of the largest components in each row of ¯C, and a corresponding P⋆ matrix (with a single 1 per row at the position of the largest component) serves as a subgradient. This transforms the problem into a convex minimization problem (D-2): minλ∈Rm+ F(λ).\nThird, it is proven that the solution of this dual problem can lead to an optimal solution for the original MIP. Theorem 3 states that if λ⋆ is an optimal dual solution of (D) and a mild assumption (Assumption 1, ensuring a unique minimizer for maxP∈Sn⟨¯C,P⟩) holds, then the primal optimal solution P⋆ obtained from Lemma 1 and the corresponding θ⋆ = (P⋆)⊤e are optimal for both the LP (9) and the MIP (8). This implies that solving the dual LP relaxation effectively solves the original integer-weighted problem.": 1365,
    "The paper introduces Domain Prompt Placement (DPP) to adaptively allocate SVDP's trainable parameters to pixels with large distribution shifts. This is achieved by measuring the degree of domain gap at the pixel level using a general uncertainty scheme. Specifically, the Monte Carlo Dropout (MC Dropout) method is employed, performing multiple (e.g., m=10) forward propagations with dropout applied only to the linear layer within the prediction head. For each pixel `exj`, this process yields `m` group probabilities `pi(eyj| exj)`. The uncertainty value `U(exj)` for each pixel is then calculated as the mean squared difference between each predicted probability `pi(eyj| exj)` and the mean prediction `µ` across all `m` rounds (Equation 2). Pixels are then sorted based on their calculated uncertainty values, and SVDP parameters are strategically placed on those pixels exhibiting high uncertainty scores, indicating larger distribution shifts. This targeted placement ensures that prompts focus on areas most affected by domain shifts, facilitating more efficient extraction of local domain-specific knowledge.": 1366,
    "The paper addresses this by employing Koopman theory within the Temporal Koopman Networks (TKNets) framework. TKNets consists of three main components: an embedding function (ϕ), measurement functions (G), and a Koopman operator (K). First, the embedding function ϕ: X → V transforms the input raw data (x) into a representation space (V). Next, the measurement functions G: V → Z map these representations into a higher-dimensional functional space, known as the Koopman space (Z). In this Koopman space, the complex, non-linear dynamics governing the system's behavior (state transitions vt+1 = F(vt)) can be approximated by a linear Koopman operator K. This operator K acts linearly on the measurement functions, such that K ◦ g(vt) = g(F(vt)) = g(vt+1). By learning this linear Koopman operator, TKNets effectively captures the temporal transition relationship between consecutive domains, allowing for the prediction of future domain characteristics based on past observations.": 1367,
    "The paper addresses this by proposing a two-phase approach to enable the use of neighborhood clustering (NC) in the Domain Adaptation of Black-Box Predictors (DABP) setting. The core challenge is that NC methods, like AaD (Attracting and dispersing), typically assume that target features from the source model already possess a semantic structure, which is not guaranteed in DABP due to the black-box nature of the source predictor.\nFirstly, the method incorporates a \"\"warm-up\"\" phase for the target model. During this initial phase, knowledge distillation (KD) is employed to train the target model. Specifically, the paper uses DINE (Domain Adaptation from Single and Multiple Black-Box Predictors), a KD-based method, to warm up the model. This KD process helps the target model to start extracting features that exhibit a certain level of semantic structure. Crucially, Selection Training (ST) is also applied during this warm-up phase to prevent the model from forgetting minority classes while it is forming this initial semantic structure.\nSecondly, after the warm-up phase, the method transitions from knowledge distillation to neighborhood clustering for the main training. The paper adopts AaD (Yang et al. 2022) as the specific NC method. AaD optimizes an upper-bound of the neighborhood clustering objective, resulting in two simple terms: one that encourages similarity among nearest neighbors (Ci) and another that discourages similarity with other samples in the mini-batch (Bi). To efficiently retrieve nearest neighbors, AaD utilizes memory banks that store all target features and their predictions, which are updated with each mini-batch. Even during this NC phase, Selection Training (ST) is continuously combined with NC to ensure the stability of minority class clustering, as the semantic structure from the warmed-up model might still not be perfectly robust for these classes. This combined strategy allows the model to learn more balanced representations than traditional KD, further alleviating the minority class forgetting issue.": 1368,
    "To detect and correct confusing class pairs and clarify classification boundaries, the paper proposes a Confusing Pair Correction strategy. This strategy is applied after the initial target domain refinement. First, the method identifies \"\"hard classes\"\" (S_hc), which are classes where confusing pairs are more likely to appear. This is done using the small loss criterion: classes with an average cross-entropy loss (L(x_t^i)) above a certain threshold (ζ) are considered hard classes.\nFor every target sample, the model's top-2 predictions are collected, forming a prediction matrix (M_k). This matrix records the most confident (rank 1) prediction (ŷ_t^i1) and the second most confident (rank 2) prediction (ŷ_t^i2) for each sample. The frequency of a specific pair of labels (α, β) in M_k is then computed as f_α,β = P(y_α == M_k[i][1] ∧ y_β == M_k[i][2]) / P(y_α == M_k[i][1]). This frequency indicates the probability that class α tends to flip into class β. The most confusing pair (α*, β*) is identified as the pair (α, β) that maximizes f_α,β, with the additional condition that f_α,β > f_β,α (indicating a directional confusion from α to β) and both α and β must belong to the previously identified set of hard classes (S_hc).\nOnce the most confusing pair (α*, β*) is identified, label correction is applied. Specifically, for noisy samples whose pseudo-labels are α* (the class that tends to flip), their labels are corrected to β* (the class they are confused with). Noisy samples are typically selected based on the small loss criterion. After these label corrections, the prototypes and all pseudo-labels are recalculated, and the model is trained using a cross-entropy loss with these newly corrected pseudo-labels. This process aims to reduce the overlap between confusing classes and establish clearer classification boundaries.": 1369,
    "The paper proposes a cycle self-refinement learning mechanism where the domain-ensemble network and the source-specific networks mutually refine each other.\nFirst, the domain-ensemble network is refined using high-confidence pseudo-labels generated by the instance-level ensemble strategy (as described in Solution 1). Specifically, target samples with high-confidence pseudo-labels are selected. To further improve generalization and robustness, a consistency regularization technique, similar to Fixmatch, is applied using strong data augmentation. The pseudo-labeling loss (`Lpsd`) for the domain-ensemble network is calculated using JS divergence between the prediction of the original target sample and its strongly augmented version. A confidence-based weight (`γxt`) is applied to each pseudo-labeled target sample to reduce the impact of noisy labels, where `γxt` is higher for samples with higher prediction confidence. This process allows the domain-ensemble network to accumulate the dominant transferable knowledge from all source domains.\nSecond, the source-specific networks are guided and refined by the domain-ensemble network through conditional feature alignment. This alignment aims to make the knowledge learned by each source-specific network more discriminative and consistent with the aggregated knowledge from the domain-ensemble network. An auxiliary classifier (`h'_si`) is introduced for each source domain. The conditional feature alignment loss (`L_align`) is designed to ensure that samples with the same categories in the source domain and target domain are classified with similar predictions by the source domain classifier. This is achieved by minimizing the cross-entropy between the auxiliary classifier's output on target features (from the domain-ensemble network) and the pseudo-labels provided by the domain-ensemble network, while simultaneously maximizing the cross-entropy between the auxiliary classifier's output on source features (from the corresponding source-specific network) and the predictions provided by that source-specific network. This adversarial-like strategy encourages the source-specific networks to adapt their features and predictions to align with the more precise and discriminative pseudo-labels from the domain-ensemble network, especially for knowledge that might have been poorly adapted initially.\nThe total training procedure combines an initial classification loss (`Lcls`) for both types of networks, the pseudo-labeling loss (`Lpsd`) for the domain-ensemble network, and the conditional feature alignment loss (`Lalign`) for the source-specific networks. This entire process is performed in an end-to-end manner, with pseudo-labels being updated in each iteration, creating a continuous cycle of mutual refinement.": 1370,
    "To ensure marginal coverage for any target distribution within the defined set Pf,ρ, the paper proposes setting the threshold `t` for the prediction set `eC(x) := {y ∈ Y | s(x,y) ≤ t}` to the supremum of the (1-α)-quantiles over all distributions in Pf,ρ, denoted as `eQ(1-α; Pf,ρ)`. Theorem 6 provides a method to compute this worst-case quantile function. It states that `eQ(α; Pf,ρ)` can be expressed as `Q(g_f,ρ^(-1)(α); F_min)`, where `Q` is the standard quantile function. Here, `F_min(x)` is defined as the minimum of the cumulative distribution functions (c.d.f.s) of the scores from the source domains, i.e., `min_{1≤i≤d} F_i(x)` where `F_i` is the c.d.f. of `s#S_i`. The function `g_f,ρ(β)` is defined as `inf_{z∈[0,1]} { βf(z/β) + (1-β)f((1-z)/(1-β)) ≤ ρ }`, and `g_f,ρ^(-1)(τ)` is its inverse, `sup{β ∈ [0,1] | g_f,ρ(β) ≤ τ}`. For specific f-divergences like χ2-divergence and Total Variation distance, `g_f,ρ` and `g_f,ρ^(-1)` have closed-form solutions. For general f-divergences, `g_f,ρ^(-1)` can be efficiently computed using a binary search algorithm by solving a one-dimensional convex optimization problem.": 1371,
    "The paper employs an explicit knowledge transfer method by utilizing the Decomposed Network models, fine-tuned on source domains, to predict potential entities and their types for sentences in the target domain. For instance, if a source model (e.g., BERT trained on OntoNotes 5.0) identifies \"\"Java\"\" as a \"\"Product\"\" entity in a target domain sentence, this information is explicitly copied and appended to the end of the target sentence as auxiliary input (e.g., \"\"Java is a Product entity. [SEP] Microsoft is a Corporation entity.\"\"). This augmented input, containing potential named entities and their types suggested by various source domain models, is fed into the `fBERT` encoder of the target domain model. Although the target domain's tagging methods for entity mentions may differ, this explicit guidance provides the model with additional context and examples, aiding in the NER task for the limited target domain data. This auxiliary information is used as input for `fBERT` but not for inference by `fMD` and `fET`.": 1372,
    "The paper tackles the text-text domain gap by implementing the Hard Cross-domain Contrastive Learning Network (HC-net). This module is designed to learn isomorphism information between the source and target domains and align their cross-domain features. The core idea is that samples belonging to the same category should be close regardless of their domain. The HC-net operates by forming positive pairs between an anchor sample from one domain (e.g., target domain) and a sample from the other domain (e.g., source domain). Specifically, it identifies the \"\"nearest\"\" samples as positive pairs based on cosine similarity, using a similarity threshold (γ). If the cosine similarity between an anchor (e.g., from target domain Dt) and a sample from the source domain (Ds) exceeds γ, they are considered a positive pair. The hard cross-domain contrastive loss (Lt→s_HC and Ls→t_HC) is formulated to pull these cross-domain positive pairs closer while pushing other samples further apart. The overall hard cross-domain contrastive loss (LHC) is a weighted sum of Lt→s_HC and Ls→t_HC, controlled by a learnable parameter (α), ensuring bidirectional alignment and reducing the discrepancy between the source and target data distributions.": 1373,
    "To minimize manual intervention and enhance the model's perception of entity start and end positions, the paper proposes two novel prompt template construction strategies: word-based templates and example-based templates. The word-based template strategy involves arranging predefined label words (e.g., \"\"person\"\", \"\"location\"\") and including \"\"other\"\" as a label word for non-entity categories. The special tokens `[ENT START]` and `[ENT END]` are then inserted on both sides of each label word to provide clear separation within the template. This method requires manual selection of these label words. The example-based template strategy, conversely, requires no manual intervention for template design. It involves randomly selecting samples from the training set until these samples collectively contain mentions of various entity classes. These selected samples are then concatenated to form the template. Within this concatenated text, `[ENT START]` and `[ENT END]` tokens are inserted on both sides of each entity mention. While the example-based template eliminates manual design, the paper notes that it can become excessively long when there is a large number of entity classes, potentially causing the input to exceed the maximum encoding length of the PLM. Therefore, the paper defaults to using word-based templates in its experiments. Both strategies aim to provide the model with semantic or contextual information that improves its sensitivity to entity spans, particularly in few-shot settings.": 1374,
    "To effectively guide smaller language models (LMs) in performing accurate idiomatic translation, the paper proposes a method called KB-CoT prompting. This approach integrates external knowledge of figurative meanings retrieved from the IDIOMKB into the translation process. The procedure begins with idiom identification within the source sentence. The paper assumes that idiom detection is a solved problem and focuses on the translation task, using idiom-sentence pairs from source datasets. Once an idiom is identified, its corresponding multilingual figurative meaning is retrieved from the pre-constructed IDIOMKB. This retrieved meaning is then incorporated into the prompt provided to the smaller LM. The prompt structure starts with a short task description for machine translation, followed by the retrieved idiom meaning. This explicit inclusion of the figurative meaning acts as a \"\"Chain-of-Thought\"\" (CoT) transition, encouraging the LM to focus on comprehending the non-literal meaning of the idiom before attempting the full sentence translation. For example, when translating a Chinese sentence containing \"\"一气呵成,\"\" the prompt includes the phrase \"\"一气呵成' means 'to complete a task or work in one go, without stopping or taking a break'.\"\" This guidance helps smaller LMs like InstructGPT (6.7B) to produce more accurate idiomatic translations compared to direct prompting, which often leads to literal translation errors. The method contrasts with earlier MT systems that directly replace idiomatic expressions, instead using the figurative meaning as a contextual hint.": 1375,
    "To encourage the separation of unrobust features, the paper introduces an \"\"easy teacher\"\" model and a feature distillation task. First, \"\"easy samples\"\" are extracted from the training set. This is done by training an underfitting shallow model (e.g., DistilBERT or DistilRoBERTa) for a few epochs (e.g., 2 epochs) on all training samples. Samples are then ranked based on their prediction confidence (the largest value in the predicted probability distribution). The top 35% of samples with high confidence are considered easy samples, as they are easily fitted by shallow models and prone to overconfident predictions. These easy samples are then used to train a new \"\"teacher model\"\" until convergence, aiming for it to capture unrobust features inherent in these samples. Unlike typical distillation methods that transfer logits, this approach distills features. For each training sample, the teacher model produces an unrobust feature `˜h`. This `˜h` is fed through a simple linear transformation to obtain `˜z`. A whitening operation `ζ(.)` (a non-parametric layer normalization without scaling and bias) is applied to both the student's unrobust feature `zσ` and the teacher's `˜z` to make them comparable. Finally, a smooth L1 loss (`Ldistill`) is used as the loss function for feature distillation, compelling `zσ` to approximate `˜z`. This process encourages `zσ` to capture the unrobust characteristics learned by the teacher, thereby further separating it from the robust features `zµ`.": 1376,
    "The model addresses contextual inconsistency through a Multi-Semantic Cross-Attention Module, which integrates semantic features of idioms to enhance comprehension across diverse contexts. First, each idiom in the candidate set is encoded using a BERT model, with the idiom represented as a sequence `{[CLS],w1,w2,w3,w4,[SEP]}`. The hidden layer representation of `[CLS]` from the final layer (l-th layer) is extracted as the idiom representation, `hl_c`. These individual idiom representations from the candidate set are then concatenated to form a comprehensive candidate idiom representation, `hl_ct`. To leverage mutual semantic relationship information, `hl_ct` undergoes a group convolution operation (3x3 kernel, `fK3x3`) to produce a contextualized key, `hk`, which captures local features and idiom structure. Subsequently, this contextualized key `hk` is concatenated with the contextual dynamic representation `hl_pt` (from the Contextual Representation Learning Module). The concatenated vector is then processed by a 1xN convolution (`fα1xN`) and a 1x1 convolution (`fβ1x1`) to generate an attention matrix, `hA`. This `hA` is learned based on the dynamic relationship between the idiom's contextual representation and the representations of idioms in the candidate set. Finally, the candidate idiom representation `hl_ct` is aggregated using the attention matrix `hA` via a 1x1 convolution (`fV1x1`) and an addition fusion operation, resulting in the multi-semantic representation `hl_vt`. This process ensures that the model fully utilizes the semantic information among candidate idioms to guide semantic learning in different contexts.": 1377,
    "The Hash Distributed Beam Search 1 (HDBS1) algorithm addresses this by distributing states among multiple workers using a hash function and enforcing strict synchronization at each search layer. Each worker maintains its own open list (Oi) and a local set of generated states (Gi). A state is assigned to a specific worker (j) using a hash function, `HASH(S[[τ]])`, based on its non-resource variables. When a worker generates a successor state, it pushes the state's information (S, τ, gτ, fτ) to a message channel (Qj) dedicated to the assigned worker j. This `PUSH` operation is non-blocking. Workers alternately receive states from their incoming channel using a non-blocking `POP` operation and expand states from their local open list. After expanding all states in its current open list, a worker sends a special `NULL` message to all other workers to signal completion of its current layer. Each worker maintains a counter (`ci`) of how many other workers have sent this `NULL` message. Once `ci` equals the total number of workers (k), indicating all workers have finished the current layer, the worker proceeds to update its local open list (Oi) from its generated states (Gi). Worker 1 then aggregates information (best solution found, minimum f-value, open list sizes, completeness flags) from all workers, computes a global dual bound, and broadcasts this aggregated information back to all workers, ensuring all workers are synchronized before starting the next layer.": 1378,
    "The paper addresses the challenge of mapping the source model's output to target domain labels through an Output Mapping Layer (Output-Mapping). Two main approaches are described. The first is source-target label mapping, which involves specifying a many-to-one surjective mapping `h : YS → YT` from source class labels (`YS`) to target class labels (`YT`). For a target class `t ∈ YT`, its prediction probability `Prob(t|fS(exT))` is calculated as the average prediction probability of a subset `B ⊂ YS` of source labels that map to `t`, i.e., `(1/|B|) * Σs∈B Prob(s|fS(exT))`. This mapping can be pre-specified, learned via frequency-based greedy mapping, or iterative greedy mapping. The second approach is adding a trainable dense layer (linear head) with parameters `ω` between the source model's output (dimension `KS`) or its penultimate output and the target model's output (dimension `KT`). This layer directly learns the mapping from source model features to target labels.": 1379,
    "The method was tested on nine datasets for SARS-CoV-2 detection using complete blood count tests. Five different machine learning algorithms (KNN, Decision Tree, Random Forest, XGBoost, MLP) were employed to evaluate the robustness of cross-adaptation. The datasets were transformed using three domain adaptation algorithms (Kernel Mean Matching, Kullback-Leibler Importance Estimation Procedure, Transfer AdaBoost for Classification). The transformed datasets were used to train the models, and the performance was measured using the F1 score. Results showed an average improvement of 10 percentage points in the F1 score across all algorithms.": 1380,
    "The paper proposes an acceleration-based technique to estimate the total amount of resources required to complete the search, building upon a physics analogy. It first introduces a velocity-based technique, where the average velocity (vk0,k) of progress is calculated as the ratio of change in progress measure (hk - hk0) to change in resources (rk - rk0) over a window of observations from k0 to k. The total estimated resources (ˆr(k)) are then calculated by extrapolating this velocity: ˆr(k) = rk + (hm - hk) / vk0,k, where hm is the target progress (1 for completion). To account for non-linear progress, the acceleration-based technique captures the change in velocity. Given a window defined by start (k0), middle (k1), and end (k2) points, the acceleration (ak) is computed as 2 * (vk0,k2 - vk0,k1) / (rk2 - rk1). A corrected velocity (v) over the entire window is then derived: v = vk0,k1 - 0.5 * ak * (rk0 + rk1). Assuming this acceleration remains constant, the total estimated resources ˆr(k) are found by solving a quadratic displacement formula: 1 = hk + v * (ˆr(k) - rk) + ak * (ˆr(k) - rk)^2 / 2. The solution for ˆr(k) is the positive root of this quadratic equation. The resource measure (rk) used in experiments is the number of nodes explored (|Vk|).": 1381,
    "A classification network, denoted as F, is integrated directly into the CycleEmotionGAN framework, rather than being trained as a separate, post-adaptation step. This network F takes an adapted image x'S (which is GST(xS)) as input and assigns an emotion label y'S. The classifier F is optimized by minimizing a cross-entropy loss, Ltask(F), which is a standard approach for multi-class classification. Specifically, Ltask(F) is calculated as the expected value over source images and their labels (xS, yS) of the negative sum over all L emotion categories of 1[l=yS] * log(σ(F(l)(GST(xS)))), where σ is the softmax function and 1 is an indicator function. This means the classifier learns to predict the original source emotion label (yS) from the adapted source image (GST(xS)). The parameters of the classifier F (ϕ) are updated during the training process, alternating with the updates for the generators and discriminators. This direct integration allows the classifier to benefit from the ongoing adaptation process and ensures that the adapted images retain their original annotation information for effective emotion classification.": 1382,
    "To encourage the discovery of discriminative, low-entropy latent factors, an unsupervised factorisation loss is applied to the Common Factorised Space (CFS) layer. This loss is derived from an uncertainty measure based on the entropy of the CFS activations. Specifically, for each instance xi from the combined input X (both source and target data), its common factor representation is FC,i = ΨθC(xi). The unsupervised factorisation loss is defined as the negative sum of the element-wise inner product of FC,i and its logarithm: - Σi=1 to N < FC,i, log(FC,i) >. This loss biases the representation FC to contain more certain predictions, meaning values closer to 0 or 1 for each discovered latent factor. By minimizing this uncertainty measure, the model is encouraged to learn a near-binary representation, which serves to weakly align the domains by promoting a shared set of certain latent attributes, thereby facilitating knowledge transfer.": 1383,
    "To address the computational expense of pairwise distance calculations in the Instance-Based method, the paper introduces a Center-Based Discriminative Loss (LCd). This loss is inspired by Center Loss but extends it to also enforce inter-class separability. The LCd has two main components:\n1.  Intra-class compactness: It penalizes the squared Euclidean distance between each deep feature (hs_i) and its corresponding class center (cyi). The penalty is applied if this distance exceeds a margin m1, using max(0, ||hs_i - cyi||^2 - m1)^2.\n2.  Inter-class separability: It penalizes the squared Euclidean distance between different \"\"batch class centers\"\" (ci and cj) if this distance is less than a margin m2, using max(0, m2 - ||ci - cj||^2)^2. The \"\"batch class centers\"\" are computed by averaging the deep features of samples belonging to each class within the current mini-batch.\nA key aspect of this method is the update mechanism for the class centers. While \"\"batch class centers\"\" are used for inter-class separability within the current batch, the cyi used for intra-class compactness are \"\"global class centers\"\" that are updated iteratively. These global centers are initialized as batch class centers in the first iteration and then updated in each subsequent iteration using a moving average approach: ct+1_j = ct_j - γ * ∆ct_j, where ∆ct_j is the difference between the current global center and the average of features for class j in the current batch, weighted by the number of samples. This allows the method to consider global cluster information while training with mini-batches. A trade-off parameter β balances the contributions of these two components. This loss is also differentiable and integrated into the overall training objective, enabling efficient learning of discriminative features.": 1384,
    "To align higher-order moments of source and target feature distributions within the unilateral transformation framework, the proposed method utilizes the Maximum Mean Discrepancy (MMD). MMD is an effective non-parametric criterion that compares two distributions by embedding each into a Reproducing Kernel Hilbert Space (RKHS). The objective function for learning the transformation parameter A includes a term that minimizes the squared MMD between the target feature distribution (Pt) and the transformed source feature distribution (Ps_transformed). Specifically, this term is expressed as 1/2 * ||Ext∼Pt[k(·,xt)] - Exs∼Ps[k(·,(A + I)xs)]||^2_Hk. The choice of kernel k is crucial for moment alignment. The paper employs a polynomial kernel of degree d, defined as k(x,y) = (1 + x^T y)^d. When a polynomial kernel of degree d is used, minimizing the MMD ensures that the two distributions, Pt and Ps_transformed, become similar up to their d-th moments. This allows for aligning higher-order statistics beyond just the mean or covariance, offering a more comprehensive alignment than methods that only align second-order moments. The parameter A is estimated by optimizing this objective function using gradient-based methods, for which the gradient of the kernel with respect to A is derived.": 1385,
    "For binary classification with the 0-1 loss, the paper proposes an efficient three-step algorithm (Algorithm 1) for estimating S-disc, $\\varsigma_{\\ell01}^{\\mathcal{H}}(\\hat{P}_T, \\hat{P}_S)$. This estimation is reduced to minimizing a cost-sensitive classification problem. The algorithm proceeds as follows:\n1.  Source learning: A classifier, denoted as $\\hat{h}_S$, is learned using the labeled source data $S$. This $\\hat{h}_S$ serves as an empirical approximation of the true source risk minimizer $h_S^*$.\n2.  Pseudo labeling: Pseudo-labeled datasets, $\\tilde{S}$ and $\\tilde{T}$, are created. For the source domain, $\\tilde{S}$ consists of pairs $(x, \\text{sign}(\\hat{h}_S(x)))$ for $x \\in S$. For the target domain, $\\tilde{T}$ consists of pairs $(x, -\\text{sign}(\\hat{h}_S(x)))$ for $x \\in T$. This step effectively transforms the S-disc estimation into a cost-sensitive classification problem where the goal is to distinguish between samples that agree with $\\hat{h}_S$ and those that disagree.\n3.  Cost-sensitive learning from pseudo labeled data: Another classifier, $h''$, is learned by minimizing a surrogate cost-sensitive risk, $J_{\\ell_{sur}}(h)$, over the pseudo-labeled data $\\tilde{S}$ and $\\tilde{T}$. The objective function $J_{\\ell}(h)$ is defined as $\\frac{1}{n_S} \\sum_{j=1}^{n_S} \\ell(h(x_j^S), h_S^*(x_j^S)) + \\frac{1}{n_T} \\sum_{i=1}^{n_T} \\ell(h(x_i^T), -h_S^*(x_i^T))$. In practice, $h_S^*$ is replaced by $\\hat{h}_S$, and a surrogate loss like the hinge loss ($\\ell_{hinge}(y, y') = \\max(0, 1 - yy')$) is used for training $h''$. The final S-disc estimate is then computed as $1 - J_{\\ell01}(h'')$, where the 0-1 loss is used for the final calculation. The computational complexity for hinge loss minimization using the sequential minimal optimization (SMO) algorithm is $O((n_T + n_S)^3)$, which is significantly more efficient than X-disc.": 1386,
    "The Coarse2Fine (C2F) Attention module is introduced to address the granularity inconsistency. For the source task, which deals with coarse-grained aspect categories lacking detailed position information, the C2F module helps capture more specific semantics and position information from the context. It achieves this by using an auxiliary pseudo-label prediction task. The source aspect representation (`ha_s`) is used to attend to the context, and the induced attention scores aggregate context information to predict the category of the source aspect itself (pseudo-label `yc`). This mechanism, calculated as `zf_i = (uf)Ttanh(Wf[hi;ha_s] + bf)` and `beta_f_i = exp(zf_i) / sum(exp(zf_i'))`, generates `va = sum(beta_f_i * hi)`. This `va` is then fed to a softmax layer for the auxiliary task prediction, minimizing `Laux`. To handle cases where no corresponding aspect term explicitly exists, a fusion gate `F = sigmoid(W[va;ha_s]+b)` is adopted. This gate adaptively controls the proportions of `ha_s` and `va` to form a more specific source aspect representation `ra_s = F * ha_s + (1 - F) * W'va`, effectively reducing the aspect granularity gap and facilitating subsequent feature alignment.": 1387,
    "The transferable curriculum is integrated into a deep domain adaptation framework by applying the binary weighting scheme `w(x_s^i)` to the loss functions of both the label classifier `G_y` and the domain discriminator `G_d`. The objective for the label classifier `G_y` (Equation 3) is modified to `E_Gy = (1/n_s) * sum(w(x_s^i) * L_y(y_s^i, G_y(G_f(x_s^i)))) + (1/n_t) * sum(H_y(G_y(G_f(x_t^j))))`. Here, `L_y` is the cross-entropy loss for source examples, weighted by `w(x_s^i)`, and `H_y` is the entropy loss for unlabeled target examples, which quantifies the uncertainty of label predictions and promotes confident predictions. The objective for the domain discriminator `G_d` (Equation 4) is `E_Gd = -(1/n_s) * sum(w(x_s^i) * log(G_d(G_f(x_s^i)))) - (1/n_t) * sum(log(1 - G_d(G_f(x_t^j))))`. This means that only the source samples indicated as noiseless and transferable by `w(x_s^i)` contribute to the domain adversarial loss for the source domain. This selective alignment prevents the domain discriminator from matching the target domain with noisy source samples, thereby mitigating negative transfer. The overall architecture (Figure 2) includes a feature extractor `G_f`, a label classifier `G_y`, and a domain discriminator `G_d`. A Gradient Reversal Layer (GRL) is used for domain adversarial training. The curriculum `w` acts as a binary weighting scheme, indicating the selection and timing of each example into training.": 1388,
    "To improve the generalization capability of the Target-Specific Network (TSN) and encourage tighter clusters of target data points, the paper introduces a Clustering Regularization term, `Lcluster(θT,C;Xt)`, into the TSN's loss function `LT`. This term aims to minimize the distance between the latent feature representations `fθT(xt)` of target instances `xt` and their corresponding accumulated class centers `ck`. The class centers `C = [c1, c2, ..., cK]` are dynamically updated in each mini-batch based on the predictions of the Adaptation Network (AN). Specifically, for each target instance `xt` in a mini-batch `Bt`, if the AN predicts it belongs to class `k` (denoted by `ŷAt = k`), then `xt` contributes to updating the center `ck`. The update rule for `ck` is `ck ← αck + (1-α)/mk * Σ δ(ŷAt,k)fθT(xt)`, where `α` is a momentum term, `mk` is the number of target instances predicted as class `k` in the mini-batch, and `δ(ŷAt,k)` is an indicator function (1 if `ŷAt = k`, else 0). Additionally, an orthogonality constraint `µ||CTC - I||F^2` is included in `Lcluster` to ensure that the class centers `C` remain dissimilar from each other, where `I` is the identity matrix and `||·||F^2` is the squared Frobenius norm. This regularization forces target data points to converge towards their respective class centers, thereby forming more discriminative and compact clusters in the latent feature space.": 1389,
    "An attention module is integrated into both the student and teacher networks to generate attention-aware features. The architecture of this module is defined by a function A(x) = T(U(D(F(x)))), where F(x) is the output of the backbone networks for an input x. D represents a down-sample operation using 2x2 average pooling, U represents an up-sample operation using bilinear interpolation, and T represents a nonlinear transformation consisting of a 1x1 convolutional layer followed by a sigmoid activation function. The output of the attention module, H(x), is then formulated as H(x) = (1 + A(x)) * F(x), where * denotes element-wise product. Since the attention map A(x) ranges from [0,1], it acts as a control gate, allowing features in F(x) to be heightened in some positions and restrained in others. This property is leveraged to assist the computation of the consistency loss (Lcon) in the target domain. Specifically, the consistency loss incorporates an attention mask matrix M (M ∈ ZH×W). This mask is defined such that M(u,v) = 1 if the attention activation from the teacher network's attention map (AT(u,v)) is greater than a predefined attention threshold (τatt), and M(u,v) = 0 otherwise. By applying this mask, only those regions in the target domain image that exhibit higher attention activation (indicating potentially higher domain gap or noteworthy features) are allowed to participate in the calculation of the consistency loss. This mechanism ensures that the model focuses its adaptation efforts on the most critical regions, thereby improving the overall performance of the domain adaptation.": 1390,
    "To identify and utilize domain-shared features, IATN incorporates a Domain Classifier. This classifier takes the sentence representation (Sr) as input and aims to predict the domain label (source or target). Crucially, a Gradient Reversal Layer (GRL) is inserted before the domain classifier. The GRL acts as a \"\"pseudo-function\"\" that passes the input (Sr) unchanged during the forward pass but reverses the sign of the gradient during the backpropagation pass. This adversarial training strategy ensures that the S-net, which generates Sr, is optimized to produce features that are *indistinguishable* by the domain classifier, thereby forcing it to learn domain-invariant or domain-shared feature representations. These domain-shared features from the sentence network (specifically the sentence pooling vector hp_s) then serve as a \"\"bridge\"\" for the aspect network (A-net). As described in Solution 1, hp_s is used in the calculation of aspect attention (beta_i) through a spot multiplication, allowing the domain-invariant sentiment tendencies captured in hp_s to guide the extraction of information from aspects, making the aspect representations more robust across domains.": 1391,
    "The paper addresses this through its \"\"Domain-speciﬁc Classiﬁer Alignment\"\" stage, which is the second alignment stage in the MFSAN framework. After learning multiple domain-invariant representations and training \"\"Domain-speciﬁc classiﬁers\"\" (Cj) for each source, the problem arises that these classifiers, trained on different source domains, might disagree on predictions for target samples, particularly those near class boundaries. To resolve this, the framework minimizes the discrepancy among all classifiers' probabilistic outputs for target domain data. This is achieved by introducing a \"\"disc loss\"\" (Ldisc), which is calculated as the average absolute difference between the probabilistic outputs of all pairs of classifiers (Ci and Cj) for target samples (xk). Formally, Ldisc = (2 / (N * (N - 1))) * Σj=1 to N-1 Σi=j+1 to N Ex∼Xt[|Ci(Hi(F(xk))) - Cj(Hj(F(xk)))|]. By minimizing this loss, the probabilistic outputs of all classifiers are encouraged to be similar, leading to more consistent predictions. For final prediction, the average of all classifier outputs is computed.": 1392,
    "To overcome the \"\"paradox of transitivity\"\" inherent in direct dot product operations for semantic compatibility, the paper introduces a Semantic Compatibility Evaluation Layer. This layer processes the context representation before it is used to measure compatibility with a target word. Specifically, the context representation (vc) is fed into a multilayer network of perceptrons, denoted as L. This network applies a ReLu (Rectified Linear Unit) non-linearity activation function: L(vc) = f2(relu(f1(vc))), where f1 and f2 represent fully connected layers. The output of this multilayer perceptron, L(vc), is then used in conjunction with the target word's embedding (v'w) to calculate the semantic compatibility score. The final compatibility score is computed using a sigmoid function: sigma(v'w * L(vc) + bu), where 'bu' is a bias term tuned on a development dataset. This multilayer perceptron network L relaxes the contextual similarity constraints that a direct dot product would impose. It allows the model to map context representations that might be originally different in the embedding space to similar embeddings that are close to the target word, thereby avoiding the transitive property issue where compatibility of a word with two different contexts would incorrectly imply similarity between those contexts.": 1393,
    "This is addressed by the Gated Modality-mixing Network. This component computes the nonverbal shift vector by learning a non-linear combination of the visual and acoustic embeddings using an attention gating mechanism. For a given word L(i) with its original word representation e(i), the visual embedding h(i)v, and the acoustic embedding h(i)a, the model first calculates modality-specific influence gates. The visual gate w(i)v is computed as σ(Whv[h(i)v;e(i)] + bv), and the acoustic gate w(i)a is computed as σ(Wha[h(i)a;e(i)] + ba), where [;] denotes vector concatenation, Whv and Wha are weight vectors, bv and ba are scalar biases, and σ is the sigmoid function. These gates dynamically control the intensity of influence from each modality. Finally, the nonverbal shift vector h(i)m is calculated by fusing the gated visual and acoustic embeddings: h(i)m = w(i)v ⋅ (Wvh(i)v) + w(i)a ⋅ (Wah(i)a) + b(i)h, where Wv and Wa are weight matrices and b(i)h is a bias vector. This mechanism allows the model to adaptively weigh the visual and acoustic information based on the specific word and its context.": 1394,
    "Two model-agnostic metrics are designed to dynamically assign instance weights:\n1. Prediction Loss: The first metric uses the prediction loss f(y, gθ(x)) of an instance as its weight. The weight w is calculated as (1/τ) * (-yT log p(y|x) + ε), where p(y|x) is the prediction probability vector, ε is a smoothness constant to prevent zero weights, and τ is a normalization constant ensuring the average weight in a mini-batch is 1. This metric is effective in early fine-tuning epochs, assigning larger weights to instances with high prediction loss, indicating they are not well-fitted by the pre-trained classifier and likely contain more target-specific knowledge.\n2. Variance of Historical Prediction Losses: The second metric uses the variance of an instance's historical prediction losses. At epoch t, the weight w is calculated as (1/τ) * (std(ht-1) + ε), where ht-1 is a vector containing historical prediction losses f up to the previous epoch, and std(ht-1) is the estimated standard deviation of these historical losses, including a confidence interval term sqrt(ς(ht-1) + ς^2(ht-1) / (|ht-1| - 1)). This metric is more effective in later epochs. Instances with large prediction losses and large variance indicate either persistent difficulty in learning target-specific knowledge or potential overfitting, prompting the model to assign them higher weights to balance learning and preserve shared knowledge.": 1395,
    "The framework structures the Generative Adversarial Network (GAN) by decomposing the generator into two parts: the shared sequence encoder `fenc` and a domain-specific RNN generator, `fTgen` for the target domain or `fSgen` for the source domain. For instance, to generate synthetic sequences `ˆxT` in the target domain from a source sequence `xS`, the model uses `GS→T = fTgen(fenc(xS,θS))`. Similarly, for inverse mapping, `GT→S = fSgen(fenc(xT,θT))`. This decomposition encourages the encoder `fenc` to learn domain-invariant features (`z`) because it is shared by both `GS→T` and `GT→S`. The GAN generator `GS→T` is trained against a target domain discriminator `DT`, which learns to distinguish generated data `ˆxT` from real target data `xT`. This adversarial training, combined with the shared encoder, forces the latent representation `z` to be truly domain-invariant, enabling effective transformation between domains without paired data.": 1396,
    "To achieve accurate distribution matching between the source and target domains, the paper proposes an Adversarial Kernel Embedding Training approach. Instead of using a predefined or linear kernel for Maximum Mean Discrepancy (MMD) measurement, a neural network `fφe` is introduced to parameterize the measure function `f` within the RKHS. This allows for learning a suitable function that maps the latent features `H` (obtained from the encoder `W`) into a space where MMD can be more effectively measured. The MMD objective is formulated as a maximization problem for `fφe`, aiming to distinguish the empirical mean embeddings of the source (`HS`) and target (`HT`) latent features. This maximization is then minimized by the encoder `W`, creating a minimax game similar to Generative Adversarial Networks (GANs). To ensure the kernel embedding is effective, two conditions are addressed: boundedness and injectivity. Boundedness is enforced by imposing a locally Lipschitz constraint on `fφe` through weight clipping. Injectivity is approximated by introducing another neural network `fφd` to parameterize the inverse function `f^-1`, minimizing `||H - fφd(fφe(H))||F^2`. The final distribution divergence loss term `Ld(W,fφe,fφd)` combines the MMD maximization with this injectivity regularization. This adversarial training process allows `fφe` to learn to distinguish distributions, while `W` learns to produce domain-invariant latent features that fool `fφe`.": 1397,
    "Optimal hashing projections for the target domain (Wt) and source domain (Ws) are learned using an alternating optimization method. This approach ensures that the learning of one domain's projection is guided by the other, and vice-versa, gradually reducing the domain disparity. For both Wt-Step and Ws-Step, the optimization involves solving a subproblem that includes the weighted discrepancy term (from Solution 1), a quantization loss term (e.g., ||Bt - Wt^T Xt||^2), and an orthogonality constraint (Wt^T Wt = I and Ws^T Ws = I). Since the sgn(·) function is non-convex, it is relaxed to its signed magnitude (x). To handle the orthogonality constraints, the method applies a Crank Nicolson-like scheme, inspired by (Wen and Yin 2013). This scheme updates the orthogonal matrix W (either Wt or Ws) iteratively using its partial derivative (G) with respect to the objective function and a skew-symmetric matrix Q (e.g., Qt = Wt^T Wt). Specifically, W(k+1) = W(k) * Q(k+1), where Q(k+1) is derived from (I + (tau/2)Q(k))^(-1) * (I - (tau/2)Q(k)). The Barzilai-Borwein (BB) method is used for iterative updates. The binary codes (Bt and Bs) are directly obtained by applying the sgn function to the respective projected data (Wt^T Xt and Ws^T Xs). The weight matrix M is updated in each iteration based on the current difference between Wt and Ws using the defined sigmoid weight function.": 1398,
    "The paper introduces an \"\"Attribute Verification Loss\"\" (Lver) specifically designed to enforce disentanglement and preserve image-level style diversity. This loss operates on pairs of latent units extracted from different stages of the encoding-decoding process. For instance, given two input images xa and xb, which are encoded to ea and eb respectively, and then processed through a swap and decode step to produce ˜xa and ˜xb, which are then re-encoded to ˜e'a and ˜e'b. The Lver computes channel-wise absolute differences between paired latent units (e.g., ai from ea and ˜a'i from ˜e'b). A fully-connected layer then predicts a verification label for each unit pair. The loss optimizes a binary classification objective: a label of 1 is assigned if the two units in a pair (e.g., ai and ˜a'i) are expected to originate from the same input image (i.e., they represent the same attribute from the same original image instance), and 0 otherwise (e.g., ai from xa and ˜b'i from xb for a swapped attribute). This mechanism pushes latent units representing the same attribute style from different images apart, even if they share the same semantic label, while pulling units from the same image closer. This ensures that the embeddings capture not just the attribute label but also the unique style variations within that attribute, thereby promoting disentanglement and preserving intra-attribute diversity.": 1399,
    "A differentiable Mixed Integer Program (MIP) layer, termed MIPaaL (Mixed Integer Program as a Layer), is integrated into an end-to-end learning pipeline to directly optimize decision quality. First, a neural network, parameterized by `θ`, acts as a predictive model `fθ`. It takes feature embeddings `φ` as input and outputs estimated objective coefficients `ˆc` (i.e., `ˆc := fθ(φ)`). These estimated coefficients `ˆc` are then fed as input to the MIPaaL layer. The MIPaaL layer, utilizing its cutting plane solver mechanism, computes the optimal decision `ˆx` for the estimated MIP problem (i.e., `ˆx := x∗(ˆc;A,b,I)`). This `ˆx` represents the predicted optimal decision based on the neural network's output. The loss function for training the entire pipeline is defined as the solution quality of the predicted optimal decision `ˆx` with respect to the *ground truth* objective coefficients `c` (i.e., `loss(ˆc,c) := cT ˆx`). This directly measures the quality of the decision, not just the accuracy of the coefficient prediction. Since the neural network's prediction (`fθ(φ)`) and the dot product in the loss function (`cT ˆx`) are differentiable, and the MIPaaL layer itself is made differentiable, the entire pipeline can be trained using standard backpropagation. Gradients are passed back through the MIPaaL layer (via the KKT conditions of its continuous surrogate) to update the parameters `θ` of the predictive neural network, thereby optimizing `fθ` to produce `ˆc` values that lead to higher quality decisions.": 1400,
    "The paper proposes the 1/4-Envy-Free Algorithm for Uniform Single-Interval Valuations. This algorithm first orders agents based on the length of their desired intervals. For each agent, the algorithm checks if an interval of value 1/4 containing the midpoint of the desired interval is available. If such an interval exists, it is allocated; otherwise, the largest available interval of value at most 1/4 is chosen. The algorithm ensures that the envy between any two agents is at most 1/4 and runs in polynomial time.": 1401,
    "AlignFlow exploits the inherent invertibility of its component mappings to guarantee exact cycle consistency. The framework defines cross-domain translation mappings as compositions of invertible functions through a shared latent space. For example, the mapping from domain A to domain B is GA→B = GZ→B ◦ GA→Z, where GA→Z is the inverse of GZ→A. The paper demonstrates that the inverse of GA→B is precisely GB→A. Specifically, G−1 A→B = (GZ→B ◦ GA→Z)−1 = G−1 A→Z ◦ G−1 Z→B = GZ→A ◦ GB→Z = GB→A. This mathematical property means that translating a data point from A to B using GA→B and then translating it back from B to A using GB→A will deterministically yield the original data point. That is, GB→A(GA→B(a)) = a for any a in domain A, and similarly GA→B(GB→A(b)) = b for any b in domain B. This exact cycle consistency is a direct consequence of the invertible design and the shared latent space, eliminating the need for additional cycle-consistency loss terms (e.g., L1 or L2 norms) that are typically used in models like CycleGAN to approximately enforce this property.": 1402,
    "Langevin dynamics is implemented using the Metropolis-adjusted Langevin Algorithm (MALA), which drives fringe examples towards high-density regions of the data distribution. By setting the step size (α) and perturbation variance (δ²) such that α > δ²/2, the algorithm approximates the continuous Langevin dynamics, effectively cooling down the samples. This process involves iteratively nudging the fringe examples towards the data manifold, ensuring that the resulting samples are similar to the original input but significantly improve the mapping to the target domain.": 1403,
    "To explicitly enhance joint distribution alignment for unlabeled target instances and alleviate mode collapse, DADA introduces a target discriminative adversarial loss, LtF(G,F) and LtG(G,F), based on the integrated classifier F(·). This loss uses conditional category probabilities to weight the domain predictions for target instances. Specifically, for a target instance xt, the domain prediction vector ˆpk(xt) for the kth category is defined as:\nˆpk(xt) = pk(xt) / (pk(xt) + pK+1(xt)) for k'=k, K+1, and 0 otherwise.\nThe target discriminative adversarial loss for F(·) is:\nLtF(G,F) = - (1/nt) * sum_{j=1 to nt} sum_{k=1 to K} [ ¯pk(xt_j) * log(ˆpk(xt_j)) ]\nAnd for G(·) it is:\nLtG(G,F) = (1/nt) * sum_{j=1 to nt} sum_{k=1 to K} [ ¯pk(xt_j) * log(1 - ˆpk(xt_j)) ]\nwhere ¯pk(xt_j) is the kth element of the conditional probability vector ¯p(xt_j) for target instance xt_j, derived from F(G(xt_j)). This formulation encourages an explicit interplay between category and domain predictions for target instances, similar to the source loss, by aligning each instance to several most related categories. Additionally, to learn more target-discriminative features and prevent trivial solutions (e.g., assigning all instances to the same category), DADA incorporates an entropy minimization principle. The entropy minimization loss, Lt_em(G,F), is applied to the conditional probability vector ¯p(xt) of target instances:\nLt_em(G,F) = (1/nt) * sum_{j=1 to nt} H(¯p(xt_j))\nwhere H(·) computes the entropy. This loss serves as a regularizer for F(·) during minimization and helps G(·) learn discriminative features during maximization, mitigating the negative effects of adversarial feature adaptation. The overall minimax problem combines these losses:\nmin F [ λ(Ls + LtF) - Lt_em ]\nmax G [ λ(Ls + LtG) - Lt_em ]\nThis combined objective ensures that the domain classifier explicitly understands classification boundaries, reducing false alignments between different categories, and by deceiving such a strong domain classifier, the feature extractor learns better aligned features.": 1404,
    "The paper proposes an Iterative Learning with Selective Pseudo-Labeling (SPL) strategy. The process begins with an initial projection matrix (P0) learned using only source data. In each iteration (k), a subset of pseudo-labeled target data (Sk) is selected from the full set of pseudo-labeled target samples (Dt). The selection is progressive: knt/T target samples are chosen in the k-th iteration, where T is the total number of iterations. To prevent bias towards \"\"easy\"\" classes, a class-wise selection strategy is employed. For each class (c), the method identifies target samples pseudo-labeled as class c (nc t) and then selects the top knc t/T samples with the highest pseudo-label probabilities from that specific class to form Sk. This ensures that samples from all classes have an opportunity to be included in the learning process. The selected subset Sk, along with the labeled source data (Ds), is then used to learn an updated projection matrix (Pk) via Supervised Locality Preserving Projection (SLPP). This updated Pk is then used to re-calculate projections and update pseudo-labels for all target data in the next iteration. This iterative refinement of both the projection and pseudo-labels, coupled with the progressive and class-wise selection, allows the model to gradually improve accuracy while minimizing the negative impact of initial mislabeled samples.": 1405,
    "To alleviate the decoder's bias towards the source domain, the paper introduces the Weight Transfer Module (WTM). This module addresses the issue where the original decoder is trained by both segmentation loss (only from source) and domain classification loss, leading to source-specific details being captured. WTM transfers the weights of the original decoders (WDec1 and WDec2) to new decoders (DecT1 and DecT2) using a simple convolution layer (WTM1 and WTM2) initialized as a unit mapping. This preserves knowledge learned from synthetic annotations. Crucially, these new transferred decoders (DecT1 and DecT2) are then trained *only* by adversarial loss, using a dedicated third domain discriminator, Dwtm. This ensures that the new decoders focus exclusively on reducing domain divergence, making the two domains symmetrical and promoting the prediction of more domain-invariant outputs. For testing, the high-level transferred decoder (DecT2) and the semantic encoder (Enc) are used together.": 1406,
    "The paper achieves effective separation and extraction of shared and private features through an elaborately designed adversarial training mechanism involving a shared feature extractor (Es), K domain-specific private feature extractors (Epj), and a discriminator (D). The shared feature extractor Es is trained to produce domain-invariant features (zs) by confusing the discriminator D. Its objective function includes an adversarial loss term that encourages Es to strengthen its feature extraction ability to make its output indistinguishable by D. Conversely, each private feature extractor Epj is designed to retain domain-informative features (zpj) specific to its respective source domain. The discriminator D's role is to discriminate between features from different domains, whether they are shared or private. By forcing Es to confuse D and Epj to be discriminable by D, the framework encourages the shared features to become domain-invariant and the private features to contain domain-specific information. This is achieved without explicit orthogonality constraints, unlike some prior methods. The final sentiment polarity classification is then performed by a classifier (C) that concatenates both the shared features from Es and the private features from Epj, ensuring that both domain-invariant and domain-specific knowledge contribute to the prediction.": 1407,
    "The paper adapts deep mutual learning (DML) for unsupervised cross-domain sentiment classification by introducing label probers (P) that mediate the knowledge transfer between the two learning groups. Unlike standard mutual learning (sML) where classifiers directly align their outputs (e.g., DKL(C2(di) || C1(di))), the proposed approach uses the label prober in each group to learn from the *peer* group's classifier. Specifically, the mutual learning loss (LML) is defined as DKL(C2(di) || P1(di)) for group 1 and DKL(C1(di) || P2(di)) for group 2. This design ensures that the sentiment classifiers (C1, C2) are primarily optimized by the supervised classification loss (LCLS) on the source domain and are *not* directly forced to align with the potentially unreliable pseudo-labels generated by their peer classifiers in the unsupervised target domain. Instead, the label probers (P1, P2) are responsible for mimicking the peer classifier's predictions and then guiding their *own* group's feature extractor. This indirect mechanism frees the classifiers from the burden of learning from potentially weak or misleading pseudo-labels, thereby preventing performance degradation and making the mutual learning process more robust in a fully unsupervised cross-domain scenario.": 1408,
    "To address the inaccuracy of pseudo labels for unlabeled target data, the paper proposes a novel soft-label based cross entropy loss. First, for the unlabeled target dataset Dt, feature vectors {fsh x}x∈Dt are extracted using the pre-trained model's domain-shared branch. Then, the k-reciprocal encoding is applied to calculate a distance matrix between samples, and the DBSCAN clustering method is used to segment the target training data into K groups, {Ck}K k=1. Unlike methods that assign a definite pseudo label ˜yt i to each sample xt i based on its cluster Ck, this approach treats pseudo labels as soft constraints. A soft label for each sample xt i is represented by K weights {wi,k}K k=1, which measure the relationship between the sample and each of the K groups. Specifically, the weight wi,k is defined as a descending function of the distance between xt i and the center of group Ck. For the group Ck that xt i belongs to (i.e., k = ˜yt i), the weight is (epsilon + 1) / (K * ||fsh xt i - Ck||^2 + 2), and for other groups (k != ˜yt i), it is 1 / (K * ||fsh xt i - Ck||^2 + 2). Here, epsilon is a hyperparameter (set to 0.9) that ensures a minimum weight for the assigned cluster and encourages its weight. This soft label distribution is then used in a cross-entropy loss function, Lcrot = - sum(wi,k * log(pid(k|xt i))) over k=1 to K, where pid(k|xt i) are probabilities output by a 2048 x K FC layer and softmax function on fsh x. This approach allows the model to have less confidence in approximated pseudo labels and mine potential associations between different groups, mitigating overfitting.": 1409,
    "The paper introduces \"\"image-translation-structure consistency\"\" to actively guide the encoder-generator network to extract structure-aware features and preserve object structure during translation. This is achieved by integrating parsing net encoders (Ep x, Ep y) and parsing nets (Px, Py) into the network, which perform a multi-class segmentation subtask. For the forward cycle, the translated image ¯y (generated as Gx(Ex(x,z),z)) is fed into the parsing net Py (which uses Ep y) to predict a segmentation mask ˆypred. A multi-class cross-entropy loss, Lseg1(Ex,Gx,Ep y,Py,X, ˆX), is then applied between this predicted segmentation mask ˆypred and the ground-truth segmentation mask ˆx of the original source image x. This loss ensures that the structure of the translated image ¯y remains consistent with the structure of the original image x. Similarly, for the backward cycle, Lseg2(Ey,Gy,Ep x,Px,Y, ˆY) is formulated, applying the multi-class cross-entropy loss between the predicted segmentation mask of the reconstructed image and the ground-truth segmentation mask of the original target image. By explicitly training the network to perform segmentation on the translated images and penalizing deviations from the source image's structure, the model is compelled to maintain object preservation, preventing artifacts or loss of object details that could harm detector training.": 1410,
    "The Multimodal Context Network is designed to learn a comprehensive multimodal representation of the context. It takes the outputs `H` from the Unimodal Context Network as input. To capture complex asynchronous spatio-temporal relations across different modalities and sentences within the context, the representations `hm,n` (for all modalities `M` of the `nth` context sentence) are concatenated. This concatenated input is then fed into an encoder module, specifically a Transformer-based encoder with 6 intermediate layers. The self-attention mechanism within this encoder allows the model to identify and weigh various temporal relations between its inputs. The output of this Multimodal Context Network is `ˆH`, which is a spatio-temporal multimodal representation of the entire context.": 1411,
    "Calibrated confidence scores are employed as a metric by first training a model on the source domain dataset (Ds). For any given input, this model produces a logits vector, denoted `z`, from its final layer. The initial, non-calibrated confidence score (`q`) for the predicted class label is derived as the maximum probability from the softmax of `z`. To address the known issue of mis-calibrated neural network confidence scores, temperature scaling is applied. This involves introducing a learnable scalar \"\"temperature\"\" parameter (`T`). The calibrated confidence score (`q_hat`) is then calculated as the maximum probability from the softmax of `z` divided by `T`. The optimal `T` is learned by minimizing the negative log-likelihood loss over a validation set from the source domain (Dval_s). Once `T` is determined, the CONF_CALIB metric is computed as the difference between the average calibrated probability scores for the predicted class on the source domain and the average calibrated probability scores for the predicted class on the target domain. This difference quantifies the shift in model certainty between the two domains.": 1412,
    "The paper integrates rich contextual information by using pre-trained word embeddings and fine-tuning a large pre-trained language model. For the recurrent neural architecture, the system receives a tokenized sentence as input and maps it to word embeddings by concatenating representations from pre-trained GloVe (Global Vectors for Word Representation) and ELMo (Embeddings from Language Models) models. GloVe provides static word embeddings, while ELMo provides deep contextualized word representations that vary depending on the word's context. These concatenated embeddings are then passed through a Bi-LSTM (Bidirectional Long Short-Term Memory) to build task- and context-dependent representations for each word. For token labeling (word-level metaphor identification), the hidden states from each direction of the Bi-LSTM are concatenated and passed through a feed-forward layer, followed by a sigmoid activation. For sentence-level score prediction (metaphoricity or emotion), the concatenated Bi-LSTM hidden states are passed through an attention function, which includes a linear layer and softmax normalization, to construct a sentence representation. This resulting vector is then passed through another feed-forward layer and used to predict a sentence-level score with sigmoid activation.\nAdditionally, the paper experiments with fine-tuning a pre-trained BERT (Bidirectional Encoder Representations from Transformers) architecture. The inputs to this network consist of BERT-specific word and position embeddings. For the word-level sequence labeling task, the outputs of BERT's last Transformer layer are fed directly to a classification layer. For sentence-level tasks, an additional attention module is used to construct sentence representations from BERT's outputs. This leverages BERT's deep bidirectional context understanding gained during its pre-training phase.": 1413,
    "The paper leverages synthetic question-answer pairs through a dedicated Question Generation (QG) module. First, the QG model is trained on the source domain's labeled data to learn the conditional probability of generating a question given a passage and its answer. For the unlabeled passages in the target domain, an NER (Named Entity Recognition) system is employed to extract potential answer spans. The pre-trained QG model then uses these target passages and extracted answer spans to generate pseudo questions, forming a pseudo-labeled dataset for the target domain (Tgen). Crucially, unlike prior methods that might directly fine-tune the MRC model's answer decoder with this synthetic data, AdaMRC uses these pseudo question-answer pairs primarily as input for the MRC encoder. The feature representations derived from these synthetic target domain data, alongside those from the real source domain data, are then fed into the Domain Classifier. This strategic use allows the Domain Classifier to learn to differentiate between source and target domain features, enabling the adversarial training process to enforce domain invariance. By not directly using the potentially noisy synthetic data for the MRC decoder's training, the framework mitigates the risk of performance degradation on the answer prediction task while still benefiting from the target domain's structural information for domain adaptation.": 1414,
    "When source domain style labels are known and align with target domain styles (lS = lT), the proposed Domain Adaptive Style Transfer (DAST) model is used to achieve adaptive, domain-aware style transfer. This model extends the shared encoder-decoder framework by introducing learnable domain vectors and domain-specific style classifiers. Two distinct domain vectors, dS for the source domain and dT for the target domain, are incorporated. The auto-encoding loss (LS,T_ae) is reformulated to condition the decoder's reconstruction on these domain vectors, alongside the content representation and style label (e.g., log pD(xi|ci,dT,li) for target). These domain vectors implicitly guide the decoder to generate sentences that possess the specific characteristics of their respective domains. To explicitly enforce precise stylized information within each domain, two separate domain-specific style classifiers, CS for the source and CT for the target, are employed. These classifiers are pre-trained on their respective domains and their parameters are fixed during the main training. A style regularization loss (LS,T_style) is applied, ensuring that transferred sentences ((cid:101)x(cid:48)i for source, (cid:101)xi for target) are correctly classified with their target styles ((cid:101)l(cid:48)i for source, (cid:101)li for target) when conditioned on the appropriate domain vector (e.g., log PCT((cid:101)li|(cid:101)xi) for target). The overall training objective (LDAST = LS,T_ae + LS,T_style) combines these components. The domain vectors and domain-specific style classifiers work in conjunction to enable the model to distinguish domain-specific features and adaptively transfer styles, generating sentences with domain-specific lexical terms while maintaining content.": 1415,
    "The paper proposes a Generative Adversarial Network (GAN) framework, Pun-GAN, which consists of a generator Gθ and a discriminator Dφ, trained alternately. The core mechanism for encouraging the generator to produce ambiguous pun sentences is a specially designed ambiguity reward `r`. This reward is calculated based on the discriminator's output probabilities for the two target word senses, `s1` and `s2`, given a generated sentence `x`. The reward function is defined as `r = (Dφ(s1|x) + Dφ(s2|x)) / (|Dφ(s1|x) - Dφ(s2|x)| + 1)`. This formula is designed to assign a higher reward to sentences where both target senses (`s1` and `s2`) have high probabilities from the discriminator, and crucially, where the probabilities for these two senses are close to each other (i.e., a small gap between `Dφ(s1|x)` and `Dφ(s2|x)`). The `+ 1` in the denominator prevents division by zero. The generator's objective is to minimize the negative expected reward, L(θ) = - Σk r(k)Gθ(x(k)|s1,s2), where `x(k)` is the `k`-th sampled sequence and `r(k)` is its reward. This objective is optimized using the policy gradient method, where the gradient ∇θL(θ) is approximated as - (1/K) Σk=1 to K r(k)∇θlog(Gθ(x(k))). The generator is pre-trained using a general corpus to provide a warm start. During adversarial training, the generator and discriminator are trained alternately, with the discriminator providing the ambiguity reward that guides the generator via reinforcement learning to produce sentences that are simultaneously interpretable with both target word senses, effectively achieving punning without direct supervision from a pun corpus.": 1416,
    "The paper leverages the modeled domain relevance to control knowledge transfer through a fine-grained knowledge fusion model, inspired by knowledge distillation. This is achieved by dynamically adjusting the hyper-parameter `α` (alpha), which balances the sequence labeling loss (`LT_SEQ`) and the knowledge distillation loss (`LT_KD`) for the target model.\nThe overall target model loss is `LT = (1 - α)LT_SEQ + αLT_KD`.\nInstead of a fixed scalar `α`, the model assigns different `α` values based on the computed relevance weights:\n1.  Sample-level Knowledge Fusion: For each target sample `i`, a specific `α_samp_i` is computed based on its sample relevance weight `wsamp_i`: `α_samp_i = σ(τ · wsamp_i + γ)`, where `σ` is the sigmoid function, `τ` is a temperature parameter, and `γ` is a bias. This `α_samp_i` is then used to weight the `LT_SEQ` and `LT_KD` for that specific sample in the overall batch loss calculation:\n    `LT_SEQ = - (1/n) * sum_i=1^n (1 - α_samp_i) * yT_i * log(ˆyT_i)`\n    `LT_KD = - (1/n) * sum_i=1^n α_samp_i * log(pT_i / pS_i)` (Note: The paper uses `log(pT_i pS_i)` which is likely a typo and should be `log(pT_i) * pS_i` or `pS_i * log(pT_i)` for cross-entropy, but the general idea is weighting the terms).\n2.  Element-level Knowledge Fusion: For each element `j` within sample `i`, a specific `α_elem_ij` is computed based on its element relevance weight `welem_i`: `α_elem_ij = σ(Wα * welem_i + bα)`, where `Wα` and `bα` are trainable parameters. This `α_elem_ij` is then applied at the element level within the loss functions:\n    `LT_SEQ = - (1/n) * sum_i=1^n sum_j=1^L (1 - α_elem_ij) * yT_ij * log(ˆyT_ij)`\n    `LT_KD = - (1/n) * sum_i=1^n sum_j=1^L α_elem_ij * pS_ij * log(pT_ij)`\n3.  Multi-level Knowledge Fusion: To combine both levels, a multi-level `α_multi` is computed as the element-wise product of `α_samp` (broadcasted to element dimensions) and `α_elem`: `α_multi = α_samp ⊙ α_elem`. This `α_multi_ij` then replaces `α_elem_ij` in the element-level loss functions (Eqs. 15 and 16). This allows for a highly granular control over knowledge transfer, enabling strongly relevant parts to learn more from the source model and weakly relevant parts to learn more from target data.": 1417,
    "The extracted low-rank sentence embeddings, denoted as C = [cT1, cT2, ..., cTD]T, are used to implement a vector-space approach to lexical centrality for humor ranking. First, a centroid (m) for the entire corpus is computed by taking the average of all sentence embedding vectors (ck). This centroid represents the \"\"lexical center\"\" of the corpus. The degree of humor for any given sentence is then indicated by its Euclidean distance from this computed centroid. Specifically, if sentence s1 has embedding x1 and sentence s2 has embedding x2, and the distance d(m, x1) is less than d(m, x2), it implies that s1 is considered funnier than s2. This approach leverages the idea that more humorous texts, or \"\"core jokes,\"\" tend to cluster closer to the lexical center of the corpus.": 1418,
    "To evaluate the impact of pivot feature selection, the paper compares two configurations of its joint neural UDA algorithm: JointMI and JointOracle. The JointMI configuration represents a standard approach where pivot features are selected by calculating the mutual information (MI) between the source features and their corresponding labels. The 100 features with the highest MI are chosen as pivots. This method relies solely on information available from the source domain. The JointOracle configuration serves as an oracle-informed system to establish an upper bound on performance achievable with optimal pivot selection. In this setup, pivot features are selected by calculating the mutual information (MI) between the *target* features and their *gold labels*. This provides an ideal set of pivots that are highly predictive in the target domain. Crucially, despite using target labels for pivot selection, the network training itself (i.e., optimizing the joint loss function) still only uses source labels for the primary supervised task, mirroring the UDA setting. By comparing the performance of JointMI and JointOracle, the study quantitatively assesses the quality of standard MI-based pivot selection and the potential for improvement with better (oracle) selection. The paper also qualitatively examines MI-selected pivots, showing examples of shared, domain-specific, and general but unselected features to illustrate the limitations of MI.": 1419,
    "To address the second research question, the paper explores various document embedding methods to determine which are most effective in capturing the necessary discourse context for metaphor identification. These methods include traditional embeddings as well as newer, more advanced techniques that are capable of encapsulating larger textual units and their semantic relationships within the discourse, thereby providing a richer input to the classifiers.": 1420,
    "To ensure that the generated sentence contains a specific target verb with a desired part-of-speech (POS), the paper proposes a POS constrained language model. This model is inspired by the asynchronous forward/backward generation model (Mou et al., 2016). Given a target verb as input, the model first generates a backward sequence starting from the target word at its designated position (t) in the sentence and ending at the beginning of the sentence (position 0). The probability of this backward sequence is denoted as p(w1 t). The output of this backward sequence is then reversed and fed as input to a forward model. This forward model then generates the remaining part of the sentence, with its probability denoted as p(wn t). The final generated sentence is a concatenation of the input and output of the forward model. To enforce the specific part-of-speech constraint, the training corpus is pre-processed. Words that are used with the specific part-of-speech of interest (e.g., verbs) are labeled with a POS tag (e.g., \"\"spells.v\"\" for \"\"spells\"\" used as a verb), while other usages remain unchanged (e.g., \"\"spells\"\"). A POS tagger (Bird et al., 2009) is used for this labeling. By training the language model on this labeled corpus, it learns to generate sentences where the input target word appears with its specified POS, such as \"\"she spells.v her husband at the wheel\"\" when \"\"spells.v\"\" is provided as input.": 1421,
    "A distant global context is integrated by inserting a topic word related to the pun word (`wp`) at the beginning of the seed sentence. This process, part of the RETRIEVE+SWAP+TOPIC method, aims to foreshadow the pun word without eliminating ambiguity. Relatedness between words is determined using a \"\"distant\"\" skip-gram model, `pθ(wj | wi)`, which is specifically trained to maximize the probability of `wj` given `wi` when they are separated by a distance `d1` to `d2` words (e.g., `d1=5`, `d2=10`). This model identifies candidate topic words `w` that are strongly associated with `wp` over a longer range. To maintain grammaticality and type consistency, the system restricts the word to be replaced in the seed sentence to nouns or pronouns. Furthermore, candidate topic words are filtered to ensure they are type-consistent with the deleted word, using WordNet path similarity (e.g., path similarity > 0.3) to compare synsets. This ensures that the inserted topic word fits semantically and grammatically within the sentence's initial context, thereby supporting the pun word globally while preserving the sentence's overall structure and ambiguity. An optional neural smoother (a single-layer LSTM sequence-to-sequence model) was also explored to fill in words around the topic word to further improve grammaticality, though its effectiveness was limited.": 1422,
    "To effectively incorporate word position knowledge, the paper introduces a binary feature for each word in the input sentence. This binary indicator specifies whether the word is located in the first half or the second half of the sentence. This feature is motivated by the observation that puns tend to appear towards the end of sentences (around 88% in homographic and 92% in heterographic datasets). Instead of mapping this binary indicator to a vector representation using an embedding table, the paper directly adopts the value of the binary indicator as part of the input to the model. This position indicator is then concatenated with the transformed character embeddings and pre-trained word embeddings to form the complete input representation for each word, which is then fed into the BiLSTM network.": 1423,
    "Solution 2:\nOnce the two classifiers, F1 and F2, have identified ambiguous features by maximizing their discrepancy, the feature encoder G is then optimized to generate more discriminative features that are pushed away from these decision boundaries. This is achieved by training G to minimize the very discrepancy that F1 and F2 were trained to maximize. The objective for G is to minimize Ex∼Dt[ (1/K) * Σ |p1i(y|x) - p2i(y|x)| ], where the sum is over K classes. This adversarial step, repeated throughout the training process, continuously forces G to produce features for the target domain that are clearly distinguishable by both classifiers, thereby reducing ambiguity and achieving category-level alignment. Additionally, the generator G is further regularized using information from unlabeled target data through a Generator Regularizer, R(G). This regularizer, formulated as lG = max(0, m - di,j)^2 if sij=0 and di,j^2 if sij=1, where di,j is the L2 distance between data points, m is a predefined margin, and sij indicates if xi and xj belong to the same class, encourages the output of R(G) to be distinguishable among classes. The underlying label for target data points (xi) used in this regularization is estimated by taking the maximum posterior probability from the two classifiers. This regularization term is weighted by a dynamic hyperparameter λ4, which is set to exp[-5(1 - t/max_epochs)^2]λ4, minimizing its effort at the beginning of training when the predictor is less accurate. This combined approach ensures that G not only resolves ambiguity but also produces high-quality, class-separable features.": 1424,
    "The paper addresses cross-domain CWS using a Word-Embedding-Based Segmenter (WEB-Segmenter), which is a non-parametric decoder that directly utilizes the CWS-optimized word embeddings. The segmentation process is formalized as a hypotheses-based Viterbi decoding problem. First, Hypothesis Generation occurs character-wise for a given sentence. At each time step, a hypothesis `ht` includes a partial segmentation container (`SEGt`), a buffer container (`BUFt`) for unsegmented characters, and the number of segmented words (`Mt`). New hypotheses `ht+1` are generated by either appending the next character to `BUFt` or by moving the sequence in `BUFt` into `SEGt` as a new word and then appending the next character to `BUFt`. Hypotheses containing out-of-vocabulary (OOV) words (words not in the target domain dictionary `D`) are discarded. To reduce search space, if the buffer container's size reaches a predefined maximum word length, its content is moved into the segmentation container. Second, Probability Calculation for each hypothesis `ht` is based on the log probability, which is derived from the product of conditional probabilities of words within the partial segmentation. The conditional probability `p(wi|SEGt,f)` of a word `wi` is calculated using the cosine similarity between its embedding `vwi` and the embeddings of preceding words `vwi-j` within a fixed window size `f`. This cosine similarity serves as the metric for probability. Third, Dynamic Beam-Size and Maximum Word Length are employed to ensure efficiency and reliability. The segmenter maintains at most `k` (initial beam-size) top hypotheses sorted by log probabilities. If, at any point, no hypotheses can be generated (due to OOV words or length limits), the segmenter dynamically increases the beam-size by 10 and the maximum word length by 1, then re-generates hypotheses from the beginning. This mechanism guarantees that at least one segmentation (the one from the baseline segmenter) is always produced. Finally, Similarity Score Look-up is used to improve decoding speed by pre-calculating and storing cosine similarities of all word pairs that co-occur at least once in the automatically segmented corpus, allowing for quick retrieval during decoding.": 1425,
    "To transfer knowledge from a large, out-of-domain corpus to a small, noisy target-domain corpus, the paper employs a domain adaptation technique using L2-regularization. First, a neural network model is trained on the TIGER corpus, which is a large newswire dataset (source domain), to obtain optimized weights, denoted as `W_hat`. These `W_hat` weights are then used as a prior during the training of the final model on the target domain, which is the German Twitter data. This is achieved by adding a penalty term, `RW = λ||W - W_hat||^2_2`, to the objective function (categorical cross-entropy loss) during the target-domain training. Here, `W` represents the weights of the model being optimized on the Twitter data, and `λ` is a hyperparameter controlling the degree of impact of the prior weights. This regularization is applied to the weights of the two main bidirectional LSTMs, the character LSTM, all embedding layers, and the final output layer. By penalizing large deviations from the `W_hat` weights, the model trained on the small target dataset is guided by the knowledge acquired from the large source dataset, preventing overfitting and improving generalization. The optimal value for `λ` was empirically determined to be 0.001.": 1426,
    "To enhance the performance of existing manually-adapted financial sentiment dictionaries, such as those compiled by Loughran and Mcdonald (L&M dictionaries), the paper introduces two distinct automatic adaptation methods: ADD (Additional Words) and RE (Reclassification).\nThe ADD method aims to expand an L&M dictionary by identifying and incorporating new, relevant words. For a specific sentiment category (e.g., negative, uncertain, or litigious), an Support Vector Machine (SVM) is trained. In this training, words already present in the L&M dictionary for that category are labeled +1, and all other words are labeled -1. Each word is represented by its domain-specific word embedding. This trained SVM is then applied to all words from the original H4N dictionary that are *not* already part of the L&M dictionary and for which word embeddings are available. The SVM scores for these new words are converted into probabilities using logistic regression. A word is added to the resulting ADD dictionary (e.g., negADD, uncADD, litADD) only if its converted SVM score surpasses a predefined confidence threshold θ (set to 0.8), ensuring that only highly confident classifications are included. This method focuses on adding entirely new words that were not part of the original L&M lexicon.\nThe RE method focuses on refining the classifications of words already present within an L&M dictionary. This is achieved by training SVMs in a five-fold cross-validation setup. For each word in the L&M dictionary, its reclassification decision is made by the SVM model that was *not* trained on the specific fold containing that word. Similar to the ADD method, SVM scores are converted into probabilities via logistic regression. A word becomes a member of the adapted RE dictionary (e.g., negRE, uncRE, litRE) if its converted SVM score from the appropriate cross-validation fold exceeds the confidence threshold θ (0.8). This process allows for a data-driven re-evaluation and potential re-labeling of words based on their actual usage contexts within the financial corpus, thereby correcting or confirming their sentiment assignments.": 1427,
    "To generate a comprehensive distribution of alternative utterances, the paper utilizes a pre-trained LSTM language model, which serves as the utterance prior p(U | O). This language model is conditioned on an input object (color or grid). Similar to the literal meaning LSTM, a linear transformation is applied to the input object to initialize the LSTM's hidden state. The LSTM then generates successive tokens of an utterance. For grid inputs, a specific enhancement is introduced: a layer of multiplicative attention, using the \"\"general\"\" scoring function from Luong et al. (2015), is applied between the LSTM output and the convolutional grid output before the final Softmax layer. This attention mechanism allows the language model to \"\"attend\"\" to individual grid cells while producing output tokens, thereby improving the quality of utterance prior samples for complex grid objects. Due to the large support of this distribution, which would make efficient computation of the pragmatic speaker's (s1) normalization term infeasible, the issue is resolved by taking a small set of samples from this pre-trained LSTM for each object in a context. These samples approximate p(U | O) each time the pragmatic listener (l1) is computed during training and evaluation.": 1428,
    "To evaluate the impact of different idiom representation methods on model performance, the paper tests three distinct approaches. The first method, \"\"Idiom Embedding,\"\" treats each idiom as a single token, and its representation is obtained through pre-training on a large corpus (Song et al., 2018). This approach assumes idioms are independent semantic units. The second method, \"\"Average Character Embedding,\"\" represents an idiom by simply averaging the embeddings of its four constituent characters. This method mimics understanding idioms purely based on their literal meanings. The third method, \"\"Average Character Embedding + MLP\"\" (Multi-Layer Perceptron), applies an MLP to the concatenation of the four character embeddings of an idiom. The output vector of this MLP is then used to represent the idiom. This method also applies a composition assumption, where the idiom's representation is a composite function of its constituent words, but with a non-linear transformation. The MLP used has a hidden layer of 400 units with a tanh activation function, taking an 800-dimension input vector (from concatenating four 200-dimension character embeddings) and outputting a 200-dimension vector. These different representations are then fed into state-of-the-art cloze test models (Language Model, Attentive Reader, Stanford Attentive Reader) to assess their performance on the ChID dataset.": 1429,
    "The paper proposes a unified reinforcement learning (RL) framework that jointly trains the Selection Distribution Generator (SDG) and a Predictor. The Predictor itself is composed of two parts: a feature extractor and a classifier. The feature extractor is responsible for learning distributed representations of the data, transforming both the guidance set (XTg) from the target domain and the source data bags (Bj) into vector collections (Φt and ΦBj, respectively). The classifier then trains on the representations of the selected source data (ΦˆBj) for the specific task (T). During this training, the classifier passes gradients back to the feature extractor, updating its parameters (Θ) and thus optimizing the data representations. The SDG and the Predictor are pipelined: the feature extractor's output (data representations) serves as input to the SDG for selection, and the Predictor (specifically, the classifier's performance on the selected data) provides feedback in the form of rewards to the SDG. This feedback mechanism, combined with the policy gradient optimization for the SDG and standard backpropagation for the Predictor, ensures that the data selection process (managed by SDG) and the representation learning (managed by the feature extractor within the Predictor) are mutually informed and optimized. The framework ensures that the selected instances are relevant to the target domain, and simultaneously, their representations are learned and refined to be task- and domain-specific.": 1430,
    "The paper employs a novel hybrid architecture combining a Mem2seq model and a pointer generator for text generation. Given a reference title and a set of predicted related entities, a bi-directional Gated Recurrent Unit (GRU) encoder processes the reference title to produce hidden states. For decoder hidden state initialization, a multihop attention mechanism is applied to the predicted related entities, which are stored as memories. The last hidden state of the reference encoder serves as the initial query, iteratively updated over multiple hops to refine the attention distribution over memories, with the final query vector initializing the GRU decoder. During each decoding step, a Memory Network computes attention weights for each related entity, iteratively refining them. This process incorporates an entity coverage vector, which is the sum of attention distributions from previous hops, to reduce repetition. A final memory-based context vector is obtained. Simultaneously, a Reference Attention mechanism, similar to existing approaches, computes attention weights for each word in the reference title based on the decoder state and a reference coverage vector (sum of attention distributions over previous decoder steps) to also reduce repetition. A reference context vector is derived. The Generator then aggregates attention weights for each word from both reference and memory attention distributions. It also computes a probability from the language model based on the decoder state, reference context, and memory context. Two gate functions act as soft switches: one balances generating a word from the vocabulary versus copying from the reference title or related entities, and another balances copying from the reference text versus related entities. The final probability of generating a token is a weighted sum determined by these gates. The model is trained with a loss function that combines negative log-likelihood of the ground truth token with coverage loss for both reference and memory distributions. During testing, a simple masking method is applied with beam search to prevent repetition of non-stop words or punctuation already generated in the current output.": 1431,
    "To automatically determine and apply appropriate rhetorical modes, the paper proposes the Automatic Control CVAE (ACCVAE). This model builds upon the MCCVAE by adding an automatic prediction mechanism for the rhetorical label `r`. Before generating the next sentence, an MLP predictor is used to estimate the probability distribution over possible rhetorical modes `p(r|hX)`, where `hX` is the last hidden state of the encoder LSTM, representing the context of the current poem sentences. The rhetorical label `r` for the next sentence is then determined by taking the `argmax` of this predicted probability distribution. Once `r` is predicted, it is embedded as `e(r)` and combined with `hX` to form the conditional variable `c = [hX; e(r)]`, identical to how `c` is constructed in the MCCVAE. The rest of the ACCVAE architecture, including the CVAE components (prior network, recognition network, decoder), operates in the same manner as the MCCVAE. The overall loss function for ACCVAE includes the KL divergence term, the decoder's cross-entropy loss, and an additional cross-entropy loss for the rhetoric predictor, allowing the model to learn to predict rhetorical modes automatically.": 1432,
    "The paper introduces a sequential training method called ASADA (Adversarial Domain Adaptation using Artiﬁcial Titles) to align and maintain source and target embedding spaces during adaptation. This method involves three main steps:\na) Pre-training with Artificial Titles: A Pointer-Generator model (See et al., 2017) with a joint vocabulary (union of 50k most frequent terms from source and target) is first pre-trained on the \"\"artiﬁcial titles\"\" generated for the unlabeled target domain (as described in Solution 1). This step primarily trains the decoder to learn the grammatical style and vocabulary of the target domain.\nb) Embedding Space Adaptation (Target-to-Source ADA): The embedding space of the pre-trained model is then adapted using Adversarial Domain Adaptation (ADA). In this step, the model continues training on the target domain data, but the source domain is introduced as auxiliary adaptation data. The domain classiﬁer attempts to differentiate between the concept representations (final forward and backward hidden states of the bidirectional LSTM encoder) of the target and source domains. A gradient reversal layer ensures that the encoder adjusts its parameters to maximize the domain classiﬁer's loss, thereby encouraging the embedded representations of the two domains to align. This step is referred to as TADA (training on Tart using ADA).\nc) Summarization Learning (Source-to-Target ADA): With a joint embedding space now defined and aligned, the model proceeds to train on the labeled source domain data, which consists of title-text pairs. During this phase, the unlabeled target domain data is used as auxiliary adaptation data. This step, referred to as SADA (training on S using ADA), ensures that the model learns how to generate summaries from the source domain while simultaneously keeping the embedded representations aligned with the target data. This sequential approach prevents abrupt changes to the embedding and decoder domains, allowing for a more gradual and effective adaptation.": 1433,
    "The paper addresses this question with the RNN MHCA (Recurrent Neural Network Multi-Head Contextual Attention) model, inspired by the Selectional Preference Violation (SPV) theory. This model identifies metaphoricity by detecting incongruity between a target word's representation and its context. The target word representation (ht) is a BiLSTM hidden state, derived from concatenated GloVe and ELMo inputs, similar to RNN HG. The key innovation is the Multi-Head Contextual Attention (MHCA) mechanism used to compute the context representation (cn t). Instead of simply concatenating adjacent hidden states, MHCA dynamically weighs contributions from context words within a defined window size (n). The BiLSTM hidden state matrix (H) is split into N equivalent pieces, each corresponding to a \"\"head.\"\" For each head (M), a context representation is computed by taking a weighted sum of the hidden states of context words (e.g., hM t-i for left context) within the window, where the weights are determined by a dot-product attention score between the target word's hidden state (hM t) and the context word's hidden state (hM t-i). Irrelevant context hidden states outside the window are masked out. The left-side context representation (−→c n t ) and right-side context representation (←−c n t ) are computed independently for each head and then concatenated to form the full attentive context representation (cn t = [−→c n t ; ←−c n t ]). This multi-head approach allows the model to attend to different parts of the hidden states of context words, potentially recalling important context information. Finally, the target word's hidden state (ht) is concatenated with this attentive context representation (cn t), and this combined vector, [ht; cn t], is passed through a softmax function to predict the label (p(ˆyt|ht,cn t ) = σ(wᵀ[ht;cn t ] + b)). This mechanism explicitly models the interaction between the target word and its context to detect semantic incongruity.": 1434,
    "The paper proposes an interleaved \"\"wake\"\" and \"\"dream\"\" cycle approach (Algorithm 1) to efficiently manage the annotation budget (B) and improve both the student learner and the active learning (AL) policy. The total annotation budget B is divided into W wake-dream cycles, with each wake phase having a length of Tw = B/W AL queries. In the \"\"wake phase\"\" (Algorithm 2), the AL policy (π) is *exploited* to select the most beneficial data points from the unlabelled pool (Dunl). These selected data points (x_t) are then sent to a human annotator for their true labels (y_t). The newly labelled data points {(x_t, y_t)} are added to the labelled dataset (Dlab), and the underlying student learner (mφ) is retrained with the updated Dlab. This process directly uses the human annotation budget to improve the student learner. Immediately following a wake phase, a \"\"dream phase\"\" (Algorithm 3) is initiated. In this phase, the *currently trained student learner* acts as an imperfect annotator to generate pseudo-labelled data. This pseudo-labelled data is then used in simulations to improve the AL policy, as detailed in Solution 1. By interleaving these two phases, the approach ensures that the entire human annotation budget is dedicated to improving the student learner, while the policy improvement happens \"\"for free\"\" (in terms of human annotation cost) via simulations with the imperfect student learner. This cyclical interaction allows for continuous adaptation and refinement of both components on the target task.": 1435,
    "The proposed model conditions a sequence tagging model on the enriched slot representations to achieve robust cross-domain transfer and handle misaligned schemas. After obtaining the contextual utterance token encodings `H` from a bidirectional Gated Recurrent Unit (BiGRU) and the enhanced slot representations (`ds` for slot description and `ea_i` for attention-weighted example values), these are combined. Specifically, for each utterance token `h_i`, its encoding is concatenated with the slot description encoding `ds` and its corresponding attention-weighted slot example encoding `ea_i`. This concatenated vector `(h_i ⊕ ds ⊕ ea_i)` forms the input to a bidirectional Long Short-Term Memory (BiLSTM) network, which acts as the Tagger. The BiLSTM processes this sequence of enriched inputs, producing hidden states `X = {x_i}`. Finally, these hidden states `x_i` are fed into a softmax layer to perform a 3-way classification (Inside, Outside, Begin - IOB tags) for each token `y_i`, indicating if and where the provided slot type occurs in the utterance. This direct conditioning on the rich semantic context of the slot allows the model to adapt to new domains and schemas by understanding the slot's meaning through its description and examples, rather than relying solely on potentially misaligned slot names.": 1436,
    "To enhance the attention mechanism's ability to align context with idiom meaning, the paper modifies the standard global attentional model (Luong et al., 2015). Specifically, the current target hidden state, denoted as `h_t`, is extended by concatenating it with `h_0`, which represents the average embedding of all words in the modern plain text meaning of the idiom. This extended target hidden state, `h_t' = V[h_t;h_0]` (where `V` is a learnable matrix), is then used to compute attention weights. The score function `score(h_t', h_k) = h_t' * W * h_k` (where `W` is a learnable matrix) compares this extended state with each source hidden state `h_k`. This modification allows the attention mechanism to incorporate explicit semantic information about the idiom's meaning when determining the relevance of source context words, thereby improving the alignment process.": 1437,
    "The model architecture synergistically combines both shared domain-invariant and unshared domain-specific features. The initial layers of the network, including Embedding, Bi-LSTM, and the multi-perspective match block, are duplicated into two kinds: shared and domain-specific. Shared layers process sentences from all domains, learning domain-invariant features as described in Solution 1. Domain-specific layers, conversely, operate only on sentences from their corresponding domains, learning highly specialized features. An unshared domain discriminator, identical in structure to the shared one but without the GRL (instead using an identity transform that multiplies gradients by +λ), is used to train these domain-specific layers to minimize domain classification loss, thus generating domain-specific representations. Each domain also has its own domain-specific aggregation and classification (fully connected) layers. The aggregation layer takes both the domain-specific and shared features as inputs. It then aggregates these features, concatenates the resulting vectors to form a combined representation, and passes this combined feature vector to the classification layers for the final task classification (e.g., question deduplication or textual entailment). The training process involves five passes, with specific loss functions governing the learning of shared and domain-specific components: the shared embedding, Bi-LSTM, and aggregation layers are learned by minimizing LSh = L4 + L5 - λL1, where L4 and L5 are task classification losses for source and target domains respectively, and L1 is the shared domain discriminator loss. Source domain layers are trained by minimizing LS = L2 + L4, and target domain layers by minimizing LT = L3 + L5, where L2 and L3 are unshared domain discriminator losses for source and target domains, respectively. This integrated training ensures that both types of features contribute to the final task performance.": 1438,
    "The paper addresses the challenge of leveraging sparse, crowdsourced pairwise comparison data and generalizing to unseen instances by integrating textual features within the Gaussian Process Preference Learning (GPPL) framework. Unlike traditional regression approaches that would first estimate scores and then train a model, GPPL directly incorporates features during the preference learning process, avoiding error propagation. For both humorousness and metaphor novelty tasks, GPPL is augmented with 300-dimensional average word embeddings (from word2vec trained on Google News). For the metaphor task, the embedding for the metaphor token is concatenated with the average word embeddings of its context sentence. Additionally, linguistic features are incorporated: average token frequency (from Wikipedia), a polysemy measure (average number of synsets from WordNet 3.0), and average bigram frequency (from Google Books Ngrams). For the metaphor task, metaphor token frequency is also appended if the frequency feature is selected. By using these features, GPPL can model the underlying utility of texts based on their linguistic characteristics, enabling it to make predictions for instances that were not part of the training set's pairwise comparisons. The Bayesian inference mechanism of GPPL is inherently robust to sparse and noisy data, allowing it to learn reliable scores even when the number of pairwise annotations is limited, and to generalize effectively by leveraging the feature space.": 1439,
    "The Concept-Aware-Pseudo-Query (CAPQ) framework unifies the strategies for enhancing feature transferability and discriminability through a cooperative training objective. The overall training objective is a weighted sum of three loss components: the Source Ranking Loss (LS), the Concept Preservation Loss (LP), and the Target Ranking Loss (LT). The LS component ensures that paired text and video samples from the source domain are embedded closely together, while non-matching pairs are pushed apart, establishing initial discriminative capabilities. The LP component, as detailed in Solution 1, focuses on making the learned features transferable and domain-agnostic by preserving knowledge from pre-trained models and aligning concept distributions across domains. This is achieved by encouraging the joint text-video embeddings to retain the ability to distinguish between external concepts, implicitly reducing video and description distribution shifts. The LT component, as detailed in Solution 2, leverages the novel mutually-exclusive pseudo-text selection mechanism to learn discriminative features for the target domain. By generating pseudo-text queries for unlabeled target videos and applying a ranking loss, it progressively aligns cross-domain visual features at the level of text descriptions, even without ground-truth target text. The framework's design ensures that the concept preservation mechanism (LP) helps select reliable pseudo-texts for the LT, restricting the accumulation of false labeling risk. This cooperative minimization of all three losses allows CAPQ to learn features that are both robustly transferable across domains and highly discriminative for retrieval tasks in the target domain, effectively bridging the cross-domain discrepancy.": 1440,
    "The paper addresses the class imbalance problem through the Group-level Class Equivalence Loss (Lcl). This loss aims to align the maximum classification scores for each category within a group between the source and target domains. First, for each group k and each category cls, a maximum pooling layer is applied to the group-specific feature Fk_l to obtain Mk_l, which represents the maximum score for that category within group k. The maximum classification score from the source domain (Mk_S) is then used as a pseudo-label. The loss function is a multi-class binary cross-entropy applied for each category u, comparing Mk_S,u with Mk_T,u. A threshold parameter (delta) is introduced to exclude very low probability values, preventing noisy or unreliable pseudo-labels from influencing the training. By encouraging the target domain's maximum scores to be similar to the source's for each category within each group, this loss helps ensure that minority categories, which might otherwise be overlooked, are properly considered during adaptation.": 1441,
    "The paper employs a two-term optimization problem to adapt the model, specifically updating the encoder (phi_u), decoder (v), and classifier (h_w). The first term in this optimization is a cross-entropy loss (Lce) applied to the generated pseudo-dataset (ZP, YP). This term ensures that the classifier maintains its generalization capabilities on the abstract knowledge represented by the prototypical distribution. The second term is a matching loss, designed to align the target domain's distribution with the prototypical distribution in the embedding space. This alignment is achieved by minimizing the Sliced Wasserstein Distance (SWD) between the empirical distribution of target domain samples (X_T) transformed into the embedding space by the encoder (phi_u(X_T)) and the empirical prototypical distribution (p_hat_J(Z_P)) derived from the generated pseudo-dataset. SWD is chosen for its computational efficiency compared to the full Wasserstein Distance, as it approximates the distance by summing the Wasserstein distances between multiple random one-dimensional projections of the high-dimensional distributions. A trade-off parameter (lambda) balances the contribution of these two loss terms. By minimizing this combined loss, the model's encoder is adapted to transform target domain data into an embedding space where its distribution closely matches the learned prototypical distribution, allowing the source-trained classifier to generalize effectively to the target domain.": 1442,
    "The paper adaptively balances multiple objectives by formulating the parameter update as a quadratic programming (QP) problem. Instead of using manually set hyper-parameters to weight different loss terms (as in conventional multitask learning), the method seeks an optimal update vector `w` that is as close as possible to the gradient of the unified contrastive loss (`gt`) while satisfying the Source Discriminative Constraint (hw, gsi >= 0) and the Target Memorization Constraint (hw, gdmi >= 0). The QP problem is defined as: min w ||w - gt||^2 subject to hw, gsi >= 0 and hw, gdmi >= 0. This problem is solved in the dual space, which reduces the complexity to a much smaller QP with only two variables (u1, u2). Once the optimal dual variables (u*1, u*2) are found, the optimal update vector `w` is computed as w = gt - u*1gs - u*2gdm. This adaptive calculation of `w` ensures that the model updates prioritize minimizing the contrastive loss while strictly adhering to the non-increasing constraints on source and old target domain classification losses, effectively balancing the objectives without fixed trade-off parameters.": 1443,
    "The self-domain adaptation framework is structured in three main procedures to leverage target domain information at inference while addressing the limitations of traditional domain adaptation (DA) and domain generalization (DG). First, during the training step, the framework learns a discriminative feature extractor (F) and classifier (C) using available labeled source domains, and simultaneously learns a domain-specific adaptor (A) through a meta-learning based adaptor learning algorithm. This pre-training ensures the adaptor is well-initialized for efficient adaptation. Unlike traditional DA, this training phase does not require access to the target domain data. Second, at inference time, the framework uniquely leverages the unlabeled test domain data. All parameters of the feature extractor, classifier, depth estimator, and autoencoder are fixed. Only the pre-learned adaptor (A) is updated using the proposed unsupervised adaptor loss (LAdap) applied to the unlabeled test domain data. This step allows the model to fine-tune itself to the specific characteristics of the unseen target domain, overcoming the DG limitation of losing target-specific information. Finally, after the adaptor optimization is complete, the entire model (with the adapted adaptor) is fixed, and predictions are made on the test domain data. This two-stage approach (training the adaptor with meta-learning, then adapting it at inference) allows the framework to benefit from target domain information without requiring it during initial training, providing a practical solution for face anti-spoofing in real-world scenarios.": 1444,
    "The paper transforms the classification problem of choosing between Standard Scaling (ST) and Curtis-Reid (CR) into a regression problem, which is then converted back into a classification decision using a learned threshold. A regression model, either a linear regressor or a random forest regressor, is trained to predict the `delta` value, which is the difference in the square-rooted attention levels (`sqrt(alpha_cr) - sqrt(alpha_st)`) based on the input feature vector `f`. Let `~delta(f)` be the predicted difference. To make the classification decision, a threshold value `tau` is introduced. If the predicted `~delta(f)` is less than or equal to `tau` (`~delta(f) <= tau`), Curtis-Reid scaling is selected; otherwise, Standard scaling is chosen. This threshold `tau` acts as an additional control parameter, allowing the system to bias the selection towards Standard scaling. A negative `tau` value, for instance, implies a more conservative approach, where Curtis-Reid is only selected if a substantial reduction in attention level is strongly predicted, thereby preventing potential performance degradations that might occur from unnecessary switches to CR. The optimal `tau` value is determined by searching a predefined domain (e.g., `[-1, 1]` in 0.01-steps) on the training set to minimize the overall attention level achieved by the classifier. For the implemented linear model, the final coefficients are `~delta(f) = 0.014 * f_coef + 0.07 * f_obj + 0.008 * f_rhs`, with a threshold `tau = -0.054`.": 1445,
    "The paper structures adversarial training to progressively align feature distributions by considering specific conditions within the target domain through Condition-Specific Adversarial Training (CSAT). Instead of a single domain classifier, CSAT employs three distinct discriminators: DC1, DC2, and DCA. DC1 and DC2 are condition-specific discriminators, each addressing a particular condition (e.g., T1 or T2). DCi operates on the stylized source images corresponding to condition 'i' (^XSi) and the real target images of condition 'i' (XTi), aiming to match their respective feature distributions (pCi(^XSi) and pCi(XTi)). DCA, on the other hand, targets transferring knowledge from the overall stylized source domain (^XS) to the overall target domain (XT), acting as a standard domain classifier. This multi-discriminator setup, where some discriminators focus on condition-specific alignment and one on global alignment, allows for a progressive reduction of the domain shift. The discriminators follow a PatchGAN fashion, and the adaptation is performed in a target-to-source direction to facilitate the later extraction of adversarial ambivalence. The encoder and decoders (DecC1, DecC2, CA) are optimized to fool these discriminators, while the discriminators are optimized to correctly classify the source of the features.": 1446,
    "To facilitate knowledge transfer and extract domain-invariant information, the paper proposes a Sparse Associative Structure Alignment mechanism. This mechanism focuses on aligning the sparse associative structures learned from the source and target domains. Since the associative strength `beta_ij_delta` (representing the association between variable `i` and variable `j` at a specific segment duration `delta`) can be viewed as a distribution, the problem of measuring structure distance is transformed into measuring distribution distance.\nThe method employs Maximum Mean Discrepancy (MMD) to align these associative structure distributions. The objective is to minimize the MMD between the `beta` distributions from the source domain (`beta_S`) and the target domain (`beta_T`). This is formulated as the loss function `L_beta = MMD(beta_S, beta_T)`. By minimizing this `L_beta` loss, the model restricts the associative structures to be similar across domains, effectively guiding the transfer of knowledge. This approach differs from traditional domain adaptation methods that align features directly; instead, it aligns the underlying sparse associative structures, which are assumed to be more stable and domain-invariant due to their inspiration from causal mechanisms. The overall objective function for the model combines this `L_beta` with the label prediction loss (`L_y`) and the segment length restriction loss (`L_alpha`) as `L = L_y + lambda * (L_alpha + L_beta)`, where `lambda` is a hyper-parameter. This joint optimization ensures that the model learns domain-invariant representations by aligning the fundamental associative structures.": 1447,
    "The overall framework, Learning to Augment (L2A), integrates a data generator, a reinforced selector, a teacher module, and a student module in an interactive feedback loop to address BERT compression in data-scarce domains. The process begins with initializing the knowledge distillation (KD) module and the reinforced selector. The data generator constructs its stationary distribution `Ps` (as described in Solution 1) and samples augmented training data `D0` from it. For each batch `xb` in `D0`, the teacher model provides its output `ft(xb)` and the student model provides `fs(xb)`, forming the state `sb`. The reinforced selector then uses its learned policy to determine an action `ab` (selection decision) for the augmented data, which influences the reward. The student model is updated using the knowledge distillation loss `LKD = Latt + Lhidden + Ldark`, which combines attention information, intermediate hidden representations, and dark knowledge (prediction outputs) from the teacher, applied to the augmented data. After the student update, a reward `rb` is obtained based on the student's performance change (as described in Solution 2). This `(sb, ab, rb)` tuple is stored in an episode history. After processing all batches in an epoch, the reinforced selector's policy `phi'` is updated using the accumulated rewards from the history. This iterative process allows the data augmentation strategy to be dynamically refined based on the student's learning progress, ensuring that the generated data is increasingly helpful for distillation. The use of a stationary distribution with constrained exploration for the generator contributes to the stability of the training process, preventing the generation of highly noisy or irrelevant samples. This automated, learning-based approach, leveraging cross-domain information, enables the student model to effectively learn from the teacher even with limited labeled data, often outperforming the teacher by focusing on relevant knowledge.": 1448,
    "The framework employs a multi-stage progressive adaptation strategy. Initially, global domain alignment is achieved by constructing a CIR that bridges the gap between source and target domains. Next, local pixel-level adaptation is performed using ACM to address finer domain discrepancies. Finally, self-supervised training with pseudo-labels generated from the adapted model refines the model's performance on the target domain. This staged approach ensures a gradual reduction of the domain gap and enhances the overall robustness of the model.": 1449,
    "The paper directly improves mutual information (MI) maximization for unsupervised hypothesis transfer by integrating the concept of multiple hypotheses with a novel regularization term, forming the Hypothesis Disparity regularized Mutual Information maximization (HDMI) approach. Unlike previous MI-based methods that might rely on pseudo-label based self-training strategies or single hypotheses, HDMI explicitly leverages a set of source hypotheses learned from the source domain and adapts them to a corresponding set of target hypotheses on unlabeled target data. The core improvement stems from the Hypothesis Disparity (HD) regularization, which is applied among the target hypotheses. This regularization term coordinates the target hypotheses by minimizing the disparity (dissimilarity) in their predicted label probability distributions. This coordination is crucial because, in an unsupervised setting, MI maximization alone with multiple hypotheses can lead to unconstrained optimization and undesirable disagreements. By enforcing this disparity minimization, HDMI ensures that the target hypotheses jointly learn better representations and preserve more transferable source knowledge, leading to better-calibrated prediction uncertainty. This entire process is designed to operate without needing to access the source data during the adaptation phase, adhering to the privacy-preserving property of hypothesis transfer learning.": 1450,
    "A novel self-supervised learning task is constructed using the self-expression layer to enhance the model's representation ability. Given an input `Z_i` to the self-expression layer and a set of outputs `G_j = ZC_j` (where `C_j` is the j-th column of the self-expression matrix `C`), the task defines `Z_i` and `G_i` as a positive pair, meaning they are considered matched. Conversely, `Z_i` and all other `G_j` (where `j ≠ i`) are treated as negative pairs. The model is then optimized using a self-supervised loss function `L_c` (Equation 8). This loss is formulated as the negative logarithm of a softmax-based classifier's output, aiming to classify `Z_i` as `G_i`. Specifically, it maximizes the similarity between `Z_i` and `G_i` while minimizing similarity with all other `G_j`. A temperature parameter `τ` controls the concentration level of the distribution. Both `Z_i` and `G_i` are L2-normalized to ensure `||Z_i|| = 1` and `||G_i|| = 1`. This self-supervised loss `L_c` is then incorporated as a regularization term into the final loss function `L_f` (Equation 9) used for training on the target data, thereby guiding the model to learn better representations.": 1451,
    "The proposed Source Data-Free Domain Adaptive Object Detection (SFOD) framework addresses the challenge of performing UDA object detection without direct access to source data. It operates by first leveraging a pre-trained model from the source domain to generate initial pseudo-labels for the unlabeled target domain. Recognizing that these pseudo-labels are inherently noisy, SFOD integrates two primary mechanisms to ensure robust training. Firstly, Self-Entropy Descent (SED) is utilized to determine an optimal confidence threshold for pseudo-label generation. This is achieved by iteratively fine-tuning the model and monitoring the mean self-entropy of the target dataset, selecting the threshold that corresponds to the first local minimum of this metric, thereby ensuring the reliability of the generated labels. Secondly, to counteract the dominance of false negatives within these noisy labels, False Negatives Simulation is employed through Mosaic data augmentation. This technique creates synthetic hard examples (e.g., small or obscured objects) from existing true positives, enhancing the model's ability to detect challenging instances. The target model is then trained in a self-learning manner using these refined pseudo-labels, optimizing a loss function (Ldet = Lrpn + Lcls + Lreg) where region proposal and bounding box regression losses utilize predictions from the pre-trained model. This comprehensive approach allows the framework to adapt effectively to the target domain without requiring the original source data.": 1452,
    "To enforce better separation between the scores of samples from private and shared sets and further reduce negative transfer, the paper employs a Sample Selective Confidence Regularization. This novel regularization term, `LCR`, is designed to decrease the network's confidence for samples likely to originate from the target private set. The core idea is that samples with a low sample transfer score `s(x)` (indicating they are likely private) should have low predicted class confidence. The `LCR` loss is applied selectively to a subset of target samples in a batch, specifically those for which `s(x)` falls below a dynamic threshold `s_bar(t)`. The threshold `s_bar(t)` is calculated as `s_bar(t) = s0 - (s0 / 2) * (t / T)`, where `s0` is the fixed hyperparameter (1.0), `t` is the current training step, and `T` is the total training steps. This dynamic threshold starts at `s0` and decreases towards `s0/2` as training advances, meaning that initially, only samples with very low scores are regularized, but as the model improves, the regularization applies to a broader range of low-scoring samples. The `LCR` loss itself is defined as `(1 / |Bt|) * sum_{i in Bt} sum_{j=1}^{|Ys|} (y(xi)j - (1 / |Ys|))^2`, where `Bt` is the set of target samples in the batch for which `s(x) < s_bar(t)`, `y(xi)j` is the classifier's pseudo-probability for sample `xi` and label `j`, and `|Ys|` is the number of source classes. This term encourages the pseudo-probabilities for low-scoring samples to be uniformly distributed across all source classes, effectively lowering the confidence in any single class prediction.": 1453,
    "Unlabeled target domain data is effectively utilized in BiSIDA through a source-guided unsupervised learning branch that integrates style transfer, pseudo-labeling, and consistency regularization. First, a target domain image (xt) undergoes a random brightness and contrast perturbation. This perturbed image is then passed through the Continuous Style-induced Image Generator (CSIIG) along with multiple randomly sampled source images (xs,i) as style references. This process, termed source-guided style transfer, transforms the target image into a set of images (~xt,i) that retain the target's semantic content but adopt various source domain styles, thereby creating high-dimensional perturbations. These transformed images (~xt,i) are individually fed into the teacher segmentation network (Ft) to generate stable probability maps (~pt,i). These probability maps are then averaged to compute a combined probability map (pl), which is subsequently sharpened using a temperature parameter (T) to produce a more confident distribution (p4_l). The pseudo-label (qt) for the original target image is derived by taking the argmax of this sharpened probability map. Finally, consistency regularization is applied: the student segmentation network (Fs) processes a brightness and contrast perturbed version of the original target image (A(xt)) to produce its own probability map (pt). An unsupervised loss (Lu) is computed between pt and the pseudo-label qt. This loss incorporates a class-balanced reweighting mechanism to address class imbalance and a confidence threshold (delta) to include only high-confidence pixels in the loss computation. This entire process ensures that the model learns from the unlabeled target data by enforcing consistent predictions across different high-dimensional stylistic perturbations, guided by robust pseudo-labels generated from the teacher network's ensemble predictions on source-style-transferred target images.": 1454,
    "The DERWENT model improves target domain classification by incorporating a weighted classification loss (li,3) that selectively utilizes labeled source data. For each sequence Si, a set of labeled data (Li) is identified, which includes data points from either the source or target domain. The classification loss is a weighted sum of the standard binary cross-entropy loss for these labeled data points. The key innovation lies in the weighting function w(x̂). For a target data point, w(x̂) is set to 1, indicating its high importance. For a source data point x̂, w(x̂) is defined as a scaled sigmoid of the cosine similarity between x̂ and the ending data point of the sequence x̂i,nsi. This weighting mechanism assigns higher importance to source data points that are more similar to the sequence's endpoint (which is a target data point in a successful Type 1 random walk, or a source data point in a successful Type 2 random walk). This reﬂects the conﬁdence in using the loss of a source data point for the target domain, effectively filtering and prioritizing source knowledge that is more relevant or \"\"closer\"\" to the target domain through the established transfer path. This allows the model to reuse valuable label information from the source domain in a principled way that accounts for domain discrepancy.": 1455,
    "The paper proposes a two-stage reinforcement learning framework that leverages the Latent Unified State Representation (LUSR) to achieve zero-shot policy transfer. The first stage focuses on representation learning. A dataset of 500,000 images is collected, comprising 100,000 random observation states from a designated source domain and 100,000 images from each of four \"\"seen\"\" target domains. This comprehensive dataset is then used to train the Cycle-Consistent VAE, as described in Solution 2. Upon completion of this training, the encoder component of the Cycle-Consistent VAE is extracted and designated as the mapping function F, which is responsible for transforming raw observation states into the LUSR. The second stage involves reinforcement learning training. An RL agent is trained exclusively within the single source domain, utilizing the Proximal Policy Optimization (PPO) algorithm. Crucially, the input to this RL agent is not the raw pixel observations but rather the LUSR (Sz) generated by the mapping function F. This training is conducted for a substantial number of steps (e.g., 10 million steps for CarRacing or 50,000 steps for CARLA). Because the LUSR is designed to be consistent across different visual domains by filtering out domain-specific variations, the policy learned by the RL agent in the source domain is inherently robust to these visual changes. Consequently, after this single-domain training, the acquired policy can be directly applied and evaluated in various target domains, including those that were \"\"seen\"\" during LUSR training and entirely \"\"unseen\"\" domains, without any further training or fine-tuning, thereby enabling zero-shot policy transfer.": 1456,
    "The model is designed to explicitly capture the inter-dependency between the punchline and its preceding context across all modalities. For the language modality, both the context and punchline are fed together as a single input sequence to the fine-tuned ALBERT encoder. This input is formatted as `[CLS]Cl[SEP]Pl`, where `Cl` represents the tokens of the context and `Pl` represents the tokens of the punchline. The `[SEP]` token serves to explicitly separate the two sources of information, enabling the ALBERT encoder to model the punchline in light of its context, similar to how it handles question-answering tasks. For the acoustic, visual, and Humor Centric Features (HCF) modalities, input representations (Xm) are similarly structured as `[PAD]Cm[SEP]Pm`, where `Cm` denotes context features and `Pm` denotes punchline features. The `[PAD]` token acts as a placeholder analogous to `[CLS]`. This consistent input formatting ensures that the unimodal Transformer encoders for acoustic, visual, and HCF modalities also process their respective punchline features in the context of the preceding information. Throughout the unimodal representation learning and subsequent multimodal fusion phases, this context-aware input structure allows the model to accurately capture and leverage the inter-dependency between the punchline and its context.": 1457,
    "The paper addresses sequential multi-domain adaptation by extending the PRUNE-TUNE approach to iteratively learn new domains within a single, unified Transformer model. This is achieved through the use of a mask matrix. For each domain (including the general domain and each specific target domain), a unique sparse sub-network is learned. The general domain's informative sub-network is learned first and frozen. Then, for each subsequent target domain, a distinct lottery sub-network is uncovered from the remaining unfixed parameters using iterative pruning and fine-tuning, as described in Solution 2. A mask matrix is introduced over all parameters in the network. This matrix indicates which parameters belong to which specific domain's sub-network, ensuring that each parameter is associated with only one domain and is updated only by learning processes for that domain. When adapting to a new domain sequentially, the general sub-network remains frozen, and the newly identified domain-specific lottery sub-network is fine-tuned. For decoding, given a source sentence and its corresponding domain identification, a binary domain mask is applied to the unified model. This mask activates only the learned sparse sub-network relevant to that specific domain, allowing the single model to support translation for multiple domains without interference or performance degradation between them. This design allows for flexible adaptation to new domains without requiring simultaneous access to data from all previously learned domains.": 1458,
    "The paper incorporates a Sample-Wise Weighted Loss (Lnmt) that directly reweights observed source-domain samples based on their estimated usefulness for target-domain translation. The usefulness of each source-domain sample (xs) is quantiﬁed by a sample weight `w_xs`, which is determined by the output of the previously described classifier C: `w_xs = C(E(tt;xs))`. This weight represents the probability that the encoded representation of the source-domain text, when conditioned on the target-domain tag, matches the target domain. A higher `w_xs` indicates greater similarity or relevance of that source sample to the target domain. This weight is then applied to the standard Neural Machine Translation (NMT) cross-entropy loss for source-domain samples, effectively up-weighting samples that are more useful for the target domain and down-weighting less useful ones. The overall training objective combines this weighted NMT loss with the Domain-Aware Adversarial Loss and the Source-Side Mixup Loss, ensuring that the model prioritizes learning from source samples that are more aligned with the characteristics of the target domain.": 1459,
    "To reduce sample selection bias and improve generalization ability, the method minimizes the distribution mismatch between the selected high-confidence pseudo-labeled samples and the remaining low-confidence target samples. This is achieved by integrating the Bi-Classifier Determinacy Maximization (BCDM) approach within the self-training process. After the target domain D is separated into a high-confidence domain Dh (with pseudo-labels assigned by the strong model) and a low-confidence domain Dl, the BCDM method is used to align these two domains. Specifically, the training process iteratively performs three steps, adapted from the original BCDM:\nStep 1: The cross-entropy loss calculation (similar to BCDM's Step A) is performed by replacing source domain samples (xs) and labels (ys) with high-confidence domain samples (xh) and their pseudo-labels (yh).\nStep 2: The CDD distance (classifier output discrepancy) is calculated between the high-confidence samples (xh) and their pseudo-labels (yh) for cross-entropy loss, and importantly, the CDD distance is also calculated with samples from the low-confidence domain (xl) instead of all target domain samples. This aims to maximize the discrepancy between classifiers on the low-confidence samples while maintaining stability.\nStep 3: The feature extractor G is minimized based on the CDD distance calculated with samples from the low-confidence domain (xl), similar to BCDM's Step C.\nBy applying BCDM to align the pseudo-labeled high-confidence domain (Dh) with the unlabeled low-confidence domain (Dl), the method explicitly reduces the distribution mismatch (dh,H(ˆDh, ˆDl)), thereby alleviating sample selection bias and ensuring better generalization across the entire target domain.": 1460,
    "AST for Domain Normalization (AST-Norm) aims to suppress domain-related information in deeper features by normalizing their AST-latent. This is achieved by altering the AST-latent of features to a fixed mean prototype. The fixed domain prototype, denoted as `zl′,g`, is computed as the expectation of the encoded amplitude spectrum over a set of features `V` from all four variants (source, target, source-stylized, target-stylized) at a specific layer `l′`: `zl′,g = E h∈V [Qe ◦ T ◦ FA(h)]`. During adaptation, features like `hl′,s` are transformed into their normalized versions, e.g., `hl′,sg = AST(hl′,s,zl′,g)`. This process effectively forces the AST-latent of different domain features towards a common prototype, thereby reducing their domain discriminability. Similar to AST-Sim, the phase preservation property of AST ensures that task-related content remains intact during this normalization process.": 1461,
    "The paper addresses this through two main modules: Conditional Kernel Guided Alignment (CKA) and Graph-based Semantic Transfer (GST). The CKA module integrates unbiased semantic knowledge into global alignment. It functions as a semantic-aware discriminator with a multi-head architecture. Extracted image features (Fs/t) from both source and target domains are first processed by the conditional kernels (Wcon) to obtain semantic activation maps (Ss/t). Then, Fs/t are fed into the CKA module, which consists of shared convolutions (fs) followed by C class-specific branches. This produces semantic-aware features (ˆFs/t,c = [fs(Fs/t) : Ss/t,c]) by concatenating the shared features with class-specific activations. Independent class-specific domain classifiers (fc) are then applied to these C separated branches, optimized with Binary Cross-Entropy (BCE) loss (Eq. 6). This setup ensures that features belonging to the same category are aligned, mitigating the domain gap in a category-to-category manner. Concurrently, the GST mechanism narrows the domain gap in the category-based feature space by transferring unbiased semantics from the source to the target domain via graph-based message propagation. The unbiased semantic paradigm (P) from the source domain is converted into an unbiased semantic graph (GP), comprising nodes (VP) representing class embeddings and edges (EP) representing inter-class quadratic relationships. GST then guides the optimization in the target domain by transferring knowledge from GP to the conditional graph (Gcon) established in the target domain. This transfer includes both node transfer, where VP guides the node embedding of the pixel-level sub-graph (Gpix ⊂ Gcon) to ensure unbiased semantic consistency for instance samples, and edge transfer, where the quadratic relationships (E(i,j)P) from GP guide the edges of the category-level graph (Gcat ⊂ Gcon), ensuring consistency in inter-class relationships. The transfer loss (Eq. 7) combines Kullback-Leibler divergence for node transfer and Cosine Embedding loss for edge transfer, generating explicit gradient flows to guide target domain optimization.": 1462,
    "To enhance feature discrimination, the paper utilizes a cluster-level memory bank in conjunction with multi-branch contrastive learning. Unlike previous methods that might save current features, this approach updates the memory bank using the ensemble feature from the temporally averaged model. At the beginning of each epoch, the memory bank is initialized by calculating cluster centroids (cj) as the mean of ensemble features (f(xt i|E[θ])) for all samples belonging to a refined cluster (Ij). During each iteration, these centroids are dynamically updated using a momentum factor (m), incorporating new ensemble features from the temporally averaged model. With these updated centroids in the memory bank, the similarity between individual sample features and class centroids can be measured using a dot product. A multi-branch contrastive loss is then applied to supervise the learning of different branch features. This loss encourages features from the same class (as defined by the cluster centroids) to be close in the feature space while pushing features from different classes apart. Specifically, for each branch's feature (f(xt i,k|θ)), the contrastive loss compares its similarity to its assigned cluster centroid (c+) against its similarity to all other cluster centroids, scaled by a temperature parameter (τ). This mechanism effectively guides the multi-branch features to become more discriminative by leveraging the stable cluster representations from the temporally averaged model.": 1463,
    "The self-labeling process is adapted to handle private source classes in universal domain adaptation (UniDA) settings through distribution weighting. In UniDA, some source classes (fCs) may not appear in the target domain, leading to potential misclassification of common or novel target samples into these non-existent target categories. To mitigate this, the paper combines the proposed method with the existing OVANet (One-vs-All Network) approach. For target samples predicted to belong to seen classes, their label distribution is calculated by averaging the predicted probabilities yi_ova from OVANet. A weight eta_k is then assigned to each seen class k: eta_k is set to 1 if the average probability (phi_k)\ns than 1/|Cs| (where |Cs| is the number of source classes), and a small value (phi < 1) otherwise. This weighting (eta_k) is then applied to the Lp(Dt) term in the mutual information maximization loss, resulting in a modified loss L0p(Dt). Specifically, the sum over seen classes in Lp(Dt) is weighted by eta_k, while the sum over novel categories remains unweighted. This ensures that classes with few samples, which might be private source classes, are down-weighted, preventing the model from classifying target samples into them via mutual information maximization. The overall objective for mutual information maximization in UniDA becomes L0MIM(Dt) = -L0p(Dt) + Le(Dt).": 1464,
    "Domain-specific knowledge is integrated into BaMCTS in several ways to enhance efficiency. Firstly, for Action Space Reduction, the full set of integer variables `I` is restricted to `Ifrac`, which is the subset of variables that are fractional in the solution of the LP relaxation of the MIP instance. This heuristic dramatically reduces the number of possible actions, leveraging the empirical observation that only a few integer variables typically take on fractional values. Secondly, in the Selection step, a variant of the UCT rule is adopted that incorporates a global action score based on pseudocosts. The scoring function for assessing a child node `S'` (extending parent `S` with variable `i`) is `SCORE(S,S') = (1-αPC)UCTscore(S,S') + αPC * ˆPCi`. Here, `ˆPCi` represents the running average of the pseudocost score for variable `i` across all candidate evaluations that involved it. Pseudocosts are historical quantities representing the objective gain from branching on a variable, indicating its effectiveness in tightening the LP relaxation. This `ˆPCi` term acts as a RAVE score, allowing the MCTS to favor variables that have historically proven effective in MIP branching. The `αPC` hyperparameter controls the weight given to this pseudocost information. Thirdly, for Expansion, beyond traditional uniform random expansion, a \"\"deterministic best score expansion\"\" rule is introduced, which selects the available action (variable) with the largest average pseudocost score, further exploiting domain knowledge. Lastly, in Backpropagation, a \"\"max-backup\"\" rule is employed, which, unlike sum-backup, keeps track of the maximum observed reward in a node's subtree. This aggressive approach is suitable for this setting as it encourages more future visits to nodes that have led to high-reward candidates, facilitating a focused local search and slight modifications to candidates for improved rewards.": 1465,
    "The paper provides a theoretical convergence analysis, stating that the estimated target contamination factor will converge to its true value in the limit. This is formalized in Theorem 3. The proof relies on two key assumptions:\n1.  Assumption 1 (Uniform Convergence): The sample of scores from the source domain is an i.i.d. sample from the real source distribution. For the target domain, it's assumed that while there might be bias in the sampled scores ($T_m$) compared to the true target distribution ($T$), this bias gradually diminishes as the number of examples ($m$) increases. Formally, as $m \\to +\\infty$, the distribution of sampled target scores ($t_m$) converges uniformly to the true target distribution ($t$) in [0,1].\n2.  Assumption 2 (Shape Similarity): The normal scores distribution of the theoretical target distribution ($T$) shares exactly the same shape with the normal scores distribution of the source domain ($S$). Formally, the KL divergence between the source normal scores distribution ($S^{\\delta_S}$) and the target normal scores distribution ($T^{\\delta_T}$) is zero, i.e., KL($S^{\\delta_S}$ || $T^{\\delta_T}$) = 0.\nGiven these assumptions, the theorem proves two main points:\n1.  Threshold Convergence: As $m \\to +\\infty$, the estimated target predictive threshold ($\\delta_T^m$) converges to the real predictive threshold ($\\delta_T$). This is shown by demonstrating that the argmin function (from Equation 1 for finding $\\delta_T^m$) converges to the true threshold. The proof involves several steps, including the uniqueness of the solution, convergence of `cut` distributions, and properties of KL divergence.\n2.  Contamination Factor Convergence: Once the threshold convergence is established, the convergence of the estimated target contamination factor ($\\hat{\\gamma}_T^m$) to the true target contamination factor ($\\gamma_T$) follows directly. The estimator $\\hat{\\gamma}_T^m$ is defined as the proportion of target examples with anomaly scores greater than $\\delta_T^m$. The proof sketches that the expectation of this estimator converges to the probability of a true target score being greater than the true threshold, which is by definition the true contamination factor. This relies on the properties of expectation, i.i.d. examples, dominated convergence theorem, and the previously proven threshold convergence.": 1466,
    "Adversarial training is applied to the shared encoder and decoder modules to encourage the model to learn domain-shared features. A gradient reversal layer is introduced after the domain classifier to reverse the gradient direction, training the model to extract domain-shared features that confuse the classifier. The domain classifier minimizes domain classification loss, while the underlying network maximizes it, creating an adversarial setup. This ensures that the shared encoder and decoder are trained to extract domain-shared features.": 1467,
    "The unified framework integrates the joint representations to simultaneously perform pun detection and pun location using simple fully-connected layers. For pun detection, which is modeled as a binary classification task, the overall embedding for the input text is constructed by concatenating the BERT `[CLS]` token's contextualized embedding `TC[CLS]` with the self-attentive embedding `TJ[ATT]` (derived from the joint embeddings of all words). This combined `TJ[CLS]` vector is then passed through a fully-connected layer `FD(·)` to derive logits for two classes (pun or no pun), and the final prediction `ˆyD` is obtained by applying a softmax function and taking the argmax. For pun location, which aims to identify the specific pun word(s), the self-attentive joint embedding `TJi,[ATT]` for each individual word `wi` is used as features. Similar to detection, each `TJi,[ATT]` is fed into a separate fully-connected layer `FL(·)` to derive two logits (pun word or not a pun word), and the prediction `ˆyL i` for each word is generated via softmax and argmax. Both tasks are optimized using a cross-entropy loss function. This allows the model to learn shared representations that are effective for both sentence-level classification and word-level tagging.": 1468,
    "The paper integrates dynamic data selection and weighting strategies into an iterative back-translation framework. The overall algorithm proceeds iteratively, typically for both translation directions (e.g., source-to-target and target-to-source).\nAt each training epoch `t`:\n1.  Score Computation: For every sentence in the monolingual corpora (both source and target), a combined score `score(s)` is computed using the dynamic data selection formula: `score(s) = λ(t)repr(s) + (1-λ(t))simp(s)`. The `λ(t)` function dynamically adjusts the balance between representativeness (`repr(s)`) and simplicity (`simp(s)`) as training progresses, shifting from prioritizing simple sentences to more representative ones.\n2.  Data Selection: Based on the computed `score(s)`, the top `p%` of sentences are selected from the monolingual corpora. This `p` is a hyper-parameter, with 30% found to be generally optimal.\n3.  Back-Translation: The selected monolingual sentences are then back-translated using the current translation model (e.g., MEF to back-translate target monolingual data into source, or MFE for source monolingual data into target), effectively synthesizing a parallel dataset.\n4.  Data Weighting: After back-translation, weights are assigned to these newly generated back-translated samples. This weighting is based on the quality metrics (e.g., Encoder Representation Similarities or Agreement Between Forward and Backward Models) and can also incorporate the quality improvement metric from previous iterations. This step aims to down-weight examples of poor quality, reducing the impact of noise.\n5.  Model Training: The translation model (e.g., MFE or MEF) is then trained using these weighted back-translated sentences. The weights influence the contribution of each synthetic sample to the model's loss function during training.\n6.  Iteration: The entire process (steps 1-5) is repeated for subsequent epochs. Crucially, the `λ(t)` value is increased at each training epoch, gradually shifting the focus of data selection. This iterative refinement allows the models to progressively learn from more complex and domain-relevant data as their translation capabilities improve.\nThis integrated approach ensures that as the back-translation models become better, they are exposed to increasingly representative in-domain data, while simultaneously mitigating the impact of noisy or low-quality translations through dynamic weighting.": 1469,
    "The paper integrates common sense knowledge to ensure meaningful comparisons in generated similes primarily through the \"\"Simile to Literal Transformation via Common-sense Property\"\" step (part of Solution 1) and its influence on the training data for the generation model. When transforming a simile into its literal counterpart, the system leverages COMET (Commonsense Transformers for Automatic Knowledge Graph Construction), which is fine-tuned on ConceptNet. COMET's \"\"HasProperty\"\" relation is specifically used to infer the implicit property shared between the TOPIC and the VEHICLE in a simile. For instance, given \"\"Love is like a unicorn,\"\" the VEHICLE \"\"unicorn\"\" is fed to COMET, which returns associated properties like \"\"very rare,\"\" \"\"rare,\"\" \"\"beautiful,\"\" etc. This process explicitly identifies the underlying common sense properties that make the comparison meaningful. By constructing the literal sentences based on these inferred properties, the training data for BART inherently encodes the relationship between a literal description (containing an explicit property) and its corresponding simile (where the property might be implicit). This ensures that when BART is fine-tuned on these pairs, it learns to generate similes that implicitly or explicitly convey a relevant common sense property, thereby making meaningful comparisons between the TOPIC and the VEHICLE.": 1470,
    "Deep learning models, specifically CNN and LSTM, are applied to the hyperbole detection task. These models are trained using three types of pre-trained word embeddings: Skip-gram, Directional Skip-gram, and BERT-based contextualized embeddings. The models' performance is evaluated using 10-fold cross-validation, with accuracy as the primary metric. The deep learning models consistently outperform traditional learners, with the best deep learning system achieving an accuracy improvement of 11.0% points over the best traditional system. Fine-tuning a pre-trained BERT model also yields better results than traditional learners but falls short of the deep learning models.": 1471,
    "To make self-training more robust against noisy pseudo-labels, the paper integrates Class-aware Feature Self-distillation (CFd) into the self-training framework. The overall loss function for domain adaptation is `L = L_S_pred + L_T'_pred + L_CFd`. `L_S_pred` is the cross-entropy loss for training the classifier with the source labeled data. `L_T'_pred` is the self-training loss, also cross-entropy, calculated on the pseudo-labeled target set `T'`. The crucial addition is `L_CFd`, which combines the Feature Self-distillation (Fd) loss and the intra-class loss. `L_CFd = J_feat_NCE(E(x;theta), x_bar) + lambda * L_intra_class`. `J_feat_NCE` ensures the Feature Adaptation Module (FAM) learns discriminative features from the PrLM, while `L_intra_class` enforces class-aware clustering, making features from the same class more cohesive. This joint optimization allows the model to learn highly discriminative features that are robust to noise. The self-training process itself includes a \"\"rank-diversify\"\" strategy to select high-confidence pseudo-labels: samples are re-ranked by entropy loss (lower entropy indicates higher confidence), and then selected in a diversified manner to ensure a balanced representation of classes in the pseudo-label set `T'`. The weight `alpha` for `L_T'_pred` is gradually increased during training. By learning robust, discriminative, and class-aware features through CFd, the negative impact of inevitable noise in pseudo-labels is alleviated, leading to a more robust self-training process.": 1472,
    "The paper demonstrates that adversarial training, when guided by automatically learned augmentation policies, can indeed improve generalization to new domains and languages. This is achieved by carefully defining the reward signal for the policy search methods:\n1.  Reward Signal for Generalization: To discover augmentation policies specifically geared towards improving generalization, the F1 score of the downstream task model is calculated not on the source domain development set, but on the *out-of-domain* or *cross-lingual development datasets*. This F1 score is then fed as the reward to the optimizer (AutoAugment's RNN controller or BayesAugment's Bayesian optimizer). This direct feedback mechanism guides the policy search to find augmentation strategies that are beneficial for the target generalization task.\n2.  Augmented Data Generation: Once the optimal augmentation policy (a combination of adversary types and their transformation probabilities) is learned, it is used to generate an adversarially augmented training dataset from the source domain (e.g., SQuAD). This augmented dataset contains both original QA samples and corresponding adversarial samples generated according to the learned policy.\n3.  Model Training: The reading comprehension model (RoBERTaBASE) is then fine-tuned on this adversarially augmented source dataset. The hypothesis is that training on data augmented with policies optimized for target domain/language performance will implicitly teach the model more robust and generalizable features.\n4.  Cross-Lingual Evaluation (Translate-Test): For cross-lingual generalization, the modified Translate-Test method (Lewis et al., 2020; Asai et al., 2018) is employed. QA samples in non-English languages are first translated to English. The RoBERTaBASE model, fine-tuned on English SQuAD with cross-lingual-optimized augmentation policies, predicts answer spans within the English context. These predicted spans are then mapped back to the original language context using alignment scores from the translation model. This allows evaluation of the English-trained model's performance on non-English data without requiring direct training on multilingual datasets.": 1473,
    "The paper proposes a comprehensive supervised multi-modal domain adaptation framework that combines multiple objectives to achieve domain-invariant and modality-discriminative feature embeddings. The final objective function, L, is a weighted sum of four components: a classification loss (Lc), a joint embedding alignment loss (Lj), a multi-modal embedding alignment loss (Lmm), and an adversarial domain classification loss (Ladv). The classification loss (Lc) is a standard cross-entropy loss applied to both source and target domains, ensuring that the model maintains discriminative performance. The joint embedding alignment loss (Lj) and multi-modal embedding alignment loss (Lmm) utilize Maximum Mean Discrepancy (MMD) to explicitly align the distributions of joint multi-modal features and individual modality features (visual and textual) across domains, respectively. The adversarial domain classification loss (Ladv) employs a domain classifier (fd) to implicitly align distributions. The domain classifier is trained to distinguish between source and target samples, while the feature mapping parameters (θF) are optimized to confuse this classifier, thereby making the learned features domain-invariant. The overall training process seeks a saddle point where the label predictors (θC) minimize the classification loss, the domain classifier (θd) minimizes the adversarial loss, and the feature mappings (θF) minimize the classification loss while maximizing the adversarial loss. This adversarial training, combined with the explicit MMD-based alignments, ensures that the learned multi-modal features are both discriminative for the VQA task and robustly transferable across domains.": 1474,
    "The JOUST framework is designed to facilitate transfer learning in low-resource scenarios, specifically investigating two setups: Domain Adaptation and Single-to-Multiple Domain Transfer. The core idea is to leverage the pre-trained and RL-optimized agents to adapt to new domains or multi-domain complexities with limited data. Two fine-tuning methods are adopted:\n1.  Naive Fine-tuning: This is a straightforward approach where the pre-trained models are directly fine-tuned on the small amount of target domain data without any specific constraints.\n2.  Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017): This method is used for regularization during fine-tuning. EWC helps prevent catastrophic forgetting of previously learned knowledge (from source domains) when adapting to new, limited target domain data, by selectively slowing down learning on parameters important for previous tasks.\nFor Domain Adaptation, a specific domain (e.g., hotel) is selected as the target. The DS and US are first trained on source data (dialogues *not* involving the target domain). Then, they are fine-tuned on a small amount of target domain adaptation data (e.g., 300 dialogues involving the hotel domain) using either Naive or EWC methods. Crucially, after this initial fine-tuning, the pair of agents continues to interact through self-play using the proposed Reinforcement Learning (RL) training regime (+RL). This RL phase allows the DS to learn effectively from simulated dialogues, overcoming the limitations of the small target domain dataset by exploring more dialogue states and actions.\nFor Single-to-Multiple Domain Transfer, the scenario involves limited multi-domain data but sufficient single-domain dialogues. All single-domain dialogues in the training set form the source data. For each target multi-domain combination (e.g., Hotel+Train), a small number of dialogues (e.g., 100) are sampled as adaptation data. The DS and US are first pre-trained on the source single-domain data, then fine-tuned on the limited multi-domain adaptation data using Naive or EWC. Subsequently, the agents improve themselves through continued interaction via RL. This process enables the agents to learn more complicated policies required for managing transitions and interactions across multiple domains, which were not present in the initial single-domain training.\nIn both transfer learning setups, the RL optimization, particularly with turn-level rewards, plays a critical role in boosting performance. It allows the agents to explore and reinforce behaviors that lead to higher success rates, generalizing well to unfamiliar states encountered in the test corpus, even with better initialization points provided by EWC.": 1475,
    "The paper proposes three strategies to integrate the contextual relations (verb-global, verb-local, verb-distant) for predicting the verb's metaphoricity:\n1.  Context concatenation: The representations of the individual context components (csubj, cobj, cCLS, vbsc) are concatenated to form a single combined context representation (c = csubj ⊕ cobj ⊕ cCLS ⊕ vbsc). A relation model (linear, bilinear, or neural tensor) is then applied to this combined context and the verb representation (v) to compute the probability of metaphoricity, p(r|v,c).\n2.  Context average: The averaged representation of all context components (c = average(csubj, cobj, cCLS, vbsc)) is used as the combined context. Similar to concatenation, a relation model is then applied to this averaged context and the verb representation to predict metaphoricity.\n3.  Context maxout: Instead of combining contexts into a single representation, this strategy computes the probability of metaphoricity for each individual contextual relation (e.g., p(r|v,cCLS), p(r|v,csubj), p(r|v,cobj), p(r|v,vbsc)) separately. The final prediction is then based on the maximum probability among these individual predictions, i.e., max{p(r|v,c)}, where c belongs to {cCLS, csubj, cobj, vbsc}.\nAdditionally, the model employs multi-task learning to jointly optimize the relation-level prediction with a sequence-level metaphor detection task. The relation-level prediction uses binary cross-entropy loss (L0), while the sequence-level prediction uses cross-entropy loss (L1) on the hidden states of the final layer and a softmax layer for all tokens. The final loss for MrBERT is the sum of these two losses (L = L0 + L1), allowing the model to benefit from both explicit relation modeling and general contextual understanding.": 1476,
    "Gradient information from the source model is exploited to enhance unsupervised domain adaptation performance by integrating it directly into the Joint Kernelized Stein Discrepancy (JKSD) criterion. The Stein operator, central to JKSD, is defined using terms involving ∇x logP(x,y), which is the gradient of the log-probability of the source distribution with respect to the input x. In the empirical estimation of JKSD (Equation 5), this gradient information appears in the Υ and Ω matrices. Specifically, Υi,j includes (∇xi k(xi,xj))ᵀ∇xj logP(xj,yj), and Ωi,j includes (∇xi logP(xi,yi))ᵀ∇xj logP(xj,yj)k(xi,xj). For the latent representations (z,y), the term ∇z logP(z,y) is computed as (1/p(y|z))yᵀ∇zGs(z) + ∇zp(z)/p(z), where ∇zGs(z) is the Jacobian matrix of the source classifier Gs with respect to the target latent representation z. This explicitly incorporates the gradient of the source model's classifier output and the gradient of the approximated marginal probability of the latent representation. Furthermore, to maximize the test power of JKSD and make the target domain samples more discriminative, an adversarial strategy is employed. Fully connected layers U and V, parameterized by θU and θV, are introduced to transform the features before applying the kernel functions (k(U(xi),U(xj)) and l(V(yi),V(yj))). The training objective involves minimizing the combined KD and JKSD loss with respect to the target model parameters (θT, θG) while simultaneously maximizing JKSD with respect to θU and θV. This adversarial training forces the model to learn more robust and discriminative features by exploiting the multi-view features, including gradients, from the source domain.": 1477,
    "The paper proposes a joint learning procedure that integrates the primary Aspect-based Sentiment Analysis (ABSA) tasks with the novel explicit self-supervised strategies. The final objective function, `Lfinal`, is a weighted sum of two main components: `Labsa` and `Laux`. `Labsa` represents the combined negative log-likelihood (NLL) losses from the three core ABSA subtasks: aspect term extraction (`Late`), opinion term extraction (`Lote`), and aspect-based sentiment classification (`Lasc`). `Laux` represents the combined NLL losses from the two auxiliary self-supervised tasks: Type-Specific Masked Term Discrimination (`Ltsmtd`) and Pairwise Relations Discrimination (`Lprd`). The hyper-parameter `α` controls the contribution of the auxiliary tasks to the total loss, allowing for flexible weighting. All parameters, especially the shared encoder parameters (`Θs`), are optimized across all these subtasks simultaneously. This joint training approach ensures that the shared encoder, which forms the foundation of the model's contextual understanding, is explicitly optimized not only for the main ABSA tasks but also for discerning term types and their pairwise relations through the self-supervised objectives. This comprehensive optimization strategy enables the model to leverage deep contextual information and explicitly exploit aspect-opinion relations, which is particularly beneficial for accurately handling sentences containing multiple aspects.": 1478,
    "The model provides interpretable insights into quotations by identifying indicative words within history queries. This is achieved through a two-level attention mechanism. First, for a given quotation `q_k` and its associated history queries `{h_1, h_2, ..., h_m_k}`, a query-level attention score (`a_k,i`) is computed. This score quantifies the relevance of each history query `h_i` to the quotation `q_k` by taking the softmax of the dot product between the quotation representation `r_q_k` and the history query representation `r_h_i`. This `a_k,i` represents the quotation-aware attention. Second, at the word-level, the self-attention weights from the Transformer encoder used for processing each history query are extracted. These weights indicate the importance of individual words within that query. Finally, the indicative words for a quotation are determined by multiplying the query-level attention scores (`a_k,i`) with the word-level self-attention scores for each word in the corresponding history query. Words with the highest combined scores are highlighted as indicative. This process allows the model to pinpoint specific terms in the queries that are most relevant to understanding the figurative meaning of the quotation, thereby offering a transparent interpretation of the model's reasoning.": 1479,
    "The paper evaluates a BERT-based (Devlin et al., 2019) model's capacity to match human judgments on the paired ideology ranking task. The model is finetuned on the 24 subreddit corpus. For each text, contextual embeddings are averaged in two ways: over all tokens and over all entity-related tokens. These averaged embeddings are concatenated and used as input to a pairwise logistic regression model. For evaluation, the majority answer from each annotator group (e.g., left-leaning annotators, news-exposed annotators) is used as the group label. The model's performance is assessed by comparing its F1 scores when matching these human-generated labels against its performance when matching the semi-supervised heuristic labels. The evaluation is conducted on both the full non-screening set and a higher consensus subset (where at least 75% of workers selected the same answer). This approach directly tests whether the model can capture the variability and nuances present in human perceptions, rather than just a single \"\"ground truth,\"\" thereby revealing the challenge for models to account for extralinguistic factors introduced during annotation.": 1480,
    "The paper integrates sentence-level and discourse-level information by combining the representations obtained from the different modules into a unified hierarchical representation. The final representation `di` for a word `xi` is formed by concatenating three components: the original BERT output hidden state `hi`, the neighboring sentence representation `N`, and the response from the position-aware global memory network `ri`. Specifically, `di = Concat(λhi + (1 − λ)ri, N)`. Here, `λ` is an empirically set weighting parameter (0.8) that balances the contribution of the original BERT output and the global memory network's response. The neighboring sentence representation `N` is obtained by feeding a context window of `2k + 1` sentences (k preceding, target, k succeeding) into a hierarchical attention architecture, where BERT is the first encoder and a transformer is the second. This `di` then serves as the input to the sentence-level label-enhanced contextualized representation module, ensuring that the label embedding and subsequent transformer processing benefit from both local sentence context, broader neighboring sentence context, and long-range discourse dependencies of the target word.": 1481,
    "The model leverages large amounts of unlabeled data through a Self-Training (ST) mechanism, which is applied iteratively. After an initial fine-tuning phase on labeled data, the model generates pseudo-labels for the large candidate set `U` collected by the Target-based Generating Strategy (TGS). Instead of using hard labels, the model employs soft pseudo-labeling, where for each unlabeled instance `ui`, a probability distribution `ˆyi` over classes is generated. This is calculated by strengthening high-confidence predictions and reducing low-confidence ones through squaring and normalizing the current predictions (`ˆyij = pij^2 / sum(pij'^2)`). This soft labeling approach helps mitigate error propagation that can occur with hard labels. The self-training objective is defined as a KL-divergence loss (`Lst = KL(ˆY ||P)`) between the generated soft pseudo-label distributions `ˆY` and the model's current predictions `P` for the unlabeled data. This `Lst` term is then incorporated into the overall objective function, allowing the model to iteratively refine its parameters by learning from both the original labeled data and the pseudo-labeled unlabeled data, thereby improving its generalization performance and reducing overfitting, especially when labeled data is limited.": 1482,
    "The paper proposes using dense captioning as a dual task to provide rich visuo-spatial and semantic learning signals during training. A pretrained dense captioning model, trained on realistic images from Visual Genome, is employed as the feedback model. Crucially, this model is *not* fine-tuned on the story visualization dataset (PororoSV) because it lacks dense caption annotations, which are prohibitively expensive to gather. Instead, the Visual Genome-based predictions from this off-the-shelf model are used as \"\"proxy\"\" annotations for the target dataset. These noisy predictions serve as ground truth for training the generative model. The generative model is optimized for two losses derived from the dense captioning feedback: a bounding box loss (L1 regression) to learn spatial information and a captioning loss (cross-entropy) for semantic information. To address the absence of explicit positional input in the captions and prevent penalization for inverted character positions, the dataset is augmented with mirror versions of the stories and their corresponding mirrored bounding box predictions. During bounding box loss computation, the model calculates the loss against both the original and mirrored bounding box predictions and retains the lower one, enforcing positional invariance.": 1483,
    "The paper systematically exploits the inherent duality between Question Generation (QG) and Passage Retrieval (IR) tasks within the back-training framework for unsupervised domain adaptation (UDA). The duality implies that the input of one task can be considered the output of the other (e.g., a passage is input for QG, but output for IR given a question). This allows for a reciprocal generation of synthetic data.\nFor adapting a QG model (PS(q|p)) to a target domain:\n*   The method leverages the availability of unlabeled questions (qu) in the target domain, which are considered \"\"natural outputs.\"\"\n*   It then uses a source-trained IR model (PS(p|q)) to retrieve corresponding \"\"noisy inputs\"\" (ˆp) for these natural target domain questions. This is done by conditioning the retriever on the target domain question (PS(p|qu)) and retrieving a passage (ˆp) from the target domain's unlabeled passage pool.\n*   The resulting pairs (ˆp, qu) are then used as synthetic training data to fine-tune the QG model.\nFor adapting an IR model (PS(p|q)) to a target domain:\n*   The method utilizes the availability of unlabeled passages (pu) in the target domain, which are considered \"\"natural outputs\"\" for the IR task (i.e., the passage is the desired output given a question).\n*   It then employs a source-trained QG model (PS(q|p)) to generate \"\"noisy inputs\"\" (ˆq) for these natural target domain passages. This involves generating a question (ˆq) conditioned on the target domain passage (PS(q|pu)).\n*   The resulting pairs (ˆq, pu) are then used as synthetic training data to fine-tune the IR model.\nThis reciprocal process allows each task to generate synthetic training data for the other by leveraging their inverse relationship and the availability of unlabeled data (questions and passages) in the target domain. This systematic exploitation of duality is a core mechanism for back-training's effectiveness in UDA for these tasks.": 1484,
    "The framework addresses two related predictive problems by modeling the joint probability p(n*,f) for a query noun (n*) and a candidate verb-syntactic frame (f) incrementally through time.\n1.  Verb syntax prediction (p(f|n*)): Given a query noun, the model infers which verb-syntactic frames are appropriate to describe it. This is operationalized by calculating the conditional probability p(f|n*) using Bayes' theorem, which involves the joint probability p(n*,f)(t) and marginalizing over all frames. The joint probability is decomposed into p(n*|S(f)(t)) * p(f)(t). The term p(n*|S(f)(t)) is computed by either the Deep Prototype Model (SFEM-DPM) or the Deep Exemplar Model (SFEM-DEM) as described in Solution 1, which assesses the likelihood of the query noun extending to the frame based on its support nouns. The prior probability p(f)(t) is determined by a frequency-based approach, specifically the proportion of unique noun arguments a verb frame has been paired with and attested in the historical corpus up to time t.\n2.  Noun argument prediction (p(n*|f)): Given a verb predicate and a syntactic relation (i.e., a frame f), the model infers what nouns can be plausibly introduced as its novel arguments in the future. This is directly operationalized by the term p(n*|S(f)(t)) from the joint probability calculation, which is the core output of the SFEM-DPM or SFEM-DEM, indicating the likelihood of a query noun being categorized under that specific frame based on its similarity to existing support nouns.\nThe entire process is trained incrementally at each time period (t) by minimizing the negative log-likelihood of the joint probability for every frame and each of its query nouns. This allows the model to learn and adapt to emerging compositions over time, leveraging the time-dependent multimodal representations and the chaining mechanisms.": 1485,
    "The Wasserstein Selective Transfer Learning (WSTL) method achieves robust and efficient selective transfer learning by synergistically combining an adversarial training mechanism between the Transfer Learning (TL) module (fω) and the Wasserstein distance-based Discriminator (fϕ) with a Reinforcement Learning (RL)-based data selector (fθ). The overall training process is iterative and adversarial. First, the discriminator fϕ is trained to optimality via stochastic gradient ascent, aiming to maximize the empirical Wasserstein distance between source and target data representations, while enforcing a Lipschitz constraint with a gradient penalty. Once the discriminator's parameters are optimized, they are fixed. Then, the TL module fω is updated. Its objective is to minimize the estimated Wasserstein distance, effectively trying to \"\"fool\"\" the discriminator by learning domain-invariant features. This adversarial training ensures that the features learned by the TL module are highly transferable and discriminative. Simultaneously, the RL-based data selector fθ interacts with this environment. It receives state representations, which are enhanced by the domain-invariant features learned by the TL module. The selector then makes decisions on which source instances to select for the current batch. These selection actions are guided by a composite reward signal: an immediate reward derived from the Wasserstein distance (provided by the discriminator, indicating how close selected source data is to the target) and a delayed reward based on the performance improvement of the TL module. The RL module updates its policy using a policy gradient algorithm to maximize this total reward. This integrated approach creates a feedback loop where the discriminator improves feature learning for the TL module, which in turn provides better state representations for the RL selector, and the discriminator also directly guides the RL selector with immediate rewards, leading to more stable training, faster convergence, and ultimately, better performance by selecting high-quality source data.": 1486,
    "Existing metaphor detection datasets are enhanced with gloss annotations through a multi-step human annotation process. First, for each benchmark dataset (TroFi, VUA, PSUCMC), a candidate set of words is constructed. For TroFi, this includes all labeled verbs. For VUA, words are randomly selected verbs. For PSUCMC, a random set of words is chosen and filtered for meaninglessness. Second, for English datasets (TroFi, VUA), human annotators look up the Merriam-Webster dictionary to fetch glosses for words in the candidate set. For the Chinese dataset (PSUCMC), glosses are extracted from the Baidu Dictionary. Third, four annotators are recruited for each dataset to independently select the most appropriate gloss that expresses the contextual meaning of the target word, comparing all candidate glosses with the given context. Finally, annotators discuss and determine the final labels after generating their individual annotations. The reliability of these annotations is verified using kappa-score, ensuring a high degree of inter-annotator agreement. This process creates datasets suitable for jointly training and evaluating both metaphor detection and gloss-based interpretation.": 1487,
    "The Domain Specification step is responsible for incorporating target-specific attributes into each domain-independent review (`xm`) to generate a target-domain review. This is framed as a text infilling problem and addressed through the following mechanisms:\n1.  Target-Domain Masked Language Model (TD-MLM): A pre-trained BERT model is adapted by re-training it with the Masked Language Modeling (MLM) task specifically on the unlabeled data from the target domain (DU). This re-trained model is referred to as TD-MLM. The TD-MLM is then used to predict words for the masked positions in the domain-independent review `xm`. Initially, for each masked token, the model predicts the word with the highest probability from its entire vocabulary.\n2.  Target-Specific Aspect Constraint: To ensure that the infilling process generates actual aspect terms relevant to the target domain and maintains coherence for multi-word aspects, the prediction space of the TD-MLM is constrained. If a masked position corresponds to a single-word aspect term in the original source review, the TD-MLM's prediction is restricted to only single-word aspect terms from the pre-extracted target-specific aspect list (A1t). If the masked span corresponds to a multi-word aspect term (e.g., `k` consecutive [MASK] tokens), the method computes the joint word probabilities for all possible `k`-word aspect terms from the target-specific aspect list (Akt). These multi-word terms are then re-ranked based on their joint probabilities, and the highest-scoring one is selected to fill the masked span.\n3.  Target-Specific Opinion Constraint: To ensure that the generated opinion terms maintain the sentiment consistency of the original source review, a sentiment-based constraint is applied. The sentiment (Positive, Negative, or Neutral) of the original masked source-specific opinion term is determined using the output from the Double Propagation algorithm. Subsequently, when the TD-MLM infills the masked opinion term, its predictions are restricted to only those target-specific opinion terms from the extracted target-specific opinion list (Ot) that possess the *same sentiment* as the original source-specific opinion term. This mechanism ensures that the sentiment expressed by the generated opinion term aligns with the sentiment of the original masked term.": 1488,
    "To integrate the linguistic idiom principle with contextual pre-trained language models and improve MWE representation, the paper explores three distinct settings involving the expansion of the model's vocabulary by adding a single token to represent each MWE. These settings are applied within the Sentence BERT architecture, which uses a siamese network structure with a mean-squared error loss over cosine similarity for training, ensuring that resultant embeddings can be compared using cosine similarity.\nThe three tokenization strategies are:\n1.  \"\"all replace\"\": All instances of an MWE in the input text are replaced with their corresponding newly added single token before being fed into the model. This strategy assumes that the MWE always functions as a single unit.\n2.  \"\"select replace\"\": This is a more nuanced approach. Each input sentence is first classified using a one-shot coarse-grained idiomaticity detection model (from Task 1, Subtask A). An instance of an MWE is replaced with its single token representation *only* if the one-shot model predicts that the MWE in that specific sentence has an idiomatic meaning. This attempts to apply the idiom principle conditionally.\n3.  \"\"no replace\"\": This serves as a baseline. There is no change to the pre-trained model (no special token added for MWEs) or its input. MWEs are processed by the model's standard word-piece tokenizer.\nFor Task 2, Subtask A (pre-training only), the \"\"all replace\"\" and \"\"select replace\"\" variations of BERT base are further pre-trained on a large corpus (Common Crawl News Dataset) containing relevant MWEs. The embeddings associated with the new MWE tokens are randomly initialized. These models, along with a standard BERT base and a BERT base with MWE tokens but no additional pre-training (randomly initialized embeddings), are then trained using the Sentence BERT architecture on standard STS datasets (STS benchmark for English, ASSIN2 for Portuguese) to enable semantic similarity comparison.\nFor Task 2, Subtask B (fine-tuning), the \"\"no replace,\"\" \"\"all replace,\"\" and \"\"select replace\"\" versions of BERT base are fine-tuned. This fine-tuning involves both the standard STS data and training data constructed from the AStitchInLanguageModels dataset using Equation 1, where the gold similarity score for `sim(E, E→c)` is 1 and for `sim(E, E→i)` is `sim(E→c, E→i)`. Crucially, for the \"\"replace\"\" versions, the tokens associated with MWEs are randomly initialized at the start of fine-tuning, testing if this sample-efficient method of learning MWE embeddings is feasible without extensive prior pre-training of these specific tokens.": 1489,
    "The UDALM procedure is designed to achieve robust performance and sample efficiency through its two-stage adaptation process and the properties of its mixed loss function. First, the initial Domain Pretraining (DPT) step (Solution 1) effectively adapts the language model to the target domain's general linguistic characteristics, providing a strong foundation. Second, the subsequent fine-tuning with the mixed classification and Masked Language Model (MLM) loss (Solution 2) allows for continuous adaptation to the target domain's language while learning the task from the source. This multi-task approach acts as a regularizer, preventing the model from overfitting on the source domain and leading to more robust training, as evidenced by smaller standard deviations in experimental results compared to other methods. The method demonstrates sample efficiency by showing robust performance improvements even when the amount of unlabeled target data is limited (e.g., down to 500 samples), making it suitable for low-resource settings. Furthermore, the mixed loss itself serves as an effective early stopping criterion for Unsupervised Domain Adaptation (UDA) training. By monitoring the minimum mixed loss during training, the procedure can determine when to stop, alleviating the need for extensive search for the optimal number of training steps. This is crucial in UDA scenarios where labeled target data for validation is unavailable. The paper shows that stopping based on the minimum mixed loss yields comparable accuracy to training for a fixed, optimal number of epochs, indicating its reliability for model validation and streamlined development.": 1490,
    "The paper integrates feedback from multiple independently trained style discriminators into a single encoder-decoder framework through a combined training objective. The overall training loss for the joint encoder-decoder model is `L = λDAE * E_x~T[-log Pθ(x|˜x)] + Σ_i=1^k λi * Lsi`. Here, `λDAE` is a hyper-parameter weighting the Denoising Autoencoder (DAE) loss, which ensures content preservation and basic rewriting capability. `Lsi` is the reinforcement learning objective derived from the `i`-th style discriminator, as described in Solution 2, which guides the generation towards the specific target style `si`. The `λi` are hyper-parameters weighting the contribution of each style discriminator's feedback. For training, the joint encoder-decoder model is exposed to a randomized mixture of data from each of the target-domain corpora. This mixture is agnostic to the individual style of each sentence, meaning the model does not explicitly know which style a given input sentence belongs to. Instead, the multiple discriminative LMs, each specialized in a particular target style, provide continuous feedback by rewarding style adherence in the transferred sentences. This setup allows for a unified and cohesive understanding of multiple styles by diversifying rewards from different discriminators across samples. By combining the DAE objective with the policy gradient feedback from multiple style-aware LMs, the model learns to simultaneously control multiple stylistic dimensions while preserving content, effectively relaxing the requirement for jointly labeled data across styles.": 1491,
    "The paper explores this by implementing Task-Adaptive Pre-Training (TAPT). This approach uses a much smaller, unlabeled corpus compared to DAPT, specifically consisting of the input documents directly from the target domain's summarization task. Similar to DAPT, TAPT continues the pre-training of the BART model using its original pre-training objective function (corrupting documents and optimizing a reconstruction loss). The key advantage of TAPT is its practicality: it is less dependent on the availability of massive unlabeled domain-related corpora and is significantly less computationally expensive to run. Despite its smaller size, the TAPT corpus is considered highly task-relevant because it directly comprises the documents that the model will eventually summarize.": 1492,
    "The generalization of metaphorical knowledge across different datasets is evaluated using a cross-dataset setup with the BERT pre-trained language model. Four distinct metaphor detection datasets are considered: LCC(en), TroFi, VUAPOS, and VUAVerbs. These datasets differ in their POS types, annotation processes, and raw sentence domains. Similar to the cross-lingual experiments, the training size for each dataset is set to the minimum among all datasets (3,838, the size of TroFi). For each pair of source and target datasets, two experiments are conducted: one using the pre-trained BERT model and another using a randomly initialized BERT model as a baseline. The same edge probing architecture is employed. By comparing in-distribution performance (training and testing on the same dataset) with out-of-distribution performance (training on one dataset and testing on another), the study assesses how well metaphorical knowledge transfers between datasets with varying characteristics. This setup helps to understand if PLMs learn generalizable knowledge or dataset-specific artifacts, particularly highlighting the impact of annotation consistency and domain differences.": 1493,
    "The paper achieves collaborative optimization through a joint training objective that integrates both the sentiment classification loss (`Lclass`) and the domain discrimination loss (`Ldomain`). The overall objective function is formulated as `min(M,p,f) {λLclass(S;θM,p,f) - min(g) Ldomain(Ŝ,T;θM,p,g)}`, where `λ` is a trade-off parameter balancing the two objectives. This structure explicitly defines a minimax game. The training procedure is iterative: in each step, the sentiment classification loss is computed using samples from the source domains, and this loss is used to update the parameters of the Pre-trained Language Model (PLM), the learnable soft prompt vectors, and the Masked Language Model (MLM) head function. Concurrently, samples from both source and target domains are fed into their respective domain discriminators to calculate the domain discrimination loss. The parameters of the PLM and the soft prompt vectors are updated by both losses: they are optimized to minimize `Lclass` (for accurate sentiment prediction) and simultaneously to maximize `Ldomain` (to learn domain-invariant features). This collaborative training ensures that the model benefits from both the domain-aware continuous vectors learned by the soft prompts and the domain-invariant representations enforced by the adversarial training, leading to a more robust and effective prediction of the [MASK] token's feature distribution for cross-domain sentiment analysis.": 1494,
    "The paper employs amnesic probing to establish a causal link between hidden state features and translation behavior. First, logistic regression probing classifiers are trained to predict the label (figurative/paraphrased vs. figurative/word-for-word) from the hidden representations of PIE tokens. Then, Iterative Null-space Projection (INLP) is used to remove specific features from the hidden states. INLP trains multiple classifiers to distinguish between figurative PIEs that are paraphrased and those translated word-for-word. After training, the vectors are projected onto the null space of the classifiers' parameters, effectively removing the information captured by these classifiers. The previously paraphrased PIE occurrences are then re-processed through the model with this information removed from their hidden states. The success of the intervention is measured by recording the percentage of translations that change from a paraphrase to a word-for-word translation. Changes in attention patterns (e.g., reduced attention within the PIE and increased interaction with context) are also observed to confirm the causal connection.": 1495,
    "The paper addresses narrative generation by fine-tuning state-of-the-art text generation models, specifically T5 (Text-to-Text Transfer Transformer) and BART (Bidirectional and Auto-Regressive Transformers). These models are conditioned on both a proverb and a set of keywords to generate a narrative. The keywords for a narrative are defined as the verbs and named entities present within that narrative, which are extracted using spaCy. The dataset is split into train and test sets under \"\"seen\"\" and \"\"unseen\"\" proverb settings, similar to the proverb prediction task. The quality of the generated narratives is evaluated using automatic metrics such as BLEU (Bilingual Evaluation Understudy) and ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence), as well as the recall of the input keywords mentioned in the generated narrative. Additionally, human evaluation is conducted on AMT to assess the generated narratives based on criteria such as relatedness to the proverb, interestingness/creativity, fluency, and overall quality using Likert scales.": 1496,
    "The influence of syntactic flexibility on model performance is assessed by defining an \"\"ICE score\"\" for idiomatic expressions (IEs). The ICE score represents the percentage of times an idiomatic phrase occurs in the test data in a form that does not match its original base form. A higher ICE score indicates greater syntactic flexibility, meaning the phrase appears more frequently in non-standard forms. The performance of the roberta-base model is evaluated for each idiom type, and Spearman correlations are calculated between the model's performance (accuracy) and the idioms' ICE scores. This analysis aims to determine if more fixed expressions (low ICE score), which might be memorized by models, or more flexible expressions (high ICE score), which require reasoning about context and interacting words, are easier or harder for NLI models to classify, particularly for non-entailment cases.": 1497,
    "To overcome the suboptimality of heuristically designed patterns and improve prediction quality, the paper introduces a Pattern Search (PS) method. This approach aims to find the best combination of different patterns. Given a specific simile dataset, denoted as DPS, the method iteratively calculates the performance of the Pattern Ensemble (as described in Solution 2) for all possible subsets of the predefined patterns (from Table 1). For each subset, the ensemble's performance is evaluated. Finally, the subset of patterns that yields the optimal performance on the DPS dataset is selected. This optimal subset, denoted as `pbest`, is then used as the input for the Masked Language Model (MLM) to predict the simile components. This systematic search ensures that the most effective combination of patterns is utilized, rather than relying on a fixed or manually chosen set.": 1498,
    "The paper proposes the Modality Aware Fusion (MAF) module, which encapsulates both the Multimodal Context-Aware Attention (MCA2) and Global Information Fusion (GIF) mechanisms. This MAF module is designed as an adapter, allowing it to be readily incorporated at multiple layers of Generative Pretrained Models (GPLMs) such as BART (Bidirectional and Auto-Regressive Transformers) and mBART (multilingual BART). The integration as an adapter module means that the pre-trained models' strong textual understanding can be augmented with multimodal cues without requiring extensive re-training of the entire base model. Through experimentation, it was found that the optimal performance was achieved when this MAF fusion module was introduced before the sixth layer of the encoder, indicating a strategic placement that efficiently retains multimodal information for accurate decoding. This approach enables the model to leverage existing powerful language models while enhancing their capability to process and explain sarcasm from multimodal inputs.": 1499,
    "To ensure valid probability distributions and effective training with soft labels, SUBDP employs two key mechanisms:\n1.  Dummy Positions for Distribution Validity: The word alignment matrices (˜A) obtained from SimAlign are processed using an `add-dummy-position` operator (∆) and a `row normalization` operator (NR) to create right-stochastic alignment matrices As→t and At→s. The `add-dummy-position` operator expands the matrix by one row and one column, introducing a \"\"dummy\"\" position. This dummy position intuitively corresponds to \"\"null words\"\" in word alignment literature. The `row normalization` operator then ensures that each row sums to one, making the matrices right-stochastic. This mathematical construction guarantees that the projected arc distribution ˆP2(tq|tp) and label distribution ˆP2(ℓ|tq→tp) are provably valid probability distributions (i.e., non-negative and summing to one) for any target word or arc, respectively. This is crucial because invalid distributions would lead to unstable or incorrect training.\n2.  Partial Cross-Entropy Loss for Optimization: When training the target language parser P2, the objective is to minimize the cross-entropy between P2's output and the projected soft silver labels ˆP2. However, the added dummy word (representing null alignment) is not used in the final dependency inference process and could introduce extra noise. To address this, SUBDP calculates a *partial cross-entropy loss*. This loss function specifically excludes elements involving dummy words. For arc prediction, the partial arc cross-entropy loss L(t)arc(P2, ˆP2) sums over only the actual words in the target sentence (p=1...|t|, q=1...|t|), ignoring the dummy position. A similar partial label cross-entropy loss L(t)label(P2, ˆP2) is computed for labels. This ensures that the optimization focuses only on the relevant, non-dummy word pairs, preventing noise from unaligned or null-aligned words from disproportionately affecting the training process.": 1500,
    "The paper addresses this by proposing the Pun Explanation task and evaluating its utility for pun classification.\n1.  Explanation Generation: A T5 (Text-to-Text Transfer Transformer) model is fine-tuned to generate pun explanations given pun sentences as input. For texts without annotated explanations, the pun sentence itself is used as the output explanation.\n2.  Pun Classification Task Setup: The task is to classify whether a given text `T` is a joke, optionally augmented with an explanation `ET`. Output labels for classification are assigned using the majority vote of AF3 (is a joke) from the ExPUN dataset. The dataset is split into 1,699/100/200 for train/dev/test, with dev and test sets having an equal distribution of jokes to non-jokes.\n3.  Models for Classification:\n    *   No explanations (Baseline): BERT-base, RoBERTa-base, and DeBERTa-base are fine-tuned to classify jokes without any explanations as input.\n    *   Gold explanations: The baseline models are augmented with human-annotated gold explanations during both training and testing. Two variants are explored for handling missing explanations or held-out explanations: (1) representing missing explanations as an empty string (\"\"w/gold expl.\"\"), or (2) randomly sampling a negative explanation from another annotated example (\"\"w/gold+sampled neg.\"\").\n    *   Generated explanations: T5-generated explanations are used during testing with models trained on gold human-annotated explanations.\n    *   ELV (Explanations as Latent Variables): A probabilistic framework where natural language explanations are treated as latent variables. It jointly trains an explanation generation module and an explanation-augmented prediction module using a variational EM framework.\n4.  Evaluation Metric: The \"\"simulatability score\"\" is used to measure explanation quality, defined as `acc(IE→O) - acc(I→O)`, where `I` is input text, `E` is explanation, and `O` is classification output. This measures the improvement in task performance when explanations are provided versus when they are not.": 1501,
    "The paper evaluates the end-to-end performance of the proposed pipeline system by integrating the best-performing components from the retrieval and generation modules. For a randomly selected set of 60 context words, the system first employs the neural pun word retrieval module, specifically the finetuned RoBERTa-large-NLI model, to retrieve the top-1 predicted pun word pair for each context. This selection is based on the neural module's superior performance in identifying compatible pun word pairs. Subsequently, the pun generation module, specifically the T5PT+FT model (which incorporates both pretraining and finetuning), is utilized to generate a pun for each context, integrating the retrieved pun word pair and its associated senses. This combined approach constitutes the full proposed pipeline. The effectiveness of this integrated system is then assessed through human evaluation, which measures the success rate of the generated puns. A human success rate benchmark is established by identifying the human-written pun in the XCUP dataset that annotators found least difficult to compose for each context. This comprehensive end-to-end evaluation provides insight into the practical performance of the entire system, from initial pun word identification to final pun generation, and directly compares it against human capabilities to highlight the inherent challenges of the task and areas for future improvement.": 1502,
    "The overall objective function for training the adapter-based secret key model is `Ladapter = LCE + LCEadapter + β·LDC([A,S],T) + λ·L'MMD(A,S,T)`. The `LCE` term is for source domain classification. An additional `LCEadapter` (Cross-Entropy Loss for adapter) is introduced: `E(x,y)∼Ds[CE(FFN(ψ(adapter(x))),y)]`. This loss is applied to source domain data passed through the adapter, ensuring that the adapter maintains task-dependent information and that the transformed embeddings still contain sufficient signals for the classification task. The `LDC` term, `LDC([A,S],T)`, encourages the domain classifier to distinguish between the combined source and target+adapter domain (`[A,S]`) and the original target domain (`T`). The `L'MMD` term, `L'MMD(A,S,T)`, similar to the prompt-based method, aims to close the distance between the target+adapter domain (`A`) and the source domain (`S`), while enlarging the distance between the source domain (`S`) and the original target domain (`T`). This setup allows the model to perform poorly on target data without the adapter, but regain high performance when the adapter is applied, as the adapter effectively makes the target data appear \"\"source-like\"\" to the model.": 1503,
    "The framework unifies homographic pun generation with homophonic pun generation by converting the homographic task into an equivalent homophonic one. This is achieved by leveraging a word-sense disambiguation (WSD) model and a reverse dictionary. For a given homographic pun word (e.g., \"\"sentence\"\") with two distinct senses (e.g., \"\"a set of words...\"\" and \"\"the punishment...\"\"), the WSD model (Bevilacqua and Navigli, 2020) is used to identify which extracted phrases (from the context word and phrase selection module) exhibit the second sense. Once the two senses are identified, two new substitute words are obtained using a reverse dictionary (Qi et al., 2020). For example, if \"\"sentence\"\" has senses related to \"\"clause\"\" and \"\"punishment,\"\" the reverse dictionary might yield \"\"clause\"\" for the first sense and \"\"conviction\"\" for the second. The original homographic pun word is then conceptually replaced by this new \"\"substitute pun word\"\" (e.g., \"\"clause\"\"), and the alternative sense is represented by a \"\"substitute alternative word\"\" (e.g., \"\"conviction\"\"). This transformation effectively re-frames the homographic pun generation as a homophonic task, allowing the same context word and phrase selection, label prediction, and steered generation modules designed for homophonic puns to be applied.": 1504,
    "The paper designs an automatic annotation procedure to obtain dialogue state labels for the synthetic dialogues generated through self-chat. Since the user simulator is trained to mark values within its utterances using special tokens (e.g., `|anatolia|>`), this inherent capability is leveraged. The procedure involves extracting these specially marked values from the user utterances. These extracted values are then matched with corresponding slots in the initial user goal. In cases where multiple values are present for a single slot, the correspondence is determined with the help of transformer attention scores. The expressed values are accumulated turn by turn until the current turn, forming the dialogue state annotation for that specific turn. This automated process ensures that the synthetic dialogues, despite being generated without manual annotation, come equipped with the necessary dialogue state labels required for training downstream models like Dialogue State Tracking (DST) models.": 1505,
    "The paper conducts an in-depth analysis of factors influencing correctness and error trends in language models' figurative language interpretation. One key finding is that models often over-rely on the predicted probability of answers (y1 and y2) alone, rather than effectively utilizing the metaphorical context. This tendency is quantified by Spearman r-values between P(yi|xi) and P(yi), showing that models frequently make the same prediction for paired sentences, although fine-tuning helps to mitigate this. The analysis also examines the influence of context phrase length (longer phrases are harder) and answer probability (more probable answers are easier). Furthermore, a qualitative analysis categorizes errors based on common-sense knowledge types: common-sense object knowledge, visual metaphors, common-sense social understanding, and cultural knowledge. It's observed that models improve significantly on object, visual, and social common-sense categories after training, but not as much on cultural examples, which tend to be more disparate. The study also identifies \"\"sarcastic\"\" metaphors as a common source of errors for both humans and LMs, where models struggle to move beyond simple word associations. Finally, an examination of GPT-3 Curie's errors reveals little overlap with human mistakes, with GPT-3 making more \"\"obvious\"\" errors on simple metaphors, suggesting a fundamental difference in reasoning compared to humans.": 1506,
    "The paper addresses the generation of hyperbolic text within masked spans by leveraging the text span infilling capability of BART (Bidirectional and Auto-Regressive Transformers), a pre-trained sequence-to-sequence model. During the training process, BART is fine-tuned specifically for this task. A masked hyperbolic sentence (where hyperbolic spans have been replaced with a `<mask>` token) serves as the encoder source input, and the original, unmasked hyperbolic sentence acts as the decoder target. This fine-tuning process teaches BART to reconstruct the original hyperbolic text, thereby shifting its probability distribution to favor the generation of hyperbolic words or phrases that are pertinent to the given context, rather than just literal text. During inference, BART takes a literal sentence with masked spans and fills these masks with potential hyperbolic words, effectively transforming the literal input into a hyperbolic version. The method tolerates some noise in the training corpus (non-hyperbolic sentences or masked spans) because fine-tuning on such examples primarily enhances BART's general reconstruction ability without negatively impacting its hyperbolic generation.": 1507,
    "To generate coherent and humorous sentences that exploit the pun word's ambiguity, the system fine-tunes a keyword-to-sentence model using T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020). T5 is selected due to its strong performance in various text-to-text tasks. The prompt provided to the T5 model is structured to include the target pun word and two context words from each of its two senses. For example, for the word 'sentence', a possible prompt could be \"\"generate sentence: sentence, judge, trial, noun, comma\"\". This explicit inclusion of context words from both senses in the input is designed to encourage the model to generate an output that allows for dual interpretation. Additionally, the system investigates the optimal position of the pun word within the prompt, experimenting with placing it at the beginning, in the middle, and at the end of the prompt to assess its impact on the quality of generated sentences. After candidate sentences are generated, a humor classifier, which is a BERT-large model (Devlin et al., 2018) fine-tuned on the ColBERT dataset (Annamoradnejad and Zoghi, 2020) for humor detection, is utilized. This classifier assists in selecting punning sentences by ranking candidates based on their humor probability. Specifically, it is used to rule out non-pun sentences by removing the bottom third candidates, while the remaining candidates are randomly sampled for evaluation.": 1508,
    "Unlike vanilla prototype-based methods that model prototypes as deterministic vectors, MANNER employs a probabilistic framework by modeling prototypes as stochastic variables. Specifically, the distribution over prototypes for each entity type `k` (`zk`) is modeled as a Gaussian distribution `N(zk | gθ(ˆHk, Hk), σ^2_1 I)`. The mean of this Gaussian distribution is obtained through a mean function `gθ`, which takes as input the adapted memory representations (`ˆHk`) and the support set representations (`Hk`). The mean function `gθ` is defined as a weighted interpolation: `γ · Neural([ˆrk, rk]) + (1 - γ) · rk`, where `ˆrk` and `rk` are the means of token representations in the adapted memory and support set, respectively, `[·,·]` is concatenation, and `Neural(·)` is a feed-forward neural network. The covariance is a scaled identity matrix `σ^2_1 I`. This approach allows the model to capture the uncertainties of prototypes, which is conducive to learning more informative prototypes and improving the robustness of few-shot models, especially during the finetuning stage by preventing overfitting to the limited data.": 1509,
    "An explicable vehicle retrieval module, named Scorer, is developed to obtain figuratively meaningful yet literally false comparison pairs for a given tenor when the vehicle is unknown. Scorer operates in two distinct steps. Step 1 focuses on identifying Figuratively Meaningful Candidates. For a given tenor, Scorer queries the Cogbank dataset to obtain its top-k most frequently used cognitive properties. These properties serve as a basis for selecting vehicle candidates. Nouns from the Cogbank dataset that share cognitive properties with the tenor are chosen as initial candidates, ensuring a figuratively meaningful simile as the matched properties can be regarded as the ground. Step 2 then aims to select a Literally False Candidate from these initial choices. This step re-ranks the candidates from Step 1 based on the Euclidean distance of their word embeddings from the tenor. Candidates with a longer Euclidean distance are ranked higher, as they are considered less literally associated with the tenor, thus promoting figurative comparisons. The final ranking score for each candidate is a combination of its rank based on the number of shared cognitive properties (figurative meaning) and its rank based on the Euclidean distance (literal falsity). This two-step process ensures that the retrieved vehicle is both semantically related to the tenor in a figurative sense and literally distinct, providing an explicable and appropriate comparison for simile generation.": 1510,
    "The MetaAdapt framework incorporates adaptive learning rates to enhance the convergence and performance of both its inner and outer loop optimizations. Specifically, it utilizes an adaptive learning rate `α` for the inner-level optimization, which governs the gradient descent steps taken to update the initial parameters (θ) to task-specific parameters (ϕ) based on sampled source tasks. For the outer-level optimization, which updates the original initial parameters (θ) based on the metaloss and meta-gradients derived from the few-shot target examples, an adaptive learning rate `β` is employed. The paper also mentions the use of cosine annealing for these adaptive learning rates, which helps in fine-tuning the step sizes during training, allowing for better convergence properties for both the task-specific updates and the overarching meta-parameter updates. This adaptive control over learning rates contributes to more stable and efficient learning across the bi-level optimization process.": 1511,
    "The paper evaluates the generalization ability of generative language models by testing the best-performing GPT-3 configuration (davinci-002 with 12 few-shot samples) on distinct datasets beyond the primary MetaphorList, and across languages.\nThe generalization assessment involves:\n1.  Generalization to non-metaphoric sentences: The model is tested on English sentences extracted from the VU Amsterdam Metaphor (VUA) Corpus, specifically those where all words are annotated as literal. This evaluates the model's capacity to correctly identify when a sequence is non-metaphoric rather than hallucinating a source domain.\n2.  Generalization across datasets and complexity: The model is tested on the LCC dataset (Mohler et al., 2016). This dataset consists of sentences from government discourse, characterized by being much longer and using more complicated, domain-specific language compared to the prototypical examples in the MetaphorList. The LCC dataset serves as a hold-out test set, meaning it was not used for model or prompt selection.\n3.  Generalization across languages: The LCC dataset includes both English and Spanish sentences. The Spanish subset is used to evaluate the model's ability to predict source domains in a different language. Although the input sentences are in Spanish, the model is expected to generate source domain predictions in English, consistent with the language used in the few-shot examples provided in the prompt. This tests the model's cross-lingual transfer capabilities for metaphor identification.": 1512,
    "The proposed framework, Bidirectional Generative Cross-domain ABSA (BGCA), trains a single, shared generative model (T5-base) in both the text-to-label and label-to-text directions. This shared model learns the association between sentences and labels from two perspectives. Initially, the model is trained in the text-to-label direction on the labeled source data (DS) to predict sentiment tuples from sentences. Subsequently, the same model is fine-tuned in the label-to-text direction, also using DS, but with sentiment tuples as input and original sentences as output. This bidirectional training allows the model to develop a more robust and comprehensive understanding of how sentiment elements relate to natural language expressions. Finally, the high-quality augmented dataset (DG), generated by the label-to-text model using noisy target domain predictions, is combined with the original labeled source data (DS). The shared model then undergoes a final training phase in the text-to-label manner on this combined dataset (`DS` + `DG`). This continuous training of a shared model across reversed directions and augmented data leverages its encoding and generating capabilities, enabling it to seamlessly switch between predicting labels and generating natural sentences. This integrated approach leads to a more accurate prediction of labels for given sentences by thoroughly learning the complex associations between them.": 1513,
    "The informativeness of a generated simile is automatically assessed by measuring the content richness of its vehicle. The underlying assumption is that vehicles with more words tend to convey richer content and create a more vivid impression. For a given simile, the informativeness score (I) is determined by the average length of all extracted vehicles within that simile. The formula is I = (1/mv) * sum(len(v)), where mv is the number of vehicles extracted from the simile, and len(v) is the length (number of words) of each vehicle. This simple yet effective metric directly quantifies the descriptive detail embedded within the simile's comparative element.": 1514,
    "The paper proposes a framework where the Generalizability Test (Solution 1) is used to gauge the type and degree of dataset shift in a target dataset. This assessment then empirically predicts which data augmentation schemes will be most effective for domain adaptation.\nThe framework's predictions are based on the following conjectures:\n*   No shift: Datasets categorized as \"\"No shift\"\" (e.g., SearchQA) are expected to show minimal improvements with most data intervention schemes because the source model already captures the target distribution.\n*   Concept shift and Covariate shift: Datasets falling under \"\"Concept shift\"\" and \"\"Covariate shift\"\" are conjectured to be more amenable to zero-shot data interventions. This is because these shifts involve changes in output distribution (Concept) or input distribution (Covariate) while some underlying compatibility with the source model's conditional distributions might still exist. Zero-shot methods, such as varying context, answer, or question distributions (e.g., using QGen or ClozeQA), are applied.\n*   Full shift: Datasets exhibiting a \"\"Full shift\"\" (where both input and output distributions do not match) are expected to benefit more significantly from few-shot interventions or in-domain annotations from the target domain. The proposed few-shot method (Data-Gen, Solution 2) serves as a proxy for annotating examples in the target domain, as these examples are generated with supervision from the target dataset.\nBy using the Generalizability Test to classify the shift type, the framework provides a principled way to select the most appropriate and effective intervention strategy (zero-shot vs. few-shot) without the need for extensive trial-and-error or large-scale target domain training.": 1515,
    "The Chain-of-Skills (COS) model supports flexible skill configurations at inference time to boost retrieval accuracy. The core idea is to dynamically combine and chain the learned skills based on the specific task requirements. For instance, the expanded query retriever and entity linker can build upon the results of single retrieval. To consolidate evidence gathered by different skills, a score alignment mechanism is employed. This mechanism addresses the issue of unaligned scores from different skills (e.g., linking scores vs. retrieval scores) by normalizing linking scores based on the same-step retrieval scores. Specifically, for a document `i`, its linking score `lsi` is aligned using the formula `lsi = lsi / max({ls} U {rs}) * max({rs})`, where `{ls}` and `{rs}` are the sets of linking and retrieval scores for top-K documents, respectively. This ensures that if a raw linking score is larger than a retrieval score, the top-1 document from each set is aligned, but if smaller, it is not scaled up, preventing common but irrelevant entities from dominating. Documents returned by multiple skills are considered more relevant and are promoted in ranking by taking the maximum of their individual aligned scores and multiplying by a coefficient `α`. Finally, the reranking skill is used to compute new scores for the merged evidence set. Documents are then sorted using a combination of their aligned retrieval/linking score and the reranking score, `si + β * rankscorei`, where `β` is a hyper-parameter. For multi-hop questions, this scoring process is conducted for second-hop evidence documents, and then two-hop scores are aggregated to sort the reasoning chains, allowing for iterative retrieval and refinement of evidence paths.": 1516,
    "The paper investigates the role of monolingual pretraining in improving idiom translation, particularly focusing on mBART (Liu et al., 2020), a model pretrained on monolingual data from many languages. The hypothesis is that multilingual pretraining can bootstrap over source and target language contexts where idioms occur, even without direct translation examples.\nThe investigation involves:\n1.  Model Initialization: Experiments compare randomly initialized models (\"\"random\"\") with models initialized using mBART (\"\"mBART\"\"). Both use the Transformer architecture with 12 encoder/decoder layers, 1024 embedding size, and 16 self-attention heads.\n2.  Training Conditions: Models are trained under different conditions:\n    *   Zero-shot: Training data includes only regular parallel data, testing performance on unseen idioms.\n    *   Joint: Training data includes regular and idiom-train data, testing performance on idioms observed in training (different context).\n    *   Upsampling: Same as joint, but idiom-train data is upsampled (e.g., 20x or 100x) to assess the impact of increased exposure.\n3.  Noise Injection during Fine-tuning: To explore the robustness and contextual reliance of models, different types of noise are injected during fine-tuning of mBART models:\n    *   mBART+mask: 10% of source tokens are masked.\n    *   mBART+replace(enc): 10% of source tokens are replaced with random ones.\n    *   mBART+replace(dec): 10% of target tokens are replaced with random ones.\n4.  Contextual Probing: An extensive analysis is conducted using probing methods where idiom words are encoded within different contexts (full context, phrase context, word context). The effect on decoder distributions and translation outputs is measured. This involves:\n    *   Translation Performance Variation: Decoding different encoder representations (where idiom words' representations are replaced with those from narrower contexts) and evaluating against reference translations using ChrF.\n    *   Translation Likelihood Variation: Varying encoder idiom representations and measuring the perplexity of reference translations under the model.\n    *   Decoder Uncertainty: Measuring the entropy of the decoder's distributions for target tokens, distinguishing between those aligned to idiom words and non-idiom words, under teacher-forcing conditions.\nThis systematic approach allows for quantifying the targeted improvements from pretraining, even in zero-shot settings, and analyzing how context affects idiom translation and model \"\"myopia\"\" versus \"\"contextuality.\"\"": 1517,
    "The paper addresses this by introducing STBridge, a novel plug-and-play approach that seamlessly extends a frozen text-conditioned R-VOS model to accommodate noisy speech inputs. STBridge incorporates a speech encoder (Wav2Vec2 augmented with linear layers to predict noise type) to extract noisy speech embeddings (`gns`) and noise embeddings (`gn`). During training, STBridge leverages triplets of video, text, and noisy speech to align the query spaces between text and speech. A crucial aspect is the semantic alignment constraint, which applies an L2-Norm loss (`Lalign`) between the pooled text embedding (`pool(gt)`) and pooled speech embedding (`pool(gs)`). This loose constraint ensures that the semantic spaces of text and speech queries are aligned. The Noise-aware Semantic Adjustment (NSA) module processes the noisy speech embeddings to produce cleaner speech embeddings (`gs`), and the Semantic Jitter Suppression (SJS) module processes text embeddings (or speech embeddings during inference) to generate robust object queries (`q`). During inference, the text branch is discarded, and the processed speech embedding `gs` is directly used as the input to the SJS module, which then generates the object query `q` for the frozen mask decoder `D(q,f)`. This design allows the well-trained R-VOS model to remain frozen, making the adaptation efficient and practical for real-world applications.": 1518,
    "The paper designs a unified framework, Domain Adaptive In-Context Learning (DAICL), that efficiently accommodates different language model architectures by devising distinct prompting and fine-tuning strategies tailored to their inherent structures. For encoder-only models, such as BERT-based architectures (e.g., XLM-RoBERTa-large), the retrieved target contexts (xT) are concatenated *after* the source input (xS) using a separation token: [xS; <SEP>; xT1; ...; xTk]. The fine-tuning strategy involves training the model to predict source input labels (e.g., using average pooling for Sentiment Analysis or a CRF layer for Named Entity Recognition) and simultaneously predicting masked tokens within the appended target contexts via Masked Language Modeling (MLM). This bidirectional structure allows the representation for each masked token to encode both target context and source input. For decoder-only models, such as LLaMA, the retrieved examples are *prepended* before the source input, forming a prompt structure like [prompt; xT; xS; y]. The fine-tuning strategy for these models uses Causal Language Modeling (CLM), where the model is trained to predict each token autoregressively across the entire prompt, including the target contexts, source input, and the response output. This single CLM objective intrinsically merges the task learning and language modeling objectives, as the loss is computed on all tokens, effectively learning both target distribution (from xT) and task discrimination (from y). Parameter-efficient approaches like Low-Rank Adaptation (LoRA) are used for fine-tuning larger decoder-only models to manage computational resources.": 1519,
    "The associative chaining method enables language models to learn systematic meta-sense extensions by focusing on the semantic proximity of meta-sense prototypes. This approach follows recent computational implementations of semantic chaining, predicting that a token `t(w,m)` with an existing meta-sense `m` can be extended to express a new meta-sense `m'` if `m` and `m'` share similar semantic feature values, meaning the semantic distance between their prototypes is small.\nThe model is trained using a contrastive learning objective. In each step, a meta-sense triplet `Mtrip = (m, m+, m-)` is sampled, where `(m, m+)` forms a systematic meta-alternation, but `(m, m-)` does not. Then, a lexical instantiation `w` of `(m, m+)` and another word `w'` with meta-sense `m-` are sampled. The language model is trained to minimize the following loss function: `Lassoc = - Σ Σ l(w, w')`, where `l(w, w') = ||h(w,m) - h(w,m+)||2 - ||h(w,m) - h(w',m-)||2`. This objective encourages the model to bring the prototype of `w`'s meta-sense `m` closer to the prototype of its systematically related meta-sense `m+`, while simultaneously pushing it away from the prototype of an unrelated meta-sense `m-` (represented by `w'`). This contrastive learning mechanism helps the model learn to identify and leverage the associative semantic relationships crucial for meaning extension.": 1520,
    "The paper achieves joint optimization of geometric and statistical domain alignment through a comprehensive overall objective function that combines multiple loss components. The total objective function in MNCRI is formulated as L = LCE + λLCon + µLMMD-IR. Here, LCE represents the standard cross-entropy supervised loss, calculated on the labeled source domain instances to ensure basic classification accuracy. LCon is the geometric domain alignment loss, derived from the mutual nearest neighbor contrastive learning paradigm (as detailed in Solution 1), which focuses on aligning common semantic characteristics and separating private ones. LMMD-IR is the statistical domain alignment loss, incorporating the instance re-weighting scheme (as detailed in Solution 2), designed to reduce distribution discrepancy while mitigating excessive alignment by considering instance relevance. The parameters λ and µ are trade-off weights that control the relative importance of the geometric and statistical alignment components, respectively, allowing for a balanced optimization. By minimizing this combined loss function, the framework simultaneously learns discriminative features for classification, aligns common semantic features geometrically, and reduces statistical distribution discrepancies in a relevance-aware manner. This joint optimization strategy enables the model to derive more robust and accurate cross-domain representations, leading to superior vulnerability detection performance.": 1521,
    "The paper proposes a robust three-step domain obfuscation approach to achieve effective domain counterfactual generation. The first step, Base Masking, initializes the mask using a frequency-based strategy adapted from prior work. This involves assigning an affinity score `ρ(w,D)` to a word `w` for a domain `D`, based on the probability of the word's presence in that domain and the non-uniformity of its probability distribution. Words with high affinity towards the source domain relative to the target domain, quantified by `ma(w,D,D') := ρ(w,D) - ρ(w,D')`, are masked if their `ma`-score exceeds a threshold `τ1`. The second step, Over-The-Top (OTT) Masking, then refines this by leveraging a fine-tuned language model (RoBERTa-base) as a domain classifier. It identifies additional domain-specific tokens by calculating attention norms (`mb(w;l)`) for words, masking those with scores above `τ2`. This step adds context awareness and inductive capabilities. Finally, the Unmasking step sequentially restores masked tokens that are not strongly domain-specific. It uses the domain classifier's confidence score to guide this process, unmasking tokens that cause minimal change in the original domain's probability (`mu(wki)` scores) until a threshold `τ3` is met. The resulting masked text, purged of source-domain-specific cues, is then fed into a T5-based encoder-decoder model for Domain Counterfactual Generation. This model, fine-tuned with an unsupervised sentence reconstruction objective and guided by a soft prompt representing the target domain, generates the counterfactual text.": 1522,
    "To generate a summary for a target domain document `x`, the paper obtains a target prefix by taking a weighted average of the source prefixes. Using the weights `W = {w1, w2, ..., wn}` computed in the previous stage, the target key prefix (`hTK`) and target value prefix (`hTV`) are calculated as `hTK = Sum(j=1 to n) [wj * hjK]` and `hTV = Sum(j=1 to n) [wj * hjV]`. This combined target prefix, `PT = {hTK, hTV}`, is then used with the frozen pre-trained language model `M` to generate the target summary `y = M(x; PT)`. The process of recomputing `hjK` and `hjV` for the target prefix involves replacing `Ej` with `ET` in the prefix tuning equation, where `ET` is initialized with the `C` most frequent sentencepiece tokens from the `m` unlabelled target domain documents. This weighted averaging scheme allows for the efficient combination of knowledge from multiple source domains into a single, generalized prefix for the target domain.": 1523,
    "The quantitative relationship between embodiment and LM performance is determined through statistical correlation and regression analyses. For each language model, its performance on the CEmb dataset is scored as a binary value (correctly or incorrectly identified interpretation). This series of binary values is then correlated with the continuous variable of embodiment ratings using the point-biserial correlation coefficient, along with its associated p-value. Additionally, linear regressions are conducted for all models (and their different sizes) to predict task performance based on embodiment score, AoA, word frequency, and word length. These regressions are performed both with and without the embodiment score feature. The coefficient of determination (R2) is then compared between these two sets of regressions. A consistently higher R2 when the embodiment score is included indicates its contribution to explaining the variance in LM performance, thereby quantifying its relationship with interpretation accuracy.": 1524,
    "Co-training with task decomposition involves decomposing the SSDA framework into two sub-components: UDA and SSL. Two distinct classifiers are trained separately—one in the UDA setup and another in the SSL setup. These classifiers iteratively teach each other by exchanging high-confidence predictions on unlabeled target data. The predictions from one classifier are added to the training set of the other classifier, allowing both classifiers to benefit from each other's knowledge. This iterative teaching process ensures that both classifiers can adapt and improve their performance in the target domain over time.": 1525,
    "The paper adapts and evaluates two dialogue system tasks, response retrieval and response generation, to specifically address similes within dialogue contexts using the Multilingual Simile Dialogue (MSD) dataset.\nFor Response Retrieval, the task is defined as a ranking problem. Given a multi-turn dialogue context and multiple response candidates (including the correct one), the model must rank the candidates such that the correct response receives the highest score. For each dialogue context in the MSD simile data (both English and Chinese), 19 negative responses are randomly selected from other dialogues. The baseline used is BERT-base. The dialogue context and each response candidate are concatenated as input to the pre-trained model. The output of the first input token (`<cls>`) is used to compute a score for the input sequence. An English dialogue retrieval model, BERT(Reddit), is trained on a large Reddit dialogue dataset, and a Chinese dialogue retrieval model, BERT(Ch), is trained on a combined LCCC and PchatbotW dataset. These trained models are then evaluated on the MSD-En and MSD-Ch simile data, respectively, and compared against their performance on their original test sets.\nFor Response Generation, two types of generation tasks are considered:\n1.  Traditional Response Generation: This task uses the dialogue context as input to generate the full response. Baselines include DialoGPT and GODEL for English data, and T5-base, BART-large, GPT-2, and CDialGPT for Chinese data. These models are evaluated on the MSD simile data and compared to their performance on the larger Reddit-dialogue and LCCC+PchatbotW test sets.\n2.  Response Completion: This is a novel generation task where the model completes a response sentence *after* a given comparator. For example, given \"\"Arguing with parents is not wise. It is like\"\" as input, the model is asked to generate \"\"throwing an egg at a rock.\"\" This task specifically tests the model's ability to understand the dialogue context and complete a simile expression. Experiments for this task are conducted on English data using DialoGPT, demonstrating how simile generation can benefit from explicit guidance (the presence of the comparator).": 1526,
    "To boost performance while maintaining parameter efficiency, the paper incorporates an efficient prompt tuning strategy, specifically prefix-tuning. In this approach, the parameters of the pre-trained language model (PLM) are fixed during training. Instead, only the parameters of newly added prefix embeddings are trained. These trainable prefix embeddings are added into each attention layer of the PLM. This method significantly reduces the number of trainable parameters (by a factor of 10x compared to fine-tuning the entire PLM), making the training process more efficient. During inference, only the main task is performed: the model queries for all slot types and directly generates the corresponding slot entities. This strategy also eliminates the need for additional span matching mechanisms, making the overall process more concise. The paper compares this prefix-tuning approach with traditional fine-tuning, demonstrating that prefix-tuning exhibits better knowledge transferability.": 1527,
    "After the initial data collection, each sample undergoes a validation process performed by a separate set of workers who are fluent in the respective language. These validators are tasked with reviewing the collected examples to ensure their quality and adherence to the specified format. Any examples found to be incoherent, offensive, or not conforming to the required structure are rejected. This two-stage process of collection by one set of native speakers and validation by another aims to maintain the integrity and cultural appropriateness of the MABL dataset.": 1528,
    "The proposed Masked Audio Text Encoder (MATE) is designed as a multi-modal Masked Language Model (MLM) rescorer that is compatible with encapsulated ASR systems. This means MATE can work with any first-pass ASR models, including Hybrid, CTC (Connectionist Temporal Classification), or Transducer architectures. The rescorer is intentionally made agnostic to the specific ASR architecture, its training mechanism, and its internal features. This is achieved by taking the output of the first-pass ASR (text transcriptions) and the raw audio input, processing them independently through a pre-trained Masked Language Model (BERT) for text and a self-supervised learning (SSL) based speech encoder (WavLM) for audio, and then integrating these representations via a cross-modal adaptation module. This decoupled design allows MATE to generalize better across different ASR systems without requiring specific knowledge or modifications related to their internal workings.": 1529,
    "The paper addresses the construction of more realistic few-shot SLU benchmarks by designing a dynamic sampling strategy. Unlike traditional uniform random sampling, this strategy allocates the number of samples for each intent and slot based on their individual learning difficulty, as determined by evaluation metrics. The dataset is built through a \"\"sampling-iterative\"\" process. In each iteration, the total number of samples is limited, but the dynamic sampling strategy assigns samples to categories (intents or slots) based on their F1 scores from the previous iteration. For categories that are easy to classify (high F1 score), fewer samples are allocated, while for hard-to-classify categories (low F1 score), more samples are allocated. The pseudo-code for this strategy shows an initialization step where the number of samples for each class (`num_samp`) is set to zero, and initial F1 scores (`F1[0][i]`) are set to 1. In subsequent iterations (`k`), the total number of new samples (`∆`) is distributed among categories. The number of samples to be added for each category `j` (`temp`) is calculated using a formula that incorporates the current F1 score (`F1[k][j]`) and a weighting factor that changes with iterations, ensuring that categories with lower F1 scores receive more samples. After sampling, a model is trained and evaluated, and the F1 scores for each class are updated, guiding the sampling in the next iteration. This iterative and adaptive sampling process ensures that the constructed benchmarks, FewShotATIS and FewShotSNIPS, better simulate real-world scenarios where data availability might be limited and learning complexities vary.": 1530,
    "To capture fine-grained alignment patterns between Metaphor Detection (MD) and Basic Sense Discrimination (BSD), the AdMul framework utilizes two Local Discriminators (Qlcd). This approach is inspired by the observation that basic senses in BSD correspond to literal senses in MD, and non-basic senses in BSD correspond to metaphorical senses in MD. Specifically, literal samples in MD (class 0) and basic samples in BSD (class 0) are forcibly pushed closer in the feature space, while metaphorical samples in MD (class 1) and non-basic samples in BSD (class 1) are clustered closer.\nThe process involves the shared feature extractor Qf generating a semantic discrepancy feature `v` from the `[CLS]` token's hidden state, and a task-specific classifier Qy then predicts a normalized label distribution `ˆy` for each sample `xi`. This `ˆy` represents the probability of `xi` belonging to class `c` (`ˆyci`). The local discriminators `Qlcd` then take `ˆyci * Qf(xi)` as input, effectively using the label distribution as an attention weight applied to the sample's feature. This allows for a \"\"soft\"\" alignment, considering the model's confidence in a sample's class.\nSimilar to the global discriminator, the training of the local discriminators is adversarial. Each `Qlcd` attempts to predict the source task (`d`) of the features belonging to its specific class `c`. Concurrently, the shared feature extractor Qf is trained to generate task-invariant features that fool these local discriminators. By aligning samples within corresponding classes across tasks, the model enhances the distinction between literal/basic and metaphorical/non-basic senses. For instance, pushing literal and basic senses closer helps to clearly separate metaphorical senses from literal ones, and improving BSD performance through this alignment further strengthens knowledge transfer to MD. A task weight `wd` is used to maintain the dominance of MD in local alignment, with `w0=1` and `w1=0.3` in experiments.": 1531,
    "To analyze and correlate specific metaphor types with distinct register properties, the compiled corpus and its differentiated metaphor annotations were leveraged by calculating the percentages of different metaphor types (conventionalised, non-conventionalised, extended, potential, and flags) as word tokens within each of the five subcorpora. These percentages were then examined for clear differences across registers. The analysis specifically focused on identifying patterns and correlations between metaphor usage and the pre-defined SFL and Biber dimensions characterizing each register. For instance, the study investigated how the level of conventionalised metaphors varied across registers (e.g., high for speeches, low for fiction). It also examined the distribution of non-conventionalised and extended metaphors, arguing that their prevalence in sermons and commentaries correlated with the \"\"highly persuasive\"\" register property. The impact of \"\"oral\"\" versus \"\"literal\"\" discourse on metaphoricity was also investigated, as was the influence of tenor dimensions like \"\"hierarchical\"\" versus \"\"equal\"\" interlocutor relationships. The methodology involved statistical analysis to determine the significance of these correlations, as indicated by p-values (e.g., p<.05, p<.01, p<.0001). For example, the correlation between highly persuasive registers and the occurrence of non-conventionalised and extended metaphors was found to be statistically significant. The analysis also considered how specific register characteristics, such as the time pressure in oral debates or the collaborative nature of dialogue, might influence the creation and distribution of certain metaphor types like extended metaphors. By systematically comparing metaphor counts across subcorpora defined by their register properties, the framework allowed for the identification of which metaphor types function as effective register markers.": 1532,
    "The paper proposes a novel pipeline for unsupervised domain adaptation that simultaneously adapts both the dense retrieval model (bi-encoder) and the rerank model (cross-encoder) within a two-stage retrieval system. The process unfolds in three interconnected steps. First, a pseudo-training dataset is constructed for the target domain. This involves generating pseudo-queries for target domain passages using a pre-trained T5 model. To ensure diversity, three pseudo-queries are generated for each passage. Negative samples are then identified using two retrieval approaches: BM25 (a sparse retriever) and a bi-encoder trained on MSMARCO (a dense retriever). For each query, challenging irrelevant passages are selected from the top-1000 retrieved by both models, forming a negative training dataset (Dneg). Positive training samples (Dpos) are created by extracting top-scoring query-passage pairs from the generated pseudo-queries using an MSMARCO-trained cross-encoder. The combined Dpos and Dneg form the final pseudo-training dataset (Dce) for the cross-encoder. Second, the cross-encoder (rerank model) is adapted to the target domain using the Dce dataset. This adaptation employs the \"\"denoise-finetuning\"\" approach (as detailed in Solution 1), which mitigates the impact of noisy labels in the pseudo-dataset through random batch warm-up and co-regularization learning. This step ensures the rerank model effectively captures domain-specific relevance. Third, the knowledge from the now domain-adapted rerank model is distilled into the dense retrieval model (bi-encoder). This is achieved using knowledge distillation with a \"\"Mix\"\" teacher model (as detailed in Solution 2), which combines the unadapted and adapted cross-encoder's margin predictions to provide a robust teaching signal. This sequential adaptation, where the improved rerank model provides better pseudo-labels for the bi-encoder's distillation, allows for a comprehensive and effective adaptation of the entire two-stage retrieval system to the target domain without requiring human-labeled data.": 1533,
    "Parameter sharing between prompt generation and incorporation is enabled by training the model using a multi-task framework. The overall loss function `L` is defined as a weighted sum of two components: `L = α · L'NER + (1 - α) · Lgen`. Here, `L'NER` denotes the normalized loss function for the Named Entity Recognition (NER) task, which can be either the cross-entropy loss for standard fine-tuning or a probability maximization loss for prompt-tuning. `Lgen` is the loss function for type-related feature selection (as described in Solution 2). `α` is a weight assigned to `L'NER`, and `(1 - α)` is assigned to `Lgen`. The model parameters are iteratively refined by optimizing this combined loss function using an AdamW optimizer. This joint training strategy ensures that the prompt generation process is optimized to produce prompts that are beneficial for the downstream NER task, and vice versa, leading to improved overall model generalization.": 1534,
    "To alleviate the catastrophic forgetting problem caused by the gradual shift in domain difficulty in curriculum learning, the paper proposes RE-GEM (Relaxed Gradient Episodic Memory), a modified version of the Gradient Episodic Memory (GEM) continual learning algorithm.\nThe core idea of GEM is to prevent the increase of losses on previously learned data (stored in an episodic memory `Mk`) while learning new data. RE-GEM modifies the gradient update rule of GEM, which is typically used when the angle between the current gradient `g` (from new data) and the reference gradient `gref` (from memory data) is greater than 90 degrees. In such cases, standard GEM projects `g` to ensure `g⊤gref ≥ 0`, prioritizing not forgetting.\nRE-GEM, however, prioritizes learning from new data while still trying to alleviate forgetting. When `g⊤gref < 0` (angle > 90 degrees), instead of projecting `g`, RE-GEM projects `gref` to `˜g` such that `˜g⊤g ≥ 0`. This projected `˜g` is calculated as `˜g = gref - (g⊤ref g / g⊤g) * g`.\nThe parameters `θ` are then updated based on *both* `g` and `˜g`. Specifically, Algorithm 2: TRAIN (which is called by Algorithm 1) incorporates this logic:\n1.  For each batch `(X, Y)` from the current training data `Pt`, a reference example `(Xref, Yref)` is sampled from the model's episodic memory `M`.\n2.  The gradient `g` is computed for the current batch, and `gref` is computed for the reference example.\n3.  If the dot product `g⊤gref` is non-negative (meaning the gradients are aligned or orthogonal, so learning new data doesn't significantly conflict with old knowledge), the model parameters `θ` are updated simply by `θ ← θ - αg` (standard gradient descent).\n4.  If `g⊤gref` is negative (meaning the gradients conflict, indicating potential forgetting), `gref` is projected to `˜g` using the formula `˜g = gref - (g⊤ref g / g⊤g) * g`. The parameters are then updated by `θ ← θ - α(g + ˜g)`. This ensures that the update direction `(g + ˜g)` still makes progress on the current task (`g`) while attempting to move in a direction that also respects previous knowledge (`˜g`).\nThis modification allows the model to successfully learn from new, harder examples while actively mitigating the forgetting of knowledge acquired from earlier, easier examples.": 1535,
    "Prompt generation and encoding are implemented to obtain representations of variables L, S*, E*, and C. Special tokens are used to mark entity pairs, and placeholders separate variables. The entire input instance is fed into the encoder to produce embeddings. These embeddings are then concatenated to form final representations for causal operations. This method facilitates the acquisition of representations for causal effect estimation.": 1536,
    "The Multimodal Figurative Language Retrieval Task is designed to specifically examine Vision and Language Pre-Trained Models' (VL-PTMs) preference for figurative images. Given a set of both figurative and partially literal images for a figurative phrase, the task's objective is to rank the images using the model's matching scores such that the figurative images are ranked higher than the partially literal ones. This task provides a deeper insight into how the model comprehends figurative language in terms of its preferences. The evaluation metric used is precision@k, where 'k' represents the number of figurative images present in the input set. For example, for the idiom \"\"ruffle someone’s feathers,\"\" the task aims to have images depicting causing discomfort ranked higher than pictures of birds and feathers. The paper also investigates if fine-tuning can improve this preference by training a supervised model on a separate set of images, aiming to reduce the models' bias towards literal interpretations.": 1537,
    "The paper introduces a new iterative label set semantics inference (ILSSI) method to address the overlapping problem of unseen and seen slots in previous evaluation methods and to alleviate the multiple slot types prediction issue. Instead of splitting test sets based on sample granularity (e.g., an utterance containing any unseen slot type is classified as an unseen sample), ILSSI splits test sets based on slot type granularity. This means that for a given utterance, each slot type is evaluated separately as either seen or unseen. The training samples are constructed as (St, At, Ui, Y'i), where St is a target slot type, At represents all other slot types, Ui is an utterance, and Y'i is the BIO label for St. Positive training samples are generated by setting each existing slot type in an utterance as St, while negative samples are generated by choosing slot types that belong to the overall set of slot types but do not appear in the utterance. During testing, ILSSI iteratively feeds these constructed training samples into HiCL, which outputs a BIO label for each target slot type. This allows for the unbiased and separate evaluation of unseen and seen slot types, as the model is trained and tested specifically for individual slot types.": 1538,
    "A top-down greedy layer-wise training procedure is employed to learn sparse domain-invariant representations. Instead of training all mask layers simultaneously, the training is performed sequentially from the top layer to the bottom layer. For each layer `(L-i)`, a new filtering layer `mL-i` is introduced on top of the `(L-i)`-th transformer layer. Crucially, during this phase, all previously trained mask layers (from `L` down to `L-i+1`) are frozen to preserve their learned parameters. This iterative process continues until the mask layer for the most bottom layer `m1` is trained. A suite of models, `θL` to `θL:1`, is collected throughout this process. The model's efficacy is determined by evaluating its performance on the validation set from the sourced domain, and the best-performing model is chosen for testing on target domains. The objective function for training combines a cross-entropy loss `Lce` with a sparsity regularization term `Llsparsity` for the current layer's mask, and for multi-class classification, an additional distance regularization term `Ldist` is included to encourage label-specific feature extraction.": 1539,
    "Semantic errors in LLM-generated logical forms are minimized through an iterative Execution-Guided Refinement (EGF) process. After an initial logical form is generated by the LLM, it is executed over the Knowledge Base (KB). If the execution returns an empty answer, this feedback is verbally provided back to the LLM. The LLM is then prompted to correct the erroneous logical form, using the previous generation and the empty answer feedback as context. This iterative process continues until a non-empty answer is obtained upon execution, or until a maximum number of iterations is reached. Each subsequent prompt includes the sequence of previous generations and their corresponding execution feedback, allowing the LLM to learn from its mistakes and refine the logical form.": 1540,
    "The overall framework, C4MMD, enhances the metaphor detection capabilities of smaller models by strategically leveraging the rich contextual understanding and world knowledge of Multi-modal Large Language Models (MLLMs) without directly fine-tuning the MLLMs for the task. This is achieved through a knowledge summarization and transfer process. The MLLMs are used as intelligent knowledge generators, producing supplementary textual information in the form of image descriptions (mI), text analyses (mT), and mixed modality interpretations (mMix) through a three-step Chain-of-Thought prompting method. This generated knowledge is then treated as additional input for the smaller, downstream model. Specifically, `mI`, `mT`, and `mMix` are concatenated with the original text `xT` and fed into the smaller model's text encoder (XLMR-Encoder). The text encoder is designed with segment encoding to differentiate between these various textual inputs, allowing the smaller model to process and integrate the MLLM's insights effectively. The downstream multi-modal fusion module then processes these enhanced features, combining them with the original image features. Furthermore, the framework incorporates two auxiliary tasks—detecting image-dominated and text-dominated metaphors—which act as regularization, forcing the smaller model to learn and utilize the MLLM-derived knowledge to identify metaphorical features within each modality before final fusion. This comprehensive strategy allows the smaller model to benefit from the MLLM's deep understanding, leading to improved metaphor detection performance.": 1541,
    "The paper addresses this by introducing an Importance Rating task for self-disclosures. The task involves estimating how important a disclosure is for others to understand the user's message and communication goals, given the disclosure span and its surrounding context. Three levels of importance are defined: low (can be removed), moderate (essential but can be abstracted), and high (must be kept as is). For disclosures in posts, the context includes the title and body; for comments, it extends to the entire comment and its parent comment. Training data for this task is human-annotated by three in-house annotators, with majority vote or \"\"moderate\"\" used for training labels in cases of disagreement. The model for this task is built by fine-tuning Llama-2-7B, using the same setup and hyperparameters as the abstraction experiment. Various input-output formats are explored, including generating reasoning (thought process) that leads to the human-assigned ratings using GPT-3.5. However, fine-tuning on the thought process was found to degrade performance. The model's accuracy is measured by considering a prediction correct if it matches any of the three human annotations, accounting for the subjective nature of the task.": 1542,
    "The method unifies the multi-stage process by combining the individual prompts for keyword extraction, dialogue simulation, and dialogue-enhanced reasoning into a single, comprehensive prompt. Instead of performing three separate inference calls, the entire sequence is executed in one go. This unified prompt (P) is constructed by concatenating P1 (for keyword extraction), P2 (for dialogue simulation), and P3 (for dialogue-enhanced reasoning). The LLM (M) then processes the task description (T), the specific question (Q), and this combined prompt (P) in a single inference call: K, S, R = M(T ⊕ Q ⊕ P). The LLM is designed to sequentially generate the keywords (K), the simulated dialogue scenario (S), and the final reasoning and answer (R) within this single response. This approach streamlines the overall process, requiring only one inference operation through the LLM to obtain the complete answer, thereby enhancing efficiency and practical applicability.": 1543,
    "The pragmatic accuracy of LLM-generated responses (`UN2`) is automatically evaluated by comparing them against the true intention reference response (`UT2`) and the incorrect literal intention reference response (`UL2`) using similarity measurements. The core metric for success is whether the similarity between the model's response and the true intention response is greater than the similarity between the model's response and the literal interpretation response, formalized as `sim(UN2, UT2) > sim(UN2, UL2)` under the given `Context (C)`. To implement this, GPT-4 is utilized as a `Contextual Similarity Evaluator`. GPT-4 is prompted with the `Context (C)`, the `speaker's non-literal utterance (UN1)`, the `True Intention (IT)`, the `Generated Response (UN2)`, and the two reference options (`UT2` and `UL2`). GPT-4 then identifies which of the two reference options (`UT2` or `UL2`) is closest to the `Generated Response (UN2)`. The options are randomly shuffled to prevent bias, and a temperature of 0 is set for GPT-4 to ensure stability in evaluation. This GPT-4-based evaluation serves as a proxy for human judgments, which are considered the gold standard and show good agreement with GPT-4's assessments. Additionally, non-contextual cosine similarity using Llama-3-8B-Instruct embeddings (obtained via LLM2Vec) is explored, though it aligns less with human annotations than GPT-4's contextual similarity.": 1544,
    "To evaluate the generalization of LLM unfunning to other languages and forms of humor, the paper applies the approach to a code-mixed English-Hindi humor dataset. GPT-4 is prompted to \"\"unfun\"\" humorous tweets from this dataset. A secondary filtering step is implemented to remove low-quality results, where GPT-4 itself re-classifies the unfunned outputs, and any still deemed humorous are discarded. This ensures the generated non-humorous examples are of high quality. The resulting synthetic unfunned data undergoes human evaluation by bilingual annotators who rate them for humor and coherence, comparing them against original non-humorous human tweets. For automatic evaluation, the performance of humor classifiers (specifically, an XLM-ROBERTA model fine-tuned on English-Hindi Twitter data) is tested. The classifiers are trained with different proportions of this synthetic non-humorous data incorporated into the original corpus. The evaluation assesses whether the classifier, when trained on the original dataset, can generalize to the human-vetted unfunned samples, thereby determining if the synthetic data provides challenging adversarial examples that expose the classifier's reliance on superficial features.": 1545,
    "The paper integrates the textual elaboration and visual binding processes into a cohesive framework called GOME, which is a collaboration between large language models and text-to-image models. The overall workflow of GOME involves three main stages. Firstly, \"\"data collection\"\" is performed by preprocessing a collection of linguistic metaphors sourced from previous research, which are then post-filtered by an LLM to ensure they are visualizable. Secondly, the \"\"grounding-based visual elaboration\"\" step utilizes an LLM (GPT-4) to generate detailed textual descriptions. This step, as described in Solution 1, employs a Chain-of-Thought (CoT) prompting method with integrated rhetorical knowledge to produce fine-grained metaphorical elements (tenor, vehicle, groundings) and visual elaborations that focus on underlying meanings rather than literal objects. Finally, in the \"\"metaphor depiction\"\" or \"\"scenario visualization\"\" step, the paired data of metaphors and their generated visual elaborations are fed into a diffusion model, specifically Stable Diffusion. This stage incorporates the novel \"\"metaphorical attribute-object binding\"\" method, as detailed in Solution 2. The elaboration outputs, including the groundings and visual descriptions, undergo a syntactic analysis process to extract the necessary object-attribute binding pairs. These extracted binding pairs then inform the inference-time optimization of the diffusion model through cross-attention control, ensuring that metaphorical attributes are accurately registered and blended with the target objects in the generated image. This integrated approach ensures that the abstract concepts and underlying groundings of metaphors are faithfully captured and depicted in the final visual illustration, addressing the issues of over-literalization and infidelity in visual metaphors.": 1546,
    "The Reconstruction module synthesizes the information processed by the Pun Grounding and Disambiguation modules to reconstruct the intended message of the pun. This involves integrating the disambiguated text with the grounded visual context to form a coherent understanding of the pun's message. The module employs a synthesis algorithm that merges these inputs, ensuring that the reconstructed message aligns with both the textual intent and the visual cues.": 1547,
    "The paper addresses this through Auto-correction with Syntax Error Feedback (Auto-SEF). This mechanism leverages the ability of a theorem prover (specifically Isabelle/ZF) to check the validity of a formal code ϕ and output a set of syntax errors {ek} = TP(ϕ) if it's invalid. These reported syntax errors are then used as feedback to guide the LLM's correction process. A specific prompt, perr, is designed to enable the LLM to recognize previously produced errors and correct mistakes. To maintain semantic consistency, retrieved examples {(si,ϕi)}s are also used during this generation step: g(s) = LLM(perr, {(si,ϕi)}s, {ek}, d(s)), where d(s) is the denoised output. This process is iterative, meaning gk+1(s) = LLM(perr, {(si,ϕi)}s, ek,1, gk(s)), with an initial state g0(s) = d(s) and ek,1 being the first item in the syntax errors reported by the theorem prover for gk(s). This iterative feedback loop allows the LLM to progressively refine the formalization results based on concrete error signals from the theorem prover.": 1548,
    "Large language models (LLMs) are leveraged to assist human annotators in identifying subtle inconsistencies through a \"\"Hybrid\"\" annotation method. In this method, GPT-4 is prompted to generate multiple possible inconsistencies between a given story and its summary. The prompts instruct GPT-4 to identify potential inconsistencies and provide arguments for why a detail might or might not be inconsistent, ensuring that the model always suggests a possible inconsistency. This approach aims to surface errors that human annotators might otherwise miss. These LLM-generated inconsistencies, along with their explanations, are then presented to three new Upwork workers. These workers read the suggested inconsistencies before labeling the summary overall and provide a short written response justifying their agreement or disagreement with each suggested inconsistency. This process is designed to support human annotators by providing specific points of potential inconsistency, thereby improving the coverage of detected errors. The outputs from this hybrid method are then manually reviewed and filtered by the authors to ensure only legitimate errors are included in the final expanded gold set of labels, acknowledging that LLM suggestions can sometimes be incorrect or misleading.": 1549,
    "During inference on unseen test data points, where the multimodal interaction type is unknown, the outputs of the specialized expert models are dynamically combined through a soft weighted fusion mechanism. The core challenge is to accurately estimate the potential multimodal interaction type for each new data point. The method assumes that determining the interaction type is a sub-task that can be performed even if the final task label prediction is difficult. To achieve this, a dedicated \"\"fusion model\"\" is trained to classify the interaction type (redundancy, uniqueness, or synergy) for each data point. This fusion model, which in practice is a fine-tuned BLIP2-based classifier, dynamically infers weights (wr, wu, ws) for each of the three expert models (fr, fu, fs). These weights reflect the estimated probability or relevance of each interaction type for the given input. The final prediction, denoted as ŷ, is then obtained by a weighted sum of the outputs from each expert model: ŷ = Σi∈{r,u,s} wi fi(x1, x2), where fi(x1, x2) is the output of expert model i for input modalities x1 and x2. The fusion model is trained using focal loss to address potential class imbalance in the interaction types. Ablation studies demonstrate that this model-based fusion generally provides the most significant improvement compared to simpler model-free fusion methods like average, maximum, or fixed-weighted fusion.": 1550,
    "To address the challenges of mention detection in Large Language Models (LLMs) for end-to-end Major Entity Identification, the paper proposes a novel two-stage prompting strategy. The first stage, \"\"Word-level MEI,\"\" focuses on overcoming the difficulty LLMs have with nested mentions and general mention detection. Instead of asking the LLM to identify full spans directly, it is prompted to detect and tag only the syntactic heads of mentions that refer to the major entities. Other words are implicitly assigned to a null entity. This is achieved through few-shot examples where contiguous sets of words annotated with the same entity are processed, and their syntactic heads are extracted using a tool like spaCy. The LLM's output for this stage is a text with words tagged with entity IDs (e.g., \"\"lady#2,\"\" \"\"Alice#1\"\"). The second stage, \"\"Head2Span (H2S) retrieval,\"\" takes the entity-tagged heads from the first stage along with the original document. A separate prompt instructs the LLM to expand each identified head into its complete mention span, including any determiners and adjectives. For example, \"\"lady#\"\" would be expanded to \"\"That lady in the BMW.\"\" This structured generation task prompts LLMs to reproduce documents while inserting MEI tags at specific locations. For proprietary models like GPT-4, the Needleman-Wunsch algorithm is used to align documents and extract tags in case of reproduction failures. For open-source models, regular expression-based constrained decoding is employed. This two-stage approach effectively decouples the challenging mention detection task from the entity linking task, allowing LLMs to leverage their strong linking capabilities while mitigating their weaknesses in precise span identification.": 1551,
    "To evaluate LLMs' ability to generate puns, the paper introduces two settings: free and constrained generation. In the free setting, LLMs can freely choose contexts based on given pun pairs. In the constrained setting, LLMs must utilize provided contextual words as much as possible. This allows for assessing LLMs' capacity to generate puns both freely and under constraints. Metrics such as Ambiguity, Distinctiveness, Surprise, One-pun-word Incorporation Rate, and Contextual Word Incorporation Rate are used to evaluate the generated puns. Additionally, manual indicators like Success Rate and Funniness Rating provide further insight into the quality of generated puns.": 1552,
    "The paper addresses the challenge of attributing unregistered proverbs by leveraging conformal prediction, a methodology for constructing prediction intervals that provide statistically rigorous guarantees. Specifically, it equips a multi-class text classification model (Logistic Regression, chosen for its performance) with conformal prediction. This approach, following the Least Ambiguous set-valued Classifier (LAC) strategy, generates a set of possible locations for each unregistered proverb, rather than a single class label. The conformity scores are calibrated on a development set, and an alpha value of 0.05 is set, ensuring that the true class is contained within the prediction set with a specified probability (found to be 97% on a separate test set). This method quantifies the uncertainty of predictions made by the machine learning model, providing a set of locations each unregistered proverb could possibly have been collected from, accompanied by mathematical guaranteed coverage. This allows experts to focus their efforts on a narrowed set of potential origins.": 1553,
    "This is addressed in the \"\"Triggering Specialization\"\" phase, which is the core of the self-specialization process. Upon establishing a set of domain-specific instructions and responses generated in the previous phases, the base model (Mbase) undergoes tuning using this self-generated data. This tuning adjusts the model's internal parameters specifically to cater to the domain's nuances. The process employs parameter-efficient fine-tuning, specifically QLoRA (Quantized Low-Rank Adaptation), which adds only a small percentage (e.g., 0.28%) of trainable parameters to an otherwise frozen model. This allows for efficient training (e.g., a few hours on a single A100 GPU) and results in a lightweight specialization module. This targeted, parameter-efficient tuning on self-generated domain-specific data transforms the generally competent base model into a domain-specialized one (Maligned) while aiming to preserve cross-task generalizability.": 1554,
    "The paper proposes an autonomous workflow to automatically synthesize a multimodal, fine-grained assembly dialogue dataset called LEGO-MRTA. This process involves several steps:\n1.  Instruction Manual Crawling: 65 multimodal instruction manuals for LEGO brick assembly are crawled from the official LEGO website. These manuals contain illustrated images and textual instructions, including a summary and sequential multimodal step instructions with highlighted theme entities.\n2.  Tool Response Generation: A commercial Large Language Model (LLM) is used with prompt templates to generate candidate user requirements and decide on serving functional tools. Simulated responses are then recorded from templates for up to 6 tools per conversation session.\n3.  VLM-Based Question Answering (QA) Construction: Step instructions from the manuals are used to construct queries, including a special token (\"\"[detection]\"\") for object detection tasks. These queries, along with aligned images, are fed into MiniGPT-v2 to generate vision-language pairs in the format \"\"<Object><Xleft><Ytop><Xright><Ybottom>\"\". This process is iterated through all instruction steps to construct Vision Question Answering (VQA) pairs.\n4.  Multimodal Context-Aware Conversation Generation: A commercial LLM is used as the core of the workflow to generate conversations grounded in both the instruction manual and simulated tool responses. Instruction manuals are first chunked (reconstructed with a summary and 10 step instructions) to manage input token limits for the LLM. A designed prompt template (P3, Table 5, Appendix A) is instantiated with these chunked manuals. The commercial LLM then generates conversations, with a system prompt informing the language agent of its responsibilities and a query prompt used for each round of requests. Historical rounds of requests are tracked by memory. This automated synthesis ensures that the simulated conversations closely resemble natural human language by grounding them in both the instruction manual and responses from the Mixed Reality (MR) environment.": 1555,
    "The paper proposes a Pun Generation with Curriculum Learning (PGCL) framework that employs a multi-stage curriculum learning approach to optimize pun structure and humor preferences sequentially. This framework comprises two main components: a structure preference optimization module (Stage 1) and a humor preference optimization module (Stage 2). The core idea is to learn these two preference targets separately, progressing from \"\"easy\"\" to \"\"hard.\"\" In Stage 1, the model first learns the pun structure preference. This involves using Direct Preference Optimization (DPO) to align the LLM to generate sentences that strictly adhere to predefined pun structural rules (e.g., presence of pun words, homophonic/homographic distinctions). The output of this stage is a structurally optimized LLM. In Stage 2, the framework then focuses on learning the humor preference. This stage utilizes an improved triplet-based DPO algorithm to align the structurally optimized LLM with humor preferences. By first ensuring structural correctness and then refining for humor, the curriculum learning paradigm allows the model to build capabilities incrementally, preventing the inherent catastrophic forgetting problem often encountered in multi-objective preference alignment. This sequential optimization ensures that the model concentrates on the harder task (humor) without compromising the performance on the easier, foundational task (structure).": 1556,
    "The framework employs two paraphrase-based methods to enhance the diversity of generated labeled data: label-variant paraphrase and label-invariant paraphrase.\nFor Label-variant Paraphrase, a paraphrasing tool is directly applied to the original target domain unlabeled text (t) to generate a new paraphrased text (t'). Since the raw text is rewritten, the subsequent pseudo-label (l') extracted from t' using the extraction model (Me) can differ from any original pseudo-label. The generation model (Mg) then produces a new sentence (t'') based on this potentially new pseudo-label (l'). This approach introduces new labels and contexts, enhancing diversity.\nFor Label-invariant Paraphrase, the goal is to enrich the text patterns of existing target domain labeled samples while keeping their original labels unchanged. A paraphrasing tool is used to rewrite the text (t') of a labeled sample (t', l') into a new text (t''). Crucially, prompts are utilized to encourage the paraphrasing tool to include the original label (l') in the rewritten text. Post-processing methods are then applied to ensure that the paraphrased text (t'') indeed includes the original label (l'). This method transforms the context of the label, synthesizing more diverse data while maintaining the original label's integrity.": 1557,
    "The paper introduces a multi-head fusion mechanism to synthesize a cohesive joint speech-text representation, recognizing that different attention heads in pre-trained models capture distinct patterns and features. Instead of uniformly assigning attention scores or concatenating full representations, this mechanism initially dissects each representation—including the original unimodal representations (`ria`, `rit`), and the disentangled modality-invariant (`hia,inv`, `hit,inv`) and modality-specific (`hia,spe`, `hit,spe`) representations—into separate outputs based on their respective attention heads. For example, a representation `rim` is broken down into `{ri,(1)m, ..., ri,(H)m}`, where `H` is the number of attention heads. These head-specific outputs are then assembled into a matrix `Mi`, which includes all original, modality-invariant, and modality-specific components. Multi-head self-attention is performed over this matrix, ensuring that each representation becomes aware of other cross-modal, cross-subspace, and cross-head representations. The outputs from this fusion process are then concatenated to form the final input for the multimodal classifier, allowing the model to leverage the differential impact of each head.": 1558,
    "To maintain training stability when integrating diverse distributional feedback, the method employs a Stable Training Technique inspired by curriculum learning. This technique addresses potential instability that can arise from merging distributions with varying scales and characteristics. The core idea is to introduce data to the model in a specific sequence, progressing from simpler to more complex concepts. The learning difficulty for each document `d` is gauged by calculating the Kullback-Leibler (KL) divergence between the normalized domain-level distribution (`Norm(Rdom)`) and the normalized observation-level distribution (`Norm(Robs(d))`). This measure, `DKL(Norm(Rdom) || Norm(Robs(d)))`, quantifies how much a specific document's distribution diverges from the overall domain distribution. By gradually introducing increasingly challenging document distributions based on this KL divergence score, the model is guided to learn progressively, enhancing convergence and adaptability to the new corpus while preventing training instability.": 1559,
    "The paper formulates a \"\"Sense Selection\"\" task that goes beyond the binary classification of Words-in-Context (WiC) by leveraging the periphrastic definitions available in the structured dictionary. Instead of simply asking if two occurrences of a word have the same meaning, the Sense Selection task presents a model with a word, its usage context, and the full set of its possible meanings (represented by their periphrastic definitions). The system is then tasked with predicting which specific word sense is most likely employed in the given context. This reframing requires and benefits from high-quality contextual representations at both the lexical and sentential levels, as the word under scrutiny must be contrasted with all candidate definitions. The task is derived directly from the structured dictionary data, where each of the 14,416 usage examples is associated with an average of 3.8 candidate definitions, making it more faithful to the source data and providing a more nuanced evaluation of lexical semantic capacities.": 1560,
    "To investigate whether an additional auxiliary objective can further boost event trigger detection (TD) domain transfer, the proposed method integrates Masked Language Modeling (MLM) into the training process. This is achieved by adding a dedicated token-level MLM head to all model variants (implicit and explicit). The parameters of this MLM head are updated during training but are not utilized during inference. The MLM procedure involves masking 15% of randomly chosen tokens in the input sequence. Of these masked tokens, 80% are replaced with a special `[MASK]` token, 10% are replaced with a random token from the vocabulary, and the remaining 10% are left unchanged. The model is then trained to predict the original tokens for these masked positions. This auxiliary MLM objective is applied to the target domain training data. By performing MLM on the target domain, the model is encouraged to adapt its language understanding capabilities to the specific characteristics and distribution of the target domain's text. This domain adaptation, combined with the OIE-based coupling of triggers and relations, aims to reduce the trigger distribution shift between domains and consequently improve TD performance, especially in low-resource target domains. The training updates for the models' parameters are performed in an alternate fashion within each epoch: first based on the target domain MLM loss, and then based on the target few-shot TD loss.": 1561,
    "The paper proposes a Domain Positioning mechanism to address the challenge of selecting the correct domain-variant adapter during the test phase when the domain ID of a sample is unknown. This mechanism consists of two sub-modules: Domain Prototype Learning and Nearest Domain Indexing.\n1.  Domain Prototype Learning: Upon entering the test stage, the model has N domain-variant adapters and a corresponding domain-invariant adapter. To recognize different domains, this module learns a representation for each domain based on the training data. For each training sample xij in domain Di, the average of the last block's hidden representations from the LLM, h(xij), is computed. Then, for each domain Di, a mean (µi) is calculated as the average of h(xij) for all samples in Di. A shared covariance matrix (Σ) is also computed across all domains, representing the overall data distribution. These µi and Σ collectively represent the learned prototypes for each domain.\n2.  Nearest Domain Indexing: For a given test sample x, its hidden representation h(x) is computed. The Mahalanobis distance is then used to measure the similarity between h(x) and each domain prototype µi, taking into account the shared covariance Σ. The domain-variant adapter corresponding to the domain prototype (µi) that yields the minimum Mahalanobis distance to h(x) is selected as the most matching adapter for the test sample. This allows the model to dynamically index the appropriate domain-variant knowledge without requiring explicit domain IDs.": 1562,
    "The paper explores four distinct methods for evaluating the quality of generated metaphor reasons:\n1.  Automatic Evaluation: This method compares the generated reasons against the original dataset's reasons using standard natural language generation metrics. The metrics employed are BLEU (B1 for BLEU-1), ROUGE (R1 for ROUGE-1, RL for ROUGE-L), and METEOR (M). These metrics utilize n-gram matching mechanisms, with BLEU and ROUGE emphasizing precision and recall respectively, and METEOR additionally considering synonyms and stems.\n2.  Text-embedded Fine-tuning Model Evaluation: This approach involves training a RoBERTa-large model on entailment datasets, specifically Semantic Textual Similarity Benchmark (STS-B) and Sentences Involving Compositional Knowledge (SICK). The fine-tuned RoBERTa-large model then evaluates the similarity between the generated reasons and the reference reasons, providing scores on a scale of 1-5. This method aims to capture higher-dimensional semantic information compared to direct vocabulary distribution calculation.\n3.  ChatGPT Evaluation: This method uses ChatGPT itself to score the \"\"similarity\"\" between the original dataset's output (reference reason) and the prediction model's output (generated reason). A specific prompt is designed for ChatGPT, asking it to discuss the use of a target word in a sentence and then rate a given prediction based on a provided answer on a scale of 1-5. This leverages ChatGPT's understanding of complex semantic information.\n4.  Manual Evaluation: This is a human-centric method where three volunteers, with a background in metaphor, independently evaluate the correct answers (reference reasons) and the model-predicted answers (generated reasons). They score the \"\"similarity\"\" on a scale of 1-5, considering aspects such as semantics, fluency, and logic. The scores from the three volunteers are then averaged to provide the final evaluation score for each prediction, aiming to reduce subjectivity.": 1563,
    "Early Exit (EE) strategies are leveraged in a dual capacity within the framework: for accelerating inference and for actively enhancing the domain adaptation process. For inference, classifiers are attached to the intermediary layers of the Pre-trained Language Model (PLM). A sample is allowed to exit early from layer `i` if its prediction confidence, `Si` (defined as the maximum probability assigned to any class by the classifier at layer `i`), exceeds a predefined threshold `α`. If `Si ≥ α`, the sample's label is assigned as `argmaxc∈Cpti(c)` at layer `i`. If the confidence remains below `α` for all intermediary layers, the sample is processed until the final layer for inference. This adaptive inference strategy inherently speeds up processing for \"\"easy\"\" samples. Crucially, in the context of domain adaptation, these attached exit classifiers play a pivotal role in facilitating the multi-level adaptation. They serve as the points at which knowledge distillation is applied across all layers, as detailed in Solution 2. By enabling this layer-wise distillation, the exits help mitigate catastrophic forgetting and mode collapse during adversarial training, ensuring that domain-invariant features are learned throughout the entire PLM, not just at the final layer. This comprehensive domain alignment, supported by the exits, allows the same confidence threshold `α` that was learned on the source validation set to be directly and effectively applied to the target dataset, as the feature representations across domains become aligned at every layer. This robust alignment, therefore, enables the source-trained exit classifiers to be utilized directly for target domain inference without requiring additional target labels.": 1564,
    "The effectiveness of the structured generation process is enhanced by strategically augmenting individual sub-tasks with specialized language models or tools, a concept referred to as LLM augmentation. While a primary Large Language Model (LLM), such as FALCON-40B, is used for general generation tasks like User utterance generation (uu) and Agent utterance generation (au), other states can leverage different, potentially more specialized, models. For instance, in the Question answerability classification (ac) and Answer sentences selection (ss) states, an instruction-tuned model like FLAN-UL2-20B can be employed as an \"\"assistant.\"\" This is because `ac` and `ss` correspond to classification-like tasks, for which instruction-tuned models might be better suited or more efficient. Each state, whether using the primary LLM or an assistant, leverages a unique set of resources, including specific prompts and a designated number of in-context learning (ICL) exemplars. This modularity allows for the selection of the most appropriate tool for each sub-task, optimizing performance and resource utilization across the entire generation pipeline.": 1565,
    "The paper addresses this through the Knowledge Distillation stage of its Meta-Knowledge Distillation (MKD) strategy, which leverages a meta-teacher model to transfer domain-invariant knowledge. After the meta-learning stage (Solution 2), the teacher network (`f_theta_t`) is initialized with the meta-updated parameters (`theta`) that have captured domain-invariant knowledge across source domains. Concurrently, a student network (`f_theta_s`) is initialized with random parameters. The student network is then trained on the target domain (`T`) using a combined loss function, `L_all`, which balances supervision from ground-truth summaries and guidance from the teacher network. This combined loss is formulated as `L_all = (1 - lambda) * L_NLL + lambda * L_KD`, where `lambda` is a hyperparameter for balancing the two loss components. `L_NLL` represents the negative log-likelihood loss, which is the standard supervised loss derived from the ground-truth target summaries. `L_KD` is the distillation loss, which quantifies the difference between the softened distributions predicted by the teacher and student networks. Specifically, `L_KD` is calculated as the mean squared error between the hidden states of the teacher and student models: `L_KD = E [Sum_l=1^L (f_theta_t(yl|y1:l-1,x) - f_theta_s(yl|y1:l-1,x))^2]`. By minimizing this combined loss, the student network effectively learns to mimic the domain-invariant knowledge encoded in the meta-teacher while also adapting to the specific characteristics of the target domain, thereby facilitating robust cross-domain knowledge transfer.": 1566,
    "The dual-pruning importance score for each large language model (LLM) weight is efficiently computed using an approximation algorithm based on the empirical Fisher information matrix. The final importance score, Sm, is defined as an approximation of the Taylor series expansion of the regularized loss, Lours(Ds), with respect to the weight Wm. Specifically, Sm ≈ |∂Lours(Ds)/∂Wm * Wm + 1/2 * [∂Lours(Ds)/∂Wm * Wm]^2|. The term O(||Wm||^3) is neglected due to the quadratic approximation. The key to efficiency lies in approximating the diagonal of the Hessian matrix, which is computationally expensive to compute directly for large models. This approximation is achieved by leveraging the empirical Fisher index, which uses the average of the squared gradient of the model's prediction. This approach allows the calculation of Sm by considering both general and domain-specific knowledge through the regularized training objective, without updating the model weights during this calculation. These importance scores then guide the pruning decisions; for example, weights with the smallest importance scores within each layer are pruned to achieve a desired sparsity level.": 1567,
    "The paper addresses this by conducting a case study focusing on English and Chinese, utilizing two types of translated data. First, Machine Translation (MT) data is created by translating every Chinese proverb, context, and answer into English using Google Translate (Zh–En). Second, Human-Adapted Translation (HT) data is generated by taking the machine-translated context and performing manual corrections for literal translation mistakes and grammatical errors. Additionally, a light adaptation is conducted by replacing culturally specific names and locations with culturally aligned equivalents (e.g., \"\"Xiao Ming\"\" to \"\"Michael\"\") to reduce potential confusion for LLMs. The zero-shot evaluation is then performed on these MT and HT datasets using best-performing multilingual models (mT0-XXL, 13B) and an English model (LLaMA-2 13B). By comparing the performance on the original source language data, the MT data, and the HT data, the paper defines the \"\"language gap\"\" as the gain in performance from MT to HT (attributable to translation quality improvements). The \"\"culture gap\"\" is then defined and quantified as the absolute difference between the accuracy on the Human-Adapted Translation (AccHT) and the maximum accuracy achieved on either the original source language (AccSrc) or the original target language (AccTgt), i.e., |AccHT − max(AccSrc, AccTgt)|. This methodology allows for isolating the performance degradation specifically due to cultural common ground differences, rather than just linguistic translation issues.": 1568,
    "The paper addresses this question by proposing an innovative curriculum learning framework that automatically measures sentence difficulty, thereby overcoming the limitations of manual evaluation. Instead of relying on human-defined difficulty, the framework uses the training loss of a pre-trained model as the measure of sentence difficulty. Specifically, for a classification task like metaphor detection, the cross-entropy loss (dM(Yi)) of a given example (Yi) with respect to a pre-trained model (M) is used. Once the difficulty of all sentences in the dataset is determined, the training examples are re-arranged in ascending order of difficulty, from easy to difficult. The training process then starts with a subset of the easiest examples (initially 50% of the data) and gradually increases the proportion of training data as the number of epochs progresses, following a linear increase schedule. This strategy ensures that the model, when its learning ability is weak in the early stages, is exposed only to simpler data, preventing it from being overwhelmed by complex examples and wasting limited data. This dynamic re-arrangement and gradual exposure to more complex data effectively addresses the problem of data sparsity by maximizing the utility of available data and significantly improving the model's convergence speed and overall training efficiency.": 1569,
    "To address this, the paper first categorizes the unique source and target domains identified in the dataset into a systemic taxonomy of 14 ontological categories, based on the work of Gordon et al. (2015). This categorization process involves three steps: (1) preprocessing, where manual fixes for typos are applied to the domains; (2) measuring the relatedness between each individual domain and each of the 14 ontological categories using Dor et al. (2018)'s TermRelater tool; and (3) assigning the category with the highest relatedness score to the respective domain. Once all source and target domains are mapped to these ontological categories, the paper conducts an analysis similar to that for metaphor types. It calculates the count of each domain category across the challenging, no effect, and reinforcing editorial effects for both conservative and liberal ideologies. Significance tests (Anova or Kruskal, followed by post-hoc independent t-test or Mann-Whitney with Bonferroni correction if p < 0.05) are then applied to these counts to identify significant differences across the effects for each domain category. Effect sizes (r) are also computed to quantify the strength of these correlations.": 1570,
    "To maximize the \"\"High-prediction-effect\"\" (the principle that higher prediction rates increase training signals and boost performance), the paper formulates an entity recognition and classification task that is jointly optimized with the Mask-Specific Language Modeling (MSLM) loss.\nFor the entity recognition task, given an input sentence `s` of `n` tokens, where each token `xi` is tagged with a BIO label (Beginning, Inside, Outside), the model is designed to accurately extract entities. A probability distribution across all BIO labels (`ˆyi`) for each token is obtained using a softmax function applied to `f(hi ◦ W(ed))`, where `f` is a non-linear function, `◦` denotes vector concatenation, `hi` is the encoded representation of token `xi` from the language model's output (`H`), and `W(ed)` is a trainable weight vector (`R1×k`). The entity detection loss (`LED`) is then computed as a cross-entropy loss over these BIO labels: `LED = −Sum(yi,j * log ˆyi,j)` for all tokens `i` and BIO labels `j`.\nFor the entity linking/classification task, once an entity is detected, its span representation (`em`) is obtained by mean-pooling the encoded representations of its constituent tokens (`hi,...,hM`). A probability distribution across all predefined entity types (`E`) for this entity is then computed using `softmax(f(em ◦ W(ec)))`, where `W(ec)` is another trainable weight vector (`R1×d`). The entity classification loss (`LEL`) is also a cross-entropy loss: `LEL = −Sum(ylm * log ˆylm)` for all entity types `l`.\nThese task-specific trainable parameters, `W(ed)` and `W(ec)`, are introduced to enrich the Masked Language Model (MLM) representations, which are subsequently used in the token class prediction layer (for BIO labels) and the entity class prediction layer (for entity types), respectively. The overall model loss (`L`) is a joint optimization of all three cross-entropy losses: `L = LMSLM + LED + LEL`. This joint optimization ensures that the model receives more signals through gradient computation during training, enhancing its ability to detect mentions and improving overall domain-specific fine-tuning performance by leveraging the extra knowledge brought in from these parameters.": 1571,
    "The paper designs DUQGen as a comprehensive unsupervised domain adaptation framework that integrates several components to achieve efficiency and superior performance. The framework first employs its novel \"\"Domain Document Selection\"\" (Solution 1) to identify a small, yet highly representative and diverse set of documents from the target collection. This is followed by \"\"Synthetic Query Generation\"\" (Solution 2), where an LLM is prompted with in-domain few-shot examples to create high-quality, domain-specific queries for these selected documents. This judicious selection and generation process significantly reduces the required scale of synthetic training data, enabling the framework to achieve strong results with only a few thousand examples (e.g., 1,000 or 5,000 query-document pairs), a factor of x1000 less than some prior methods. After generating positive query-document pairs, \"\"Negative Pairs Mining\"\" is performed using hard negative mining: synthetic queries are parsed to a first-stage retriever (e.g., BM25, ColBERT, Contriever) to retrieve top-x documents, and the bottom-numneg documents are selected as negatives. Finally, the pre-trained neural ranker (e.g., MonoT5-3B or ColBERT) is \"\"Fine-tuning\"\" using this efficiently generated synthetic data, adapting the same hyperparameters used in the original MS-MARCO pre-training. This integrated approach ensures that the fine-tuning data is both representative and diverse, preventing overfitting and catastrophic forgetting, and allowing the ranker to effectively leverage target domain information, leading to consistent improvements over zero-shot baselines and substantial outperformance of prior unsupervised domain adaptation methods.": 1572,
    "The paper addresses this by devising an approach for Scaffolding Support based on Mastery Level of LLM, which automatically detects the LLM's current capability and provides knowledge support when a question is beyond its mastery level. This approach involves three main components:\n1.  Mastery Level Verifier: This module estimates the LLM's mastery level by examining the consistency of its answers. For a given scaffolding question prompt (pi) and inputs (Xi), the LLM (M) generates multiple answers (Ai) by performing the query multiple times with a temperature (tp).\n    *   For Forward Reasoning, semantic textual similarity (calculated using T5) is used to determine if a sufficient number of answers (nmin, based on a consistency ratio rc) are semantically similar above a threshold (smin). If a consistent subset (Aci) exists, the question is within mastery.\n    *   For Relation Reasoning, answers are projected to \"\"Yes,\"\" \"\"No,\"\" or \"\"Uncertain\"\" using predefined rules. Consistency is determined if a sufficient number of answers project to the same \"\"Yes\"\" or \"\"No\"\" category.\n    If consistent answers (Aci) are found, the question is within the LLM's mastery level; otherwise, it is deemed beyond its capability.\n2.  Metaphor Knowledge Scaffolding: If the Mastery Level Verifier determines that the question (pi) surpasses the LLM's mastery level, this module provides a scaffolding knowledge prompt (bi) from a pre-defined Metaphor Knowledge Base (KB). This knowledge prompt is concatenated with the original question and inputs (pi || Xi || bi) to help the LLM generate an answer. The LLM then generates multiple answers (Asi) with this additional knowledge.\n3.  Filtering: Finally, a single answer (aoi) is selected. If consistent answers (Aci) were found by the verifier, one is randomly selected from Aci. If the question was beyond mastery and scaffolding knowledge was provided, one answer is randomly selected from the denoised answers in Asi. This dynamic assessment and targeted support mechanism ensures that the LLM receives relevant guidance precisely when needed, facilitating its reasoning process.": 1573,
    "The paper designs two pre-training tasks to help the model learn the prompt format and improve its ability to distinguish relation types.\nThe first task is Prompt-based Masked Language Model (Prompt MLM). This task adapts the standard masked language model (MLM) approach by applying it specifically to the context prompt and label prompt components of the input. Words within the context prompt or labels within the label prompt are randomly selected and replaced with a `[MASK]` token. If a label consists of multiple tokens, all its tokens are masked. The model is trained to predict these masked tokens, which enables it to fit the prompt's structure and learn to extract useful information from these specific prompt components.\nThe second task is Relation Contrastive Discrimination. This task aims to optimize the distribution of relation representations in the semantic space, thereby enhancing the model's ability to differentiate confusing relation types. For a given input `X` with relation type `R`, samples are constructed as follows: positive samples are `K` randomly chosen instances with the same label as `R`; negative samples include other instances within the batch. Crucially, hard negative samples are constructed by leveraging the similar relation filter to identify and include instances that contain relation types similar to `R`. These hard negatives are added to the negative samples to guide the model to focus on distinguishing between easily confused labels. Supervised Contrastive Learning (SCL) is then employed, considering multiple positive samples, to learn robust representations. The final loss function for pre-training combines both objectives: `Lfinal = αLs + (1−α)LMLM`, where `Ls` is the SCL loss and `α` is a hyperparameter set to 0.6.": 1574,
    "To obtain robust and reliable label predictions, especially for fuzzy target samples and private categories, the paper proposes a hybrid prototype completion and self-training mechanism. After identifying partial target samples as representative instances of private classes (`Dp_t`), the method abandons the noisy predictions of the source classifier for the remaining fuzzy target samples. Instead, a pseudo-classifier discrimination method is introduced in the embedding space. This pseudo-classifier combines source prototypes and target private prototypes.\nFirst, `k`-means clustering with `kp` clusters is performed on the embedding features of samples in `Dp_t` to obtain target private cluster centroids, denoted as `{νt_j}`. Source class centroids, `{µs_j}`, are obtained from the labeled source domain. The objective is to move all target samples (both `Dp_t` and `Dt \\ Dp_t`) closer to a geometrically appropriate centroid through self-training. Samples in `Dp_t` are moved towards their corresponding target private cluster centroid `ν`, while samples in `Dt \\ Dp_t` are moved towards either a source prototype `µ` or a target private prototype `ν` based on minimizing allocation uncertainty.\nA confidence-guided prototype contrastive loss (`LPro−Con`) is designed to unify these objectives. Assuming all embedding features (`z_i`) and class centroids (`ε_j`) are L2 normalized, the loss is defined as:\n`LPro−Con = - Σ_{i=1}^{nt} Σ_{j=1}^{ks+kp} w_i,j log(s_i,j)`\nwhere `s_i,j = exp(z_i ε_j / τ) / (Σ_{l=1}^{ks} exp(z_i µs_l / τ) + Σ_{k=1}^{kp} exp(z_i νt_k / τ))`.\nHere, `ε_j` represents either a source prototype `µs_j` (for `1 ≤ j ≤ ks`) or a target private prototype `νt_{j-ks}` (for `ks + 1 ≤ j ≤ ks + kp`).\n`w_i,j` is an expanded soft label vector. For a sample `b` in `Dp_t` belonging to the `r`-th target private cluster, `w_b,ks+r = 1` and other `w_b,k = 0`. For a sample `c` in `Dt \\ Dp_t`, `w_c,j` is calculated based on the contrastive scores `s_c,j` to make its distribution sharper and more concentrated than `s_c,j`.\nMinimizing `LPro−Con` acts as a self-training process, improving cluster purity and emphasizing high-confidence assignments. The class centroids (`{µs_j}` and `{νt_j}`) are treated as variables and updated automatically via back-propagation. Initial centroids are obtained after the first stage of training: `µs,initial` from source labels and `νt,initial` from k-means on `Dp_t`. If the Hartigan’s dip test indicates no significant bimodal distribution (i.e., no private categories detected), only source prototypes `{µs_j}` are used for self-training and inference. The final predicted label for a target sample is determined by the index corresponding to the maximum component of its contrastive score vector `s_i`.": 1575,
    "Instead of setting a single global threshold for uncertainty, the paper proposes a category-aware heterogeneous threshold vector (ˆδ ∈ RLs) to identify \"\"unknown\"\" samples more effectively. The motivation is that the diversity of \"\"known\"\" categories necessitates distinct thresholds.\nThe process involves two main steps:\n1.  Logarithmic Total Evidence Distribution Collection: For each known source category 'k' (1 ≤ k ≤ Ls), the logarithmic total evidence scores (logS) of samples belonging to that category are collected. This forms a set Ωk = {logSs_i | ys_i = k}.\n2.  Threshold Estimation: A Gaussian distribution is fitted to each Ωk to obtain the mean estimation (ˆυk) and standard deviation estimation (ˆσk) for that specific category. Based on the \"\"three-sigma\"\" rule (which states that approximately 99.7% of data points fall within three standard deviations of the mean in a normal distribution), the category-aware threshold ˆδk is set as ˆυk - 2 × ˆσk. This threshold is designed to contain more than 95% of source samples within each category, effectively ignoring minor outliers and providing a robust, data-driven threshold for each known class.\nDuring inference, a target sample 'i' is classified as \"\"unknown\"\" if its logarithmic total evidence (logSt_i) for its most confident predicted class (argmax αt_ik) falls below the corresponding category-aware threshold (ˆδj). Otherwise, it is classified into the \"\"known\"\" category 'j'. This approach is data-based and learned from the source domain, avoiding the need for tricky hyperparameter selection and adapting to the inherent diversity of categories.": 1576,
    "The derived understanding that Softmax weights converge to class-wise means is practically applied to accelerate the learning of the Softmax layer in transfer learning through a novel initialization procedure. This procedure, termed Weights Initialized with Centred Means (WICM), involves initializing the weights of the Softmax layer with the scaled centered class-wise means (M_hat) of the target data representations. The centered class-wise means (m_hat_l) are computed as m_l - (1/k) * sum(m_j), where m_l are the empirical class-wise means of the representations on the target domain. The matrix M_hat is then formed by normalizing the matrix M (containing m_l) by its operator norm (kMPk), where P is a centering matrix. This initialization leverages the theoretical finding that the learned weights (w_l) tend to align with these centered means. Experiments show that WICM yields slightly better accuracy than random initialization (Weights Learned From Scratch, WFS) and, crucially, leads to faster convergence of the Softmax layer training, especially as the target domain data becomes more similar to the source domain (i.e., as noise variance decreases).": 1577,
    "The paper proposes a free energy alignment loss (Lfea) as a regularization term to implicitly diminish the domain gap. This loss leverages the observed free energy biases between source and target domains, where target samples generally have higher free energy values than source samples. The Lfea is defined as max(0, F(x;theta) - lambda), where F(x;theta) is the free energy of a target sample x with model parameters theta, and lambda is the average free energy over source data (estimated via exponential moving average during training). By minimizing this loss, the model is encouraged to pull down the free energy of target samples towards the average free energy of source data. This regularization term is applied to unlabeled target data during the training process, alongside the negative log-likelihood loss (Lnll) applied to labeled source data and any newly labeled target data. The full learning objective combines these two losses: min_theta (E_x~S union Tl [Lnll(x;y;theta)] + beta * E_x~Tu [Lfea(x;theta)]), where beta is a loss weight hyperparameter. This implicit alignment of free energy distributions helps reduce the domain shift, complementing the active sampling strategy by making the target data more compact around the source domain's energy landscape.": 1578,
    "GearNet is designed as a universal paradigm that can be easily incorporated into various existing backbone methods, including those for de-noising (e.g., Co-teaching), unsupervised domain adaptation (e.g., DANN), and weakly supervised domain adaptation (e.g., TCL). The integration is achieved by adapting the backbone method's original loss function within GearNet's dual learning structure. For any chosen backbone method, two models, fθ and ˜f˜θ, are initialized using that backbone algorithm. The backbone's specific loss function, denoted as ℓbone for fθ and ˜ℓbone for ˜f˜θ, replaces the conventional supervised learning loss (ℓsuper or ˜ℓsuper) in GearNet's overall loss formulation. GearNet's total loss for training fθ (ℓtotal) and ˜f˜θ (˜ℓtotal) then becomes a combination of this backbone loss and the symmetric Kullback-Leibler (KL) divergence loss (ℓguide or ˜ℓguide) for consistency regularization, weighted by a trade-off hyperparameter β. This modular design allows GearNet to leverage the noise reduction or domain adaptation capabilities of the backbone method while simultaneously enhancing robustness and generalization through its bilateral knowledge transfer and consistency regularization, making it a general framework applicable across different types of domain adaptation and noisy label learning algorithms.": 1579,
    "To address the unknown number of hidden subdomains, the paper proposes aggregating multiple target predictors, h†t(xt;K), each obtained for different values of K (kt,ks). The target domain predictive model becomes h∗t(xt;A) = ∑ksupsks=1 ∑ksuptkt=1 σkt,ks(A)h†t(xt;(kt,ks)), where σkt,ks(A) is a Softmax function that encourages sparsity in the weighting factor A. In a weakly supervised setting, A∗ is estimated by minimizing the prediction error over labeled target data: A∗ = argminA (1/nw) ∑(xt,yt)∈D l(h∗t(xt;A),yt). In an unsupervised setting, A∗ is estimated by minimizing the one-dimensional Wasserstein distance between the output distributions of the aggregated target classifier and the source model: A∗ = argminA W(h∗t(Xt;A),hs(Xs)). Both optimizations are performed using gradient descent. To enhance robustness, a Bagging method is adopted, where ˜S and A are estimated over 10 Bagging datasets, and their average values are used for final estimation. The initialization for A is set to privilege the single-subdomain case (kt=ks=1) with a higher weight, while distributing the remaining weight uniformly among other combinations.": 1580,
    "The MIP-GNN's variable bias predictions are used to guide multiple components of a branch-and-cut solver, specifically node selection and warm-starting, with potential for branching variable selection. For guided node selection, a confidence score, score(bpi) = 1 - |bpi - bbpie|, is defined for each prediction bpi, where bbpie rounds to the nearest integer. Predictions closer to 0 or 1 receive a higher score. The score of a node N in the search tree, node-score(N; bp), is calculated as the sum of confidence scores (or their complements) for the variables fixed at that node. If a fixed variable xi at node N aligns with the rounded prediction (xi = bbpie), it contributes score(bpi); otherwise, it contributes 1 - score(bpi). Nodes with higher scores are prioritized, indicating better alignment with the model's bias predictions. To balance primal and dual bound progress, the solver periodically selects the node with the best bound instead of relying solely on bias predictions (e.g., every 100 nodes). For warm-starting, the bias predictions are used to construct a feasible solution. A rounding threshold pmin (e.g., 0.5 to 1) is defined. A variable's bias prediction bpi is rounded to the nearest integer if its confidence score(bpi) ≥ pmin. Since not all variables may be rounded, a \"\"solution repair\"\" mechanism (a common MIP solver feature) is leveraged to complete the partially integer solution. The process iterates over a grid of pmin values (e.g., {0.99, 0.98, ..., 0.68}) to generate multiple candidate warm-start solutions. Any resulting integer-feasible solution can then be used to warm-start the branch-and-bound search.": 1581,
    "To address the challenge of selectively integrating prior knowledge and avoiding negative transfer, the paper introduces a gate network within the Domain Discriminative Feature Enrichment Layers. After the transferred prior feature (`zp`) is obtained (from the domain-invariant feature transfer process), it is combined with the discriminative target feature (`zt`). A gate network, `Gg`, takes both `zp` and `zt` as input. The gate's output, `g`, is computed as a sigmoid function applied to a linear transformation of the concatenated `zp` and `zt` (`g = (cid:27)((cid:18)T_g [zp;zt] + bg)`), where `(cid:18)g` and `bg` are the weights and biases of the gate network. The sigmoid function ensures that `g` produces an importance score between 0 and 1 for the prior knowledge. The final discriminative representation for the target domain is then formed by element-wise multiplying the gate's output `g` with the prior feature `zp`, and then adding the target feature `zt` (`g (cid:12) zp + zt`). This gate mechanism dynamically controls the information flow from the prior knowledge to the target domain, allowing the model to leverage useful prior information while suppressing irrelevant or harmful knowledge, thereby preventing negative transfer and preserving target domain characteristics.": 1582,
    "The RDA system includes a Damage Severity Assessment module responsible for determining the level of damage depicted in an image. This module employs a fine-tuned VGG16 model. The VGG16 model is initially pre-trained on the large-scale ImageNet dataset to learn general image features. Subsequently, it is fine-tuned on a specialized damage-related labeled dataset, which categorizes images into three distinct classes: severe damage, mild damage, and little-to-no damage (referred to as \"\"none\"\"). Images that have been identified as unique and relevant by the preceding deduplication and junk filtering modules are then processed by this fine-tuned model to automatically classify their damage severity level in real-time.": 1583,
    "To develop a robust training strategy for the post-refinement network, especially given the scarcity of initial disparity maps (Dc) for training (e.g., only 16 for HCI 4D Light Field Dataset), the paper proposes a unique data augmentation approach. This strategy involves three main components: LFs augmentation, Dc augmentation, and Iterative training.\n1. LFs augmentation: To increase the diversity of light field images (LFs), standard image augmentation methods are borrowed and applied. These include random color channel re-distribution, random brightness adjustments, random contrast adjustments, and random rotations by multiples of 90 degrees.\n2. Dc augmentation: This is a crucial part for diversifying the input disparity maps. The initial disparity map Dc is augmented by adding appropriate noise. This noise comes in three forms:\n    * Scale noise (s): The error between Dc and the ground truth (Dg) is scaled, D'c = 2 * s * (Dc - Dg) + Dg, where s is a random number in [-1,1]. This maintains the original distribution while augmenting.\n    * Translation noise (t): The entire Dc is translated, D'c = 0.2 * t + Dc, where t is a random number in [-1,1]. This helps the decoder learn attention better.\n    * Pixel-wise random noise (r): A weak perturbation is introduced, D'c = r * (Dc - Dg) + Dg, where r is pixel-wise random noise sampled from a truncated normal distribution [0.95, 1.05]. This provides more diverse Dc inputs. During training, scale and translation noise are applied N (<=3) times, followed by pixel-wise noise once.\n3. Iterative training: The refined output disparity map (Df) can be taken as a new initial disparity map (Dc) and fed back into BpCNet for further iterative training. Experiments showed that one iteration is sufficient. This effectively generates more diverse training samples for the network.": 1584,
    "The optimization process incorporates two primary loss functions: feature reconstruction loss (`Lrec`) and feature orthogonal loss (`Lorth`), alongside the standard pixel-level Mean Squared Error (MSE) loss for density estimation (`Lden`). The `Lrec` enforces similarity between re-encoded features (`˜f` and `˜z`) and their pre-encoded counterparts (`f` and `z`). This is achieved by flattening the features into a `RHW×C` space and computing their correlation matrix `R`. `Lrec` is designed as a cross-entropy loss that maximizes the diagonal entries of `R`, ensuring pixel-level features are highly correlated. It also includes a \"\"hard predicted region reinforcement\"\" mechanism for `˜f`: for regions with large prediction errors in the density map, their corresponding pixel-level vectors in `˜f` are given increased weight in `Lrec`, focusing learning on challenging areas. The `Lorth` minimizes the squared Frobenius norm of the pixel-level feature similarity between `˜f` and `˜z`, and also between `f` and `z`. This loss explicitly pushes domain-invariant and domain-specific features apart in the embedding space, ensuring their disentanglement. The overall optimization objective is a weighted sum: `L = Lden + λrecLrec + λorthLorth`, where `λrec` and `λorth` are hyperparameters. The network training follows a gradient-based meta-learning approach, where the source set is split into meta-train and meta-test sets based on the dynamically divided sub-domains. An intermediate model is updated on the meta-train set, and then tested on the meta-test set, with both losses contributing to the final model parameter update.": 1585,
    "The paper designs a specific optimization algorithm for generating adversarial perturbations (δ) that aims to improve transferability and smoothness in a cross-domain setting. The process is iterative, starting with the original benign image (xt) as the initial adversarial example (x(0)t). In each iteration (k), a perturbation (δ(k)) is calculated based on the gradient of the Lefd+cos objective function with respect to the current adversarial example (x(k)t). Specifically, δ(k) is computed by taking the Sign of the gradient (▽Lefd+cos) and then convolving it with a 15x15 Gaussian kernel (G). This convolution step, applied outside the Sign function, is a key modification from previous methods (e.g., Dong et al. 2019) and serves to suppress high-frequency noise in the generated perturbations, making them smoother and potentially more transferable. The adversarial example for the next iteration (x(k+1)t) is then updated by adding a scaled version of this smoothed perturbation (α · δ(k)) to the current adversarial example (x(k)t). A Clip operation is applied to ensure that the Lp norm constraint (∥x′t − xt∥p ≤ ϵ) on the perturbation is maintained, keeping the adversarial changes imperceptible. Additionally, the optimization incorporates random linear transformations (T), including random cropping, random padding, and random resizing, applied to the input images at each iteration. Unlike some prior methods, this transformation is applied with a probability of 1.0, introducing extra data diversity into the optimization process to alleviate overfitting and further enhance the transferability of the crafted adversarial examples. The final adversarial example (x′t) is the result of N iterations (x(N)t).": 1586,
    "To achieve efficient real-time inference for high-resolution images, the paper proposes a fast inference pipeline that leverages 3D Look-Up Tables (3D-LUTs). While the ColorMLP directly processes each pixel during training (the \"\"Direct\"\" pipeline), this becomes computationally expensive for high-resolution images (e.g., 4K resolution with 12 megapixels). During inference, instead of applying the ColorMLP to every pixel, the predicted ColorMLP parameters (Θ) are first used to generate an equivalent 3D-LUT. This is done by feeding the parameters of an identity 3D-LUT (where each color maps to itself) as input to the ColorMLP, resulting in a generated 3D-LUT (VG). Once VG is generated, the color transformation for the original content image (Ic) is performed by applying this 3D-LUT (LUT(Ic, VG)). Since 3D-LUTs are ultrafast operators with O(1) complexity per pixel and require zero floating-point operations, this intermediary step significantly improves inference efficiency, especially for high resolutions. The generation of the 3D-LUT itself is a one-time cost per image pair, and the subsequent application of the 3D-LUT is extremely fast, making the overall process suitable for real-time high-resolution image processing.": 1587,
    "To enhance the understanding and alignment of multimodal representations in the semantic space, the model devises a dual-semantic guided training strategy. In addition to minimizing task-specific cross-entropy losses for metaphor recognition (`Lmr`), sentiment analysis (`Lsa`), intention detection (`Lid`), and offensiveness detection (`Lod`), the overall loss function (`L`) includes a dual-semantic guided loss (`Ldg`). This `Ldg` is a contrastive loss designed to effectively leverage cross-modal information. It operates by bringing related image-text pairs closer in the forward direction and pushing unrelated pairs apart in the reverse direction within the semantic space. Specifically, for a given meme sample `i`, its context-aware multimodal representation (`hTi` and `hIi`) is contrasted with other multimodal representations (`hTk` and `hIk`) from within the same batch. The `Ldg` is formulated as a negative logarithm of a normalized exponential similarity, where `sim` is the cosine similarity between `hTi` and `hIi`, scaled by a temperature parameter `τ`, divided by the sum of exponential similarities with all other pairs in the batch. This mechanism forces the model to learn how to better differentiate and capture the semantic information among different meme samples, thereby improving alignment and comprehension.": 1588,
    "The paper addresses this by proposing a multi-teacher knowledge distillation framework. Two teacher models are trained independently: one (Frequency Teacher) using images adapted by the Frequency Domain Transfer (FT) module (Solution 1), and another (Spatial Teacher) using images adapted by the Spatial Domain Transfer (ST) module (Solution 2). Once these two teacher models are trained, a student model is trained for inference using an entropy-based dynamic multi-teacher distillation strategy. During student training, a source domain image is input into both trained teacher models to obtain their respective predicted segmentation category probability maps. The student network then predicts its own probability map. The training loss for the student network, termed LMKD, combines the standard segmentation losses (cross-entropy and Dice loss) with a knowledge distillation loss. This knowledge distillation loss is based on the Kullback-Leibler (KL) divergence between the student's predicted probability map and the probability maps from both teachers. A weighting mechanism, based on the entropy of the teachers' predictions, dynamically balances the influence of each teacher, allowing the student to learn robustly from both frequency-domain and spatial-domain adapted information.": 1589,
    "The paper addresses the computational complexity and relevance of patch-level correlation matching by introducing an Importance Sampling (ImpS) strategy. This strategy selectively focuses on semantically meaningful regions, thereby reducing the number of features considered. The criterion for high importance is based on edge locations within the image, as significant intensity changes are considered more informative. Specifically, for a generated sample Gn-α(zi) and a set of possible locations Y, the probability of selecting a feature at location 'y' is proportional to a value νi y. This value is derived from the Sobel filtered image Φ(Gn-α(zi), y), which highlights edges. Additionally, locations where the magnitude of the displacement vector (from optical flow) exceeds a certain threshold βi are excluded, as these large displacements often indicate errors in optical flow estimation and are less reliable. The threshold βi is set as the η-quantile of the flow distribution. Consequently, each position's selection probability is proportional to its edge response value, ensuring that the correlation matching prioritizes regions with rich semantic content. In contrast, the pairing feature locations (qi,j n-α) are uniformly selected. This selective sampling reduces computational overhead and minimizes the influence of less relevant or noisy regions like backgrounds or visual artifacts.": 1590,
    "The paper addresses this by introducing two novel contrastive loss functions: Content Contrastive Loss (Lcontent) and Style Contrastive Loss (Lstyle). For the content encoder, Lcontent is proposed to improve discrimination within a class and encourage the extraction of domain-invariant content features. For a content feature ˆcy i extracted from the translated target image, the positive sample is set to the original source content cx i, while remaining features at other pixels serve as negative samples. This loss, formulated as an InfoNCE (Oord, Li, and Vinyals 2018), pushes the content encoder to produce more diverse content representations. For the style memory, Lstyle is introduced to enable it to learn robust class-wise style representations. Similar to Lcontent, for a style feature ˆsx i (which is the memory style from the target-to-source mapping pipeline), the positive sample is the source component style sx i, and other pixels' features are negative samples. This loss directly supervises the memory style ˆsx, improving the stability of cycle consistency learning for reconstructing the original image. Both contrastive losses are applied symmetrically in the target-to-source pipeline as well. By using these contrastive losses instead of simple L1 distances (as in MUNIT), the model is explicitly encouraged to learn more discriminative and diverse representations for both content and style, leading to improved performance and stability.": 1591,
    "The effectiveness and consistency of the disentangled feature translation are guaranteed through the application of several reconstruction losses. First, a pixel-level reconstruction loss (Lre) ensures that the decoders (Gs and Gt) can reconstruct the original input images (xs and xt) from their extracted liveness and content features. This loss is formulated as the L1 norm between the reconstructed image and the original image. Second, a cycle-consistency loss (Lcyc) is employed, inspired by CycleGAN. This loss ensures that if features are translated from one domain to another (e.g., source to target), they can be translated back to the original domain, and the reconstructed image should match the original input. For example, translating xs to ˆxt and then back to ˆxcyc s should result in ˆxcyc s being close to xs. Third, a latent reconstruction loss (Llat) is applied to ensure that the liveness features extracted from the original input and the pseudo-labeled images remain identical after translation. This means that the liveness feature of an original image (e.g., EL s(xs)) should be close to the liveness feature extracted from its translated version (e.g., EL t(ˆxt)). These three reconstruction losses collectively regularize the feature mapping process, ensuring that the disentanglement and translation are effective and consistent, leading to robust feature representations.": 1592,
    "The Embedding Fusion module addresses the challenge of extracting fine-grained signals and refining information fusion. Instead of directly combining transferred user embeddings (e.g., uBA_i from Domain B to A) with in-domain embeddings (uA_i), P2FCDR employs a novel gated selecting vector. This vector is designed to identify and emphasize features highly relevant to the target domain, thereby mitigating negative transfer. First, the L1 distance, ∆A_i = |uA_i - uBA_i|, is used to represent the similarity between the in-domain user embedding and the corresponding cross-domain embedding at the feature level. A smaller L1 distance indicates closer alignment to the target domain. Recognizing that different feature dimensions contribute differently to the target, a two-layer fully-connected neural network is utilized. This network takes ∆A_i as input and automatically learns to derive the gated selecting vector, sA_i, for each user i in Domain A. The sA_i vector, with values between 0 and 1 (due to a Sigmoid activation in the final layer), controls the information flow for each dimension. The refined user representation, ˜uA_i, is then obtained through a weighted combination: ˜uA_i = sA_i * uA_i + (1 - sA_i) * uBA_i. This mechanism allows the model to selectively emphasize or de-emphasize specific features from either the in-domain or cross-domain embedding, ensuring that only useful signals are integrated and negative transfer is avoided.": 1593,
    "The paper addresses this by proposing a Robustness Conductor (RC) module within the Robust Cross-Domain Recommendation (RCDR) framework. When privacy-preserving mechanisms perturb source domain data, the resulting knowledge transferred to the target domain can be noisy and potentially degrade performance. RC aims to minimize the impact of this noisy data by disentangling the features of different dimensions within the latent user and item representations (ZT and VT) in a batch. For user features (ZT), RC first applies Z-score normalization to each column (dimension) of the batched features, resulting in `¯ZT`. Then, it measures the cross-correlation matrix `C¯ZT` among these normalized dimensions. RC enforces robustness by minimizing a regularization term `LRC = ∥C¯ZT − I∥F^2 + ∥C¯VT − I∥F^2`, where `I` is the identity matrix. This objective encourages the diagonal elements of the cross-correlation matrix to approximate 1 (ensuring consistency within the same dimension) and off-diagonal elements to approximate 0 (enforcing mutual independence between different dimensions). By making the noisy and redundant representations clean and independent, RC acts as a flexible plugin that robustly enhances the recommendation prediction in the target domain, even when leveraging perturbed source domain knowledge.": 1594,
    "The task coordinator (CDTC) is optimized using reinforcement learning, specifically the REINFORCE policy gradient algorithm, to improve its selection of suitable meta-tasks based on feedback from the target domain.\nFirst, a reward (R) signal is defined to quantify the appropriateness of the selected meta-tasks. After CDTC selects B meta-tasks, the graph base learner (with parameters θOLD) performs a temporary update using these selected tasks to obtain new parameters (θNEW). The reward R is then calculated based on the performance improvement on the prompt tasks in the target domain: R = tanh((1/Mt) * Σ(j=1 to Mt) (Lc(˜τj,θNEW) − Lc(˜τj;θOLD))). Here, Lc is the cross-entropy loss measured on the prompt tasks (˜τj). A positive reward indicates that the selected meta-tasks led to an improvement in the graph base learner's performance on the target domain, thereby reinforcing CDTC to choose similar meta-tasks in the future.\nSecond, the parameters (ψ) of CDTC (including Wa, Wg, Wc, and w from task representation learning, refinement, and selection modules) are updated using the REINFORCE algorithm. Due to the non-differentiability of the task sampling process, policy gradient is employed: ψ(k+1) = ψ(k) − γ∇ψ(k)logπ(ψ(k))(R − b). Here, γ is the learning rate for CDTC, b is a baseline function (e.g., moving average of rewards) used to reduce variance, and π represents the CDTC's policy that determines task sampling probabilities. This update steers CDTC towards selecting meta-tasks that yield higher rewards.\nFinally, an alternative optimization strategy is employed to jointly train the graph base learner (θ) and CDTC (ψ) during the meta-training phase. At each iteration, candidate meta-tasks are randomly sampled from the source domain. CDTC then assigns sampling probabilities and selects a batch of meta-tasks. These selected meta-tasks are used to execute a temporary update of the graph base learner, and the calculated reward is used to optimize CDTC. The temporary update of the graph base learner can be performed with a larger learning rate (γ) to allow CDTC to collect feedback signals from a wider range of performance changes. CDTC is typically \"\"warmed up\"\" in the initial iterations before the graph base learner is updated in conjunction with CDTC's optimization.": 1595,
    "The paper generates the implicit shared property (`p`) for each simile sentence by combining two perspectives: external knowledge and contextual information. Recognizing that the vehicle is the most valuable component for inferring shared properties, the method first leverages the HasProperty relation from COMET (Commonsense Transformers for Automatic Knowledge Graph Construction), an external knowledge source, to retrieve properties associated with the vehicle.\nConcurrently, to incorporate contextual understanding, the original simile sentence `s` is rewritten into a masked sentence `s'` by replacing the comparator (e.g., \"\"like\"\") with \"\"as [MASK] as\"\". For example, \"\"Her hair felt like silk\"\" becomes \"\"Her hair felt as [MASK] as silk\"\". This masked sentence is then fed into RoBERTaLARGE, a pre-trained masked-word-prediction language model, to generate candidate properties based on the surrounding context. To ensure the quality of the generated properties, the top-10 predictions from both COMET and RoBERTaLARGE are considered. Their scores are normalized into a range of [0, 0.5]. Only properties with a score greater than dynamically adjusted thresholds (θknowledge and θcontext) are retained. The final score for each (s, t, p, v) instance is the sum of these normalized scores, ranging from 0 to 1, which contributes to the probabilistic modeling of the knowledge base.": 1596,
    "The paper addresses this through the Geometric Adaptation (GA) module (denoted as `gamma`), which is a core component within the Geometry-Aware Adaptation stage. After the structure-texture and coordinate-color disentanglements, the GA module is designed to transfer domain-invariant geometric knowledge from source geometric features (Fs_geo) to enhance target geometric features (Ft_geo). The GA module operates in a residual form with an attention mechanism. It first concatenates the source and target geometric features ([Fs_geo; Ft_geo]). An attention operation is then applied to these concatenated features. The output of this attention mechanism is finally added to the original target geometric features (Ft_geo) to produce the enhanced target feature (eFt_geo). This design allows the module to compute the geometric shape similarity between objects in the source and target patches. By leveraging the attention mechanism, it effectively extracts and transfers geometric features from objects with similar shapes, even if they are from different domains. This enhanced geometric information is then transformed back to the 2D pixel space to refine the target pseudo-labels and improve segmentation, especially for challenging small object classes. The training incorporates a Semantic-Geometric Consistency (SGC) loss (LSGC) to minimize perceptual differences between semantic and geometric predictions across domains, further solidifying the knowledge transfer.": 1597,
    "The paper proposes an adversarial min-max optimization objective for Multi-Source Survival Domain Adaptation (MSSDA), inspired by Domain-Adversarial Neural Networks (DANN). The objective aims to find a feature extractor `ϕθ` (mapping input `X` to feature space `V`) and a ranker `rh` (from hypothesis space `H`) that minimizes a specific loss, while a second ranker `rh'` tries to maximize a discrepancy. The optimization problem is formulated as:\n`max_{ϕθ,h∈H, ||w||1=1} min_{h'∈H} [ Σ_{i=1}^K wi C-index(rh;Msi) - λ1 (SDI(rh, rh';Mt) - Σ_{i=1}^K wi SDI(rh, rh';Msi)) - λ2 ||w||2 ]`\nHere, `rh` and `rh'` are ranking functions. The first term, `Σ_{i=1}^K wi C-index(rh;Msi)`, is a weighted sum of C-index scores on all source domains, minimized by `h` to ensure `rh` is a good ranker on source data, realized by minimizing the negative log-partial likelihood. The second term, `-λ1 (SDI(rh, rh';Mt) - Σ_{i=1}^K wi SDI(rh, rh';Msi))`, is an explicit realization of the weighted discordance-based distance (`wi DSDI-disc(Pt,PSi)`). The `min_{h'}` part makes `h'` try to maximize the discrepancy between the target and the weighted source distributions, while the `max_{ϕθ,h}` part makes `ϕθ` and `h` try to minimize it, effectively aligning the distributions. The third term, `-λ2 ||w||2`, is a regularization on the learned weights vector `w`, which specifies the weight for each source domain concerning the target domain. This min-max game aims to find a feature extractor `ϕθ` and a ranker `rh` that achieve feature invariance between the target domain and the weighted combination of source domains.": 1598,
    "The path-based framework offers significant flexibility in incorporating nonlinear metrics and various types of constraints.\nFor nonlinear metrics, the framework allows any path-level metric to enter the Mixed-Integer Program (MIP) as input parameters. Once a decision rule (path) j is defined in the feature graph, the set of samples Sj that satisfy this rule is known. The user-specified metric associated with this rule, denoted as ξj (e.g., misclassification error, F1-score, squared loss), can be computed for these samples. These ξj values then directly enter the objective function of the Restricted Master Program (RMP) as coefficients for the zj variables. This means that even if the underlying metric is nonlinear (like F1-score), it is pre-computed for each candidate rule and enters the RMP as a linear cost, allowing the MIP to remain linear. The paper also notes that these nonlinear metrics can enter the RMP as constraints, with an example of incorporating F1-score provided in the appendix.\nFor constraints, the path-based model provides an elegant and unified way to handle various types:\n1.  Path-level constraints: These constraints apply to individual decision rules. For example, a cost-sensitive decision tree might have a budget C for medical tests, where each rule j has an associated cost ρj. This can be expressed as `ρj * zj <= C` for all j. Such constraints are processed within the K-shortest path (KSP) subproblem. When searching for paths with negative reduced cost, the KSP algorithm can incorporate these path-level constraints as feasibility checks or by modifying the arc costs, ensuring that only paths satisfying these conditions are generated and added to the RMP. If these constraints are expressed as polyhedral inequalities `sum_{j=1 to L} ρmj * zj >= qm`, their dual variables (τm) influence the reduced cost calculation, guiding the subproblem to generate paths that are more likely to be feasible.\n2.  Attribute-level constraints: These involve complex nonlinear conditions on features or disallowing certain feature combinations. The framework handles these by integrating them as feasibility checks directly within the KSP subproblem during the extension of a partial path to the next node in the feature graph. For instance, if a medical domain requires a specific hierarchy (e.g., temperature checks before advanced tests), this can be enforced by appropriately arranging the nodes in the feature graph, ensuring that paths violating this hierarchy are never constructed. This allows for fine-grained control over the structure and content of the decision rules.": 1599,
    "The paper unifies the Cooperative Learning (CLE) and Adversarial Learning (ALE) mechanisms into a single, comprehensive optimization framework called Cooperative and Adversarial LEarning (CALE). This framework integrates the regularization terms derived from CLE and ALE with the standard discriminability and transferability losses, optimizing them jointly within a min-max paradigm.\nThe overall loss function for CALE is formulated as: `L = Ldisc + Ltran + λ[Rdisc + Rtran]`.\n1.  Discriminability Loss (Ldisc): This component enhances the discriminability of features. It includes a supervised classification loss on labeled source domain data and a self-training loss on unlabeled target domain data using pseudo-labels. Specifically, `Ldisc(θF,θG) = E(xs i)∼Dsℓce (G(F(xs i)),ys i) + Ext i∼Dt I[max(ˆyt i)≥τ]ℓce (G(F(xt i)),PL(ˆyt i))`, where `ℓce` is the cross-entropy loss, `θG` are parameters of the discriminative module (G), `F` is the feature extractor, `PL` denotes pseudo-labels, and `τ` is a self-training threshold.\n2.  Transferability Loss (Ltran): This component enhances the transferability of features by aligning domain distributions. In this paper, it's implemented via adversarial confusing, where a domain discriminator (H) attempts to distinguish source from target features, while the feature extractor (F) tries to confuse it. Formally, `Ltran(θF,θH) = Exs i∼Ds log[H(F(xs i))] + Ext i∼Dt log[1 − H(F(xt i))]`, where `θH` are parameters of the transfer module (H).\n3.  Discriminability Regularization (Rdisc): This term combines the cooperative and adversarial regularization for discriminability. It is defined as `Rdisc(θF,θG) = Exi∼Ds∪Dt [ℓCLE disc(xi;θF,θG) + ℓALE disc(xi;θF,θG)]`. `ℓCLE disc` is the cooperative loss for discriminability (from Solution 1), and `ℓALE disc` is the adversarial loss for discriminability (from Solution 2).\n4.  Transferability Regularization (Rtran): This term combines the cooperative and adversarial regularization for transferability. It is defined as `Rtran(θF,θH) = Exi∼Ds∪Dt [ℓCLE tran(xi;θF,θH) + ℓALE tran(xi;θF,θH)]`. `ℓCLE tran` is the cooperative loss for transferability (from Solution 1), and `ℓALE tran` is the adversarial loss for transferability (from Solution 2).\nThe `λ` is a trade-off parameter balancing the regularization terms. The entire model is optimized using a min-max paradigm: `min θF ,θG max θH Ldisc(θF,θG) + Ltran(θF,θH) + λ[Rdisc(θF,θG) + Rtran(θF,θH)]`.\nThe training procedure, outlined in Algorithm 1, involves iteratively calculating these losses and regularization terms, generating cooperative and adversarial examples, and updating the model parameters (θF, θG, θH) via gradient backpropagation. The `Dist.` function used for consistency losses can be cross-entropy or KL-divergence, depending on the dataset. This unified framework ensures that the learning of discriminability and transferability is not only simultaneous but also mutually beneficial, leading to robust and effective domain adaptation.": 1600,
    "A comprehensive domain adaptation strategy for multivariate time-series data is achieved by integrating both local sensor-level and global feature-level alignment mechanisms within the SEnsor Alignment (SEA) framework. This is realized through the combination of endo-feature alignment (local) and exo-feature alignment (global). The endo-feature alignment, as detailed previously, focuses on aligning individual sensor features and their correlations across domains, preventing misalignment at the granular sensor level. Complementing this, the exo-feature alignment addresses domain discrepancy at the global level. It operates by stacking the learned sensor features (`ps 1,...,ps N` for source and `pt 1,...,pt N` for target) and mapping these stacked features to global features (`hs` and `ht`) using a function `fexo(·)`. These global features are then aligned using a method similar to Deep CORAL, which minimizes the squared matrix Frobenius norm of the covariance matrices between the source (`Cs`) and target (`Ct`) global features. This `minLEXO = (1/4F^2) ||Cs - Ct||^2 F` loss enforces restrictions on the global sensor features to reduce overall domain discrepancy. The entire SEA model is trained by minimizing a combined loss function `minL = LC + λEXOLEXO + LEndo`. Here, `LC` is the task-specific cost function (e.g., Mean Square Error for regression or Cross Entropy for classification) computed on the labeled source data, `LEXO` is the exo-feature alignment loss, and `LEndo` is the endo-feature alignment loss (which itself is `λSCALSCA + λSFALSFA`). The hyperparameter `λEXO` tunes the effect of the global alignment. This integrated approach ensures that domain adaptation is performed comprehensively, addressing both the fine-grained sensor-level differences and the broader global feature discrepancies.": 1601,
    "The paper proposes a Test-Time Adaptation (TTA) scheme that differs from existing methods by fixing all network parameters and only optimizing the input image of the segmentation network. This optimization uses the data projected by SSDP as an initialization. Specifically, given a projected target domain image, its pseudo label is first computed by averaging predictions from the classifier `H` and the class prototypes (based on features from the second last layer). Then, one thousand pixels of each class are randomly sampled from this image without replacement to construct the pixel-to-pixel contrastive loss as defined in Equation (7). The projected data is then iterated once by gradient descent to minimize this loss, leading to a refined prediction of the segmentation mask. This process updates the input data to better align with the learned model's feature space, rather than adapting the model itself.": 1602,
    "The paper designs two reweighted loss functions that utilize the `wt` and `ws` weights derived from the m-PPOT transport plan.\n1.  Reweighted Entropy Loss (Lent): This loss operates on target domain data to manage prediction confidence. It consists of two parts:\n    *   `Lpe` (positive entropy loss): `Lpe = - sum_j(wt_j * sum_i(p_ij * log(p_ij)))`, where `p_ij` is the predicted probability of target sample `j` belonging to class `i`. This term increases the confidence of predictions for target samples identified as \"\"known\"\" (i.e., those with high `wt_j`).\n    *   `Lne` (negative entropy loss): `Lne = - sum_j(wu_j * sum_i(p_ij * log(p_ij)))`, where `wu_j = [1 - wt_j]+` is a weight indicating the \"\"unknownness\"\" of target sample `j`. This term suppresses overconfident predictions for target samples likely to be \"\"unknown\"\" by increasing their entropy. Only a fraction (25%) of `wu_j` with larger values are retained to avoid misclassifying \"\"known\"\" samples as \"\"unknown\"\".\n2.  Reweighted Cross-entropy Loss (Lrce): This loss is applied to the source domain data for classification. `Lrce = - sum_i(ws_i * sum_j(1(y_i = j) * log(sigma(h(f(xs_i)))_j)))`, where `ws_i` is the weight for source prototype `i` (representing class `i`), `y_i` is the true label, and `sigma(h(f(xs_i)))_j` is the predicted probability for class `j`. This loss uses `ws` to weigh the classification of source samples, effectively focusing on common classes and minimizing the first term of the theoretical bound for POT. The total training loss is `L = Lrce + Lent + lambda1 * Lot`, where `Lent = lambda2 * Lpe - lambda3 * Lne`.": 1603,
    "The paper addresses this by implementing a Two-level Domain Alignment module, which uses asymmetrical KL divergence to align distributions at two levels: semantic features and similarity patterns. Unlike conventional adversarial methods that might struggle with task-relevant semantic features or introduce information from the target to the source, this approach focuses on unidirectional alignment. First, the semantic features (`^F`) obtained from the TSE module are partitioned into `^FXS`, `^FQS`, and `^FQT` for the support set, source query set, and target query set, respectively. The KL divergence is then minimized between the semantic features of the source query set (`^FQS`) and the target query set (`^FQT`), defined as `Lsfa = KL(^FQS; ^FQT)`. This `Lsfa` loss aims to align the distributions of the high-level semantic features themselves. Second, the method also minimizes the KL divergence to align the similarity patterns (`fpc_qS` for source query set and `fpc_qT` for target query set) for each class `c`, defined as `Lspa = sum(KL(fpc_qS; fpc_qT))`. This `Lspa` loss ensures that the classification predictions or \"\"similarity patterns\"\" across domains are aligned, which is crucial for effective classification. The total objective function for training the model combines these alignment losses with the classification loss (`Lcls`) and the class matching loss (`Lclm`), using hyperparameters (`λsfa`, `λspa`, `λclm`) to balance their contributions.": 1604,
    "To learn invariant representations and improve generalization, the paper regularizes prediction consistency among the original image and all augmented images using Jensen-Shannon (JS) divergence. The total training loss `L` is defined as `L = LERM + λ · JS(x0,x1,x2,...,xn)`, where `LERM` is the empirical risk minimization loss (cross-entropy loss `ℓCE(F(x),y)`), `λ` is a trade-off parameter, `x0` is the original image, and `x1, x2, ..., xn` are `n` augmented images generated from `x0` using SADA. The JS divergence `JS(x0,x1,x2,...,xn)` is calculated as `(1 / (n+1)) * Σ(i=0 to n) KL(F(xi)||¯p)`, where `KL` is the Kullback-Leibler divergence, `F(xi)` is the model's prediction probability for image `xi`, and `¯p` is the average prediction probability across all `n+1` images (`(1 / (n+1)) * Σ(i=0 to n) F(xi)`). This consistency regularization encourages the model to produce similar predictions for the original and augmented versions of an image, thereby promoting the learning of more robust and invariant features that are less sensitive to domain shifts.": 1605,
    "The paper proposes a Linguistic Features Enhanced (LFE) approach to effectively explore and integrate linguistic features. A POS Adaptation Network is designed to acquire Part-of-Speech (POS) features. This network includes a POS Encoder (PE) that encodes each word into a POS feature vector. SpaCy's pre-trained POS tagger is used to automatically mark POS labels for words in both the support and query sets. The POS feature vector (ep_i) for each word is then averaged with the fusion word vector (ef_i) from the Task Adaptation Network to create a final enhanced word embedding. This final word embedding is then used in the few-shot CRF framework. By explicitly introducing POS features, the model leverages universal cross-domain knowledge, which plays a guiding role in constructing robust word embeddings and improving performance in few-shot slot tagging tasks.": 1606,
    "Finetuned Transformer models are evaluated for their effectiveness in identifying offensiveness in jokes by reformulating the multiclass humor subtype classification problem into a binary offensiveness detection task. In this reformulated task, Dark Jokes and Dirty Jokes are collectively labeled as \"\"offensive content\"\" due to their vulgar, insensitive, or graphic nature. Conversely, Wholesome Jokes and News articles are categorized as \"\"inoffensive content.\"\" The most accurate finetuned model from the multiclass classification task, DeBERTa, is then specifically assessed on this binary classification problem. Its performance is directly compared against a state-of-the-art offensiveness detection model, twitter-roberta-base-offensive, which is derived from the TweetEval benchmark. The comparison is based on accuracy, precision, recall, and micro-averaged F1 score. By demonstrating that the finetuned DeBERTa model achieves significantly higher accuracy and F1 scores on this specific offensiveness detection task, the paper establishes its superior capability over existing methods in identifying offensive humor.": 1607,
    "LogFormer achieves parameter-efficient knowledge transfer to target log domains through its adapter-based tuning strategy. This strategy utilizes a flexible component called the Adapter. The adapter is designed with a parallel structure, meaning it is inserted parallel to both the Log-Attention layer and the feedforward layer within the Log-Attention Encoder. This parallel design allows the adapter to leverage input information more effectively in conjunction with the original complete encoders. The internal structure of an adapter consists of two projection layers: a down-projection matrix `Wdown` that maps the hidden vector from dimension `d` to a smaller dimension `m`, and an up-projection matrix `Wup` that maps it back from `m` to `d`. By setting `m << d`, the number of parameters added per adapter is significantly limited. The adapter also incorporates a skip-connection operation internally, calculating the output vector `h'` as `h' = Wup * tanh(Wdown * h) + h`, where `h` is the given hidden vector. During the adapter-based tuning stage, only the parameters of these adapters (`Wdown` and `Wup`) are updated on the target domain. Crucially, the parameters of the pre-trained Log-Attention and feedforward layers are frozen. This approach provides a plug-in mechanism that reuses the pre-trained model while drastically reducing the number of trainable parameters and, consequently, the training costs, all while maintaining or improving performance.": 1608,
    "The paper introduces a Mutual Learning via Consistency Constraint (CC) to address the issue of misidentified known-class samples, which can arise when the decision space for known classes shrinks due to strong unknown-class identification. The core idea is to leverage the observed relationship between the confidence of the closed-set classifier and the positive score of the open-set classifier.\nFor a known-class sample xt_j that is misidentified as unknown, it typically exhibits high confidence for its true known class (l) from the closed-set classifier (pc(l|xt_j)), but a low positive score from the corresponding open-set sub-classifier (po(l|xt_j)). Conversely, for true unknown-class samples, both the closed-set confidence and open-set positive scores are consistently low. This observation highlights an inconsistency for misidentified known-class samples.\nTo correct these misclassifications, a novel consistency constraint loss, Lcc(xt_j), is proposed. It is defined as the negative sum over all known classes (l=1 to K) of the product of the closed-set confidence pc(l|xt_j) and the open-set positive score po(l|xt_j). Minimizing this loss encourages consistency. Specifically, through gradient descent, the open-set positive score po(l|xt_j) for a given class l increases proportionally to the closed-set confidence pc(l|xt_j). This means that for misidentified known-class samples (which have high closed-set confidence), their open-set positive scores are significantly increased, pushing them towards correct known-class identification. For true unknown-class samples, since both pc(l|xt_j) and po(l|xt_j) are already low, the increment is slight, preserving their unknown classification. This mutual learning process optimizes the open-set classifiers and decision boundaries to better separate both known and unknown classes.": 1609,
    "The paper proposes a two-stage training framework for G-NAS, built upon the Faster R-CNN architecture, to effectively integrate Differentiable Neural Architecture Search (NAS) with the Generalizable loss (G-loss). This framework comprises a \"\"search stage\"\" and an \"\"augment stage.\"\" In the search stage, the detector network `F(θ)` and the searchable prediction head super-net `S(ω,α)` are initialized. During this stage, for each training batch `(x,y)`, both the G-loss `Lg(θ,ω,α)` and the total training loss `Ltrain = Ldet + Lcls + Lreg + λg · Lg` are calculated. Crucially, both the network parameters (`θ`, `ω`) and the architectural parameters (`α`) are updated simultaneously using Stochastic Gradient Descent (SGD). This concurrent optimization allows the NAS process to be guided by the OoD-aware `Lg`, encouraging the selection of architectures that promote generalization. Upon completion of the search stage, the optimal architectural parameters `α*` (specifically, the architecture from the last epoch) are saved. In the subsequent augment stage, a new detector network `F(θ)` is initialized, and its prediction head `S(ω,α*)` is reconstructed using the fixed `α*` obtained from the search stage, meaning only the chosen operations are included. For each training batch, `Lg(θ,ω,α*)` and `Ltrain` are calculated again, but this time, only the network parameters (`θ`, `ω`) are updated via SGD, while `α*` remains fixed. This two-stage approach first identifies a generalizable architecture under the guidance of G-loss and then trains a robust model with this fixed, optimized architecture, ensuring both architectural design and network weights contribute to improved Out-of-Domain generalization.": 1610,
    "The paper proposes the Gaze Frontalization-based Auxiliary Learning (GFAL) Framework to leverage gaze frontalization as a constraint for gaze estimation, specifically targeting improved cross-domain generalization. The core idea is to embed the gaze frontalization process, forcing the extracted gaze features to represent a state where the eyeball looks at the front (camera), i.e., the (0,0) direction. This provides a unique, fixed anchor point in the continuous gaze label space, which helps alleviate the problem of infinite and continuous gaze labels causing overfitting. The framework consists of three main parts: a Gaze Estimation Network, a Gaze Frontalization Module, and a Consistency Loss. The Gaze Estimation Network performs the primary task of predicting gaze and head orientation. The Gaze Frontalization Module, as described in Solution 1, generates a frontalized image `xfro` by manipulating the features. The Consistency Loss, as described in Solution 2, ensures the fidelity of this frontalization. By optimizing the total loss function `L = LG + LH + λfroLfro + λG∗conLG∗con + λH∗conLH∗con`, the framework guides the feature extractor `F` to learn representations that are robust and generalizable. The frontal gaze (0,0) acts as a stable anchor, allowing the model to learn a more constrained and generalizable mapping from images to gaze directions, thereby improving performance on unseen target domains without requiring any target domain information during training.": 1611,
    "The independent training and content preservation mechanisms are integrated into a cohesive framework by applying Keyframe Manifold Constraint Gradients (KMCGs) as a corrective step within the Dual Diffusion Implicit Bridges (DDIBs) inference process. The overall system, depicted in Figure 1, first uses the independently trained source diffusion model to convert the source motion sequence into a latent encoding. This latent encoding is then fed as the starting condition to the independently trained target diffusion model. During the backward diffusion (generation) phase of the target model, the KMCGs are applied. This means that while the DDIBs handle the primary style transfer by bridging the latent spaces of the independently trained models, KMCGs specifically intervene in the reverse diffusion process of the target model to enforce content constraints derived from the source keyframes. The diffusion models themselves are based on the EDGE architecture, which is a transformer-based diffusion model that accepts conditional feature vectors and generates motion sequences using a cross-attention mechanism. This integration ensures that the system benefits from the scalability and data privacy of DDIBs' independent training while KMCGs mitigate the content preservation issues inherent in DDIBs when dealing with complex motions. The framework's design ensures that the introduction of content constraints does not compromise the strength of style transfer or the naturalness of the generated motion, as validated by experimental results.": 1612,
    "The paper introduces ProperMix to augment limited synthetic data and ensure it accurately approximates the target data distribution while maintaining fidelity. Unlike direct application of standard Mixup, which can lead to out-of-distribution mixtures, ProperMix modifies the mixing process. The mixing formula used is `Xpropermix = clip(Xsyn + αXreal, -1, 1)`, where `Xsyn` represents synthetic data, `Xreal` is real training data (if available to the attacker, otherwise another synthetic sample), and `α` is a mixing parameter uniformly sampled between `αmin` and `αmax` in each training step to encourage variation. A clipping function is incorporated to prevent \"\"overmixing,\"\" and the scaling factor typically applied to synthetic images in standard Mixup is removed to ensure a robust generator. The crucial aspect of ProperMix is its trainable nature: the mixed images (`Xpropermix`) are sent to the server, and the generator is then optimized using the gradients returned from the server. This feedback mechanism forces the generator to adjust the distribution of the mixtures towards the desired data distribution, minimizing the cross-entropy loss. Consequently, the mixed data remains \"\"in-distribution,\"\" and the target model exhibits a higher confidence score on these ProperMix-generated mixtures compared to standard Mixup, indicating better approximation of the target data distribution.": 1613,
    "The proposed Bi-level ATtention ENsemble (Bi-ATEN) module coordinates intra-domain and inter-domain weight learning within a unified architecture to achieve agile Multi-Source-Free Domain Adaptation (MSFDA) with minimal parameter tuning. The framework processes a target sample by first extracting bottleneck features from multiple source models. Instead of directly using specific source classifiers, it computes all possible cross-domain outputs by passing each bottleneck feature through all available source classifiers. The intra-domain weights (α_i) are then learned to select the most compatible classifier for each bottleneck feature, generating an \"\"unbiased\"\" domain output (˜y_i^t). This addresses the feature-output mismatch. Subsequently, the inter-domain ensemble weights (β) are learned to combine these unbiased domain outputs into the final classification result (¨y^t). This bi-level approach allows for fine-grained adaptation at the instance level (intra-domain) while maintaining broader domain consistency (inter-domain). Crucially, both source backbones and source classifiers remain frozen during the entire training process, with only the source bottlenecks being slightly tuned. This design significantly reduces the number of trainable parameters and computational cost compared to methods that tune entire source models. The training process employs an alternate strategy: in certain epochs, intra-domain weights (α_i) are manually set to one-hot vectors (as in the simplified ATEN module) to balance intra-domain compatibility and domain-consistent adaptation, especially for target domains with varying domain gaps. The overall objective function combines the inter-domain loss (L_inter), which includes cross-entropy and IM loss on the final ensemble output, and the intra-domain loss (L_intra), which applies IM loss to each unbiased domain output, weighted by a trade-off hyperparameter (λ).": 1614,
    "The framework exploits the clustering effect in the feature space to provide additional adaptation guidance through a Nearest-Centroid-Augmented Objective. This objective integrates nearest-centroid classification into the training process, particularly for under-adapted and model-confident samples.\nThe steps are as follows:\n1.  Nearest-Centroid Classification: A soft logits vector r is obtained for each sample by calculating the softmax of the cosine similarity between the sample's feature q and the mean feature (µc) for each class c. Specifically, rc = exp(q^Tµc/t) / Σ_l=1^L exp(q^Tµl/t), where µc is the average of per-class features based on the model's predictions. This provides a measure of how close a sample's feature is to each class centroid.\n2.  Training Target Refinement (˜y): The training target (˜y) for each sample is determined based on its separation status:\n    *   For samples in the easily-adaptable part (DWe), the training target is the Onehot form of the source-predicted label (ˆz).\n    *   For samples in the model-confident part (DWc), the training target is a weighted combination of the model's self-predicted label (ˆp, Onehot form of p = hT(x)) and the nearest-centroid logits (r). The weight is based on the model's confidence: max(p) · ˆp + (1 - max(p)) · r. This allows the nearest-centroid information to refine predictions for less confident model-confident samples.\n    *   For under-adapted samples (DU), the nearest-centroid output (r) is directly used as their training target. This leverages the inherent clustering structure in the feature space to guide the adaptation of these challenging samples.\n3.  Classification Loss (L_cls): The classification loss is defined as the cross-entropy loss between the output of the weakly-augmented branch (hT(α(x))) and its corresponding refined training target (˜y). Additionally, an entropy regularizer term R(Ex∈DT hT(α(x))) is included to encourage diversification of model outputs, a common practice in UDA.\n4.  Overall Objective: The final overall objective (L_total) is a weighted sum of the classification loss and the graph contrastive learning loss: L_total = L_cls + η · L_cont, where η is a hyper-parameter controlling the weight of the contrastive loss. This integrated objective ensures that the model learns to classify samples while simultaneously aligning features and exploiting the clustering effect for robust adaptation.": 1615,
    "An efficient algorithm based on the cutting plane method is developed to solve the reformulated dual linear program (D-2), which is a convex minimization problem: minλ∈Rm+ F(λ). The cutting plane method (Algorithm 1) is an iterative approach that uses a separation oracle to restrict the feasible set until convergence. For problem (D-2), the separation oracle (line 4 in Algorithm 1) is efficiently obtained from the subgradients of F(λ), as detailed in Lemma 1. This lemma provides a closed-form expression for computing F(λ) and its subgradients, which are derived from the row-wise maximum components of the matrix ¯C. This efficient computation of subgradients is critical for the cutting plane method's performance. Corollary 5 analyzes the complexity, stating that the cutting plane method can solve problem (D-2) within ˜O(n^2 + |D|^2|Y|^2n · log(R/ε)) flops and O(n|D||Y|) space. This theoretical complexity demonstrates its efficiency and scalability, particularly when compared to traditional LP algorithms like the simplex method or interior point methods, which typically have O(n^3) time complexity for similar problems. The method's efficiency is further enhanced by leveraging sparse matrix multiplication benefits.": 1616,
    "The paper designs Domain Prompt Updating (DPU) to efficiently optimize prompt parameters differently for each target sample. Recognizing that each sample exhibits a unique domain shift, DPU adapts the prompt update rate. First, an image-level uncertainty value `U(x)` is calculated by averaging the pixel-wise uncertainty values `U(exj)` across the entire image (Equation 4). This image-level uncertainty reflects the overall degree of domain shift for that specific target domain sample. Based on this value, the prompt parameters `pt` are updated using an exponential moving average (EMA) strategy (Equation 5). The EMA updating rate `β` is adaptively adjusted for each sample using the formula `β = 1 - (U(x) × θ)`, where `θ` is a scaling factor (e.g., 0.01) to bring the uncertainty value to a comparable order of magnitude as common EMA update parameters. This means that samples with higher image-level uncertainty (indicating a larger domain shift) will have a smaller `β` (and thus a larger `(1-β)`), leading to a more aggressive update of prompt parameters. Conversely, samples with lower uncertainty will have a larger `β`, resulting in a more stable update. This adaptive optimization allows the prompts to efficiently and stably adapt to the varying domain shifts encountered in individual target domain samples during the Test-Time Adaptation process.": 1617,
    "The learning objective is formulated through the optimization loss J, defined in Equation (4), which implicitly minimizes the distance between class-conditional semantic centroids in the Koopman space. During training, for each step, data sets Si and Si+1 are randomly chosen from two consecutive domains as support and query sets, respectively. For each class k in the support set Si, a forecasted centroid (ck_i+1) for domain i+1 is computed. This centroid is the mean vector of the support instances belonging to Sk_i, after being passed through the composite function K ◦ G ◦ ϕ. The query instances from Si+1 are passed through G ◦ ϕ. The predictive distribution for Di+1 is then given by a softmax over the negative Euclidean distances between the query instance embeddings and all forecasted class centroids (Equation 3). The model optimization proceeds by minimizing the negative log probability of the query instances (Equation 4). As shown in Theorem 4.3, this loss J is an approximation of the KL term in the generalization bound (Equation 2) and an inter-class distance loss. By minimizing J, the method effectively aligns the distribution of forecasted domains (DK_i+1(z|y)) with the distribution of real domains (Di+1(z|y)), thereby directly reducing the generalization bound.": 1618,
    "In practical scenarios where only empirical distributions are available, the paper provides a method to maintain marginal coverage guarantees. For each source domain Si, `mi` i.i.d. examples `Vij = s(Xij,Yij)` are sampled, and `ˆFi` is defined as the empirical c.d.f. corresponding to `Fi`. `ˆFmin(x)` is then defined as `min_{1≤i≤d} ˆFi(x)`. Proposition 7 quantifies the error when estimating `Fmin` with `ˆFmin`, providing a probabilistic bound on `|Fmin(x) - ˆFmin(x)|`. Building on this, Theorem 8 provides a marginal coverage guarantee for the prediction set `eC` when `t` is set to `eQ(1-α; ˆPf,ρ) = Q(g_f,ρ^(-1)(1-α); ˆFmin)`. To achieve a marginal coverage of at least `1-α`, Corollary 9 introduces a correction mechanism. It states that if the significance level `α` is replaced by a corrected `α'` such that `α' = 1 - g_f,ρ(ϵ + g_f,ρ^(-1)(1-α / (1 - 2 * sum(e^(-2*mi*ϵ^2)))) )`, where `ϵ` is a chosen small positive value, then the prediction set `eC(Xn+1)` constructed with this `α'` will satisfy `P(Yn+1 ∈ eC(Xn+1)) ≥ 1 - α`. This `ϵ` is chosen by solving a minimization problem `min h(ϵ)` where `h(ϵ) = ϵ + g_f,ρ^(-1)(1-α / (1 - 2 * sum(e^(-2*mi*ϵ^2))))`, subject to `0 < ϵ ≤ 1` and `h(ϵ) ≤ 1`. This effectively adjusts the confidence level to account for the uncertainty introduced by using empirical distributions.": 1619,
    "The paper introduces Knowledge Progressive Networks (KPNs) for implicit knowledge transfer. When training the k-th domain model (which can be any source or the target domain), the hidden embeddings (`h(j)`) from previously trained j-th domain models are transferred to the k-th domain. Specifically, for an input token `xi`, the knowledge transferred from the j-th domain to the k-th domain is represented as `h(j:k) = LN(Dropout(W(j:k)h(j)))`, where `W(j:k)` is a weight matrix and `LN` is Layer Normalization. For the target model, these transferred knowledge embeddings from all previous source domains (`h(j:k)`) are aggregated. This aggregation is performed by summing the weighted transferred embeddings (`αj * h(j:k)`) with the current hidden embedding (`h(k)`) and applying Layer Normalization: `˜h(k) = LN(h(k) + Σj<k αj * h(j:k))`. The parameter `αj` is learnable and adjusts the weight of knowledge from each j-th domain. This updated aggregated embedding `˜h(k)` is then used as the feature input for both the mention detection (`fMD`) and entity typing (`fET`) tasks, allowing the target model to implicitly leverage knowledge from multiple source domains.": 1620,
    "The paper enhances prediction confidence and improves class disambiguation for unlabeled target data through the Entropy Minimization Network (EM-net). While the source domain has labels for training an optimal predictor, the target domain lacks them. The EM-net minimizes the entropy of the model's predictions on unlabeled target domain instances. The loss function, Lt_EM, calculates the negative sum of P(Ti)logP(Ti) for each candidate target word Ti, where P(Ti) is the predicted probability of the target word. By minimizing this entropy, the model is encouraged to make more confident, sharper predictions (i.e., probabilities closer to 0 or 1) for target domain samples. This process effectively widens the margin between positive and negative clusters, which helps the optimal decision boundary learned from the source domain to generalize better to the target domain. Crucially, the EM-net is applied from a \"\"latter epoch\"\" during training, rather than from the beginning. This delayed application allows the model to first learn good representations and establish a general decision boundary through contrastive learning (IC-net and HC-net) before being forced to make confident predictions on uncertain instances, thereby preventing premature label decisions and improving overall performance.": 1621,
    "Entity classification is performed in a parameter-free manner by leveraging the dot product between the contextualized representations of text tokens and the embeddings of the boundary tokens from the prompts. After the extended input sequence `X'` (original text + prompts) is processed by the Encoder (PLM), the final hidden layer output `H` provides the representation for each token. To enhance the model's awareness of start and end positions, `H` is passed through two separate Multi-Layer Perceptrons (MLPs), `Wstart` and `Wend`, resulting in `Hstart` and `Hend` respectively. The representations of the special `[ENT START]` and `[ENT END]` tokens from the prompts, denoted as `S = [s1, s2, ..., sm+1]` for start tokens and `E = [e1, e2, ..., em+1]` for end tokens, serve as prototypes for each entity class. For any given text token `xi`, its probability of being the start or end of an entity of a specific type is calculated by taking the dot product of its `Hstart` or `Hend` embedding with the corresponding prototype embedding from `S` or `E`. Specifically, `Pstart = softmax(Hstart ⋅ S)` and `Pend = softmax(Hend ⋅ E)`. This approach means that the classification of entity types is achieved by measuring the similarity between text token embeddings and the learned embeddings of the boundary markers, effectively using the boundary marker embeddings as classifiers without introducing additional trainable parameters. This parameter-free classification mechanism contributes to the model's ability to transfer to new domains without requiring structural adjustments and significantly improves inference efficiency by avoiding complex decoding processes or multiple QA turns. To address the extreme imbalance between non-entity tokens and entity boundary tokens in the label sequence, Focal Loss is employed as the training objective.": 1622,
    "The paper develops an automatic evaluation metric for idiomatic translation quality that aims to align closely with human judgments, particularly in a reference-free setting, by leveraging the capabilities of GPT-4. This addresses the limitations of traditional metrics like BLEU and COMET, which struggle to capture the nuances of idiomatic expressions. The core of the solution is a GPT-4-powered evaluation prompt. This prompt is meticulously designed to instruct GPT-4 on how to assess idiomatic translation quality. It includes explicit Evaluation Criteria based on a 1-3 point scale: 1 point for ignoring, mistranslating, or only translating the literal meaning; 2 points for conveying basic figurative meaning with minor imperfections; and 3 points for exceptional translation accurately conveying figurative meaning, context, and cultural nuances. The prompt explicitly asks GPT-4 to \"\"Focus on the idiom’s figurative meaning.\"\" For evaluation, GPT-4 is provided with the source English sentence, the idiom within it, and the Chinese translation to be evaluated. It is then instructed to generate only the score. To validate this metric, a small-scale evaluation set is manually constructed across Chinese-to-English (Zh→En), English-to-Chinese (En→Zh), and Japanese-to-English (Ja→En) language pairs. Translations are generated by various LMs (InstructGPT, BLOOMZ, InstructGPT003), and 20 sentence and translation pairs are annotated by humans for each point on the 1-3 scale. The correlation between GPT-4's scores and human annotations is then measured using Pearson's r, Spearman's ρ, and Kendall's τ, demonstrating that GPT-4 can serve as an effective evaluator for idiomatic expressions.": 1623,
    "The proposed framework, TACIT (Target-Agnostic framework for Cross-domain text classIficaTion), addresses this by integrating the robust feature disentanglement and unrobust feature distillation within a unified training process that exclusively uses source domain data. The overall loss function (`L`) is a joint loss combining the cross-entropy loss (`Lce`) for robust feature classification, the VAE loss (`Lvae`) for feature disentanglement, and the distillation loss (`Ldistill`) for unrobust feature separation. Weighted coefficients (`λ1`, `λ2`) are used to balance these losses. During training, all parameters of the teacher model are frozen, as it serves only to provide prior knowledge of unrobust features. In the inference phase, only the encoder part of the student model is used. It predicts the label of a new sample solely based on the robust feature `zµ`, regardless of whether the sample originates from the source or target domain. By adaptively decoupling robust and unrobust features and encouraging the separation of unrobust features through distillation, TACIT aims to learn highly generalizable robust features from the source domain alone, eliminating the need for any unlabeled target domain data or domain adversarial training. This design allows for seamless application across diverse target domains without target-specific training requirements.": 1624,
    "The Hash Distributed Beam Search 2 (HDBS2) algorithm addresses this by relaxing the strict layer synchronization of HDBS1 while still using a distributed memory approach with message passing. Similar to HDBS1, states are assigned to workers via a hash function. However, instead of waiting for all workers to finish a layer before proceeding, each worker immediately sends its information (best solution, f-value, open list size, completeness) to all other workers via dedicated channels (Rj) once it completes its own processing for the current layer. This allows workers to potentially proceed to the next layer sooner. When a worker generates a successor state, it checks if the assigned worker (j) has already finished its previous layer (i.e., if j is in its `Li` set, which tracks workers from which information about the previous layer has been received). If worker j has finished, the successor state is immediately pushed to worker j's incoming channel (Qj). If worker j has not yet finished its previous layer, the successor state is temporarily stored in a local buffer (`Pij`). Once worker i receives the completion information from worker j, all buffered states in `Pij` are then pushed to `Qj`. Each worker aggregates the information received from other workers about the previous layer to update its local best solution, global dual bound, and completeness flags. This relaxed synchronization, combined with local buffering, aims to reduce idle time and improve overall concurrency.": 1625,
    "Resource-efficient cross-domain machine learning is achieved by keeping the pre-trained source model `fS` frozen and intact during the entire process, meaning its parameters are unchanged and not finetuned. Instead, only the parameters of the newly introduced Input Transformation Layer (`θ`) and Output Mapping Layer (`ω`) are trained. The training process involves using a target-domain training set `{x(i)T, y(i)T}n i=1` to evaluate a task loss `Loss(byT, yT |θ,ω)`, such as cross-entropy. The reprogrammed model prediction `byT` is obtained by `Output-Mapping(fS(Input-Transform(xT |θ))|ω)`. The parameters `θ` and `ω` are then optimized end-to-end using an optimization algorithm, typically a gradient-based method. This approach significantly reduces the number of trainable parameters compared to finetuning the entire source model, making it particularly suitable for small-data regimes and scenarios with constrained model development costs. In black-box settings where `fS` is inaccessible for back-propagation, gradient-free methods like zeroth-order optimization can be used for training `θ` and `ω`.": 1626,
    "The paper integrates the online tree-size estimates into a clairvoyant restart strategy by defining a specific criterion for triggering a restart. A restart is decided at step k if the current resource consumption (rk) multiplied by a parameter γ (set to 100 in experiments) is less than the estimated total resources (ˆr(k)), i.e., γ · rk < ˆr(k). This condition implies that if the estimated total tree size is significantly larger than the current resources consumed, a restart might be beneficial. The estimate ˆr(k) is updated at every leaf node. To mitigate the high variance of ˆr(k) and ensure robust decisions, two safeguards are implemented: first, the restart condition must be satisfied for 50 consecutive steps (k's) before a restart is triggered; second, no restart is performed until at least 1000 final leaves (|Fk|) have been explored. This second safeguard ensures that enough search history is available for SCIP to build a better tree after a restart. The strategy disables restarts at the root node, focusing solely on dynamic, in-search restarts.": 1627,
    "The effective strategy for jointly optimizing the distinct objectives of domain adaptation, emotion semantic consistency, and target classification is an alternating stochastic gradient descent (SGD) procedure. The overall CycleEmotionGAN model involves parameters for two generators (GST and GTS), two discriminators (DT and DS), and one classifier (F). The optimization process alternates between two main steps:\n1.  Updating Generators (GST, GTS): In this step, the parameters of the discriminators (φT, φS) and the classifier (ϕ) are kept fixed. An SGD step is performed to update the generator parameters (θST, θTS) based on the combined loss LaCycleGAN. This combined loss includes the adversarial losses for both generators (LGAN(GST,DT) and LGAN(GTS,DS)), the cycle-consistency loss (Lcyc(GTS,GTS)), and the emotional semantic consistency loss (LESC(GST)). This step aims to make the adapted images indistinguishable from the target domain while preserving their emotional semantics and ensuring cycle consistency.\n2.  Updating Discriminators (DT, DS) and Classifier (F): In this step, the parameters of the generators (θST, θTS) are kept fixed.\n    *   Discriminators: SGD steps are performed to update the discriminator parameters (φT for DT and φS for DS) based on their respective adversarial losses (LGAN(GST,DT) and LGAN(GTS,DS)). The discriminators learn to distinguish between real and generated images.\n    *   Classifier: An SGD step is performed to update the classifier parameters (ϕ) based on the classification loss Ltask(F). This trains the classifier to accurately predict the emotion labels of the adapted source images.\nThis alternating optimization ensures that all components of the model are trained in a coordinated manner, allowing the generators to adapt images while preserving semantics, the discriminators to guide the adaptation towards the target distribution, and the classifier to learn robust emotion recognition on the adapted domain.": 1628,
    "The paper introduces a novel graph Laplacian-based loss to regularize the deep feature extractor ΦθM. Unlike conventional graph losses that build graphs on lower-level features to regularize higher-level ones, this approach reverses the direction. It constructs a graph Laplacian, denoted ∆FC, based on the similarity structure within the higher-level Common Factorised Space (CFS) features FC. This ∆FC is then used to regularize the lower-level features F = ΦθM(·) extracted by the feature extractor. The graph loss is expressed as Tr(F T∆FCF). This top-down regularization ensures that the feature extractor learns representations whose similarity structure reflects that of the more discriminative and aligned latent factors in the CFS layer. This is particularly important for retrieval tasks where deep features F are used as image representations, as it propagates the \"\"trustworthy\"\" knowledge from the CFS down to the feature extraction process.": 1629,
    "The paper proposes a Joint Domain Alignment and Discriminative Feature Learning (JDDA) framework that synergistically combines these two objectives. The overall training objective is defined by a total loss function L(Θ|Xs,Ys,Xt) = Ls + λ1Lc + λ2Ld.\n1.  Source Loss (Ls): This is the standard classification loss (e.g., softmax cross-entropy) applied to the labeled source data (Xs, Ys). It ensures the model learns to classify source samples correctly.\n2.  Domain Discrepancy Loss (Lc): The Correlation Alignment (CORAL) loss is used to minimize the distribution discrepancy between source (Hs) and target (Ht) features in the bottleneck layer. Lc = CORAL(Hs,Ht) aligns the covariance matrices of the source and target features, reducing domain shift.\n3.  Discriminative Loss (Ld): This is either the Instance-Based Discriminative Loss (LId) or the Center-Based Discriminative Loss (LCd) as described in Solutions 1 and 2. It is applied to the source features (Hs) to enforce intra-class compactness and inter-class separability.\nThe parameters λ1 and λ2 are trade-off parameters that balance the contributions of the domain discrepancy loss and the discriminative loss, respectively. The entire network, including shared feature extractors and classifiers, is trained end-to-end using mini-batch Stochastic Gradient Descent (SGD) and backpropagation. The discriminative loss, applied to source features, encourages the shared feature space to be well-structured with distinct clusters. This structured feature space then facilitates the domain alignment process, as aligning compact and well-separated clusters is more effective than aligning scattered features. Consequently, the target features, being aligned with the discriminative source features, also become more discriminative, leading to improved classification performance on the unlabeled target domain.": 1630,
    "The paper effectively performs domain adaptation by transforming only the source feature space without resorting to dimensionality reduction, thereby mitigating the risk of information loss. Unlike many existing unsupervised domain adaptation methods that project both source and target data into a lower-dimensional latent common space (e.g., Transfer Component Analysis, Subspace Alignment), the proposed method operates directly on the original D-dimensional feature space. It applies a unilateral transformation, F(x) = (A + I)x, exclusively to the source features. This transformation aims to convert the source features into the target domain's feature space, making the source and target distributions of input features more similar. By avoiding dimensionality reduction, the method ensures that all potentially important information present in the original features is retained and transferred. This approach contrasts with methods that find common parts of two domains in a reduced dimension, which can inadvertently discard unique or discriminative information. Once the transformation matrix A is learned by minimizing the discrepancy between the transformed source and original target feature distributions, any off-the-shelf supervised classifier can be trained using the transformed labeled source data, which are now aligned with the target domain's characteristics.": 1631,
    "The paper provides theoretical guarantees for S-disc in several aspects:\n1.  Consistency and Convergence Rate: Theorem 4 establishes the consistency of the empirical S-disc estimator, $\\varsigma_{\\ell}^{\\mathcal{H}}(\\hat{P}_T, \\hat{P}_S)$, showing it converges to the true S-disc, $\\varsigma_{\\ell}^{\\mathcal{H}}(P_T, P_S)$, as the number of source ($n_S$) and target ($n_T$) samples increase. This holds under the assumption that the loss function $\\ell$ is bounded. Corollary 6 further specifies the convergence rate for the 0-1 loss, showing it is $O(n_T^{-1/2} + n_S^{-1/2})$ under a mild condition related to the Rademacher complexity of the hypothesis class, which is naturally satisfied by linear-in-parameter models (Lemma 5).\n2.  Tighter Generalization Error Bound: Theorem 7 derives a generalization error bound in the target domain based on S-disc. For any hypothesis $h \\in \\mathcal{H}$, the target risk $R_{\\ell}^{T}(h, f_T)$ is bounded by $R_{\\ell}^{S}(h, h_S^*) + R_{\\ell}^{T}(h_S^*, f_T) + \\varsigma_{\\ell}^{\\mathcal{H}}(P_T, P_S)$. This bound is shown to be tighter than the existing bound based on X-disc (Mansour, Mohri, and Rostamizadeh 2009a, Theorem 8) because S-disc is never larger than X-disc.\n3.  Finite-Sample Generalization Error Bound: Theorem 8 extends the generalization error bound to the finite-sample case for the 0-1 loss. It shows that the regret in the target domain, $R_{\\ell}^{T}(h, f_T) - R_{\\ell}^{T}(h_T^*, f_T)$, is bounded by terms involving the empirical loss in the source domain, the difference between $h_T^*$ and $h_S^*$ in the target domain, the empirical S-disc $\\varsigma_{\\ell}^{\\mathcal{H}}(\\hat{P}_T, \\hat{P}_S)$, and Rademacher complexity terms that decay with sample size. This implies that if $h_T^*$ is sufficiently close to $h_S^*$, minimizing the empirical source error and selecting a good source based on S-disc leads to good target generalization.": 1632,
    "Contrastive Feature Alignment (CFA) is employed to bridge the distribution gap across domains and prevent false alignment, especially when only a small amount of target labeled data is available. CFA consists of two main components: semantic alignment (SA) and semantic separation (SS). Semantic alignment aims to ensure identical distributions of feature representations from different domains but belonging to the same class. Semantic separation further alleviates false alignment by guaranteeing that distributions from both different classes and domains are as dissimilar as possible. Mathematically, this is achieved by parameterizing the two networks as `gs` and `gt` and defining a contrastive function `omega(u,v)`. This function is `||u-v||^2` if the sentiment labels `ys_k` and `yt_k'` are the same, promoting similarity for same-class features. Conversely, it is `max(0, D - ||u-v||^2)` if the sentiment labels are different, promoting dissimilarity for different-class features, where `D` is a separation parameter (set to 1). The overall CFA loss `Lcfa` is the sum of `omega(gs(xs_k,as_k),gt(xt_k',at_k'))` over all relevant pairs, effectively aligning and separating features based on their semantic class.": 1633,
    "The learning of the domain adaptation model and the construction of the transferable curriculum are jointly optimized through an alternating minimax optimization procedure. This process delivers a saddle-point solution for the Transferable Curriculum Learning (TCL) model. The optimization involves two main steps that are alternated:\n1.  Model Parameter Update: The parameters of the feature extractor (`theta_f`) and label classifier (`theta_y`) are updated by minimizing the combined objective `E_Gy - E_Gd`. Simultaneously, the parameters of the domain discriminator (`theta_d`) are updated by minimizing `E_Gd`. This is a standard minimax game for domain adversarial learning, but crucially, the losses `E_Gy` and `E_Gd` are weighted by the current transferable curriculum `w(x_s^i)`.\n2.  Curriculum Weight Update: With the model parameters (`theta_f`, `theta_y`, `theta_d`) fixed from the previous step, the transferable curriculum weights `w(x_s^i)` are updated. This update is based on the criterion `w(x_s^i) = 1(l_i + lambda * tau_i <= gamma)`, where `l_i` and `tau_i` are re-calculated using the current state of the `G_y` and `G_d` models. This ensures that the curriculum adapts to the evolving understanding of sample easiness and transferability as the model learns.\nThis alternating process allows the model to progressively improve by filtering out noisy samples and transferring relevant ones, while the curriculum itself becomes more refined based on the model's current performance and understanding of domain similarity.": 1634,
    "The paper proposes a collaborative learning framework to jointly optimize the Adaptation Network (AN) and the Target-Specific Network (TSN), enabling them to mutually reinforce each other's performance. This is achieved by incorporating a mutual learning term into both networks' loss functions and training them iteratively. The AN's overall loss `LA(θA;X)` includes `Lmutual(θA;Xt,θT)`, which measures the similarity between the AN's predictions `pθA(xt)` and the TSN's predictions `pθT(xt)` for target instances. Simultaneously, the TSN's loss `LT(θT;Xt)` includes `Lmutual(θT;Xt,θA)`, which measures the similarity between the TSN's predictions `pθT(xt)` and the AN's predictions `pθA(xt)`. During training, in each mini-batch step, both networks evaluate their predictions. The AN is updated using its loss `LA`, which includes the term encouraging consistency with the TSN. Subsequently, the TSN is updated using its loss `LT`, which includes the term encouraging consistency with the AN, along with the clustering regularization and virtual adversarial training terms. This iterative process allows the AN to guide the TSN by providing soft labels, and as the TSN specializes and potentially outperforms the AN on target data, its improved predictions can, in turn, provide better targets for the\nAN. This collaborative feedback loop, where `θA` and `θT` are updated using the Adam optimizer, allows both networks to learn from each other, with the target-specific network often achieving superior performance due to its specialization, which then further benefits the adaptation network.": 1635,
    "IATN applies interactive attention learning through two distinct attention mechanisms: Interactive Word Attention for sentences and Aspect Attention for aspects. For Interactive Word Attention, each hidden state (hi_s) of the sentence is combined with the aspect pooling vector (hp_a) to form a joint representation. A score function, defined as `tanh(hi_s · Ws + bias_s)`, is applied to this combined representation to generate a score for each word. These scores are then normalized using a softmax function to produce attention weights (alpha_i). These alpha_i weights dynamically quantify the contribution of each word in the sentence, influenced by the associated aspects. The final sentence representation (Sr) is a weighted sum of the sentence hidden states using these alpha_i weights. Similarly, for Aspect Attention, each hidden state (hi_a) of the aspect is spot-multiplied with the sentence pooling vector (hp_s). A score function, `tanh(hi_a · hp_s · Wa + bias_a)`, is applied to this product, and the results are normalized via softmax to yield attention weights (beta_i). These beta_i weights dynamically determine the importance of each word within the aspect, influenced by the overall sentence context. The final aspect representation (Ar) is a weighted sum of the aspect hidden states using these beta_i weights. This interactive and dynamic weighting ensures that both sentence words and aspect words contribute proportionally to the final sentiment representation based on their relevance to each other and the overall sentiment.": 1636,
    "A generalized idiom usage recognition model is developed using a transfer learning approach that eliminates the need for annotated idiom usage examples. The core component, the semantic compatibility model, is first trained on large raw text corpora (e.g., Wikipedia) with the objective of predicting the semantic compatibility between a context and a single word. This training uses a modified Continuous Bag-of-Words (CBOW) architecture with negative sampling, incorporating the novel context representation (bidirectional LSTM and attention) and the semantic compatibility evaluation layer (multilayer perceptron). The loss function maximizes the probability of positive (compatible) context-word pairs and minimizes the probability of negative (incompatible) pairs. Once this generalized semantic compatibility model is trained, it is applied to idiom usage recognition. Since idioms are multi-word expressions, a literal representation of the idiom is first constructed. Two methods are explored for this: Average of Word Embeddings (AWE), which averages embeddings of all words in the idiom, and Average of Keyword Embeddings (AKWE), which averages embeddings of only key words (e.g., nouns in verb-noun combinations). This literal representation (vl) is then treated as the \"\"target word\"\" for the pre-trained semantic compatibility model. The compatibility score between the context representation (vc) and the idiom's literal representation (vl) is calculated using the formula: sigma(v'l * L(vc) + bu), where 'bu' is a bias term tuned on a small development dataset. If this score is greater than 0.5, the idiom instance is classified as literal usage; otherwise, it is labeled as figurative usage. This two-stage process allows the model to generalize across different idioms without requiring idiom-specific annotated training data.": 1637,
    "The Multimodal Shifting component handles this integration. It dynamically shifts the original word embedding e(i) by incorporating the nonverbal shift vector h(i)m. The multimodal-shifted word representation e(i)m is calculated as e(i)m = e(i) + αh(i)m. A crucial aspect is the scaling factor α, which is applied to the nonverbal shift vector to ensure its magnitude is not excessively large compared to the original word embedding. This scaling factor is determined by α = min(β, 1), where β is a threshold hyper-parameter tuned via cross-validation. This mechanism allows the model to adjust the word representation in the embedding space based on nonverbal cues, while maintaining the direction of the shift vector and controlling its impact. The resulting sequence of multimodal-shifted word representations E is then used by a word-level LSTM to encode an utterance-level multimodal representation for downstream tasks.": 1638,
    "The paper proposes a Soft-mixing variant to adaptively combine the prediction loss and the variance of historical prediction losses metrics. This variant calculates the instance weight w as (1/τ) * [β * (-yT log p(y|x)) + (1 - β) * std(ht-1) + ε]. Here, β is a balancing ratio that dynamically decreases linearly from 1 to 0 across the fine-tuning epochs. In the early epochs, when β is close to 1, the prediction loss metric (which emphasizes learning target-specific knowledge) dominates the weighting. As fine-tuning progresses and β decreases towards 0, the variance of historical prediction losses metric (which helps preserve shared knowledge and address overfitting) gains more influence. This adaptive combination allows the model to dynamically shift its focus from aggressively learning new target knowledge to balancing it with the preservation of shared knowledge, leading to optimized performance.": 1639,
    "To explicitly regularize the learning of domain-invariant representations and ensure semantic consistency, the paper proposes a \"\"Perceptual Consistency Regularisation\"\" loss, `Lpercep`. This regularization leverages the shared sequence encoder `fenc` as a perceptual function. The objective is to minimize the L-2 distance between the hidden representation of an original sequence and the hidden representation of its generated counterpart in another domain. For example, for a source sequence `xS`, its hidden representation is `zS = fenc(xS,θS)`. When `xS` is transformed to `ˆxT = GS→T(xS,θS)`, the hidden representation of this generated sequence is `ˆzS = fenc(ˆxT,θT)`. The `Lpercep` term `||fenc(xS,θS) - fenc(GS→T(xS,θS),θT)||2` encourages `zS` and `ˆzS` to be semantically similar, thereby enforcing that the transformation preserves the core semantic features captured by the encoder. A similar constraint is applied for transformations from the target to the source domain. This mechanism ensures that the encoder learns robust, semantically meaningful domain-invariant features.": 1640,
    "The paper addresses the challenge of training a classifier with limited labeled target domain data by employing a semi-supervised learning approach based on label propagation. A classification network `C`, implemented as a fully-connected network with a Softmax function, is trained on the latent representations `H` (output of encoder `W`). The loss function `Lcg(W,C)` for the classifier combines two main components: a standard cross-entropy loss for both source domain labeled data (`YS`) and the few labeled target domain instances (`YTl`), and a manifold regularization term. The manifold regularization is introduced by adding `tr(H^T Lg H)`, where `Lg` is a Laplacian matrix constructed from the augmented data `X`. `Lg` is composed of `LgS` (Laplacian for source) and `LgT` (Laplacian for target), which are built using a 5-Nearest Neighbor (5-NN) graph with 0/1 weights. This regularization encourages instances that are close in the latent feature space to have similar labels, effectively propagating label information from the few labeled target instances to the abundant unlabeled ones, and also leveraging the manifold structure of the data. This bridges the gap between domains via labels and helps train a more robust classifier for the target domain despite data scarcity.": 1641,
    "The framework achieves simultaneous multi-attribute transfer and style preservation through a unified auto-encoder and cycle-consistent Generative Adversarial Network (GAN) architecture. The generator G, composed of an encoder (Enc), a swap module (Swap), and a decoder (Dec), first encodes input images into disentangled latent units, where each unit corresponds to a specific attribute or attribute-irrelevant information. The Swap module then selectively exchanges these latent units between two input images based on predefined strategies, effectively transferring attributes. For training, two swapping strategies are employed: one where attributes with different values are swapped (S = A ∪ N), and another where only a subset of common attributes are swapped (S = A). The decoder then synthesizes new images from these modified embeddings. A discriminator D is used to enhance image quality (via adversarial loss) and ensure proper attribute transfer (via attribute classification loss). Crucially, a cycle-consistency loss is applied, re-encoding the synthesized images and re-swapping the attributes back to reconstruct the original inputs, which helps preserve pixel-wise information and image details. The integration of the novel attribute verification loss further ensures that the disentanglement is robust and that image-level style diversity is maintained, allowing the model to transfer multiple attributes simultaneously without affecting unchanged attributes or background details, all within a single, end-to-end trainable model.": 1642,
    "AlignFlow provides a flexible training objective that combines Maximum Likelihood Estimation (MLE) and adversarial training. The most general AlignFlow objective, LAlignFlow, is formulated as a sum of adversarial GAN loss terms and MLE terms. Specifically, LAlignFlow(GB→A, CA, CB; λA, λB) = LGAN(CA, GB→A) + LGAN(CB, GA→B) - λA * LMLE(GZ→A) - λB * LMLE(GZ→B). Here, LGAN represents the adversarial loss (e.g., cross-entropy GAN loss) for distinguishing real samples from generated ones, with critics CA and CB for domains A and B respectively. LMLE represents the maximum likelihood estimation loss, which requires a tractable prior density (e.g., isotropic Gaussian) over the latent space Z and uses the change-of-variables formula for likelihood evaluation. The hyperparameters λA ≥ 0 and λB ≥ 0 control the strength of the MLE terms for domains A and B. This objective is minimized with respect to the parameters of the generators (GA→B, GB→A, GZ→A, GZ→B) and maximized with respect to the parameters of the critics (CA, CB). By adjusting λA and λB, the framework can be configured for pure adversarial training (when λA = λB = 0, making the prior over Z inactive), pure MLE training (when λA = λB → ∞, effectively ignoring adversarial terms), or a hybrid approach. This flexibility allows the model to leverage the benefits of both likelihood-based training (e.g., better density estimation, meaningful latent space) and adversarial training (e.g., high-quality sample generation, likelihood-free training).": 1643,
    "The score function can be estimated using the reconstruction residual of CycleGAN, eliminating the need for training an additional denoising autoencoder (DAE). Most image translation methods, including CycleGAN, inherently possess an autoencoding structure due to the cycle consistency constraint. The input image is mapped to the target domain by the generator (G) and then reconstructed back to the input domain by the decoder (F). The combination of G and F forms an autoencoder, which can be used to approximate the score function. This approach leverages the existing architecture of the IT model, providing an efficient way to estimate the score function without requiring additional training.": 1644,
    "The DADA framework is extended for partial and open set domain adaptation to address their specific challenges.\nFor Partial Domain Adaptation (DADA-P), where the target label space is subsumed by the source, the issue of negative transfer from outlier source categories is addressed. DADA-P introduces a reliable category-level weighting mechanism applied to the source discriminative adversarial loss. First, an averaged conditional probability vector ¯c is computed by averaging ¯p(xt) over all target data. This ¯c is then normalized by its largest element. A category weight vector c is derived by a convex combination of this normalized vector and an all-ones vector 1:\nc = λ * (¯c / max(¯c)) + (1 - λ) * 1\nwhere λ ∈ [0,1] is a hyper-parameter to suppress detection noise in early training. This category weight vector c is then applied to the source discriminative adversarial loss (Ls) for any source instance:\nLs(G,F) = - (1/ns) * sum_{i=1 to ns} [ c_ys_i * ( (1 - pK+1(xs_i)) * log(pys(xs_i)) + pK+1(xs_i) * log(1 - pys(xs_i)) ) ]\nAdditionally, to avoid negative transfer caused by increased predicted probabilities on outlier source categories when minimizing Lt_em, DADA-P minimizes Lt_em over F(·) only. The overall objective for DADA-P is:\nmin F [ λ(Ls + LtF) + Lt_em ]\nmax G [ λ(Ls + LtG) - Lt_em ]\nThis allows DADA-P to alleviate negative transfer while promoting joint distribution alignment in the shared label space.\nFor Open Set Domain Adaptation (DADA-O), where the source label space is subsumed by the target, the goal is to classify target instances as either \"\"known\"\" or \"\"unknown\"\" categories. DADA-O modifies the target adversarial loss to train the classifier to classify all target instances as the \"\"unknown\"\" category with a small probability q. Assuming pK(xt) is the predicted probability for the unknown category (Kth element of p(xt)), the modified target adversarial loss when minimized over F(·) is:\nLtF(G,F) = - (1/nt) * sum_{j=1 to nt} [ q * log(pK(xt_j)) - (1 - q) * log(pK+1(xt_j)) ]\nwhere 0 < q < 0.5. When maximized over G(·), the original discriminative loss LtG from (4) is still used. Replacing LtF in the main minimax problem (7) with this modified loss for DADA-O allows for a balance between domain adaptation for known instances and outlier rejection for unknown instances. The choice of q is crucial: a too small q prevents correct classification of unknown instances, while a too large q leads to misclassification of known instances. An appropriate q helps the feature extractor separate unknown target instances from known ones while aligning joint distributions in the shared label space.": 1645,
    "The paper combines two distinct pseudo-labeling methods: Nearest Class Prototype (NCP) and Structured Prediction (SP). NCP assigns pseudo-labels based on the distance of a target sample's projection to the mean vector of projected source samples for each class (source class prototypes). This method tends to be confident in samples close to the source data. SP, as described in Solution 1, leverages the intrinsic clustering structure of the target domain, being confident in samples close to target cluster centers, regardless of their proximity to source data. To take advantage of the complementary nature of these two methods, the paper proposes a simple combination rule: for each target sample (xt) and each class (y), the final conditional probability p(y|xt) is determined by taking the maximum of the probabilities derived from NCP (p1(y|xt)) and SP (p2(y|xt)). The final pseudo-label (ˆyt) for a target sample is then assigned as the class with the highest combined probability (argmax p(y|xt)). This \"\"max\"\" combination strategy allows the system to leverage the strengths of both local (NCP) and global/structural (SP) information, leading to more robust and accurate pseudo-labeling.": 1646,
    "The paper addresses this challenge in the Two-Stage Training based Unsupervised Domain Adaptation (2ST-UDA) framework by employing a gradual pseudo-label acquisition and training mechanism. After the initial Weighting Scheme based Unsupervised Domain Adaptation (WS-UDA) model (including Es, Epj, D, C) is pre-trained, 2ST-UDA leverages its ability to annotate pseudo labels for target instances. To mitigate the impact of potentially wrong pseudo labels, the framework introduces a dynamic confidence threshold (Δ), initially set high (e.g., 0.98). Only target samples whose pseudo-labels, derived from both the weighted predictions of all source classifiers (ˆyS) and the predicted results of a target classifier (ˆyT), are above this threshold are initially trusted and added to a pseudo-label set (Tl). This set is then used to train a target-specific extractor (Et). During training, the confidence threshold Δ is gradually decreased by a constant amount (η, e.g., 0.02) in each iteration, allowing more pseudo labels to be acquired over time. The training process for Et continues iteratively, using mini-batches from the growing Tl. The process terminates when the number of pseudo labels added to Tl does not significantly increase (e.g., less than N=10 new labels) or when the threshold Δ drops below a certain minimum (e.g., 0.5), ensuring that only sufficiently confident pseudo labels are used to refine the target-specific extractor.": 1647,
    "The paper proposes Deep Adversarial Mutual Learning (DAML), an end-to-end framework that integrates adversarial learning and mutual learning. The framework consists of two identical groups, each comprising a Feature Extractor (FE), a Domain Discriminator (D), a Sentiment Classifier (C), and a Label Prober (P).\n1.  Domain Adaptation (Adversarial Learning): Each group's Feature Extractor (FE) is trained to produce domain-invariant features using an adversarial learning approach. A Domain Discriminator (D) attempts to distinguish between source and target domain features. The FE, in turn, aims to \"\"fool\"\" the D by generating features that make it difficult for D to identify the domain. This min-max game is implemented using a Gradient Reversal Layer (GRL) which reverses the gradient of the domain discriminative loss (LDOM) during back-propagation to the FE, ensuring the FE learns domain-independent representations.\n2.  Sentiment Classification (Supervised Learning): Each Sentiment Classifier (C) is trained on the labeled source domain data using a standard cross-entropy classification loss (LCLS).\n3.  Mutual Learning (Target Domain Sentiment): The novel Label Probers (P) facilitate mutual learning. For target domain documents, the prober in one group (e.g., P1) learns to mimic the sentiment predictions of the classifier from the *other* group (e.g., C2) via a mutual learning loss (LML), which is a KL divergence. This LML guides the learning of the feature extractor in the prober's *own* group (e.g., FE1).\nThe overall objective for each group (e.g., LG1) combines these losses: LG1 = LCLS1 + λD * LDOM1 + λML * LML1. The parameters (λD, λML) control the influence of domain adaptation and mutual learning. The two groups are mutually learned in an end-to-end manner through an alternating optimization strategy, where gradients from all three loss components (LCLS, LDOM, LML) contribute to updating the feature extractors, enabling them to learn both domain-invariant data distributions and sentiment information from both source and target domains.": 1648,
    "The paper proposes a novel \"\"cycle-structure consistency\"\" mechanism to address the limitation of traditional cycle consistency in multimodal settings, where enforcing exact image reconstruction (X to Xrec) would encourage the mapping to ignore the random latent vector and thus limit diversity. Instead, this approach only encourages the *segmentation masks* of the original and reconstructed images to be as close as possible, rather than the images themselves. This means that while the transferred style of the reconstructed image is not constrained, its underlying structure is preserved. Specifically, for the forward cycle (X → ¯Y → Xrec), the reconstructed image Xrec is generated by Gy(Ey(Gx(Ex(x,z),z))). The parsing net Px (using Ep x) then predicts a segmentation mask ˆxrec from Xrec. The Lcycle1(Ex,Gx,Ey,Gy,Ep x,Px,X, ˆX) loss is a multi-class cross-entropy loss calculated between ˆxrec and the ground-truth segmentation mask ˆx of the original image. A similar Lcycle2(Ey,Gy,Ex,Gx,Ep y,Py,Y, ˆY) loss is applied for the backward cycle (Y → ¯X → Yrec), comparing ˆyrec with ˆy. By focusing the cycle consistency on structural information (segmentation masks) rather than pixel-level similarity, the model is allowed to generate diverse styles in the intermediate translation step (¯Y or ¯X) while still ensuring that the fundamental object structures are maintained upon reconstruction, preventing the collapse of the latent space and enabling multimodal outputs.": 1649,
    "The Contextual Memory Fusion Network (C-MFN) integrates the learned contextual information (`H` and `ˆH`) into the Memory Fusion Network (MFN), which primarily models the punchline. This integration is achieved through a novel initialization trick for MFN's two types of memories: the System of LSTMs and the Multi-view Gated Memory. For the System of LSTMs, which comprises `M` unimodal memories (one for each modality in the punchline), the LSTM cell state of modality `m` is initialized using `Dm(hm,1≤n<NC)`. Here, `Dm` is a fully connected neural network that maps the unimodal context information (`hm,n`) to the cell state of the `mth` LSTM. The Multi-view Gated Memory, which stores multimodal information, is initialized based on a non-linear projection `D(ˆH)`, where `D` is also a fully connected neural network. This initialization allows the MFN to start its punchline processing already conditioned on the comprehensive unimodal and multimodal contextual representations, enabling a more informed humor prediction. The MFN then processes the punchline word-by-word, updating its memories, and the final humor prediction is conditioned on the last state of both the System of LSTMs and the Multi-view Gated Memory using an affine mapping with Sigmoid activation.": 1650,
    "The reverse classification accuracy (RCA) measure is refined into RCA* to mitigate the impact of error accumulation. This is achieved by introducing a comparative baseline that accounts for the inherent error introduced by the pseudo-labeling and re-training process itself. First, a task classifier C is trained on the annotated source domain dataset (Ds). This classifier C is then used to pseudo-label the unlabeled target data (Dt), creating a pseudo-labeled target dataset. A new classifier, `C_hat`, is subsequently trained using this pseudo-labeled Dt, employing the same architecture and training algorithm as C. To establish the baseline, C is also used to pseudo-label a held-out dataset that originates from the same distribution as Ds. A second new classifier, `C_prime`, is then trained using this pseudo-labeled held-out source domain data. Finally, RCA* is calculated as the difference between the accuracy of `C_prime` and `C_hat` when both are evaluated on a held-out subset of the source domain (D'_s). This differential measurement isolates the performance drop attributable to domain shift by subtracting the error accumulated purely from the pseudo-labeling and re-training process.": 1651,
    "The paper achieves joint optimization by defining a combined objective function L(θe, θd, θc) that integrates both the MRC task loss and the domain classification loss. The MRC task loss, denoted as LD(θe, θd), is calculated as the negative log-likelihood of predicting the correct answer span, and it is applied exclusively to the labeled data from the source domain. This ensures that the encoder (θe) and decoder (θd) are trained to perform the core Machine Reading Comprehension task effectively. The domain classification loss, LC(θe, θc), is the negative log-likelihood of predicting the correct domain label, computed over both the source domain data and the pseudo-generated target domain data. The overall objective function is formulated as L = LD - λLC, where λ is a trade-off parameter that balances the two objectives. To optimize this combined objective, the framework employs a gradient-reversal layer. This layer is strategically placed between the encoder and the domain classifier. During backpropagation, the gradient-reversal layer inverts the sign of the gradients for the domain classification loss with respect to the encoder's parameters. This mechanism ensures that while the domain classifier minimizes LC (becoming better at distinguishing domains), the encoder simultaneously maximizes LC (becoming better at \"\"fooling\"\" the classifier), thereby learning domain-invariant features. All components (encoder, decoder, and domain classifier) are jointly optimized in an end-to-end fashion, allowing for robust domain adaptation while maintaining strong MRC performance.": 1652,
    "The paper proposes an alternating training process for the fine-grained knowledge fusion model, integrating the source and target model learning. The process is outlined in Algorithm 1:\n1.  Initialization: Parameters of both the source model (`θS`) and target model (`θT`) are initialized. Optionally, they can be pre-trained on source domain data (warm-up).\n2.  Alternating Training Episodes: The training proceeds in episodes until convergence.\n    *   Source Model Training Phase: For a specified number of `I` steps (teach step), the source model is trained ahead. In each step, a batch of samples from the source data is used to compute the source model's sequence labeling loss (`LS`) and its sample classification loss (`LS_sc`). `θS` is updated based on these losses.\n    *   Target Model Training Phase: After the `I` steps of source model training, the parameters of the source model (`θS`) are fixed (a \"\"gradient block\"\" is applied, meaning no gradients flow back to `θS`). The source model is then used to predict soft targets (`pS`) for the target domain data.\n        *   A batch of samples from the target data is sampled.\n        *   The relevance modeling (as described in Solution 1) is performed to compute `wsamp` (sample relevance weight) and `welem` (element relevance weight).\n        *   Based on these relevance weights, the fine-grained `α` values (`α_samp`, `α_elem`, or `α_multi`) are computed (as described in Solution 2).\n        *   The target model predicts its probability distributions (`pT`).\n        *   The target model's sequence labeling loss (`LT_SEQ`) and knowledge distillation loss (`LT_KD`) are computed using the fine-grained `α` values.\n        *   The total target model loss (`LT`) and its sample classification loss (`LT_sc`) are computed.\n        *   `θT` is updated based on these losses.\nThis alternating strategy ensures that the source model provides stable and updated soft targets for the target model, while the target model learns to fuse knowledge based on the fine-grained relevance, preventing negative transfer by controlling the influence of the source model.": 1653,
    "To address small sample humor recognition, the tensor embedding method is integrated with a semi-supervised label propagation procedure. First, using the lexical similarity captured by the tensor embeddings, a similarity graph (G) is constructed. For each data point (sentence embedding), its K nearest neighbors are identified. An affinity matrix (W) is then formed, where Wij is 1 if data points i and j are connected (i.e., i is a neighbor of j or vice versa), and 0 otherwise. Label propagation then iteratively updates the label assignments (F) for all data points. The update rule is F(t + 1) = αWF(t) + (1 - α)Y, where F(t) is the label matrix at iteration t, α is a propagation coefficient, and Y is the initial label matrix (containing known labels for labeled data and zeros for unlabeled data). This process propagates labels from known data points to their neighbors in a weighted average manner until convergence, yielding F*. For each data point xi, its final predicted label yi is determined by taking the argmax of the corresponding row in F*. This allows the method to effectively predict labels for a large portion of unlabeled data using only a small initial set of known labels.": 1654,
    "The paper introduces an Adjustable Joint Beam Search algorithm (Algorithm 2) to address the challenge of generating sentences that convey a metaphorical sense by jointly considering a target verb and its literal \"\"fit word.\"\" Unlike standard beam search, which optimizes for a single input, this algorithm takes two inputs: the target verb (w1, e.g., \"\"devoured.v\"\") and its fit word (w2, e.g., \"\"enjoyed.v\"\"). The goal is to generate a common context where the target verb is used metaphorically, implying its contextual sense aligns with the literal sense of the fit word. The process begins by initializing the decoder states for both inputs. At each time step (t) during decoding, the algorithm calculates score distributions for both inputs (scores1 and scores2) from their respective beam states. These scores are then combined using a weighted summation: `scores = α · scores1 + β · scores2`. The adjustment factors, alpha (α) and beta (β), are crucial for accounting for the differing influences of the target verb and fit word. These factors are calculated based on the word frequencies (wf) in the corpus: `α = σ(1 - wf(word1) / (wf(word1) + wf(word2)))` and `β = σ(1 - wf(word2) / (wf(word1) + wf(word2)))`, where σ is the sigmoid function. This calculation ensures that words with lower frequency (which are more likely to be metaphorical according to H2) have a relatively higher influence on the adjustment factors. The algorithm then selects the top k candidates from this joint score distribution, ensuring that half of the chosen words are duplicates but originate from distinct beams, leading to one-to-one correspondence between the two sequences. Although the sequences select the same word at each step, their underlying hidden states remain distinct. This joint decoding process allows the model to find an intersection between the metaphorical usage of the target verb and the literal usage of its fit word, resulting in a sentence that accommodates both.": 1655,
    "The paper addresses pun detection and location jointly by formulating them as a single sequence labeling problem. This is achieved by assigning a tag (from the designed BPA scheme) to each word in a sentence. The model architecture adopted for this task is a bidirectional Long Short Term Memory (BiLSTM) network stacked on top of a Conditional Random Fields (CRF) layer. Given an input context (x1, x2, ..., xn), the model aims to generate the corresponding tag sequence (y1, y2, ..., yn). The input to the model for each word is a concatenation of three types of features: transformed character embeddings (obtained from character-level LSTM networks and highway networks), pre-trained word embeddings (Glove), and the binary position indicators. This combined input is fed into the BiLSTM network, which captures contextual information across the sequence. The output of the word-level BiLSTM (Z = (z1, z2, ..., zn)) is then passed to the CRF layer. The CRF layer is responsible for capturing label dependencies across the sequence, defining a conditional probability P(y|x) over possible tag sequences. During training, the negative log-likelihood summed over all training instances is minimized. During testing, the Viterbi algorithm is used to efficiently find the optimal tag sequence (y*) that maximizes P(y|x), thereby jointly performing pun detection (by checking for the presence of a 'P' tag) and location (by identifying the word tagged 'P').": 1656,
    "The paper enhances Part-of-Speech tagging for noisy user-generated text by combining multiple types of input representations. This is achieved by concatenating several distinct vector sequences at each word position to form the comprehensive input to the core bidirectional Long Short-Term Memory (LSTM) layers. These input components include: 1) a feature embedding vector, which is derived from manually-defined linguistic features (as described in Solution 1); 2) the output from a character-level encoder, which is implemented as a bidirectional LSTM processing character embeddings to capture sub-word information and handle out-of-vocabulary words; 3) pre-trained FastText vectors, which provide sub-word embeddings and allow for obtaining vectors for out-of-vocabulary words; and 4) pre-trained Word2Vec vectors, specifically trained on a large corpus of German Tweets to capture domain-specific word semantics. By concatenating these diverse representations, the model benefits from explicit linguistic knowledge, sub-word morphology, and broad as well as domain-specific word semantics, providing a rich and robust input for the LSTMs to process the noisy and informal characteristics of user-generated text.": 1657,
    "The paper integrates pragmatic reasoning directly into the training objective by optimizing the pragmatic likelihood maxθ logl1(t | U,O;θ) for the full l1 neural Rational Speech Acts (RSA) architecture. This involves backpropagating gradients through the entire architecture, including the nested RSA layers (l1, s1, l0). The optimization problem is formulated to include several normalization terms (Zl1, Zs1, Zl0) which are computed using the log-sum-exp (LSE) function. Specifically, the objective includes terms that:\n1.  Maximize the literal applicability of the observed utterance to its target and maximize the margin against distractors (logLθ U,Ot − logZl0(U | O;θ)). This forms the non-pragmatic base.\n2.  Make pragmatic adjustments by enforcing a margin between the literal listener's (l0) predictions for low-cost alternative utterances (U') and the observed utterance (U) on the target referent (−logZs1(t | O;θ)). This term is crucial as it pushes the literal applicability (Lθ U',Ot') upward for distractors, simulating the inference that if a cheaper alternative was not used, it must apply to distractors. This mechanism allows the model to learn from the omission of utterances.\n3.  Ensure the pragmatic listener's (l1) predictions are well-calibrated by enforcing a margin between the pragmatic speaker's (s1) prediction for the true utterance and predictions for distractors (−logZl1(U | O;θ)). This ensures the true utterance is down-weighted for distractors after the speaker's cost-sensitive adjustments.\nBy optimizing this comprehensive objective, the model learns semantic parameters (θ) that are better calibrated for pragmatic usage, leveraging indirect pragmatic evidence to reduce the data complexity of learning.": 1658,
    "The paper establishes an incremental paper writing framework by chaining three distinct generation tasks, where the output of a preceding task serves as the input for the subsequent one. First, the system performs title-to-abstract generation, taking an input title along with predicted related entities (obtained from the link prediction module) to produce a paper abstract. Second, it executes abstract-to-conclusion and future work generation, using the previously generated abstract as the primary input to create the conclusion and future work section. Finally, the framework performs conclusion and future work-to-new title generation, utilizing the generated conclusion and future work section to predict a new title for a potential follow-on paper. All these generation tasks leverage the same underlying memory-attention network architecture, adapting the input and output types as required. This consistent architecture across stages allows for a modular and coherent approach to building upon generated content, enabling the system to incrementally draft different key elements of a scientific paper.": 1659,
    "The paper addresses the challenge of maintaining topic coherence and leveraging generation history by incorporating a Topic Memory Component. This component is designed to store both user-provided topic keywords and the evolving generation history, acting as a RAM-like mechanism. The topic memory `M` is a matrix of size `K' x dh`, where `K'` is the number of memory slots and `dh` is the slot size. Before generating each line, topic words and input text are written into this memory, remaining unchanged during sentence generation. During the decoding process, an Addressing Function `A(M,q)` is used to read from the memory. This function calculates probabilities (`αk`) for each memory slot `Mk` being selected, based on a query vector `q`. For the topic memory, the query `q` is constructed as `[st-1; c; z]`, combining the decoder's previous hidden state (`st-1`), the conditional variable (`c`), and the latent variable (`z`). The addressing function uses a non-linear layer `σ` and a parameter `b` to compute `zk = bTσ(Mk,q)`. The final memory output `ot` at time step `t` is a weighted sum of the memory slots, where the weights are the reading probabilities `α'k` obtained from the addressing function: `ot = sum(α'k * Mk)`. This `ot` is then incorporated into the rhetorically controlled decoder to guide word generation, ensuring semantic consistency with the topics and previous context.": 1660,
    "To constrain the decoding process and improve efficiency and accuracy in idiom generation, the decoder layer incorporates a \"\"candidate character table.\"\" This table is pre-generated and stores all unique characters that can appear at specific positions within standard idioms. For example, when the decoder is generating the first character of an idiom, it only considers characters present in the table designated for Position 1. This mechanism naturally ignores any ineligible characters that are not part of the valid character set for that particular position in an idiom, significantly decreasing the decoding space and guiding the generation towards structurally correct idioms.": 1661,
    "The reliability of pseudo-labels in self-training is improved by leveraging adversarial confidence to distinguish and adaptively handle easy versus hard adaptation regions, a concept termed Adversarial Ambivalence. The method first obtains an initial adaptation model M0. For a given target image xt, M0 produces predictions pC1(xt), pC2(xt), and pCA(xt), along with attention weights W(xt). Attentive Pseudo-Label Assignment (APLA) generates initial pseudo-labels yt by combining these predictions, weighted by WCi(xt), and applying a confidence threshold (ηp) to filter out low-confidence pixels. The key innovation is using the dense output from the discriminator DCA (which follows PatchGAN) to obtain an \"\"adaptation confidence map\"\" d(xt). This map indicates regions where the discriminator is uncertain, implying hard adaptation regions. The pseudo-labels are then refined by incorporating these pixel-wise confidence weights: the standard cross-entropy loss (LCE(xt)) is weighted by 1/d_hw(xt), effectively giving more weight to regions where the discriminator is less confident (i.e., harder regions). Additionally, for regions identified as \"\"hard adaptation regions\"\" (where yt_hw(xt) = 0, meaning no pseudo-label was assigned due to low confidence), a separate adversarial loss (LT_adv(xt)) is applied. This loss encourages the network to push these hard adaptation regions into the shared feature space, preventing them from being ignored. This dual approach of weighting easy regions and applying adversarial training to hard regions effectively exploits the adversarial ambivalence to improve self-training.": 1662,
    "The paper unifies task-level knowledge transfer, model-level knowledge transfer, and self-supervised learning into a principled framework through a two-stage training strategy and a comprehensive loss function. First, the meta-learner, which incorporates task-level knowledge transfer (Solution 1) and model-level knowledge distillation (Solution 2), is pre-trained using Algorithm 1. This pre-training provides a robust initialization for the parameters of the target model's encoder and decoder. Second, the target model is fine-tuned using Algorithm 2 on the target data. During this fine-tuning stage, the knowledge transferred from the deeper network (teacher model) continues to guide the representation learning of the shallower target model through the `W_ij` weights in the loss function. Simultaneously, the newly constructed self-supervised task (Solution 3) is integrated into the final loss function `L_f` (Equation 9). This unified loss function combines the reconstruction error, the self-representation loss, the knowledge distillation term (via `W_ij`), and the self-supervised loss `L_c`. By minimizing `L_f`, the framework simultaneously leverages external data through meta-learning and knowledge distillation, and enhances representation ability via self-supervision, all within a single end-to-end trainable system. After training, the self-expression matrix `C` is obtained, from which an affinity matrix is computed (e.g., `1/2 * (abs(C) + abs(C.T))`), and then spectral clustering is applied for the final clustering result.": 1663,
    "The proposed Simulate-then-Normalize strategy integrates AST-Sim and AST-Norm into the CNN segmentor to address OCDA. The CNN segmentor is divided into three parts: Φ (early layers), Ψ (middle layers), and C (classifier). AST-Sim (denoted as ASTcs) is inserted after Φ at layer `l`, and AST-Norm (denoted as ASTdn) is inserted after Ψ at layer `l′`. The strategy is based on the insight that AST-Sim is most effective at a layer `l` with high Domain-Discriminability Metric (DDM) where disentanglement is good, while AST-Norm is applied at a later layer `l′` closer to the final task output to further suppress residual domain information.\nThe training proceeds in two main stages:\n1.  Pre-adaptation Training:\n    *   The network starts from a pre-trained standard source model and pre-trained auto-encoder based AST-networks (ASTcs(ae) and ASTdn(ae)).\n    *   Source-side supervised pathway: Source data `xs` is forwarded through the integrated network `C◦ASTdn(ae) ◦ Ψ ◦ ASTcs(ae) ◦ Φ(xs)` to obtain predictions `ys`. A supervised cross-entropy loss `Ls=LCE(ys,ygt)` is used to finetune the parameters `θ` of Φ, Ψ, and C. During this, content-related gradients flow unobstructed through the frozen AST networks.\n    *   AST auto-encoder finetuning pathway: Both `xs` and `xt` are forwarded to obtain input-output pairs for finetuning `θcs` and `θdn` of the AST auto-encoders. This involves minimizing reconstruction losses: `min θcs (LAST(hl,s,ˆhl,s) + LAST(hl,t,ˆhl,t))` and `min θdn (LAST(hl′,s,ˆhl′,s) + LAST(hl′,t,ˆhl′,t))`.\n2.  Adaptation via Simulate-then-Normalize:\n    *   The finetuned `θcs` and `θdn` from pre-adaptation are frozen to preserve their auto-encoding behavior, while `θ` (parameters of Φ, Ψ, C) are updated.\n    *   The fixed domain prototype `zl′,g` is recomputed using Equation 3, and pseudo-labels `ypgt` are recomputed using Equation 8 after every epoch.\n    *   Four data-flow pathways are used:\n        *   Simulate step: At layer `l`, AST-Sim generates `hl,s✮` (source stylized with target style) and `hl,t✮` (target stylized with source style) using the cross-domain pairing strategy. The original `hl,s` and `hl,t` are also available.\n        *   Normalize step: At layer `l′`, AST-Norm normalizes the features from the simulate step, producing `hl′,sg`, `hl′,s✮g`, `hl′,tg`, and `hl′,t✮g` by fixing their latent to `zl′,g`.\n        *   Losses:\n            *   Supervised losses for source-side predictions: `min θ (Lsg=LCE(ysg,ygt))` and `min θ (Ls✮g=LCE(ys✮g,ygt))`.\n            *   Pseudo-label self-training losses for target predictions: `min θ (Ltg=LCE(ytg,ypgt))` and `min θ (Lt✮g=LCE(yt✮g,ypgt))`.\n    *   Pseudo-label Extraction: Reliable pseudo-labels `ypgt` are obtained by pruning target predictions based on a consistency check between `ytg` and `yt✮g`. Only pixels `xt` where `argmaxc ytg = argmaxc yt✮g` contribute to the target losses.\nThis strategy aims to first encourage the network to focus on task-specific cues by simulating diverse domain styles (AST-Sim) and then explicitly remove domain-related information in deeper layers (AST-Norm), leading to improved domain generalization without adversarial alignment or clustering.": 1664,
    "The benefits of explicit semantic incongruence modeling and large-scale unlabeled data utilization are synergistically combined through a two-stage training procedure within the CATE model. In Stage I, the pre-trained model (e.g., RoBERTa) is fine-tuned using the available labeled data. During this stage, the overall objective function includes the standard classification loss (`Lcls`) for labeled data and the Contrastive Objective (`Lco`) to explicitly learn the distinction between literal and metaphorical meanings. This initial stage focuses on building a strong foundational model that understands the core linguistic phenomenon of metaphoricity. In Stage II, the model transitions to leveraging unlabeled data. The fine-tuned model from Stage I is used to generate soft pseudo-labels for the large candidate set `U` collected by the Target-based Generating Strategy (TGS). Subsequently, the model undergoes iterative refinement, where it is trained using an augmented dataset comprising both the original labeled data and the newly pseudo-labeled unlabeled data. The overall objective function for Stage II combines `Lcls`, `Lco`, and the Self-Training loss (`Lst`) for the unlabeled data, weighted by hyperparameters `alpha` and `beta`. This iterative process allows the model to continuously improve its generalization capabilities by learning from a much larger and diverse dataset, while the contrastive objective ensures that the fundamental discrimination between literal and metaphorical senses remains a core learning focus throughout the entire training process.": 1665,
    "To improve semantic alignment between generated images and input captions, particularly recognizing subtle differences between frames in a story, the paper introduces an intra-story contrastive loss. This loss is adapted from existing inter-modal contrastive losses. The alignment and refinement module first computes a pairwise cosine similarity matrix between all image sub-regions and word tokens. A soft attention mechanism then assigns weights to words for generating each image sub-region, resulting in a weighted word-context vector for each sub-region. The score function, `Sword(xk, sk)`, for an image `xk` and its aligned sentence `sk` is defined as the logarithm of the sum of exponentials of cosine similarities between each image sub-region and its corresponding word-context vector. The contrastive loss, `Lword`, is then formulated as a negative log-softmax over these scores. Critically, for story visualization, negative images are sampled from *adjacent frames* within the same story. This forces the model to learn to distinguish between visually similar but semantically distinct frames, thereby improving the fine-grained semantic alignment between words and image regions within the narrative context.": 1666,
    "To address catastrophic forgetting, the paper proposes applying RecAdam (Chen et al., 2020) during the second phase of pre-training. RecAdam is an optimization method built upon the Adam optimizer (Kingma and Ba, 2015). It reconstructs the objective function to allow a gradual shift to the target task while retaining previously learned knowledge. The modified loss function is a weighted sum: `Loss = λ(t) · LossT + (1 − λ(t)) · LossS`. Here, `LossT` represents the target task objective function (e.g., summarization loss for SDPT, or reconstruction loss for DAPT/TAPT). `LossS` is a regularization term designed to simulate the first pre-training step of the model and prevent forgetting. It is simplified as a quadratic penalty: `LossS = (1/2γ) * Σ(θi - θ*i)^2`, where `θ` are the current model parameters and `θ*` are the fixed original parameters of the pre-trained model. The weighting coefficient `λ(t)` is an annealing function `1 / (1 + exp(-k · (t - t0)))`, which controls the balance between `LossT` and `LossS` over time steps `t`, with `k` and `t0` as hyperparameters. By incorporating `LossS`, RecAdam encourages the model's parameters to stay close to their initial pre-trained values, thus helping to \"\"recall\"\" the knowledge gained in the first pre-training phase while learning new domain or task-specific information.": 1667,
    "To identify narratives with similar motifs, the paper leverages the learned models from the proverb prediction task. For each narrative, these models output a probability distribution over the 250 proverbs in the dataset. The similarity between two narratives is then modeled by computing the distance between their respective probability distributions over proverbs. Various distance metrics are explored for this purpose, including cosine distance, Jenson-Shannon divergence (JSD), L2 (Euclidean) distance, and L1 (Manhattan) distance. The narrative closest to the input narrative (in terms of the chosen distance metric) is predicted as the most similar. This approach implicitly assumes that narratives related to the same proverb share similar motifs. An additional experiment is performed using a pre-trained Sentence-BERT model to obtain direct representations of each narrative, and then cosine distance is calculated between these representations to find similar narratives, serving as a baseline that does not rely on an intermediate proverb prediction step.": 1668,
    "The paper addresses this by proposing the Keyword-Conditioned Pun Generation task.\n1.  Task Definition: The task takes human-annotated pun keywords `K`, the pun word `pw`, and an alternate pun word `aw` (where `pw=aw` for homographic puns) as input. Optionally, pun wordsense annotations `Spw` and `Saw` from SemEval2017 Task 7 are also included. The goal is to produce novel and fluent puns that incorporate these inputs.\n2.  Data Preparation: The task uses 1,482 samples from ExPUN that have both human-annotated keywords (AF6) and pun wordsense annotations from SemEval2017 Task 7. 100 samples are reserved for testing, and the rest for training. Human-annotated keywords are aggregated using the proposed Keyword Aggregation Algorithm. The effect of automatically-extracted keywords is also evaluated by using the RAKE algorithm on pun text.\n3.  Models:\n    *   AmbiPun (Baseline): A current state-of-the-art homographic pun generation model is used without further fine-tuning. The input prompt format is \"\"generate sentence: K, pw, aw\"\".\n    *   Fine-tuned T5 (T5FT): A T5-base model is fine-tuned on ExPUN. The input prompt is \"\"generate a pun that situated in K, using the word pw, pw means Spw, aw means Saw.\"\" The output is the pun itself.\n    *   Fine-tuned T5 with Pre-training (T5PT+FT): To improve keyword incorporation, T5 is pre-trained on non-pun text. For a given pun word, 200 sentences containing it are extracted from BookCorpus, and RAKE is used to extract keywords. Examples are constructed where inputs are automatically extracted keywords and outputs are sentences from BookCorpus including pun words. This pre-trained model is then fine-tuned on ExPUN.\n4.  Evaluation Metrics:\n    *   Automatic Metrics: Word incorporation rate for pun words (`pw`), keywords (`K`), and both, measuring the model's ability to include all input keywords.\n    *   Human Evaluation: Amazon Mechanical Turk workers label whether a generated pun is \"\"successful\"\" (text supports both senses of the pun word). A qualifier ensures workers correctly label >=80% of 20 manually annotated samples. Fleiss' kappa (κ=0.49) is reported for inter-annotator agreement.": 1669,
    "The paper introduces a Hyperbole Ranker to select the best candidate from multiple over-generated hyperbolic versions. This ranker aims to achieve a satisfying trade-off between hyperbolicity and paraphrase quality. For each candidate sentence, two scores are computed: a hyperboles score and a paraphrase score. The hyperboles score, denoted as `hypo(c)`, is assigned by a BERT-based hyperbole detection model that was fine-tuned on the HYPO and HYPO-L datasets. This score represents the prediction probability of a sentence being hyperbolic. The paraphrase score, denoted as `para(c)`, evaluates content preservation. It is calculated using a distilled RoBERTa-base model, pre-trained on large-scale paraphrase data and further fine-tuned on the HYPO dataset (treating `hypo`-`para` pairs as positive and `hypo`-`non_hypo` as negative examples). This model computes the cosine similarity between the literal input and the candidate sentence. An intuitive scoring function `score(s) = hypo(s) * para(s)` is then used to combine these two metrics. Additionally, candidates are filtered out if their paraphrase score is below a specific threshold (γ=0.8) or if it is too close to 1 (ϵ=0.001), preventing the system from simply copying the literal input. The remaining candidates are then ranked according to their combined score, and the one with the highest score is selected as the final output. If all candidates are filtered, the one with the highest hyperboles score among all candidates is chosen.": 1670,
    "MANNER decomposes the overall Named Entity Recognition (NER) task into two sub-tasks: span detection and entity typing. Span detection aims to predict the position tags of tokens (e.g., \"\"B\"\", \"\"I\"\", \"\"O\"\", \"\"E\"\", \"\"S\"\" from the BIOES tagging scheme), while entity typing aims to predict the entity types of tokens (e.g., Person, Location). For span detection, a linear classifier is applied to the contextualized token representations obtained from a pretrained language model (e.g., BERT) to compute the probability distribution of position tags `pθ(a|x)`. For entity typing, the probability distributions of entity types `pθ(e|x,Z)` are computed based on the principle of prototypical networks, using the sampled prototypes `Z` (from the variational prototype distributions). Finally, the results of span detection and entity typing are combined to obtain the final label distribution `pθ(y|a,e)`. The predicted label distribution for each token `yi` is proportional to the product of its predicted position tag probability `pθ(ai|x)` and its predicted entity type probability `pθ(ei|x,Z)`. This decomposition is beneficial because it avoids learning prototypes for non-entities (the \"\"O\"\" class), which can be noisy and meaningless, thereby improving overall performance.": 1671,
    "Data cartography-based regularization employs a novel mixup strategy informed by training dynamics to denoise pseudo-labels generated by the source domain classifier. Training dynamics are used to characterize each training sample into easy-to-learn, ambiguous, and hard-to-learn groups. Easy-to-learn samples, which are consistently predicted correctly across epochs, are selected to interpolate with the generated pseudo-labels during the mixup process. This approach ensures that incorrect pseudo-labels are effectively denoised by mixing them with the most informative and correct data samples, thereby improving the overall performance in the few-shot setting.": 1672,
    "The established relationship between Softmax weights and class-wise means facilitates efficient source model selection in transfer learning by proposing a method that bypasses the need for multiple, costly Softmax layer trainings. This method, referred to as Centred Means (CM), involves evaluating the performance of different source models by directly using the scaled centered class-wise means (M_hat) of the target data representations as the Softmax layer weights, without any further training. For each potential source model, its pre-trained representation extractor is applied to the target domain data to compute the empirical class-wise means. These means are then used to construct the M_hat matrix, which replaces the Softmax layer weights. The accuracy achieved with these \"\"fixed\"\" weights is then used to compare and select the best source model. This approach significantly reduces computational overhead because it eliminates the need to train the Softmax layer multiple times for each candidate source model, which is a common bottleneck in traditional transfer learning pipelines.": 1673,
    "The RDA system integrates a Human-in-the-loop mechanism, facilitated by the MicroMappers crowdsourcing system, to continuously improve model performance through expert feedback. Data items processed by the automated system are sampled in batches for human verification and supervision. The sampling strategy prioritizes images classified as severe or mild damage, with \"\"none\"\" class images also sampled when positive classes are less available, ensuring a diverse set of images for review. Human experts, who are trained emergency managers, examine these sampled images via a web interface that displays the image along with the system's prediction (e.g., Damage/No Damage, and if Damage, then Mild/Severe). Experts either agree or disagree with the machine's prediction. In cases of disagreement, they are required to provide the correct label (severe, mild, none, or \"\"Don't know or can't judge\"\"). This expert-provided feedback, specifically the correct labels for images where the system made mistakes, is then used to retrain the damage assessment model, thereby enhancing its performance and adapting it to the specific characteristics and distributions of the target domain data encountered during real-world disaster events.": 1674,
    "The framework is extended to handle multiple unlabeled target domains by adapting the architecture and training procedure. Instead of a single target domain, the model incorporates N unlabeled target domains (T1, ..., TN). For each target domain, separate liveness encoders (EL ti) and content encoders (EC ti) are used to extract their respective features. Similarly, N separate liveness discriminators (DL ti) are applied for domain adversarial training, ensuring that the liveness features are domain-invariant across the source and all target domains. The feature translation process is extended to a cyclic chain: liveness features are transferred from the source domain (S) to T1, then from T1 to T2, and so on, finally from TN back to S. Different decoders (Gti) are used for each target domain to generate pseudo-labeled images (ˆxti). The synthetic pseudo-labeled images from all target domains ({ˆx∗ ti}N i=1) are then combined to train a single robust classifier (M). This multi-target extension, termed Single Source to Multiple Targets (SS2MT), allows the model to capture liveness information from a wider range of scenarios, leading to more robust and generalizable liveness features compared to single-target adaptation or blending multiple targets into one.": 1675,
    "The paper designs two probabilistic metrics, plausibility and typicality, to model simile triplets (t, p, v) and provide a nuanced understanding of simile phenomena. Plausibility (P(t,p,v)) measures the quality of a simile triplet based on the confidence scores of its supporting instances. It is calculated using a noisy-or model: P(t,p,v) = 1 - product(1 - S(si,t,p,v)) for all `η` supporting instances `si`, where S(si,t,p,v) is the confidence score of each instance (P(p|si,t,v)). This model ensures that if any instance has a high confidence score, the triplet's plausibility is high, and it approaches zero only if all supporting instances have very low confidence.\nTypicality is measured by two metrics: T(p|t,v) for simile understanding and T(t,v|p) for simile generation. T(p|t,v) quantifies how typical a property `p` is for a given topic-vehicle pair (t,v), while T(t,v|p) measures how typical a (t,v) pair is for a given property `p`. Both are formulated as normalized scores incorporating the triplet's frequency (N(t,p,v)) and plausibility (P(t,p,v)). Specifically, T(p|t,v) = (N(t,p,v) * P(t,p,v)) / sum(N(t,p',v) * P(t,p',v)) over all properties p' for (t,v). Similarly, T(t,v|p) = (N(t,p,v) * P(t,p,v)) / sum(N(t',p,v') * P(t',p,v')) over all (t',v') pairs for property p. These metrics allow the knowledge base to retain even less frequent but plausible similes, providing richer information than simple frequency counts.": 1676,
    "The paper proposes an adaptive method to approximate the transport ratios `epsilon` (total mass to transport) and `delta` (ratio of common class samples in source) which are difficult to calculate precisely in practice. Two scalars, `delta1` and `delta2`, are introduced as thresholds. The transport ratio `epsilon` is estimated as the proportion of target samples whose maximum prediction confidence `s(x) = max(softmax(h(f(x))))` is greater than `delta1`. Specifically, `epsilon = (1/n) * sum_i(1(s(xt_i) > delta1))`. The ratio `delta` for the source domain is estimated as the proportion of source prototypes (classes) whose `ws` weight (derived from the transport plan, indicating \"\"knownness\"\") is greater than `delta2`. Specifically, `delta = (1/L) * sum_j(1(ws_j > delta2))`. These estimated ratios `epsilon_i` and `delta_i` are updated iteratively using an exponential moving average: `epsilon_i = alpha1 * epsilon_i + (1 - alpha1) * epsilon_i-1` and `delta_i = alpha2 * delta_i + (1 - alpha2) * delta_i-1`. The updated `epsilon_i` is then used as the transport ratio for `POT_epsilon`, and `epsilon_i / delta_i-1` serves as the coefficient for Equation (5) in the m-PPOT calculation.": 1677,
    "The FairWASP framework is extended to satisfy pairwise demographic parity constraints, resulting in FairWASP-PW. This extension addresses a different but equivalent definition of demographic parity, where the selection rates are constrained to be approximately equal across any two groups D=d1 and D=d2, rather than aligning with the overall marginal distribution of Y. The new constraint is J(pZ;θ(y|d1), pZ;θ(y|d2)) ≤ ϵ for all d1,d2 ∈ D and y ∈ Y.\nThe solution involves three key steps:\n1.  Connection between constraints: Lemma 6 establishes a connection between the original constraints (2) and the new pairwise constraints (11). It shows that the feasible set Θϵ for the pairwise constraints is equivalent to the union of feasible sets Θ¯ϵ;t for the original constraints (2) over all possible marginal distributions t ∈ [0,1]|Y|, where ¯ϵ = √1+ϵ - 1. This means Θϵ = ∪t∈[0,1]Y Θ¯ϵ;t.\n2.  Reformulation of Problem (12): Using this connection, the optimization problem (12) with pairwise constraints is reformulated. It is shown to be equivalent to simultaneously optimizing over both the weights θ and a target marginal distribution t. This leads to the problem minθ∈Rn,t∈[0,1]Y Wc(pZ;θ,pZ;e) subject to θ ∈ In ∩ Θ¯ϵ;t. This problem can be further expressed as a minimization over t: min t∈[0,1]Y HI(t;¯ϵ), where HI(t;¯ϵ) represents the optimal objective value for the original FairWASP problem (4) when the target marginal distribution is t and the fairness tolerance is ¯ϵ.\n3.  Zero-th Order Optimization Methods: The minimization problem min t∈[0,1]Y HI(t;¯ϵ) is solved using zero-th order optimization methods, such as the Nelder-Mead method. This approach is efficient because the dimension of t is low (|Y|), and the value of HI(t;¯ϵ) can be computed efficiently via the dual problem (D). After the initial computation of the dual problem, subsequent computations for different values of t only require ˜O(n|Y|^2|D|^2 log(R/ε)) flops, as the cost matrix remains unchanged. Once the optimal t⋆ is found, fixing t to t⋆ in the reformulated problem (16) allows for finding the optimal integer weights θ⋆ using the methods developed for the original FairWASP problem, preserving the optimality of integer weights.": 1678,
    "The paper proposes two methods to analyze in which source domain knowledge transfer occurs: Source Domain Perturbation Sensitivity (SDPS) and Source Domain Fisher Sensitivity (SDFS).\n1.  Source Domain Perturbation Sensitivity (SDPS): This intuitive approach involves injecting Gaussian noise into the hidden representation `h(j)` output by each j-th source domain model. The noise variance is scaled proportionally to the variance of each feature embedding. The `SDPS(j)` is defined as `1 / α^2(j)`, where `α^2(j)` is the noise-injected precision of the embedding in the j-th source domain that causes a predefined `α%` F1 score drop in the target domain. A higher `SDPS(j)` indicates greater sensitivity and thus a more significant contribution from that source domain. This method requires multiple forward passes and averaging over epochs.\n2.  Source Domain Fisher Sensitivity (SDFS): This method provides a faster and more theoretical analysis based on the Fisher Information Matrix. For the entity typing task, the Fisher Information Matrix `F(j)` is calculated for the hidden embedding `h(j)` from the j-th source domain. The `SDFS(j,m)` for the m-th dimension of `h(j)` is `F(j)(m,m)`. The overall `SDFS(j)` for a domain is the sum of `SDFS(j,m)` over all dimensions. A higher `SDFS` value indicates that the model's predictions are more sensitive to changes in the embeddings from that source domain, implying a greater influence or knowledge transfer.": 1679,
    "A regression-based approach is employed to predict model performance drop on new, unlabeled target domains by leveraging pre-existing labeled evaluation datasets. This method assumes the availability of a small, fixed number of labeled evaluation datasets, denoted `Do`, which are distinct from the source domain (Ds) and the new target domain (Dt). For each of these `Do` datasets, two key pieces of information are obtained: the actual performance drop of the model (which was trained on Ds and then evaluated on `Do`), and the value of a chosen domain-shift detection metric (e.g., PAD*, CONF_CALIB, RCA*) calculated between Ds and `Do`. These pairs of domain-shift metric values and corresponding actual performance drops are then used to fit a simple linear regression line. Once this regression line is established, it serves as a predictive model. When a new target domain `Dt` is encountered for which no labels are available, the chosen domain-shift detection metric is calculated between Ds and `Dt`. This calculated metric value is then input into the pre-fitted regression line, which directly yields a prediction of the performance drop the model is expected to suffer when exposed to examples from `Dt`.": 1680,
    "To generate words with fine-grained rhetorical awareness, the paper proposes a Rhetorically Controlled Decoder. This decoder operates on the assumption that each word in a poem sentence has a latent type: either a \"\"content word\"\" or a \"\"rhetorical word.\"\" Instead of using a single generation distribution, the decoder first calculates a word type distribution `P(τt | st,z,c)` over these latent types at each time step `t`, given the decoder's hidden state `st`, the latent variable `z`, and the conditional variable `c`. This distribution is computed using a softmax layer on `W0[st;z;c] + b0`. Subsequently, the decoder computes type-specific generation distributions over the entire vocabulary: `P(yt | τt = content, st,ot,z,c)` and `P(yt | τt = rhetoric, st,z,c)`. These are also calculated using softmax layers, but with separate parameter matrices (`Wcontent` and `Wrhetoric`) for each type, indicating that each word type has its own generation parameters. The final probability of generating a word `P(yt | st,ot,z,c)` is a mixture of these type-specific distributions, weighted by their respective type probabilities: `P(yt | τt = content,...) * P(τt = content | ...) + P(yt | τt = rhetoric,...) * P(τt = rhetoric | ...)`. This mechanism allows the model to convey more information about the potential word to be generated, enriching the diversity and meaningfulness of the rhetorical output.": 1681,
    "After the decoder generates a sequence of characters, the paper employs a \"\"Prediction Layer\"\" to accurately map these translated character sequences to standard idioms. Since the generated sequence is considered a \"\"pseudo target language\"\" and might not perfectly match a standard idiom, this layer uses \"\"edit distance\"\" (Navarro, 2001) to find the most similar idiom from a predefined set of standard idioms. The idiom from the standard set that has the minimum edit distance to the generated character sequence is selected as the final recommended idiom, ensuring that the output is a recognized and valid idiom.": 1682
  },
  "count": 1683,
  "timestamp": 1759254327,
  "vector_dim": 1536
}