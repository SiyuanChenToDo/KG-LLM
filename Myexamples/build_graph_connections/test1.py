import os
import sys
import json
import time # æ–°å¢: ç”¨äºç”Ÿæˆæ—¶é—´æˆ³
import numpy as np
from itertools import product
from camel.storages import Neo4jGraph
from camel.embeddings import OpenAICompatibleEmbedding

# =================================================================================
# 1. ç¯å¢ƒä¸æ•°æ®åº“é…ç½® (Environment and Database Configuration)
# =================================================================================
# --- è¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå˜é‡å·²æ­£ç¡®è®¾ç½® ---
os.environ["OPENAI_COMPATIBILITY_API_KEY"] = "sk-c1a6b588f7d543adb0412c5bc61bdd7b"
os.environ["OPENAI_COMPATIBILITY_API_BASE_URL"] = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# --- Neo4j æ•°æ®åº“è¿æ¥ä¿¡æ¯ ---
# æ³¨æ„ï¼šå¯†ç å·²æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œè¯·ä½¿ç”¨æ‚¨è‡ªå·±çš„çœŸå®å¯†ç ã€‚
n4j = Neo4jGraph(
    url="neo4j+s://b3980610.databases.neo4j.io",
    username="neo4j",
    password="ta_T6_9gzxTfrTiWjRuUhO7Lm6fBbQG8TwxnSqHpoqk",
)

# --- åµŒå…¥æ¨¡å‹åˆå§‹åŒ– ---
embedding_model = OpenAICompatibleEmbedding(
    model_type="text-embedding-v2",
    api_key=os.environ["OPENAI_COMPATIBILITY_API_KEY"],
    url=os.environ["OPENAI_COMPATIBILITY_API_BASE_URL"],
)

print("âœ… ç¯å¢ƒå’Œæ•°æ®åº“é…ç½®å®Œæˆã€‚")

# =================================================================================
# 2. æ ¸å¿ƒå‚æ•°é…ç½® (Core Parameter Configuration)
# =================================================================================
CONFIG = {
    "WEIGHTS": {
        "abstract": 0.4,
        "core_problem": 0.6
    },
    "SIMILARITY_THRESHOLD": 0.75,
    "NEW_RELATIONSHIP_TYPE": "POSSIBLY_RELATED",
    # --- ç¼“å­˜è·¯å¾„å’Œç»´åº¦é…ç½® ---
    # æ–‡ä»¶å°†ä¿å­˜ä¸º embedding_cache.json å’Œ embedding_cache.npy
    "EMBEDDING_CACHE_BASE_PATH": "embedding_cache", 
    "EMBEDDING_MODEL_DIM": 1536 # DashScope text-embedding-v2 çš„ç»´åº¦é€šå¸¸ä¸º 1536
}
print(f"âœ… æ ¸å¿ƒå‚æ•°é…ç½®å®Œæˆï¼Œæƒé‡: {CONFIG['WEIGHTS']}, é˜ˆå€¼: {CONFIG['SIMILARITY_THRESHOLD']}")

# =================================================================================
# 3. è¾…åŠ©åŠç¼“å­˜å‡½æ•° (Helper and Caching Functions)
# =================================================================================
def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚"""
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0
    return dot_product / (norm_vec1 * norm_vec2)

def embed_text_batch(texts_to_embed):
    """åˆ†æ‰¹å¤„ç†æ–‡æœ¬å¹¶ç”ŸæˆåµŒå…¥ã€‚"""
    batch_size = 25
    all_embeddings = []
    for i in range(0, len(texts_to_embed), batch_size):
        batch_texts = texts_to_embed[i:i + batch_size]
        print(f"  > æ­£åœ¨ä¸º {len(batch_texts)} ä¸ªæ–‡æœ¬è°ƒç”¨ API... (æ‰¹æ¬¡ {i//batch_size + 1})")
        try:
            batch_embeddings = embedding_model.embed_list(objs=batch_texts)
            all_embeddings.extend(batch_embeddings)
        except Exception as e:
            print(f"  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            # ç¡®ä¿å³ä½¿å¤±è´¥ï¼Œä¹Ÿèƒ½ä¿æŒåˆ—è¡¨é•¿åº¦ä¸€è‡´ï¼Œä½¿ç”¨ None å ä½
            all_embeddings.extend([None] * len(batch_texts))
    return all_embeddings

def load_embedding_cache(base_path, dim):
    """ä»æ–‡ä»¶åŠ è½½åµŒå…¥ç¼“å­˜ (JSON for keys, NumPy for vectors)ã€‚"""
    json_path = base_path + ".json"
    npy_path = base_path + ".npy"
    
    cache = {}
    
    if os.path.exists(json_path) and os.path.exists(npy_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # 1. Load keys (text) and indices from JSON
            text_to_index = metadata.get('text_to_index', {})
            
            # 2. Load vectors array from .npy file
            vectors_array = np.load(npy_path)
            
            # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
            if vectors_array.shape[0] != len(text_to_index) or vectors_array.shape[1] != dim:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ '{npy_path}' ç»“æ„ä¸ä¸€è‡´ ({vectors_array.shape})ï¼Œå¿½ç•¥ç¼“å­˜ã€‚")
                return {}

            # 3. Reconstruct cache (text: vector list)
            for text, index in text_to_index.items():
                # å­˜å‚¨ä¸º list ä»¥ä¾¿åœ¨ç¼“å­˜å­—å…¸ä¸­ä¿æŒä¸€è‡´æ€§
                cache[text] = vectors_array[index].tolist() 
            
            print(f"âœ… æˆåŠŸä» '{base_path}.json/.npy' åŠ è½½ {len(cache)} æ¡åµŒå…¥ç¼“å­˜ã€‚")
            return cache
        
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ã€‚è¿”å›ç©ºç¼“å­˜ã€‚")
            return {}
    
    print(f"ğŸ’¡ æœªæ‰¾åˆ°å®Œæ•´ç¼“å­˜æ–‡ä»¶ '{base_path}.json/.npy'ï¼Œè¿”å›ç©ºç¼“å­˜ã€‚")
    return {}

def save_embedding_cache(base_path, cache):
    """å°†åµŒå…¥ç¼“å­˜ä¿å­˜åˆ°æ–‡ä»¶ (JSON for keys, NumPy for vectors)ã€‚"""
    json_path = base_path + ".json"
    npy_path = base_path + ".npy"
    
    if not cache:
        print("âš ï¸ ç¼“å­˜ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜ã€‚")
        return

    # 1. Prepare metadata and vectors array
    text_to_index = {}
    vectors_list = []
    
    for i, (text, vector_list) in enumerate(cache.items()):
        text_to_index[text] = i
        vectors_list.append(vector_list)
        
    vectors_array = np.array(vectors_list)
    metadata = {
        'text_to_index': text_to_index,
        'count': len(cache),
        'timestamp': int(time.time()),
        'vector_dim': vectors_array.shape[1] if vectors_array.size > 0 else 0
    }
    
    try:
        # 2. Save vectors using NumPy binary format (é€Ÿåº¦æ›´å¿«)
        np.save(npy_path, vectors_array)

        # 3. Save metadata using JSON
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… åµŒå…¥ç¼“å­˜å·²æ›´æ–°å¹¶ä¿å­˜åˆ° '{base_path}.json/.npy'ã€‚")
        
    except Exception as e:
        print(f"âŒ å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


def get_embeddings_with_caching(texts, cache):
    """
    ä¸ºæ–‡æœ¬åˆ—è¡¨è·å–åµŒå…¥ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜ã€‚
    è¿”å›ä¸è¾“å…¥æ–‡æœ¬é¡ºåºä¸€è‡´çš„åµŒå…¥åˆ—è¡¨ï¼ˆnumpy arraysï¼‰å’Œæ›´æ–°åçš„ç¼“å­˜ã€‚
    """
    texts_to_embed_map = {} # ä½¿ç”¨ map ä¿è¯å”¯ä¸€æ€§
    for i, text in enumerate(texts):
        if text not in cache:
            texts_to_embed_map[text] = None
            
    if texts_to_embed_map:
        print(f"  > ç¼“å­˜æœªå‘½ä¸­ {len(texts_to_embed_map)} ä¸ªå”¯ä¸€æ–‡æœ¬ï¼Œå‡†å¤‡ç”Ÿæˆæ–°åµŒå…¥...")
        unique_texts_to_embed = list(texts_to_embed_map.keys())
        new_embeddings = embed_text_batch(unique_texts_to_embed)
        
        for text, embedding in zip(unique_texts_to_embed, new_embeddings):
            if embedding is not None:
                cache[text] = np.array(embedding).tolist() # è½¬ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
    else:
        print("  > ç¼“å­˜å‘½ä¸­æ‰€æœ‰æ–‡æœ¬ï¼æ— éœ€ API è°ƒç”¨ã€‚")

    # ä»ç¼“å­˜ä¸­æ„å»ºä¸åŸå§‹è¾“å…¥é¡ºåºä¸€è‡´çš„ç»“æœåˆ—è¡¨
    final_embeddings = [np.array(cache.get(text)) if cache.get(text) is not None else None for text in texts]
    return final_embeddings, cache

print("âœ… è¾…åŠ©åŠç¼“å­˜å‡½æ•°å·²å®šä¹‰ã€‚")

# =================================================================================
# 4. ä¸»é€»è¾‘ (Main Logic)
# =================================================================================
def main():
    """æ‰§è¡ŒèŠ‚ç‚¹è·å–ã€åµŒå…¥ã€åŠ æƒæ¯”è¾ƒå’Œé“¾æ¥åˆ›å»ºçš„æ•´ä¸ªæµç¨‹ã€‚"""
    
    # --- æ–°å¢: åŠ è½½åµŒå…¥ç¼“å­˜ (ä½¿ç”¨æ–°çš„ base path å’Œ dim) ---
    embedding_cache = load_embedding_cache(CONFIG["EMBEDDING_CACHE_BASE_PATH"], CONFIG["EMBEDDING_MODEL_DIM"])

    # --- æ­¥éª¤ 1: è·å– Paper èŠ‚ç‚¹å’Œ Solution èŠ‚ç‚¹ (å…³é”®: æ·»åŠ  file_id) ---
    print("\nğŸš€ æ­¥éª¤ 1: å¼€å§‹ä» Neo4j è·å– Paper å’Œ Solution èŠ‚ç‚¹...")
    try:
        # **æ›´æ–°ç‚¹ 1: Paper æŸ¥è¯¢ä¸­æ·»åŠ  p.file_id**
        paper_query = """
        MATCH (p:paper)
        WHERE p.abstract IS NOT NULL AND p.core_problem IS NOT NULL AND p.file_id IS NOT NULL
        RETURN elementId(p) AS node_id, p.abstract AS abstract, p.core_problem AS core_problem, p.file_id AS file_id
        """
        paper_nodes = n4j.query(query=paper_query)
        print(f"  - æˆåŠŸè·å– {len(paper_nodes)} ä¸ª Paper èŠ‚ç‚¹ã€‚")

        # **æ›´æ–°ç‚¹ 2: Solution æŸ¥è¯¢ä¸­æ·»åŠ  s.file_id**
        # å‡è®¾ Solution èŠ‚ç‚¹ä¹Ÿç›´æ¥æœ‰ file_id å±æ€§ï¼Œä¸ Paper èŠ‚ç‚¹ä¸€è‡´ã€‚
        solution_query = """
        MATCH (s)
        WHERE (s:solution_1 OR s:solution_2 OR s:solution_3 OR s:solution_4) AND s.file_id IS NOT NULL
        WITH s, labels(s)[0] AS label
        WITH s, label, s.file_id AS file_id,
             CASE label
               WHEN 'solution_1' THEN s.solution_1
               WHEN 'solution_2' THEN s.solution_2
               WHEN 'solution_3' THEN s.solution_3
               WHEN 'solution_4' THEN s.solution_4
               ELSE null
             END AS text_content
        WHERE text_content IS NOT NULL
        RETURN elementId(s) AS node_id, label, text_content AS text, file_id
        """
        solution_nodes = n4j.query(query=solution_query)
        print(f"  - æˆåŠŸè·å– {len(solution_nodes)} ä¸ª Solution èŠ‚ç‚¹ã€‚")
        
        if not paper_nodes or not solution_nodes:
            print("âš ï¸ ç¼ºå°‘ Paper æˆ– Solution èŠ‚ç‚¹ï¼Œæ— æ³•ç»§ç»­ã€‚è¯·æ£€æŸ¥æ•°æ®åº“ã€‚")
            return
            
    except Exception as e:
        print(f"âŒ Neo4j æŸ¥è¯¢å¤±è´¥: {e}")
        return

    # --- æ­¥éª¤ 2: ä½¿ç”¨ç¼“å­˜æœºåˆ¶ä¸ºèŠ‚ç‚¹å±æ€§ç”Ÿæˆæˆ–è·å–å‘é‡åµŒå…¥ ---
    print("\nğŸš€ æ­¥éª¤ 2: å¼€å§‹ä¸ºèŠ‚ç‚¹å±æ€§ç”Ÿæˆæˆ–è·å–å‘é‡åµŒå…¥...")
    
    all_paper_texts = [text for p in paper_nodes for text in (p['abstract'], p['core_problem'])]
    all_solution_texts = [s['text'] for s in solution_nodes]
    
    # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–‡æœ¬
    all_paper_embeddings, embedding_cache = get_embeddings_with_caching(all_paper_texts, embedding_cache)
    all_solution_embeddings, embedding_cache = get_embeddings_with_caching(all_solution_texts, embedding_cache)

    # å°†åµŒå…¥å‘é‡åˆ†é…å›å¯¹åº”çš„èŠ‚ç‚¹
    for i, p_node in enumerate(paper_nodes):
        p_node['vectors'] = {
            "abstract": all_paper_embeddings[i*2],
            "core_problem": all_paper_embeddings[i*2 + 1]
        }
    for i, s_node in enumerate(solution_nodes):
        s_node['vector'] = all_solution_embeddings[i]
        
    print("âœ… å‘é‡åµŒå…¥å·²å…¨éƒ¨åˆ†é…ç»™å¯¹åº”èŠ‚ç‚¹ã€‚")

    # --- æ­¥éª¤ 3: è®¡ç®—åŠ æƒç›¸ä¼¼åº¦å¹¶åˆ›å»ºé“¾æ¥ (å…³é”®: è¿‡æ»¤ file_id) ---
    print("\nğŸš€ æ­¥éª¤ 3: å¼€å§‹è®¡ç®—åŠ æƒç›¸ä¼¼åº¦å¹¶åˆ›å»ºé“¾æ¥...")
    
    link_creation_count = 0
    
    for paper, solution in product(paper_nodes, solution_nodes):
        
        # **å…³é”®è¿‡æ»¤é€»è¾‘: ç¡®ä¿ Paper å’Œ Solution æ¥è‡ªä¸åŒçš„è®ºæ–‡**
        if paper['file_id'] == solution['file_id']:
            # print(f"  - ğŸ’¡ è·³è¿‡ Paper({paper['file_id']}) å’Œ Solution({solution['file_id']})ï¼Œå› ä¸ºå®ƒä»¬å±äºåŒä¸€ç¯‡è®ºæ–‡ã€‚")
            continue 

        if paper['vectors']['abstract'] is None or paper['vectors']['core_problem'] is None or solution['vector'] is None:
            # print(f"  - âš ï¸ è·³è¿‡ Paper({paper['node_id']}) å’Œ Solution({solution['node_id']})ï¼Œå› ä¸ºç¼ºå°‘å‘é‡ã€‚")
            continue

        sim_abstract = cosine_similarity(paper['vectors']['abstract'], solution['vector'])
        sim_core_problem = cosine_similarity(paper['vectors']['core_problem'], solution['vector'])
        
        weighted_score = (sim_abstract * CONFIG['WEIGHTS']['abstract'] + 
                          sim_core_problem * CONFIG['WEIGHTS']['core_problem'])
        
        if weighted_score >= CONFIG['SIMILARITY_THRESHOLD']:
            link_creation_count += 1
            print(f"  âœ¨ å‘ç°æ½œåœ¨**è·¨è®ºæ–‡**å…³è” (æ€»åˆ†: {weighted_score:.4f}):")
            print(f"     - Paper ID: {paper['file_id']} ({paper['node_id']})")
            print(f"     - Solution ID: {solution['file_id']} ({solution['node_id']} - {solution['label']})")
            
            # ä½¿ç”¨ MERGE è¯­å¥åœ¨ Neo4j ä¸­åˆ›å»ºå…³ç³»
            merge_query = f"""
            MATCH (p:paper), (s)
            WHERE elementId(p) = '{paper['node_id']}' AND elementId(s) = '{solution['node_id']}'
            MERGE (p)-[r:{CONFIG['NEW_RELATIONSHIP_TYPE']}]->(s)
            SET r.weightedScore = {weighted_score:.4f},
                r.abstractSimilarity = {sim_abstract:.4f},
                r.coreProblemSimilarity = {sim_core_problem:.4f},
                r.createdAt = timestamp()
            """
            try:
                n4j.query(query=merge_query)
                print(f"     âœ… æˆåŠŸåœ¨å›¾ä¸­åˆ›å»º '{CONFIG['NEW_RELATIONSHIP_TYPE']}' å…³ç³»ã€‚")
            except Exception as e:
                print(f"     âŒ åˆ›å»ºå…³ç³»å¤±è´¥: {e}")

    if link_creation_count == 0:
        print("\nâœ… å®Œæˆè®¡ç®—ï¼Œæœªå‘ç°ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„è·¨è®ºæ–‡ Paper-Solution å¯¹ã€‚")
    else:
        print(f"\nğŸ‰ æµç¨‹ç»“æŸï¼æ€»å…±åˆ›å»ºäº† {link_creation_count} æ¡æ–°çš„**è·¨è®ºæ–‡** '{CONFIG['NEW_RELATIONSHIP_TYPE']}' å…³ç³»ã€‚")
        
    # --- æ–°å¢: ä¿å­˜æ›´æ–°åçš„ç¼“å­˜ ---
    save_embedding_cache(CONFIG["EMBEDDING_CACHE_BASE_PATH"], embedding_cache)

# =================================================================================
# 5. è„šæœ¬å…¥å£ (Script Entrypoint)
# =================================================================================
if __name__ == "__main__":
    print("=====================================================")
    print("=== Neo4j Paper-Solution åŠ æƒè¯­ä¹‰é“¾æ¥è„šæœ¬å¯åŠ¨ (å·²ä¼˜åŒ–è·¨è®ºæ–‡è¿‡æ»¤) ===")
    print("=====================================================")
    main()
