# `test_graph.py` 框架与核心执行流程分析

本文档详细解析 `test_graph.py` 脚本的内部工作机制。该脚本实现了一个复杂的、基于检索增强生成（RAG）的问答系统，它融合了向量数据库的语义搜索和知识图谱的结构化查询。

## 1. 系统概述

该脚本的核心思想是**混合式检索增强生成（Hybrid RAG）**。它不依赖单一信息源，而是通过两个并行的检索阶段来收集证据，然后将这些证据整合后，交由一个大型语言模型（LLM）进行最终的综合与回答。

- **主证据源（Primary Evidence）**：利用**向量数据库Faiss**对本地知识库（JSON文件）进行语义相似度搜索，快速找到与用户问题最相关的文本片段。
- **补充上下文（Supplementary Context）**：利用**知识图谱（Neo4j）**，从用户问题中提取关键实体，并在图数据库中查询这些实体的关联信息，提供结构化的背景知识。

最终，这两部分信息被整合在一起，形成一个丰富的上下文，交由一个配置了高级指令的 `ChatAgent` 来生成最终的答案。

## 2. 核心组件 (Core Components)

脚本依赖于 `camel` 框架下的多个核心模块来完成其功能：

| 模块/类 | 用途 |
| :--- | :--- |
| `ModelFactory` | 用于创建和配置大型语言模型实例，如此处的 `Qwen` 模型。 |
| `Neo4jGraph` | Neo4j 数据库的客户端，负责连接数据库并执行 Cypher 查询。 |
| `FaissStorage` | 一个基于 FAISS 的本地向量存储实现，用于构建、加载和查询向量索引。 |
| `OpenAICompatibleEmbedding` | 嵌入模型客户端，用于将文本转换为向量表示。这里通过兼容 OpenAI 的接口调用了阿里云 DashScope 的服务。 |
| `KnowledgeGraphAgent` | 一个专门的 Agent，用于从自然语言文本中提取知识图谱的节点（实体）和关系。 |
| `ChatAgent` | 通用的对话 Agent，负责根据系统提示（System Prompt）和用户输入（包含检索到的上下文）来生成最终的连贯回答。 |
| `UnstructuredIO` | 用于将原始文本数据转换为 `camel` 框架内部处理的标准化 `Element` 对象。 |

## 3. 核心执行流程

整个脚本的执行流程可以清晰地划分为四个主要阶段：初始化设置、双路并行检索、上下文整合、以及最终的生成与输出。



---

### **阶段一：初始化与环境配置 (Initialization & Setup)**

此阶段为后续所有操作奠定基础。

1.  **API 密钥与环境变量**：脚本首先从环境中加载或直接设置了访问各类服务所需的 API 密钥，包括阿里云 DashScope（用于语言模型和嵌入服务）和 Neo4j 数据库的凭证。
2.  **数据库连接**：初始化 `Neo4jGraph` 实例，建立与云端 Neo4j AuraDB 的连接。
3.  **模型实例化**：
    *   通过 `ModelFactory` 创建一个 `Qwen` 大语言模型实例 (`qwen_model`)，并配置其温度等参数以控制生成的多样性。
    *   初始化 `OpenAICompatibleEmbedding` 实例，配置其使用阿里云 DashScope 的 `text-embedding-v2` 模型作为嵌入工具。
4.  **Agent 和工具实例化**：创建 `KnowledgeGraphAgent` 和 `UnstructuredIO` 等核心组件的实例。

---

### **阶段二：双路并行检索 (Dual-Channel Retrieval)**

这是系统的核心信息获取阶段，分为向量搜索和知识图谱查询两条并行的路径。

#### **路径 A：向量搜索 (Vector Search - Primary Evidence)**

此路径的目标是快速定位与用户查询在语义上最相关的原始文本。

1.  **构建/加载向量索引**：
    *   脚本会为 `final_custom_kg_papers.json` 文件中每篇论文的多个**关键属性**（如 `abstract`, `core_problem`, `related_work` 等）分别建立独立的 FAISS 向量索引。
    *   **首次运行**：脚本会遍历 JSON 文件，提取每篇论文指定属性的文本内容，调用嵌入模型将其转换为向量，然后存入对应的 `FaissStorage` 实例中并持久化到本地磁盘 (`Myexamples/vdb/camel_faiss_storage`)。为了遵循 API 的限制，嵌入过程被设计为**分批处理**（每批25个文本）。
    *   **后续运行**：如果检测到索引文件已存在，则直接从磁盘加载，大大提高了启动效率。
2.  **执行查询**：
    *   将用户的查询字符串（`query`）同样进行向量化，得到查询向量。
    *   使用该查询向量，**遍历所有属性的 FAISS 索引**，在每个索引中查找 `top_k=1` 的最相似结果。
3.  **结果格式化**：
    *   收集所有索引的查询结果，并格式化成人类可读的字符串。每个结果都包含其来源属性、相似度分数、论文ID以及内容摘要。
    *   所有结果被整合到 `vector_result` 变量中，作为“主证据”。

#### **路径 B：知识图谱查询 (Knowledge Graph - Supplementary Context)**

此路径的目标是提供结构化的、与查询实体相关的背景信息。

1.  **实体提取**：
    *   将用户查询文本传递给 `KnowledgeGraphAgent` (`kg_agent`)。
    *   `kg_agent` 利用其内置的 LLM 能力，从查询中识别并提取出关键的实体（Nodes）。
2.  **动态 Cypher 查询**：
    *   脚本遍历 `kg_agent` 提取出的每个实体。
    *   对于每个实体，它会动态构建一个强大而灵活的 **Cypher 查询**。该查询的特点是：
        *   **全属性模糊搜索**：它会在 Neo4j 数据库中**所有节点**的**所有属性**中，搜索包含该实体关键词的节点。
        *   **邻居节点发现**：一旦找到匹配的节点，查询会进一步获取其一度（1-hop）相连的邻居节点及其关系。
        *   **结果限制**：为了防止信息过载，每个关键词最多返回 5 条关系描述。
3.  **结果整合**：
    *   执行查询并收集所有返回的关系描述字符串。
    *   将结果存入 `kg_result` 变量中，作为“补充上下文”。

---

### **阶段三：上下文整合与提示工程 (Context Structuring & Prompt Engineering)**

此阶段将前两阶段检索到的异构信息整合成一个结构化的、可供 LLM 理解的上下文。

1.  **构建结构化上下文** (`structured_context`)：
    *   创建一个格式化的字符串，其中清晰地标识出“PRIMARY EVIDENCE”（来自向量搜索）和“SUPPLEMENTARY CONTEXT”（来自知识图谱）。
    *   这种结构化设计有助于引导 LLM 区分信息来源的主次和可靠性。
2.  **设计高级系统提示** (`advanced_system_prompt`)：
    *   脚本定义了一个非常详尽的系统提示，将 `ChatAgent` 的角色设定为“AI 研究助理专家”。
    *   该提示包含了详细的**响应策略**、**答案结构**、**质量保证**和**沟通风格**要求，例如：
        *   **信息优先级**：明确指示模型应主要依赖“主证据”，仅在必要时使用“补充上下文”。
        *   **专家级阐述**：要求模型在回答的基础上，对证据中提到的技术概念进行更深入的“专家阐述”。
        *   **严禁捏造**：强调所有回答必须严格基于所提供的证据。

---

### **阶段四：增强生成与输出 (Augmented Generation & Output)**

这是流程的最后一步，生成最终的答案。

1.  **创建最终用户提示**：
    *   将用户的原始 `query` 和前面构建的 `structured_context` 组合成一个最终的用户提示。
2.  **调用 ChatAgent**：
    *   初始化一个 `ChatAgent`，并将高级系统提示 (`advanced_system_prompt`) 传递给它。
    *   调用 `agent.step()` 方法，传入最终的用户提示。
3.  **输出结果**：
    *   `ChatAgent` 根据其角色设定和收到的包含丰富上下文的提示，生成一个结构化、全面且深入的回答。
    *   脚本最后将此回答打印到控制台。

## 4. 结论

`test_graph.py` 不仅仅是一个简单的问答脚本，它是一个精心设计的、小而全的 RAG 系统原型。其核心优势在于：

- **混合检索**：结合了向量搜索的语义匹配能力和图数据库的结构化关联能力，使信息检索更全面。
- **信息分层**：通过将信息源区分为“主证据”和“补充上下文”，为 LLM 的信息综合提供了清晰的指引，提高了答案的准确性和可靠性。
- **高级提示工程**：通过一个详尽的系统提示，精确地控制了 LLM 的角色、行为和输出格式，确保了最终答案的高质量。
- **模块化设计**：利用 `camel` 框架的组件，实现了清晰的、可扩展的架构。
