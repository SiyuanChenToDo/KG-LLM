#!/usr/bin/env python3
"""
æŠ€æœ¯è¯æ®æå–å™¨ - çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆç‰ˆ
=====================================

å……åˆ†åˆ©ç”¨ç»†ç²’åº¦çŸ¥è¯†å›¾è°±çš„é«˜è´¨é‡äººå·¥æ ‡æ³¨ä¿¡æ¯ï¼Œä»ä¸‰ç§èŠ‚ç‚¹ç±»å‹ä¸­æå–
å…·ä½“çš„æŠ€æœ¯è¯æ®ï¼Œç”¨äºå»ºç«‹çœŸæ­£æœ‰æ„ä¹‰çš„è·¨è®ºæ–‡è¿æ¥ã€‚

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ğŸ¯ æ·±åº¦é›†æˆnode_attributes_api.pyï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰èŠ‚ç‚¹å±æ€§
2. ğŸ“Š åŸºäºpaperèŠ‚ç‚¹çš„ä¸°å¯Œå…ƒæ•°æ®ï¼ˆauthors, conference, yearç­‰ï¼‰
3. ğŸ” åˆ©ç”¨research_questionçš„è¯¦ç»†é—®é¢˜æè¿°å’Œç®€åŒ–ç‰ˆæœ¬
4. âš™ï¸ ä»solutionçš„å®Œæ•´å’Œç®€åŒ–æè¿°ä¸­æå–æŠ€æœ¯ç»„ä»¶
5. ğŸ§  æ™ºèƒ½çš„è·¨è®ºæ–‡æŠ€æœ¯è¿ç§»æœºä¼šå‘ç°
6. ğŸ“ ç”Ÿæˆé«˜è´¨é‡çš„æŠ€æœ¯æ”¹é€ è¯æ®å’ŒéªŒè¯å»ºè®®
"""

import re
import json
import asyncio
import os
import sys
from typing import Dict, List, Tuple, Optional, Set, Union
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import logging
from pathlib import Path

# å¯¼å…¥èŠ‚ç‚¹å±æ€§API
sys.path.append(str(Path(__file__).parent.parent))
from node_attributes_api import (
    get_solution_attributes_by_name, 
    get_paper_attributes_by_name, 
    get_research_question_attributes_by_name,
    get_solution_attributes,
    find_paper_by_title,
    get_paper_research_questions,
    get_research_question_solutions,
    print_node_attributes
)

logger = logging.getLogger(__name__)

@dataclass
class TechnicalComponent:
    """æŠ€æœ¯ç»„ä»¶ - å¢å¼ºç‰ˆ"""
    name: str                    # ç»„ä»¶åç§° 
    type: str                   # ç»„ä»¶ç±»å‹ï¼šloss_function, module, algorithm, constraint, etc.
    description: str            # è¯¦ç»†æè¿°
    source_context: str         # æºä¸Šä¸‹æ–‡
    parameters: List[str]       # å‚æ•°åˆ—è¡¨
    mathematical_form: str      # æ•°å­¦è¡¨è¾¾ï¼ˆå¦‚æœæœ‰ï¼‰
    
    # ğŸ†• æ–°å¢å±æ€§ï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¿¡æ¯
    source_paper_info: Dict = field(default_factory=dict)  # æ¥æºè®ºæ–‡ä¿¡æ¯
    source_solution_info: Dict = field(default_factory=dict)  # æ¥æºè§£å†³æ–¹æ¡ˆä¿¡æ¯
    technical_domain: str = ""   # æŠ€æœ¯é¢†åŸŸ
    complexity_level: str = ""   # å¤æ‚åº¦çº§åˆ«ï¼šlow, medium, high
    implementation_details: List[str] = field(default_factory=list)  # å®ç°ç»†èŠ‚
    related_concepts: List[str] = field(default_factory=list)  # ç›¸å…³æ¦‚å¿µ
    performance_metrics: List[str] = field(default_factory=list)  # æ€§èƒ½æŒ‡æ ‡
    
@dataclass 
class TechnicalChallenge:
    """æŠ€æœ¯æŒ‘æˆ˜ - å¢å¼ºç‰ˆ"""
    name: str                   # æŒ‘æˆ˜åç§°
    domain: str                 # é¢†åŸŸ
    description: str            # æŒ‘æˆ˜æè¿°
    constraints: List[str]      # çº¦æŸæ¡ä»¶
    desired_properties: List[str] # æœŸæœ›çš„è§£å†³æ–¹æ¡ˆç‰¹æ€§
    
    # ğŸ†• æ–°å¢å±æ€§ï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¿¡æ¯
    source_paper_info: Dict = field(default_factory=dict)  # æ¥æºè®ºæ–‡ä¿¡æ¯
    source_rq_info: Dict = field(default_factory=dict)     # æ¥æºç ”ç©¶é—®é¢˜ä¿¡æ¯
    challenge_category: str = ""  # æŒ‘æˆ˜ç±»åˆ«
    difficulty_level: str = ""    # éš¾åº¦çº§åˆ«
    research_context: str = ""    # ç ”ç©¶èƒŒæ™¯
    existing_approaches: List[str] = field(default_factory=list)  # ç°æœ‰æ–¹æ³•
    evaluation_criteria: List[str] = field(default_factory=list)  # è¯„ä¼°æ ‡å‡†

@dataclass
class TransferOpportunity:
    """è¿ç§»æœºä¼š - å¢å¼ºç‰ˆ"""
    source_component: TechnicalComponent
    target_challenge: TechnicalChallenge
    adaptation_evidence: str    # æ”¹é€ è¯æ®
    adaptation_mechanism: str   # æ”¹é€ æœºåˆ¶
    validation_suggestion: str  # éªŒè¯å»ºè®®
    transfer_feasibility: float # è¿ç§»å¯è¡Œæ€§ (0-1)
    
    # ğŸ†• æ–°å¢å±æ€§ï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„æ·±åº¦åˆ†æ
    cross_paper_context: Dict = field(default_factory=dict)  # è·¨è®ºæ–‡ä¸Šä¸‹æ–‡
    technical_gap_analysis: str = ""  # æŠ€æœ¯å·®è·åˆ†æ
    innovation_potential: float = 0.0  # åˆ›æ–°æ½œåŠ› (0-1)
    implementation_complexity: str = ""  # å®ç°å¤æ‚åº¦
    expected_performance_gain: str = ""  # é¢„æœŸæ€§èƒ½æå‡
    risk_assessment: str = ""  # é£é™©è¯„ä¼°
    related_work_analysis: str = ""  # ç›¸å…³å·¥ä½œåˆ†æ

class TechnicalEvidenceExtractor:
    """æŠ€æœ¯è¯æ®æå–å™¨ - çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆç‰ˆ"""
    
    def __init__(self, kg_file_path: Optional[str] = None):
        """åˆå§‹åŒ–æŠ€æœ¯è¯æ®æå–å™¨
        
        Args:
            kg_file_path: çŸ¥è¯†å›¾è°±æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨APIè‡ªåŠ¨æ£€æµ‹
        """
        self.kg_file_path = kg_file_path
        
        # ğŸ”§ è®¾ç½®æŠ€æœ¯æ¨¡å¼åº“ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
        self.extraction_stats = {
            "components_extracted": 0,
            "challenges_extracted": 0,
            "opportunities_found": 0
        }
        # æ·»åŠ ç¼“å­˜æœºåˆ¶é¿å…é‡å¤æå–
        self._component_cache = {}
        self._challenge_cache = {}
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å¼
        self._setup_technical_patterns()
        self._setup_component_types()
        
        logger.info("æŠ€æœ¯è¯æ®æå–å™¨åˆå§‹åŒ–å®Œæˆ - çŸ¥è¯†å›¾è°±æ·±åº¦é›†æˆç‰ˆ")
    
    def _setup_enhanced_extractors(self):
        """ğŸ†• è®¾ç½®åŸºäºçŸ¥è¯†å›¾è°±çš„å¢å¼ºæå–å™¨"""
        # è®ºæ–‡å…ƒæ•°æ®åˆ†ææƒé‡
        self.paper_metadata_weights = {
            'title': 0.25,
            'abstract': 0.40, 
            'authors': 0.15,
            'conference': 0.10,
            'year': 0.05,
            'core_problem': 0.05
        }
        
        
        # æ€§èƒ½æŒ‡æ ‡å…³é”®è¯
        self.performance_keywords = [
            'accuracy', 'precision', 'recall', 'f1', 'auc', 'map', 'bleu', 'rouge',
            'perplexity', 'loss', 'error', 'mse', 'mae', 'iou', 'dice', 'psnr'
        ]
    
    def _setup_cross_paper_analysis(self):
        """ğŸ†• è®¾ç½®è·¨è®ºæ–‡åˆ†æåŠŸèƒ½"""
        # åˆ›æ–°æ½œåŠ›è¯„ä¼°å› å­
        self.innovation_factors = {
            'novelty': 0.30,      # æ–°é¢–æ€§
            'impact': 0.25,       # å½±å“åŠ›
            'feasibility': 0.20,  # å¯è¡Œæ€§
            'generality': 0.15,   # é€šç”¨æ€§
            'efficiency': 0.10    # æ•ˆç‡
        }
        
        # é£é™©è¯„ä¼°ç±»åˆ«
        self.risk_categories = {
            'technical': ['implementation', 'scalability', 'compatibility'],
            'performance': ['degradation', 'instability', 'overfitting'],
            'resource': ['computational', 'memory', 'time'],
            'domain': ['transferability', 'generalization', 'adaptation']
        }
    
    def _setup_evidence_templates(self):
        """ğŸ†• è®¾ç½®è¯æ®ç”Ÿæˆæ¨¡æ¿"""
        self.evidence_templates = {
            'technical_transfer': [
                "ä»{source_paper}çš„{source_method}ä¸­æå–çš„{component_type}æŠ€æœ¯ï¼Œé€šè¿‡{adaptation}æ”¹é€ åï¼Œå¯ä»¥è§£å†³{target_paper}ä¸­çš„{target_challenge}é—®é¢˜ã€‚",
                "åŸºäº{source_conference}å‘è¡¨çš„{source_method}ï¼Œå…¶{core_technique}æœºåˆ¶ä¸º{target_domain}é¢†åŸŸçš„{specific_challenge}æä¾›äº†æ–°çš„è§£å†³æ€è·¯ã€‚",
                "å°†{source_authors}æå‡ºçš„{technical_component}ä¸{target_context}ç›¸ç»“åˆï¼Œé¢„æœŸèƒ½å¤Ÿåœ¨{performance_metric}ä¸Šè·å¾—{expected_gain}çš„æå‡ã€‚"
            ],
            'methodological_insight': [
                "ä»{source_method}çš„{key_insight}ä¸­è·å¾—å¯å‘ï¼Œ{target_method}å¯ä»¥é‡‡ç”¨ç±»ä¼¼çš„{strategy}æ¥å¤„ç†{target_challenge}ã€‚",
                "{source_paper}åœ¨{source_domain}ä¸­çš„æˆåŠŸç»éªŒè¡¨æ˜ï¼Œ{approach}å¯¹äº{target_scenario}å…·æœ‰è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚"
            ],
            'architectural_adaptation': [
                "å€Ÿé‰´{source_architecture}çš„{design_principle}ï¼Œä¸º{target_application}è®¾è®¡{adapted_architecture}ï¼Œé¢„æœŸèƒ½å¤Ÿ{benefit}ã€‚",
                "é€šè¿‡å¯¹{source_model}çš„{modification_type}æ”¹é€ ï¼Œä½¿å…¶é€‚åº”{target_constraints}ï¼ŒåŒæ—¶ä¿æŒ{preserved_properties}ã€‚"
            ]
        }
    
    def _setup_technical_patterns(self):
        """è®¾ç½®æŠ€æœ¯æ¨¡å¼è¯†åˆ«"""
        # ğŸ”§ çŸ¥è¯†å›¾è°±æ–‡ä»¶è·¯å¾„ï¼ˆå¤šä¸ªå¯èƒ½çš„ä½ç½®ï¼‰
        self.kg_file_paths = [
            '/root/autodl-tmp/LightRAG/data/final_custom_kg_papers.json',
            '/root/autodl-tmp/LightRAG/data/test_custom_kg_papers.json',
            'data/final_custom_kg_papers.json',
            'data/custom_kg_papers.json', 
            'data/test_custom_kg_papers.json',
            'data/custom_kg_fixed.json',
            'data/custom_kg.json',
            '../data/final_custom_kg_papers.json',
            '../data/custom_kg_papers.json', 
            '../data/test_custom_kg_papers.json',
            '../data/custom_kg_fixed.json',
            '../data/custom_kg.json',
            '../../data/final_custom_kg_papers.json',
            '../../data/custom_kg_papers.json', 
            '../../data/test_custom_kg_papers.json',
            '../../data/custom_kg_fixed.json',
            '../../data/custom_kg.json'
        ]
        # æŸå¤±å‡½æ•°æ¨¡å¼
        self.loss_patterns = [
            r'(\w+(?:\s+\w+)*)\s*loss',
            r'L[_\s]*(\w+)',
            r'(\w+(?:\s+\w+)*)\s*æŸå¤±',
            r'(\w+(?:\s+\w+)*)\s*constraint',
            r'(\w+(?:\s+\w+)*)\s*regularization'
        ]
        
        # ç®—æ³•/æ¨¡å—æ¨¡å¼  
        self.algorithm_patterns = [
            r'(\w+(?:\s+\w+)*)\s*algorithm',
            r'(\w+(?:\s+\w+)*)\s*module',
            r'(\w+(?:\s+\w+)*)\s*mechanism',
            r'(\w+(?:\s+\w+)*)\s*framework',
            r'(\w+(?:\s+\w+)*)\s*approach',
            r'(\w+(?:\s+\w+)*)\s*method'
        ]
        
        # æ•°å­¦è¡¨è¾¾æ¨¡å¼
        self.math_patterns = [
            r'([A-Z]_\w+)\s*=\s*([^.]+)',
            r'(\w+)\s*\(\s*([^)]+)\s*\)',
            r'argmin|argmax|min|max|sum|âˆ‘|âˆ'
        ]
        
        # æŠ€æœ¯æŒ‘æˆ˜æ¨¡å¼
        self.challenge_patterns = [
            r'challenge[s]?\s*(?:of|in|for)?\s*([^.]+)',
            r'problem[s]?\s*(?:of|in|for)?\s*([^.]+)',
            r'issue[s]?\s*(?:of|in|for)?\s*([^.]+)',
            r'limitation[s]?\s*(?:of|in|for)?\s*([^.]+)',
            r'difficulty\s*(?:of|in|for)?\s*([^.]+)'
        ]
    
    def _setup_component_types(self):
        """è®¾ç½®æŠ€æœ¯ç»„ä»¶ç±»å‹"""
        self.component_keywords = {
            'loss_function': ['loss', 'objective', 'cost', 'regularization', 'penalty'],
            'attention_mechanism': ['attention', 'self-attention', 'cross-attention', 'multi-head'],
            'contrastive_learning': ['contrastive', 'contrast', 'positive', 'negative', 'pairs'],
            'gradient_method': ['gradient', 'backprop', 'optimization', 'descent', 'update'],
            'normalization': ['normalization', 'batch norm', 'layer norm', 'instance norm'],
            'activation': ['relu', 'sigmoid', 'tanh', 'activation', 'nonlinearity'],
            'regularization': ['dropout', 'weight decay', 'regularization', 'constraint'],
            'embedding': ['embedding', 'representation', 'encoding', 'feature'],
            'domain_adaptation': ['adaptation', 'transfer', 'domain', 'shift', 'alignment']
        }
    
    def _setup_domain_keywords(self):
        """è®¾ç½®é¢†åŸŸå…³é”®è¯"""
        self.domain_keywords = {
            'computer_vision': ['image', 'video', 'visual', 'CNN', 'convolution', 'detection'],
            'nlp': ['text', 'language', 'word', 'sentence', 'semantic', 'transformer'],
            'multimodal': ['multimodal', 'cross-modal', 'vision-language', 'text-video'],
            'time_series': ['temporal', 'time', 'sequence', 'sequential', 'time-series'],
            'graph': ['graph', 'node', 'edge', 'network', 'relation'],
            'retrieval': ['retrieval', 'search', 'ranking', 'similarity', 'matching'],
            'continual_learning': ['continual', 'incremental', 'lifelong', 'catastrophic'],
            'domain_adaptation': ['domain', 'adaptation', 'transfer', 'source', 'target']
        }

    def _get_related_paper_info(self, entity_name: str) -> Optional[Dict]:
        """è·å–ä¸èŠ‚ç‚¹ç›¸å…³çš„è®ºæ–‡ä¿¡æ¯"""
        try:
            # ä»entity_nameæ¨å¯¼è®ºæ–‡ID
            if '_SOL_' in entity_name:
                paper_id = entity_name.split('_SOL_')[0]
            elif '_RQ_' in entity_name:
                paper_id = entity_name.split('_RQ_')[0]
            else:
                paper_id = entity_name
            
            # å°è¯•é€šè¿‡APIè·å–è®ºæ–‡ä¿¡æ¯
            papers = find_paper_by_title(paper_id)
            if papers:
                return papers[0]
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥è·å–paperå±æ€§
            return get_paper_attributes_by_name('') or {}
            
        except Exception as e:
            logger.warning(f"è·å–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _assess_complexity(self, text: str) -> str:
        """ç®€åŒ–çš„å¤æ‚åº¦è¯„ä¼°"""
        return 'medium'  # ç»Ÿä¸€è¿”å›ä¸­ç­‰å¤æ‚åº¦
    
    def _extract_performance_metrics(self, text: str) -> List[str]:
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = []
        text_lower = text.lower()
        
        for metric in self.performance_keywords:
            if metric in text_lower:
                metrics.append(metric)
        
        return list(set(metrics))  # å»é‡
    
    def _extract_implementation_details(self, text: str) -> List[str]:
        """æå–å®ç°ç»†èŠ‚"""
        details = []
        
        # æŸ¥æ‰¾å®ç°ç›¸å…³çš„æ¨¡å¼
        implementation_patterns = [
            r'implement[ed]?\s+([^.]+)',
            r'use[sd]?\s+([^.]+)',
            r'apply\s+([^.]+)',
            r'employ[ed]?\s+([^.]+)',
            r'adopt[ed]?\s+([^.]+)'
        ]
        
        for pattern in implementation_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                detail = match.group(1)
                if detail and isinstance(detail, str):
                    detail = detail.strip()
                    if len(detail) > 5 and len(detail) < 100:
                        details.append(detail)
        
        return details[:5]  # é™åˆ¶æ•°é‡
    
    def _extract_related_concepts(self, text: str) -> List[str]:
        """æå–ç›¸å…³æ¦‚å¿µ"""
        concepts = []
        
        # æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µçš„æ¨¡å¼
        concept_patterns = [
            r'based\s+on\s+([^.]+)',
            r'inspired\s+by\s+([^.]+)',
            r'similar\s+to\s+([^.]+)',
            r'related\s+to\s+([^.]+)',
            r'extends?\s+([^.]+)'
        ]
        
        for pattern in concept_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                concept = match.group(1)
                if concept and isinstance(concept, str):
                    concept = concept.strip()
                    if len(concept) > 3 and len(concept) < 50:
                        concepts.append(concept)
        
        return concepts[:5]  # é™åˆ¶æ•°é‡
    
    def _enrich_component_context(self, component: TechnicalComponent, solution_data: Dict, paper_info: Optional[Dict]):
        """ä¸°å¯Œç»„ä»¶ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        component.source_context = solution_data.get('entity_name', '')
        component.source_solution_info = solution_data
        component.source_paper_info = paper_info or {}
        
        # è®¾ç½®æŠ€æœ¯é¢†åŸŸ
        component.technical_domain = 'general'
        
    
    def extract_technical_components(self, solution_data: Dict) -> List[TechnicalComponent]:
        """
        ä»solutionèŠ‚ç‚¹ä¸­æå–æŠ€æœ¯ç»„ä»¶
        
        Args:
            solution_data: solutionèŠ‚ç‚¹æ•°æ®
            
        Returns:
            List[TechnicalComponent]: æŠ€æœ¯ç»„ä»¶åˆ—è¡¨
        """
        try:
            # ğŸ”§ ä½¿ç”¨èŠ‚ç‚¹å±æ€§APIè·å–çœŸå®solutionå±æ€§
            entity_name = solution_data.get('entity_name', '')
            
            # ğŸ”§ èŠ‚ç‚¹å­˜åœ¨æ€§éªŒè¯
            if not entity_name or not entity_name.strip():
                logger.warning(f"solutionèŠ‚ç‚¹ç¼ºå°‘entity_name")
                return []
            
            # ğŸ”§ ç¼“å­˜æœºåˆ¶ï¼šå…ˆæ£€æŸ¥ç¼“å­˜
            cache_key = f"components_{entity_name}"
            if cache_key in self._component_cache:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æŠ€æœ¯ç»„ä»¶ for {entity_name}")
                return self._component_cache[cache_key]
            
            solution_attrs = get_solution_attributes_by_name(entity_name)
            
            if solution_attrs:
                # åŸºäºçœŸå®å±æ€§æå–æŠ€æœ¯ç»„ä»¶
                full_solution = solution_attrs.get('full_solution', '')
                simplified_solution = solution_attrs.get('simplified_solution', '')
                components = self._extract_real_technical_components(full_solution, simplified_solution, entity_name, solution_attrs)
                
                # ç¼“å­˜ç»“æœ
                self._component_cache[cache_key] = components
                logger.info(f"æå–åˆ° {len(components)} ä¸ªæŠ€æœ¯ç»„ä»¶ from {entity_name}")
                
                return components
            else:
                logger.warning(f"æ— æ³•è·å–solutionå±æ€§: {entity_name}")
                # ç¼“å­˜ç©ºç»“æœï¼Œé¿å…é‡å¤æŸ¥è¯¢
                self._component_cache[cache_key] = []
                return []
                
        except Exception as e:
            logger.error(f"æŠ€æœ¯ç»„ä»¶æå–å¤±è´¥: {e}")
            return []

    def _extract_real_technical_components(self, full_solution: str, simplified_solution: str, 
                                         entity_name: str, solution_data: Dict) -> List[TechnicalComponent]:
        """ä»çœŸå®solutionæè¿°ä¸­æå–æŠ€æœ¯ç»„ä»¶"""
        components = []
        
        # åˆ†æsimplified_solutionä¸­çš„å…³é”®æŠ€æœ¯ç‚¹
        if simplified_solution:
            tech_points = [point.strip() for point in simplified_solution.split(';') if point.strip()]
            
            for point in tech_points:
                # æå–æŠ€æœ¯ç»„ä»¶åç§°å’Œç±»å‹
                component_name, component_type = self._parse_technical_point(point)
                
                if component_name:
                    # ä»full_solutionä¸­æ‰¾åˆ°ç›¸å…³æè¿°
                    detailed_desc = self._find_detailed_description(component_name, full_solution)
                    
                    component = TechnicalComponent(
                        name=component_name,
                        type=component_type,
                        description=detailed_desc or point,
                        source_context=f"æ¥è‡ª {entity_name}",
                        parameters=self._extract_parameters_from_description(detailed_desc or point),
                        mathematical_form=self._extract_mathematical_form(detailed_desc or point),
                        source_paper_info=self._get_paper_info_from_solution_data(solution_data),
                        technical_domain='general',
                        complexity_level='medium'
                    )
                    components.append(component)
        
        return components

    def _parse_technical_point(self, tech_point: str) -> Tuple[str, str]:
        """è§£ææŠ€æœ¯ç‚¹ï¼Œæå–ç»„ä»¶åç§°å’Œç±»å‹"""
        tech_point = tech_point.lower().strip()
        
        # æŠ€æœ¯ç±»å‹æ˜ å°„
        type_mappings = {
            'loss': 'loss_function',
            'divergence': 'loss_function', 
            'distance': 'loss_function',
            'pool': 'data_structure',
            'representation': 'feature_representation',
            'embedding': 'feature_representation',
            'network': 'neural_architecture',
            'module': 'neural_module',
            'mechanism': 'algorithm',
            'strategy': 'algorithm',
            'framework': 'system_architecture',
            'selector': 'component',
            'encoder': 'neural_module',
            'normalization': 'preprocessing'
        }
        
        # è¯†åˆ«æŠ€æœ¯ç±»å‹
        component_type = 'algorithm'  # é»˜è®¤ç±»å‹
        for keyword, mapped_type in type_mappings.items():
            if keyword in tech_point:
                component_type = mapped_type
                break
        
        # æå–ç»„ä»¶åç§°ï¼ˆä¿ç•™åŸå§‹æè¿°çš„å…³é”®éƒ¨åˆ†ï¼‰
        component_name = tech_point.replace(';', '').strip()
        
        return component_name, component_type

    def _find_detailed_description(self, component_name: str, full_solution: str) -> str:
        """ä»å®Œæ•´solutionä¸­æ‰¾åˆ°ç»„ä»¶çš„è¯¦ç»†æè¿°"""
        # æŸ¥æ‰¾åŒ…å«ç»„ä»¶åç§°çš„å¥å­æ®µè½
        sentences = full_solution.split('.')
        relevant_sentences = []
        
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in component_name.lower().split()):
                relevant_sentences.append(sentence.strip())
        
        return '. '.join(relevant_sentences[:2])  # è¿”å›å‰ä¸¤ä¸ªç›¸å…³å¥å­

    def _get_paper_info_from_solution_data(self, solution_data: Dict) -> Dict:
        """ä»solutionæ•°æ®ä¸­è·å–è®ºæ–‡ä¿¡æ¯"""
        source_id = solution_data.get('source_id', '')
        if source_id:
            try:
                # é€šè¿‡source_idæŸ¥æ‰¾å¯¹åº”çš„paperä¿¡æ¯
                paper_attrs = get_paper_attributes_by_name(f"paper_{source_id}")
                if paper_attrs:
                    return {
                        'title': paper_attrs.get('title', ''),
                        'authors': paper_attrs.get('authors', ''),
                        'conference': paper_attrs.get('conference', ''),
                        'year': paper_attrs.get('year', ''),
                        'abstract': paper_attrs.get('abstract', '')
                    }
            except:
                pass
        return {}

    def _extract_loss_functions(self, text: str) -> List[TechnicalComponent]:
        """æå–æŸå¤±å‡½æ•°ç»„ä»¶"""
        components = []
        
        for pattern in self.loss_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                name = match.group(1)
                if name and isinstance(name, str):
                    name = name.strip()
                    if len(name) > 2:  # è¿‡æ»¤è¿‡çŸ­çš„åŒ¹é…
                        # æå–å‘¨å›´çš„ä¸Šä¸‹æ–‡
                        start = max(0, match.start() - 50)
                        end = min(len(text), match.end() + 100)
                        context = text[start:end].strip()
                        
                        # æŸ¥æ‰¾æ•°å­¦è¡¨è¾¾
                        math_form = self._extract_math_near_text(text, match.start(), match.end())
                        
                        component = TechnicalComponent(
                            name=name,
                            type='loss_function',
                            description=context,
                            source_context='',
                            parameters=self._extract_parameters(context),
                            mathematical_form=math_form
                        )
                        components.append(component)
        
        return components
    
    def _extract_algorithms(self, text: str) -> List[TechnicalComponent]:
        """æå–ç®—æ³•/æ¨¡å—ç»„ä»¶"""
        components = []
        
        for pattern in self.algorithm_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                name = match.group(1)
                if name and isinstance(name, str):
                    name = name.strip()
                    if len(name) > 3:  # è¿‡æ»¤è¿‡çŸ­çš„åŒ¹é…
                        # åˆ¤æ–­ç»„ä»¶ç±»å‹
                        component_type = self._classify_component_type(name, text)
                        
                        # æå–å‘¨å›´çš„ä¸Šä¸‹æ–‡
                        start = max(0, match.start() - 30)
                        end = min(len(text), match.end() + 80)
                        context = text[start:end].strip()
                        
                        component = TechnicalComponent(
                            name=name,
                            type=component_type,
                            description=context,
                            source_context='',
                            parameters=self._extract_parameters(context),
                            mathematical_form=''
                        )
                        components.append(component)
        
        return components
    
    def _extract_constraints(self, text: str) -> List[TechnicalComponent]:
        """æå–çº¦æŸæ¡ä»¶"""
        components = []
        
        # æŸ¥æ‰¾çº¦æŸæ¨¡å¼
        constraint_patterns = [
            r'constraint[s]?\s*(?:that|:)?\s*([^.]+)',
            r'subject\s*to\s*([^.]+)',
            r'such\s*that\s*([^.]+)',
            r'ensure[s]?\s*(?:that)?\s*([^.]+)',
            r'guarantee[s]?\s*(?:that)?\s*([^.]+)'
        ]
        
        for pattern in constraint_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                constraint_desc = match.group(1)
                if constraint_desc and isinstance(constraint_desc, str):
                    constraint_desc = constraint_desc.strip()
                    if len(constraint_desc) > 10:
                        component = TechnicalComponent(
                            name=f"Constraint: {constraint_desc[:50]}...",
                            type='constraint',
                            description=constraint_desc,
                            source_context='',
                            parameters=[],
                            mathematical_form=''
                        )
                        components.append(component)
        
        return components
    
    def _classify_component_type(self, name: str, context: str) -> str:
        """æ ¹æ®åç§°å’Œä¸Šä¸‹æ–‡åˆ†ç±»ç»„ä»¶ç±»å‹"""
        name_lower = name.lower()
        context_lower = context.lower()
        
        for comp_type, keywords in self.component_keywords.items():
            for keyword in keywords:
                if keyword in name_lower or keyword in context_lower:
                    return comp_type
        
        return 'algorithm'  # é»˜è®¤ç±»å‹
    
    def _extract_parameters(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å‚æ•°"""
        parameters = []
        
        # æŸ¥æ‰¾å‚æ•°æ¨¡å¼
        param_patterns = [
            r'parameter[s]?\s*([^.]+)',
            r'with\s+([a-zA-Z_]\w*)\s*=',
            r'where\s+([a-zA-Z_]\w*)\s+(?:is|represents)',
            r'([a-zA-Z_]\w*)\s*âˆˆ\s*[^,.]+'
        ]
        
        for pattern in param_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                param = match.group(1)
                if param and isinstance(param, str):
                    param = param.strip()
                    if len(param) <= 20:  # è¿‡æ»¤è¿‡é•¿çš„åŒ¹é…
                        parameters.append(param)
        
        return list(set(parameters))  # å»é‡
    
    def _extract_math_near_text(self, text: str, start: int, end: int) -> str:
        """æå–æŒ‡å®šä½ç½®é™„è¿‘çš„æ•°å­¦è¡¨è¾¾"""
        # åœ¨åŒ¹é…ä½ç½®å‰å200å­—ç¬¦å†…æŸ¥æ‰¾æ•°å­¦è¡¨è¾¾
        search_start = max(0, start - 200)
        search_end = min(len(text), end + 200)
        search_text = text[search_start:search_end]
        
        for pattern in self.math_patterns:
            match = re.search(pattern, search_text)
            if match:
                return match.group(0)
        
        return ''

    def extract_technical_challenges(self, paper_data: Dict, rq_data: Optional[Dict] = None) -> List[TechnicalChallenge]:
        """ä»paper/RQèŠ‚ç‚¹ä¸­æå–æŠ€æœ¯æŒ‘æˆ˜ - çŸ¥è¯†å›¾è°±å¢å¼ºç‰ˆ
        
        Args:
            paper_data: paperèŠ‚ç‚¹æ•°æ®ï¼ˆåŒ…å«å®Œæ•´çš„çŸ¥è¯†å›¾è°±å±æ€§ï¼‰
            rq_data: research_questionèŠ‚ç‚¹æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´ç²¾ç¡®çš„æŒ‘æˆ˜æå–ï¼‰
            
        Returns:
            æŠ€æœ¯æŒ‘æˆ˜åˆ—è¡¨ï¼ˆåŒ…å«ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
        """
        challenges = []
        
        # è·å–ç›¸å…³æ–‡æœ¬ - å……åˆ†åˆ©ç”¨çŸ¥è¯†å›¾è°±å±æ€§
        entity_type = paper_data.get('entity_type', '')
        
        if entity_type == 'paper':
            text_sources = {
                'core_problem': paper_data.get('core_problem', ''),
                'basic_problem': paper_data.get('basic_problem', ''),
                'abstract': paper_data.get('abstract', ''),
                'title': paper_data.get('title', ''),
                'preliminary_innovation_analysis': paper_data.get('preliminary_innovation_analysis', ''),
                'authors': ', '.join(paper_data.get('authors', [])) if isinstance(paper_data.get('authors'), list) else str(paper_data.get('authors', '')),
                'conference': paper_data.get('conference', ''),
                'year': str(paper_data.get('year', ''))
            }
        elif rq_data:
            text_sources = {
                'research_question': rq_data.get('research_question', ''),
                'simplified_research_question': rq_data.get('simplified_research_question', ''),
                'paper_context': paper_data.get('abstract', ''),
                'paper_title': paper_data.get('title', '')
            }
        else:
            text_sources = {}
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        full_text = ' '.join(filter(None, text_sources.values()))
        
        if not full_text.strip():
            logger.warning(f"èŠ‚ç‚¹ {paper_data.get('entity_name', 'Unknown')} ç¼ºå°‘æ–‡æœ¬å†…å®¹")
            return challenges
        
        # è¯†åˆ«é¢†åŸŸå’ŒèƒŒæ™¯
        domain = self._identify_domain(full_text)
        research_context = self._extract_research_context(paper_data, rq_data)
        
        # 1. æå–æ˜ç¡®æåˆ°çš„æŒ‘æˆ˜ï¼ˆå¢å¼ºç‰ˆï¼‰
        explicit_challenges = self._extract_explicit_challenges_enhanced(text_sources, domain, paper_data, rq_data)
        challenges.extend(explicit_challenges)
        
        # 2. ä»é—®é¢˜æè¿°ä¸­æ¨æ–­éšå«æŒ‘æˆ˜ï¼ˆå¢å¼ºç‰ˆï¼‰
        implicit_challenges = self._infer_implicit_challenges_enhanced(text_sources, domain, paper_data, rq_data)
        challenges.extend(implicit_challenges)
        
        # ä¸ºæ¯ä¸ªæŒ‘æˆ˜è¡¥å……ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        for challenge in challenges:
            self._enrich_challenge_context(challenge, paper_data, rq_data)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.extraction_stats["challenges_extracted"] += len(challenges)
        
        logger.debug(f"ä»{entity_type}æå–åˆ° {len(challenges)} ä¸ªå¢å¼ºæŠ€æœ¯æŒ‘æˆ˜")
        return challenges
    
    def _identify_domain(self, text: str) -> str:
        """ç®€åŒ–çš„é¢†åŸŸè¯†åˆ«"""
        return 'general'  # ç»Ÿä¸€è¿”å›é€šç”¨é¢†åŸŸ
    
    def _identify_domain_enhanced(self, text: str, paper_data: Dict) -> str:
        """å¢å¼ºçš„é¢†åŸŸè¯†åˆ« - ç»“åˆè®ºæ–‡å…ƒæ•°æ®"""
        # åŸºç¡€æ–‡æœ¬é¢†åŸŸè¯†åˆ«
        text_domain = self._identify_domain(text)
        
        # åŸºäºä¼šè®®/æœŸåˆŠçš„é¢†åŸŸæ¨æ–­
        conference = paper_data.get('conference', '').upper()
        venue_domain_mapping = {
            'CVPR': 'computer_vision', 'ICCV': 'computer_vision', 'ECCV': 'computer_vision',
            'ACL': 'nlp', 'EMNLP': 'nlp', 'NAACL': 'nlp', 'COLING': 'nlp',
            'ICML': 'general_ml', 'NIPS': 'general_ml', 'ICLR': 'general_ml',
            'SIGIR': 'retrieval', 'WWW': 'retrieval', 'CIKM': 'retrieval'
        }
        
        for venue_key, domain in venue_domain_mapping.items():
            if venue_key in conference:
                return domain
        
        return text_domain
    
    def _extract_research_context(self, paper_data: Dict, rq_data: Optional[Dict]) -> str:
        """ğŸ†• æå–ç ”ç©¶èƒŒæ™¯ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # è®ºæ–‡èƒŒæ™¯
        if paper_data:
            authors = paper_data.get('authors', [])
            if isinstance(authors, list) and authors:
                context_parts.append(f"ä½œè€…å›¢é˜Ÿ: {', '.join(authors[:3])}")
            
            conference = paper_data.get('conference', '')
            year = paper_data.get('year', '')
            if conference and year:
                context_parts.append(f"å‘è¡¨äº: {conference} {year}")
        
        # ç ”ç©¶é—®é¢˜èƒŒæ™¯
        if rq_data:
            rq_text = rq_data.get('research_question', '')
            if rq_text:
                context_parts.append(f"æ ¸å¿ƒç ”ç©¶é—®é¢˜: {rq_text[:100]}...")
        
        return ' | '.join(context_parts)
    
    def _extract_explicit_challenges_enhanced(self, text_sources: Dict[str, str], domain: str, 
                                            paper_data: Dict, rq_data: Optional[Dict]) -> List[TechnicalChallenge]:
        """ğŸ†• å¢å¼ºç‰ˆæ˜ç¡®æŒ‘æˆ˜æå–"""
        challenges = []
        
        for source_type, text in text_sources.items():
            if not text:
                continue
                
            for pattern in self.challenge_patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    challenge_desc = match.group(1)
                    if challenge_desc and isinstance(challenge_desc, str):
                        challenge_desc = challenge_desc.strip()
                        if len(challenge_desc) > 10:
                            # æå–çº¦æŸæ¡ä»¶
                            constraints = self._extract_constraints_from_challenge(challenge_desc)
                            
                            # æå–æœŸæœ›ç‰¹æ€§
                            desired_props = self._extract_desired_properties(challenge_desc)
                            
                            # è¯„ä¼°æŒ‘æˆ˜éš¾åº¦
                            difficulty = self._assess_challenge_difficulty(challenge_desc, paper_data)
                            
                            challenge = TechnicalChallenge(
                                name=f"{domain} Challenge: {challenge_desc[:30]}...",
                                domain=domain,
                                description=challenge_desc,
                                constraints=constraints,
                                desired_properties=desired_props,
                                source_paper_info=paper_data,
                                source_rq_info=rq_data or {},
                                challenge_category=self._categorize_challenge(challenge_desc),
                                difficulty_level=difficulty,
                                research_context=self._extract_research_context(paper_data, rq_data)
                        )
                        challenges.append(challenge)
        
        return challenges
    
    def _infer_implicit_challenges_enhanced(self, text_sources: Dict[str, str], domain: str,
                                          paper_data: Dict, rq_data: Optional[Dict]) -> List[TechnicalChallenge]:
        """ğŸ†• å¢å¼ºç‰ˆéšå«æŒ‘æˆ˜æ¨æ–­"""
        challenges = []
        
        # åŸºäºé¢†åŸŸçš„å¸¸è§æŒ‘æˆ˜æ¨¡å¼ï¼ˆæ‰©å±•ç‰ˆï¼‰
        domain_challenge_patterns = {
            'domain_adaptation': [
                'domain shift', 'distribution mismatch', 'transferability', 
                'domain gap', 'source-target alignment', 'cross-domain generalization'
            ],
            'multimodal': [
                'modality gap', 'alignment', 'fusion', 'correspondence',
                'cross-modal understanding', 'semantic consistency'
            ],
            'computer_vision': [
                'occlusion', 'illumination variation', 'scale variation',
                'viewpoint changes', 'background clutter', 'object detection accuracy'
            ],
            'nlp': [
                'semantic ambiguity', 'context understanding', 'long-range dependencies',
                'out-of-vocabulary words', 'syntactic complexity'
            ]
        }
        
        full_text = ' '.join(filter(None, text_sources.values())).lower()
        
        if domain in domain_challenge_patterns:
            for challenge_keyword in domain_challenge_patterns[domain]:
                if challenge_keyword.lower() in full_text:
                    challenge = TechnicalChallenge(
                        name=f"{domain}: {challenge_keyword}",
                        domain=domain,
                        description=f"Implicit challenge related to {challenge_keyword} in {domain}",
                        constraints=[],
                        desired_properties=[],
                        source_paper_info=paper_data,
                        source_rq_info=rq_data or {},
                        challenge_category='implicit',
                        difficulty_level='medium',
                        research_context=self._extract_research_context(paper_data, rq_data)
                    )
                    challenges.append(challenge)
        
        return challenges
    
    def _assess_challenge_difficulty(self, challenge_desc: str, paper_data: Dict) -> str:
        """ğŸ†• è¯„ä¼°æŒ‘æˆ˜éš¾åº¦"""
        difficulty_indicators = {
            'high': ['novel', 'unprecedented', 'unsolved', 'fundamental', 'complex', 'challenging'],
            'medium': ['difficult', 'non-trivial', 'significant', 'important', 'substantial'],
            'low': ['simple', 'straightforward', 'basic', 'minor', 'incremental']
        }
        
        challenge_lower = challenge_desc.lower()
        
        for level, indicators in difficulty_indicators.items():
            for indicator in indicators:
                if indicator in challenge_lower:
                    return level
        
        # åŸºäºå‘è¡¨ä¼šè®®æ¨æ–­éš¾åº¦
        conference = paper_data.get('conference', '').upper()
        top_venues = ['NIPS', 'ICML', 'ICLR', 'CVPR', 'ICCV', 'ECCV', 'ACL', 'EMNLP']
        if any(venue in conference for venue in top_venues):
            return 'high'
        
        return 'medium'
    
    def _categorize_challenge(self, challenge_desc: str) -> str:
        """ğŸ†• æŒ‘æˆ˜åˆ†ç±»"""
        categories = {
            'data': ['data', 'dataset', 'annotation', 'labeling', 'collection'],
            'model': ['model', 'architecture', 'network', 'representation'],
            'optimization': ['training', 'optimization', 'convergence', 'gradient'],
            'evaluation': ['evaluation', 'metric', 'benchmark', 'assessment'],
            'scalability': ['scalability', 'efficiency', 'speed', 'memory'],
            'generalization': ['generalization', 'robustness', 'transfer', 'adaptation']
        }
        
        challenge_lower = challenge_desc.lower()
        
        for category, keywords in categories.items():
            if any(keyword in challenge_lower for keyword in keywords):
                return category
        
        return 'general'
    
    def _enrich_challenge_context(self, challenge: TechnicalChallenge, paper_data: Dict, rq_data: Optional[Dict]):
        """ğŸ†• ä¸°å¯ŒæŒ‘æˆ˜ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # è®¾ç½®æ¥æºä¿¡æ¯
        challenge.source_paper_info = paper_data
        challenge.source_rq_info = rq_data or {}
        
        # æå–ç°æœ‰æ–¹æ³•
        if paper_data:
            abstract = paper_data.get('abstract', '')
            challenge.existing_approaches = self._extract_existing_approaches(abstract)
            
            # è®¾ç½®è¯„ä¼°æ ‡å‡†
            challenge.evaluation_criteria = self._extract_evaluation_criteria(abstract)
            
            # è®¾ç½®ç ”ç©¶åŠ¨æœº
            challenge.research_motivation = self._extract_research_motivation(paper_data)
    
    def _extract_existing_approaches(self, text: str) -> List[str]:
        """ğŸ†• æå–ç°æœ‰æ–¹æ³•"""
        approaches = []
        approach_patterns = [
            r'existing\s+(?:methods?|approaches?)\s+([^.]+)',
            r'previous\s+(?:work|studies?)\s+([^.]+)',
            r'traditional\s+(?:methods?|approaches?)\s+([^.]+)',
            r'current\s+(?:methods?|approaches?)\s+([^.]+)'
        ]
        
        for pattern in approach_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                approach = match.group(1)
                if approach and isinstance(approach, str):
                    approach = approach.strip()
                    if len(approach) > 10 and len(approach) < 100:
                        approaches.append(approach)
        
        return approaches[:3]  # é™åˆ¶æ•°é‡
    
    def _extract_evaluation_criteria(self, text: str) -> List[str]:
        """ğŸ†• æå–è¯„ä¼°æ ‡å‡†"""
        criteria = []
        criteria_patterns = [
            r'evaluat(?:e|ed|ion)\s+(?:using|with|on)\s+([^.]+)',
            r'measur(?:e|ed)\s+(?:using|with|by)\s+([^.]+)',
            r'assess(?:ed)?\s+(?:using|with|by)\s+([^.]+)'
        ]
        
        for pattern in criteria_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                criterion = match.group(1)
                if criterion and isinstance(criterion, str):
                    criterion = criterion.strip()
                    if len(criterion) > 5 and len(criterion) < 50:
                        criteria.append(criterion)
        
        return criteria[:3]  # é™åˆ¶æ•°é‡
    
    def _extract_research_motivation(self, paper_data: Dict) -> str:
        """ğŸ†• æå–ç ”ç©¶åŠ¨æœº"""
        motivation_sources = [
            paper_data.get('core_problem', ''),
            paper_data.get('basic_problem', ''),
            paper_data.get('preliminary_innovation_analysis', '')
        ]
        
        motivation_text = ' '.join(filter(None, motivation_sources))
        
        # æŸ¥æ‰¾åŠ¨æœºç›¸å…³çš„å¥å­
        motivation_patterns = [
            r'motivat(?:ed|ion)\s+by\s+([^.]+)',
            r'inspired\s+by\s+([^.]+)',
            r'aim(?:s|ed)?\s+to\s+([^.]+)',
            r'goal\s+(?:is|was)\s+to\s+([^.]+)'
        ]
        
        for pattern in motivation_patterns:
            match = re.search(pattern, motivation_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return motivation_text[:200] + '...' if len(motivation_text) > 200 else motivation_text
    
    def _extract_explicit_challenges(self, text: str, domain: str) -> List[TechnicalChallenge]:
        """æå–æ˜ç¡®æåˆ°çš„æŠ€æœ¯æŒ‘æˆ˜"""
        challenges = []
        
        for pattern in self.challenge_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                challenge_desc = match.group(1)
                if challenge_desc and isinstance(challenge_desc, str):
                    challenge_desc = challenge_desc.strip()
                    if len(challenge_desc) > 10:
                        # æå–çº¦æŸæ¡ä»¶
                        constraints = self._extract_constraints_from_challenge(challenge_desc)
                        
                        # æå–æœŸæœ›ç‰¹æ€§
                        desired_props = self._extract_desired_properties(challenge_desc)
                        
                        challenge = TechnicalChallenge(
                            name=f"{domain} Challenge: {challenge_desc[:30]}...",
                            domain=domain,
                            description=challenge_desc,
                            constraints=constraints,
                            desired_properties=desired_props
                        )
                        challenges.append(challenge)
        
        return challenges
    
    def _infer_implicit_challenges(self, text: str, domain: str) -> List[TechnicalChallenge]:
        """ä»é—®é¢˜æè¿°ä¸­æ¨æ–­éšå«çš„æŠ€æœ¯æŒ‘æˆ˜"""
        challenges = []
        
        # åŸºäºé¢†åŸŸçš„å¸¸è§æŒ‘æˆ˜æ¨¡å¼
        domain_challenge_patterns = {
            'domain_adaptation': [
                'domain shift', 'distribution mismatch', 'transferability', 
                'domain gap', 'source-target alignment'
            ],
            'multimodal': [
                'modality gap', 'alignment', 'fusion', 'correspondence',
                'cross-modal understanding'
            ],
            'continual_learning': [
                'catastrophic forgetting', 'plasticity-stability', 
                'memory efficiency', 'task interference'
            ],
            'time_series': [
                'temporal dependency', 'drift', 'non-stationarity',
                'sequence modeling', 'long-term dependency'
            ]
        }
        
        if domain in domain_challenge_patterns:
            for challenge_keyword in domain_challenge_patterns[domain]:
                if challenge_keyword.lower() in text.lower():
                    challenge = TechnicalChallenge(
                        name=f"{domain}: {challenge_keyword}",
                        domain=domain,
                        description=f"Implicit challenge related to {challenge_keyword}",
                        constraints=[],
                        desired_properties=[]
                    )
                    challenges.append(challenge)
        
        return challenges
    
    def _extract_constraints_from_challenge(self, challenge_desc: str) -> List[str]:
        """ä»æŒ‘æˆ˜æè¿°ä¸­æå–çº¦æŸæ¡ä»¶"""
        constraints = []
        
        constraint_indicators = [
            'must', 'should', 'cannot', 'limited', 'restricted',
            'without', 'only', 'require', 'need'
        ]
        
        sentences = challenge_desc.split('.')
        for sentence in sentences:
            for indicator in constraint_indicators:
                if indicator in sentence.lower():
                    constraints.append(sentence.strip())
                    break
        
        return constraints
    
    def _extract_desired_properties(self, challenge_desc: str) -> List[str]:
        """ä»æŒ‘æˆ˜æè¿°ä¸­æå–æœŸæœ›çš„è§£å†³æ–¹æ¡ˆç‰¹æ€§"""
        properties = []
        
        property_indicators = [
            'robust', 'efficient', 'accurate', 'fast', 'scalable',
            'generalizable', 'transferable', 'adaptive', 'stable'
        ]
        
        for prop in property_indicators:
            if prop in challenge_desc.lower():
                properties.append(prop)
        
        return properties

    def find_enhanced_transfer_opportunities(self, 
                                           components: List[TechnicalComponent],
                                           challenges: List[TechnicalChallenge]) -> List[TransferOpportunity]:
        """
        å‘ç°æŠ€æœ¯ç»„ä»¶åˆ°æŒ‘æˆ˜çš„è¿ç§»æœºä¼š
        
        Args:
            components: æŠ€æœ¯ç»„ä»¶åˆ—è¡¨
            challenges: æŠ€æœ¯æŒ‘æˆ˜åˆ—è¡¨
            
        Returns:
            è¿ç§»æœºä¼šåˆ—è¡¨
        """
        opportunities = []
        
        for component in components:
            for challenge in challenges:
                # è®¡ç®—è¿ç§»å¯è¡Œæ€§
                feasibility = self._calculate_transfer_feasibility(component, challenge)
                
                if feasibility > 0.3:  # å¯è¡Œæ€§é˜ˆå€¼
                    # ç”Ÿæˆæ”¹é€ è¯æ®
                    adaptation_evidence = self._generate_adaptation_evidence(component, challenge)
                    
                    # ç”Ÿæˆæ”¹é€ æœºåˆ¶
                    adaptation_mechanism = self._generate_adaptation_mechanism(component, challenge)
                    
                    # ç”ŸæˆéªŒè¯å»ºè®®
                    validation_suggestion = self._generate_validation_suggestion(component, challenge)
                    
                    opportunity = TransferOpportunity(
                        source_component=component,
                        target_challenge=challenge,
                        adaptation_evidence=adaptation_evidence,
                        adaptation_mechanism=adaptation_mechanism,
                        validation_suggestion=validation_suggestion,
                        transfer_feasibility=feasibility
                    )
                    opportunities.append(opportunity)
        
        # æŒ‰å¯è¡Œæ€§æ’åº
        opportunities.sort(key=lambda x: x.transfer_feasibility, reverse=True)
        
        logger.debug(f"å‘ç° {len(opportunities)} ä¸ªè¿ç§»æœºä¼š")
        return opportunities
    
    def _calculate_transfer_feasibility(self, 
                                      component: TechnicalComponent, 
                                      challenge: TechnicalChallenge) -> float:
        """è®¡ç®—è¿ç§»å¯è¡Œæ€§"""
        feasibility = 0.0
        
        # 1. ç±»å‹åŒ¹é…åº¦
        type_compatibility = self._get_type_challenge_compatibility(component.type, challenge.domain)
        feasibility += 0.4 * type_compatibility
        
        # 2. è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_sim = self._calculate_text_similarity(
            component.description, challenge.description
        )
        feasibility += 0.3 * semantic_sim
        
        # 3. å‚æ•°é€‚é…æ€§
        param_adaptability = self._assess_parameter_adaptability(component, challenge)
        feasibility += 0.2 * param_adaptability
        
        # 4. çº¦æŸå…¼å®¹æ€§
        constraint_compatibility = self._assess_constraint_compatibility(component, challenge)
        feasibility += 0.1 * constraint_compatibility
        
        return min(1.0, feasibility)
    
    def _get_type_challenge_compatibility(self, component_type: str, challenge_domain: str) -> float:
        """è·å–ç»„ä»¶ç±»å‹ä¸æŒ‘æˆ˜é¢†åŸŸçš„å…¼å®¹æ€§"""
        compatibility_matrix = {
            # æŸå¤±å‡½æ•°ç›¸å…³
            ('loss_function', 'domain_adaptation'): 0.9,
            ('loss_function', 'continual_learning'): 0.8,
            ('loss_function', 'multimodal'): 0.7,
            ('loss_function', 'time_series'): 0.6,
            
            # æ³¨æ„åŠ›æœºåˆ¶ç›¸å…³
            ('attention_mechanism', 'multimodal'): 0.9,
            ('attention_mechanism', 'nlp'): 0.9,
            ('attention_mechanism', 'computer_vision'): 0.8,
            ('attention_mechanism', 'time_series'): 0.7,
            
            # å¯¹æ¯”å­¦ä¹ ç›¸å…³
            ('contrastive_learning', 'domain_adaptation'): 0.9,
            ('contrastive_learning', 'multimodal'): 0.8,
            ('contrastive_learning', 'retrieval'): 0.9,
            
            # å…¶ä»–æ ¸å¿ƒç»„ä»¶
            ('embedding', 'multimodal'): 0.8,
            ('embedding', 'nlp'): 0.9,
            ('embedding', 'graph'): 0.8,
            ('regularization', 'domain_adaptation'): 0.7,
            ('regularization', 'continual_learning'): 0.8,
            ('gradient_method', 'optimization'): 0.9,
        }
        
        return compatibility_matrix.get((component_type, challenge_domain), 0.5)
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å åº¦
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _assess_parameter_adaptability(self, 
                                     component: TechnicalComponent, 
                                     challenge: TechnicalChallenge) -> float:
        """è¯„ä¼°å‚æ•°çš„é€‚é…æ€§"""
        if not component.parameters:
            return 0.5  # æ²¡æœ‰å‚æ•°æ—¶è¿”å›ä¸­ç­‰é€‚é…æ€§
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦å¯ä»¥é€‚é…åˆ°ç›®æ ‡é¢†åŸŸ
        adaptable_params = 0
        total_params = len(component.parameters)
        
        for param in component.parameters:
            # ç®€å•å¯å‘å¼ï¼šé•¿å‚æ•°åé€šå¸¸æ›´å…·ä½“ï¼Œé€‚é…æ€§è¾ƒä½
            if isinstance(param, str) and len(param) <= 10:
                adaptable_params += 1
        
        return adaptable_params / total_params if total_params > 0 else 0.5
    
    def _assess_constraint_compatibility(self, 
                                       component: TechnicalComponent, 
                                       challenge: TechnicalChallenge) -> float:
        """è¯„ä¼°çº¦æŸå…¼å®¹æ€§"""
        if not challenge.constraints:
            return 1.0  # æ²¡æœ‰çº¦æŸæ—¶å®Œå…¨å…¼å®¹
        
        # æ£€æŸ¥ç»„ä»¶æ˜¯å¦èƒ½æ»¡è¶³æŒ‘æˆ˜çš„çº¦æŸ
        compatibility_score = 1.0
        
        for constraint in challenge.constraints:
            # ç®€å•å¯å‘å¼ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å†²çªçš„å…³é”®è¯
            conflict_keywords = ['cannot', 'must not', 'impossible', 'limited']
            for keyword in conflict_keywords:
                if keyword in constraint.lower() and keyword in component.description.lower():
                    compatibility_score *= 0.7  # é™ä½å…¼å®¹æ€§
        
        return compatibility_score
    
    def _generate_adaptation_evidence(self, 
                                    component: TechnicalComponent, 
                                    challenge: TechnicalChallenge) -> str:
        """ç”Ÿæˆæ”¹é€ è¯æ®"""
        evidence_parts = []
        
        # 1. æ ¸å¿ƒæœºåˆ¶è¯´æ˜
        evidence_parts.append(f"æºæ–¹æ³• '{component.name}' çš„æ ¸å¿ƒæœºåˆ¶æ˜¯ {component.description[:100]}...")
        
        # 2. ç›®æ ‡æŒ‘æˆ˜åˆ†æ
        evidence_parts.append(f"ç›®æ ‡æŒ‘æˆ˜ '{challenge.name}' éœ€è¦è§£å†³ {challenge.description[:100]}...")
        
        # 3. è¿ç§»ç†ç”±
        if component.type == 'loss_function' and 'adaptation' in challenge.domain:
            evidence_parts.append("æŸå¤±å‡½æ•°ç±»å‹çš„ç»„ä»¶é€šå¸¸å…·æœ‰è‰¯å¥½çš„é¢†åŸŸé€‚é…èƒ½åŠ›ã€‚")
        elif component.type == 'attention_mechanism' and challenge.domain in ['multimodal', 'nlp']:
            evidence_parts.append("æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šæ¨¡æ€å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å…·æœ‰é€šç”¨æ€§ã€‚")
        else:
            evidence_parts.append(f"{component.type} ç±»å‹çš„ç»„ä»¶å¯ä»¥é€šè¿‡é€‚å½“ä¿®æ”¹åº”ç”¨äº {challenge.domain} é¢†åŸŸã€‚")
        
        return " ".join(evidence_parts)
    
    def _generate_adaptation_mechanism(self, 
                                     component: TechnicalComponent, 
                                     challenge: TechnicalChallenge) -> str:
        """ç”Ÿæˆå…·ä½“çš„æ”¹é€ æœºåˆ¶"""
        mechanisms = []
        
        # åŸºäºç»„ä»¶ç±»å‹ç”Ÿæˆå…·ä½“çš„æ”¹é€ å»ºè®®
        if component.type == 'loss_function':
            if 'domain' in challenge.domain:
                mechanisms.append(f"å°† {component.name} æ‰©å±•ä¸ºé¢†åŸŸè‡ªé€‚åº”ç‰ˆæœ¬ï¼Œæ·»åŠ åŸŸåˆ¤åˆ«å™¨çº¦æŸ")
            elif 'temporal' in challenge.description.lower():
                mechanisms.append(f"åœ¨ {component.name} ä¸­å¼•å…¥æ—¶åºä¸€è‡´æ€§çº¦æŸï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶")
            else:
                mechanisms.append(f"ä¿®æ”¹ {component.name} çš„æ­£è´Ÿæ ·æœ¬å®šä¹‰ï¼Œé€‚åº”ç›®æ ‡ä»»åŠ¡çš„ç‰¹æ®Šéœ€æ±‚")
        
        elif component.type == 'attention_mechanism':
            if 'cross-modal' in challenge.description.lower():
                mechanisms.append(f"å°† {component.name} æ‰©å±•ä¸ºè·¨æ¨¡æ€æ³¨æ„åŠ›ï¼Œå¤„ç†ä¸åŒæ¨¡æ€é—´çš„å¯¹é½")
            else:
                mechanisms.append(f"è°ƒæ•´ {component.name} çš„æ³¨æ„åŠ›æƒé‡è®¡ç®—ï¼Œå¢åŠ å¯¹ç›®æ ‡é¢†åŸŸç‰¹å¾çš„æ•æ„Ÿæ€§")
        
        elif component.type == 'constraint':
            mechanisms.append(f"å°†çº¦æŸæ¡ä»¶ '{component.name}' é‡æ–°è¡¨è¿°ä¸ºé€‚ç”¨äº {challenge.domain} çš„å½¢å¼")
        
            mechanisms.append(f"é€šè¿‡å‚æ•°è°ƒæ•´å’Œç»“æ„ä¿®æ”¹ï¼Œä½¿ {component.name} é€‚åº” {challenge.domain} çš„éœ€æ±‚")
        
        return " ".join(mechanisms) if mechanisms else f"é€šè¿‡é¢†åŸŸç‰¹åŒ–æ”¹é€  {component.name}"
    
    def _generate_validation_suggestion(self, 
                                      component: TechnicalComponent, 
                                      challenge: TechnicalChallenge) -> str:
        """ç”ŸæˆéªŒè¯å»ºè®®"""
        suggestions = []
        
        # 1. åŸºå‡†å¯¹æ¯”
        suggestions.append(f"åœ¨ {challenge.domain} æ ‡å‡†æ•°æ®é›†ä¸Šå¯¹æ¯”æ”¹é€ åçš„ {component.name} ä¸åŸå§‹ç‰ˆæœ¬çš„æ€§èƒ½")
        
        # 2. æ¶ˆèå®éªŒ
        if component.parameters:
            suggestions.append(f"è¿›è¡Œæ¶ˆèå®éªŒï¼ŒéªŒè¯ {component.name} ä¸­å„å‚æ•°åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„è´¡çŒ®")
        
        # 3. ç‰¹å®šæŒ‡æ ‡éªŒè¯
        if 'adaptation' in challenge.domain:
            suggestions.append("ä½¿ç”¨åŸŸé€‚é…ç‰¹å®šæŒ‡æ ‡ï¼ˆå¦‚A-distanceã€MMDï¼‰éªŒè¯æ”¹é€ æ•ˆæœ")
        elif 'temporal' in challenge.description.lower():
            suggestions.append("è¯„ä¼°æ”¹é€ æ–¹æ³•åœ¨æ—¶åºä¸€è‡´æ€§å’Œé•¿æœŸä¾èµ–å»ºæ¨¡ä¸Šçš„è¡¨ç°")
        elif 'multimodal' in challenge.domain:
            suggestions.append("åœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œå¯¹é½ä»»åŠ¡ä¸ŠéªŒè¯æ”¹é€ æ–¹æ³•çš„æœ‰æ•ˆæ€§")
        
        return " ".join(suggestions)

    def generate_enhanced_cross_paper_links(self, opportunities: List[TransferOpportunity]) -> List[Dict]:
        """ğŸ†• ç”Ÿæˆå¢å¼ºçš„è·¨è®ºæ–‡å…³è”è¾¹"""
        links = []
        
        for opportunity in opportunities:
            if opportunity.transfer_feasibility > 0.5:  # é«˜è´¨é‡é˜ˆå€¼
                # ğŸ”§ ä½¿ç”¨çŸ¥è¯†å›¾è°±ä¸°å¯Œä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„è¾¹æè¿°
                source_paper = opportunity.source_component.source_paper_info
                target_paper = opportunity.target_challenge.source_paper_info
                
                link = {
                    'source_entity': opportunity.source_component.source_context,
                    'target_entity': target_paper.get('entity_name', ''),
                    'relationship_type': 'technical_transfer',
                    'evidence': self._generate_enhanced_evidence(opportunity),
                    'mechanism': opportunity.adaptation_mechanism,
                    'validation': opportunity.validation_suggestion,
                    'feasibility_score': opportunity.transfer_feasibility,
                    'source_paper_info': {
                        'title': source_paper.get('title', ''),
                        'authors': source_paper.get('authors', []),
                        'conference': source_paper.get('conference', ''),
                        'year': source_paper.get('year', '')
                    },
                    'target_paper_info': {
                        'title': target_paper.get('title', ''),
                        'authors': target_paper.get('authors', []),
                        'conference': target_paper.get('conference', ''),
                        'year': target_paper.get('year', '')
                    },
                    'technical_details': {
                        'component_type': opportunity.source_component.type,
                        'challenge_domain': opportunity.target_challenge.domain,
                        'complexity_level': getattr(opportunity.source_component, 'complexity_level', 'medium'),
                        'difficulty_level': getattr(opportunity.target_challenge, 'difficulty_level', 'medium')
                    }
                }
                links.append(link)
        
        logger.info(f"ç”Ÿæˆ {len(links)} ä¸ªé«˜è´¨é‡è·¨è®ºæ–‡å…³è”è¾¹")
        return links
    
    def _generate_enhanced_evidence(self, opportunity: TransferOpportunity) -> str:
        """ğŸ†• ç”Ÿæˆå¢å¼ºçš„æŠ€æœ¯è¯æ®"""
        component = opportunity.source_component
        challenge = opportunity.target_challenge
        
        # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆè¯æ®
        template_type = self._select_evidence_template(component, challenge)
        template = self.evidence_templates[template_type][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿
        
        # å¡«å……æ¨¡æ¿å˜é‡
        evidence = template.format(
            source_paper=component.source_paper_info.get('title', 'Unknown Paper')[:50],
            source_method=component.name,
            component_type=component.type,
            adaptation=f"{component.type} adaptation",
            target_paper=challenge.source_paper_info.get('title', 'Unknown Paper')[:50],
            target_challenge=challenge.name[:30],
            source_conference=component.source_paper_info.get('conference', 'Unknown'),
            core_technique=component.type.replace('_', ' '),
            target_domain=challenge.domain,
            specific_challenge=challenge.description[:50]
        )
        
        return evidence
    
    def _select_evidence_template(self, component: TechnicalComponent, challenge: TechnicalChallenge) -> str:
        """ğŸ†• é€‰æ‹©åˆé€‚çš„è¯æ®æ¨¡æ¿"""
        if component.type in ['loss_function', 'algorithm', 'constraint']:
            return 'technical_transfer'
        elif 'architecture' in component.type or 'network' in component.type:
            return 'architectural_adaptation'
        else:
            return 'methodological_insight'

    def get_extraction_statistics(self) -> Dict:
        """ğŸ†• è·å–æå–ç»Ÿè®¡ä¿¡æ¯"""
        return self.extraction_stats.copy()
    
    def reset_statistics(self):
        """ğŸ†• é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        for key in self.extraction_stats:
            self.extraction_stats[key] = 0

    def find_transfer_opportunities(self, components: List[TechnicalComponent], 
                                  challenges: List[TechnicalChallenge]) -> List[TransferOpportunity]:
        """ğŸ”„ å…¼å®¹æ€§æ–¹æ³•ï¼šè°ƒç”¨å¢å¼ºçš„è¿ç§»æœºä¼šå‘ç°æ–¹æ³•"""
        return self.find_enhanced_transfer_opportunities(components, challenges)

    def generate_enhanced_edge_description(self, 
                                         opportunity: TransferOpportunity) -> str:
        """ç”Ÿæˆå¢å¼ºçš„è¾¹æè¿°ï¼ŒåŒ…å«å…·ä½“çš„æŠ€æœ¯è¯æ®"""
        component = opportunity.source_component
        challenge = opportunity.target_challenge
        
        description = (
            f"æŠ€æœ¯è¿ç§»æœºä¼šï¼š{component.name} ({component.type}) â†’ {challenge.name} "
            f"(å¯è¡Œæ€§: {opportunity.transfer_feasibility:.3f}). "
            f"æ”¹é€ æœºåˆ¶: {opportunity.adaptation_mechanism[:100]}..."
        )
        
        return description

    async def generate_llm_based_evidence(self, opportunity: TransferOpportunity) -> str:
        """ğŸ†• ä½¿ç”¨LLMåŸºäºçœŸå®èŠ‚ç‚¹å±æ€§ç”ŸæˆæŠ€æœ¯è¯æ®"""
        component = opportunity.source_component
        challenge = opportunity.target_challenge
        
        # æ„å»ºLLMåˆ†ææç¤º
        prompt = f"""
åŸºäºä»¥ä¸‹çœŸå®çš„æŠ€æœ¯ä¿¡æ¯ï¼Œåˆ†ææŠ€æœ¯è¿ç§»çš„å¯è¡Œæ€§å’Œå…·ä½“æœºåˆ¶ï¼š

æºæŠ€æœ¯ç»„ä»¶ï¼š
- åç§°: {component.name}
- ç±»å‹: {component.type}
- æè¿°: {component.description}
- æ¥æºè®ºæ–‡: {component.source_paper_info.get('title', '')}
- æŠ€æœ¯é¢†åŸŸ: {component.technical_domain}

ç›®æ ‡æŠ€æœ¯æŒ‘æˆ˜ï¼š
- æŒ‘æˆ˜: {challenge.name}
- é¢†åŸŸ: {challenge.domain}
- æè¿°: {challenge.description}
- æ¥æºè®ºæ–‡: {challenge.source_paper_info.get('title', '')}

è¯·åˆ†æï¼š
1. è¿™ä¸¤ä¸ªæŠ€æœ¯ä¹‹é—´çš„å…·ä½“å…³è”æ€§
2. æŠ€æœ¯è¿ç§»çš„å¯è¡Œæœºåˆ¶
3. é¢„æœŸçš„æ”¹è¿›æ•ˆæœ
4. å®æ–½çš„å…·ä½“æ­¥éª¤

è¦æ±‚ï¼šåŸºäºçœŸå®æŠ€æœ¯å†…å®¹ï¼Œé¿å…æ¨¡æ¿åŒ–æè¿°ï¼Œæä¾›å…·ä½“å¯æ“ä½œçš„åˆ†æã€‚
"""

        # ç›´æ¥è¿”å›ç®€åŒ–çš„æŠ€æœ¯è¿ç§»æè¿°ï¼Œä¸ä½¿ç”¨LLM
        return f"æŠ€æœ¯è¿ç§»ï¼š{component.name} â†’ {challenge.name} (å¯è¡Œæ€§: {opportunity.transfer_feasibility:.3f})"

    def _infer_technical_domain(self, description: str) -> str:
        """ç®€åŒ–çš„æŠ€æœ¯é¢†åŸŸæ¨æ–­"""
        return 'general'  # ç»Ÿä¸€è¿”å›é€šç”¨é¢†åŸŸ

    def _extract_parameters_from_description(self, description: str) -> List[str]:
        """ğŸ†• ä»æè¿°ä¸­æå–å‚æ•°"""
        parameters = []
        
        # æŸ¥æ‰¾å¸¸è§å‚æ•°æ¨¡å¼
        param_patterns = [
            r'temperature\s+parameter\s+\(([^)]+)\)',
            r'parameter\s+([Î±-Ï‰Ï„Î»Î¼Ïƒ]\w*)',
            r'threshold\s+([0-9.]+)',
            r'learning\s+rate\s+([0-9.e-]+)',
            r'batch\s+size\s+([0-9]+)',
            r'top-k\s+([0-9]+)',
            r'dimension\s+([0-9]+)'
        ]
        
        for pattern in param_patterns:
            matches = re.findall(pattern, description, re.IGNORECASE)
            parameters.extend(matches)
        
        return parameters[:5]  # é™åˆ¶å‚æ•°æ•°é‡

    def _extract_mathematical_form(self, description: str) -> str:
        """ğŸ†• æå–æ•°å­¦å½¢å¼"""
        # æŸ¥æ‰¾æ•°å­¦è¡¨è¾¾å¼
        math_patterns = [
            r'KL-divergence',
            r'Wasserstein\s+distance',
            r'symmetric\s+KL',
            r'softmax\s+function',
            r'running\s+mean',
            r'cosine\s+similarity',
            r'euclidean\s+distance'
        ]
        
        for pattern in math_patterns:
            if re.search(pattern, description, re.IGNORECASE):
                return pattern.replace('\\s+', ' ')
        
        return ''
