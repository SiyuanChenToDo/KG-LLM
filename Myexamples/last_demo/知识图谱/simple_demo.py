import asyncio
import os
import inspect
import logging
import logging.config
import random
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
from lightrag.kg.shared_storage import initialize_pipeline_status
from build_custom_kg import load_custom_kg
from cross_paper_linking import integrate_cross_paper_linking
from dotenv import load_dotenv
from pathlib import Path
import networkx as nx

# å¯¼å…¥èŠ‚ç‚¹å±æ€§æŸ¥è¯¢API
import sys
import os
# æ·»åŠ æ­£ç¡®çš„è·¯å¾„ä»¥å¯¼å…¥API
current_dir = os.path.dirname(os.path.abspath(__file__))  # å½“å‰æ–‡ä»¶ç›®å½•
parent_dir = os.path.dirname(current_dir)  # Mydemoç›®å½•
sys.path.insert(0, parent_dir)

try:
    from node_attributes_api import (
        get_paper_attributes, 
        get_research_question_attributes, 
        get_solution_attributes,
        find_paper_by_title,
        get_paper_research_questions,
        print_node_attributes
    )
    API_AVAILABLE = True
    print("âœ… èŠ‚ç‚¹å±æ€§APIå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥èŠ‚ç‚¹å±æ€§API: {e}")
    API_AVAILABLE = False

load_dotenv(dotenv_path=".env", override=False)

WORKING_DIR = "./dickens"

#æ—¥å¿—
def configure_logging():
    """Configure logging for the application"""

    # Reset any existing handlers to ensure clean configuration
    for logger_name in ["uvicorn", "uvicorn.access", "uvicorn.error", "lightrag"]:
        logger_instance = logging.getLogger(logger_name)
        logger_instance.handlers = []
        logger_instance.filters = []

    # Get log directory path from environment variable or use current directory
    log_dir = os.getenv("LOG_DIR", os.getcwd())
    log_file_path = os.path.abspath(os.path.join(log_dir, "lightrag_ollama_demo.log"))

    print(f"\nLightRAG compatible demo log file: {log_file_path}\n")
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

    # Get log file max size and backup count from environment variables
    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))  # Default 10MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))  # Default 5 backups

    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(levelname)s: %(message)s",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "console": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                },
                "file": {
                    "formatter": "detailed",
                    "class": "logging.handlers.RotatingFileHandler",
                    "filename": log_file_path,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "encoding": "utf-8",
                },
            },
            "loggers": {
                "lightrag": {
                    "handlers": ["console", "file"],
                    "level": "INFO",
                    "propagate": False,
                },
            },
        }
    )

    # Set the logger level to INFO (å‡å°‘å†—ä½™æ—¥å¿—)
    logger.setLevel(logging.INFO)  # ğŸ”¥ æ”¹å›INFOçº§åˆ«ï¼Œå‡å°‘å†—ä½™è¾“å‡º
    # Enable verbose debug if needed
    set_verbose_debug(os.getenv("VERBOSE_DEBUG", "false").lower() == "true")  # ğŸ”¥ å…³é—­è¯¦ç»†è°ƒè¯•


if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)


async def initialize():
    # æå–é…ç½®å‚æ•°ä»¥ä¾¿è°ƒè¯•
    embed_model = os.getenv("EMBEDDING_MODEL", "bge-m3:latest")  # æ·»åŠ :latestæ ‡
    embed_host = os.getenv("EMBEDDING_BINDING_HOST", "http://localhost:11434")
    llm_model = os.getenv("LLM_MODEL", "qwen3:14b")
    llm_host = os.getenv("LLM_BINDING_HOST", "http://localhost:11434")
    embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))

    # ä½¿ç”¨é»˜è®¤å­˜å‚¨åç«¯
    graph_backend = "json"  # ä½¿ç”¨é»˜è®¤çš„NetworkX/JSONåç«¯
    vector_backend = "json"  # ä½¿ç”¨é»˜è®¤çš„NanoVectorDBåç«¯

    # æ‰“å°é…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
    print("\n===== RAG åˆå§‹åŒ–é…ç½® =====")
    print(f"å·¥ä½œç›®å½•: {WORKING_DIR}")
    print(f"LLMæ¨¡å‹: {llm_model}")
    print(f"LLMæœåŠ¡åœ°å€: {llm_host}")
    print(f"åµŒå…¥æ¨¡å‹: {embed_model}")
    print(f"åµŒå…¥æœåŠ¡åœ°å€: {embed_host}")
    print(f"åµŒå…¥ç»´åº¦: {embedding_dim}")
    print(f"GRAPH_BACKEND: {graph_backend}")
    print(f"VECTOR_BACKEND: {vector_backend}")
    print(f"==========================\n")


    # ä½¿ç”¨é»˜è®¤çš„å­˜å‚¨åç«¯
    graph_storage_name = "NetworkXStorage"  # NetworkX/JSONå­˜å‚¨
    vector_storage_name = "NanoVectorDBStorage"  # é»˜è®¤å‘é‡å­˜å‚¨

    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=ollama_model_complete,
        llm_model_name=llm_model,
        llm_model_max_token_size=8192,
        llm_model_kwargs={
            "host": llm_host,
            "options": {"num_ctx": 8192},
            "timeout": int(os.getenv("TIMEOUT", "600")),  # å¢åŠ è¶…æ—¶æ—¶é—´
        },
        embedding_func=EmbeddingFunc(
            embedding_dim=embedding_dim,
            max_token_size=int(os.getenv("MAX_EMBED_TOKENS", "8192")),
            func=lambda texts: ollama_embed(
                texts,
                embed_model=embed_model,
                host=embed_host,
                timeout=60,  # æ·»åŠ è¶…æ—¶è®¾ç½®
            ),
        ),
        # ğŸ”¥ ç¦ç”¨rerankä»¥é¿å…è­¦å‘Šä¿¡æ¯ï¼ˆå¦‚éœ€å¯ç”¨ï¼Œå‚è€ƒRERANK_é…ç½®æŒ‡å—.mdï¼‰
        rerank_model_func=None,
        # è¦†ç›–åç«¯ï¼ˆå­—ç¬¦ä¸²æ–¹å¼ï¼‰
        graph_storage=graph_storage_name,
        vector_storage=vector_storage_name,
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


async def print_stream(stream):
    async for chunk in stream:
        print(chunk, end="", flush=True)


async def _demonstrate_lightrag_query_only(rag):
    """åªæ¼”ç¤ºLightRAGç¯å¢ƒæŸ¥è¯¢ï¼ˆæ–‡ä»¶è®¿é—®å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
    print(f"\nğŸ”„ æ–¹æ³•2: åœ¨LightRAGç¯å¢ƒä¸­æŸ¥è¯¢ (æœ€å®Œæ•´)")
    print("-" * 50)
    
    try:
        graph = rag.chunk_entity_relation_graph
        all_labels = await graph.get_all_labels()
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªpaperèŠ‚ç‚¹
        paper_nodes = []
        for label in all_labels:
            node_data = await graph.get_node(label)
            if node_data and node_data.get("entity_type") == "paper":
                paper_nodes.append((label, node_data))
                break
        
        if paper_nodes:
            label, node_data = paper_nodes[0]
            print(f"âœ… ä»LightRAGè·å–è®ºæ–‡: {node_data.get('title', 'N/A')}")
            print(f"   èŠ‚ç‚¹ID: {label}")
            print(f"   å®ä½“ç±»å‹: {node_data.get('entity_type', 'N/A')}")
            
            # è·å–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å…³ç³»
            edges = await graph.get_node_edges(label)
            print(f"   å…³è”å…³ç³»æ•°: {len(edges)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªå…³ç³»
            for i, (src, tgt) in enumerate(edges[:3], 1):
                edge_data = await graph.get_edge(src, tgt)
                if edge_data:
                    keywords = edge_data.get("keywords", "æ— å…³é”®è¯")
                    # ç¡®å®šå…³ç³»æ–¹å‘
                    if src == label:
                        print(f"     å…³ç³»{i}: è¯¥è®ºæ–‡ â†’ {tgt[:30]}... (å…³é”®è¯: {keywords})")
                    else:
                        print(f"     å…³ç³»{i}: {src[:30]}... â†’ è¯¥è®ºæ–‡ (å…³é”®è¯: {keywords})")
        
        # ç»Ÿè®¡ä¸åŒç±»å‹èŠ‚ç‚¹çš„è¿æ¥åº¦
        print(f"\nğŸ“Š èŠ‚ç‚¹è¿æ¥ç»Ÿè®¡:")
        node_connections = {"paper": [], "research_question": [], "solution": []}
        
        for label in all_labels[:20]:  # åˆ†æå‰20ä¸ªèŠ‚ç‚¹
            node_data = await graph.get_node(label)
            if node_data:
                entity_type = node_data.get("entity_type", "unknown")
                if entity_type in node_connections:
                    edges = await graph.get_node_edges(label)
                    node_connections[entity_type].append(len(edges))
        
        for node_type, connections in node_connections.items():
            if connections:
                avg_connections = sum(connections) / len(connections)
                print(f"   {node_type}èŠ‚ç‚¹å¹³å‡è¿æ¥æ•°: {avg_connections:.1f}")
    
    except Exception as e:
        print(f"âŒ LightRAGæŸ¥è¯¢å¤±è´¥: {e}")


async def perform_cross_paper_analysis(rag):
    """æ‰§è¡Œè·¨è®ºæ–‡å…³è”åˆ†æ"""
    print("\n" + "="*70)
    print("ğŸ”— è·¨è®ºæ–‡å…³è”åˆ†æ")
    print("="*70)
    
    try:
        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(stage, current, total):
            percentage = (current / total * 100) if total > 0 else 0
            print(f"\rğŸ”„ {stage}: {current}/{total} ({percentage:.1f}%)", end="", flush=True)
        
        # æ‰§è¡Œè·¨è®ºæ–‡å…³è”åˆ†æ
        print("ğŸš€ å¼€å§‹è·¨è®ºæ–‡å…³è”åˆ†æ...")
        
        # ğŸ”§ ä½¿ç”¨åˆç†é˜ˆå€¼ç¡®ä¿é«˜è´¨é‡è·¨è®ºæ–‡è¿æ¥
        threshold = 0.5
        print(f"ğŸ“Š ä½¿ç”¨é˜ˆå€¼: {threshold} ({threshold*100:.0f}%) - ç¡®ä¿é«˜è´¨é‡Solutionâ†’Paperè¿æ¥")
        added_count, new_edges = await integrate_cross_paper_linking(
            rag_instance=rag,
            similarity_threshold=threshold,  # ğŸ”§ ä½¿ç”¨åˆç†é˜ˆå€¼
            progress_callback=progress_callback
        )
        
        print(f"\nâœ… è·¨è®ºæ–‡å…³è”åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å‘ç° {len(new_edges)} ä¸ªè·¨è®ºæ–‡å…³è”")
        print(f"ğŸ“ˆ æˆåŠŸæ·»åŠ  {added_count} æ¡æ–°è¾¹åˆ°çŸ¥è¯†å›¾è°±")
        
        if new_edges:
            print(f"\nğŸ” è·¨è®ºæ–‡å…³è”ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for i, edge in enumerate(new_edges[:5], 1):
                print(f"   {i}. {edge['source'][:30]}... â†” {edge['target'][:30]}...")
                print(f"      ç›¸ä¼¼åº¦: {edge['similarity_score']:.3f}")
                print(f"      æè¿°: {edge['description']}")
                print()
        
        # ä»GraphMLè¯»å–å›¾ç»Ÿè®¡
        try:
            graphml_path = Path(WORKING_DIR) / "graph_chunk_entity_relation.graphml"
            G = nx.read_graphml(str(graphml_path))
            total_nodes = G.number_of_nodes()
            total_edges = G.number_of_edges()
            cross_edges = sum(
                1 for _, _, d in G.edges(data=True)
                if (d.get("edge_type") == "cross_paper") or ("cross_paper" in str(d.get("relationship", "")))
            )
            print(f"ğŸ“Š æ›´æ–°åçš„å›¾ç»Ÿè®¡:")
            print(f"   æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
            print(f"   è·¨è®ºæ–‡è¾¹æ•°: {cross_edges}")
        except Exception as e:
            # å›é€€åˆ°è½»é‡ç»Ÿè®¡
            graph = rag.chunk_entity_relation_graph
            all_labels = await graph.get_all_labels()
            print(f"ğŸ“Š æ›´æ–°åçš„å›¾ç»Ÿè®¡:")
            print(f"   æ€»èŠ‚ç‚¹æ•°: {len(all_labels)}")
            print(f"   æ³¨æ„: GraphMLæ–‡ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡è¾¹æ•°ç»Ÿè®¡")
        
    except Exception as e:
        print(f"\nâŒ è·¨è®ºæ–‡å…³è”åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def demonstrate_node_query_methods(rag):
    """æ¼”ç¤ºä¸‰ç§èŠ‚ç‚¹å±æ€§æŸ¥è¯¢æ–¹æ³•çš„åŒºåˆ«"""
    print("\n" + "="*70)
    print("ğŸš€ æ¼”ç¤ºä¸‰ç§èŠ‚ç‚¹å±æ€§æŸ¥è¯¢æ–¹æ³•")
    print("="*70)
    
    if not API_AVAILABLE:
        print("âŒ èŠ‚ç‚¹å±æ€§APIä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # ğŸ”§ æ£€æŸ¥çŸ¥è¯†å›¾è°±æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    try:
        from node_attributes_api import get_kg_file_path
        kg_file_path = get_kg_file_path()
        print(f"âœ… æ‰¾åˆ°çŸ¥è¯†å›¾è°±æ–‡ä»¶: {kg_file_path}")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è®¿é—®çŸ¥è¯†å›¾è°±æ–‡ä»¶: {e}")
        print("ğŸ“‹ å°†åªæ¼”ç¤ºæ–¹æ³•2ï¼ˆLightRAGç¯å¢ƒæŸ¥è¯¢ï¼‰")
        # åªæ¼”ç¤ºLightRAGæŸ¥è¯¢ï¼Œç„¶åè¿”å›
        await _demonstrate_lightrag_query_only(rag)
        return
    
    # ===================
    # æ–¹æ³•1: ç›´æ¥ä»JSONæ–‡ä»¶æŸ¥è¯¢ (æœ€å¿«é€Ÿ)
    # ===================
    print("\nğŸ“ æ–¹æ³•1: ç›´æ¥ä»JSONæ–‡ä»¶æŸ¥è¯¢ (æœ€å¿«é€Ÿ)")
    print("-" * 50)
    
    # è·å–ç¬¬ä¸€ç¯‡è®ºæ–‡çš„å±æ€§
    try:
        paper = get_paper_attributes(0)
    except Exception as e:
        print(f"âŒ è·å–paperå±æ€§å¤±è´¥: {e}")
        paper = None
    if paper:
        print(f"âœ… æˆåŠŸè·å–è®ºæ–‡: {paper['title']}")
        print(f"   ä½œè€…: {paper.get('authors', 'N/A')}")
        print(f"   å¹´ä»½: {paper.get('year', 'N/A')}")
        print(f"   ä¼šè®®: {paper.get('conference', 'N/A')}")
        
        # è·å–è¯¥è®ºæ–‡çš„ç ”ç©¶é—®é¢˜
        rqs = get_paper_research_questions(paper['entity_name'])
        print(f"   ç ”ç©¶é—®é¢˜æ•°: {len(rqs)}")
        for i, rq in enumerate(rqs, 1):  # æ˜¾ç¤ºæ‰€æœ‰ç ”ç©¶é—®é¢˜
            print(f"     RQ{i}: {rq.get('research_question', 'N/A')}")
    
    # æ ¹æ®å…³é”®è¯æœç´¢è®ºæ–‡
    attention_papers = find_paper_by_title("Attention")
    print(f"\nğŸ” åŒ…å«'Attention'çš„è®ºæ–‡: {len(attention_papers)}ç¯‡")
    for paper in attention_papers[:2]:  # æ˜¾ç¤ºå‰2ç¯‡
        print(f"   ğŸ“„ {paper['title']}")
    
    # ===================
    # æ–¹æ³•2: åœ¨LightRAGç¯å¢ƒä¸­æŸ¥è¯¢ (æœ€å®Œæ•´)
    # ===================
    print(f"\nğŸ”„ æ–¹æ³•2: åœ¨LightRAGç¯å¢ƒä¸­æŸ¥è¯¢ (æœ€å®Œæ•´)")
    print("-" * 50)
    
    try:
        graph = rag.chunk_entity_relation_graph
        all_labels = await graph.get_all_labels()
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªpaperèŠ‚ç‚¹
        paper_nodes = []
        for label in all_labels:
            node_data = await graph.get_node(label)
            if node_data and node_data.get("entity_type") == "paper":
                paper_nodes.append((label, node_data))
                break
        
        if paper_nodes:
            label, node_data = paper_nodes[0]
            print(f"âœ… ä»LightRAGè·å–è®ºæ–‡: {node_data.get('title', 'N/A')}")
            print(f"   èŠ‚ç‚¹ID: {label}")
            print(f"   å®ä½“ç±»å‹: {node_data.get('entity_type', 'N/A')}")
            
            # è·å–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å…³ç³»
            edges = await graph.get_node_edges(label)
            print(f"   å…³è”å…³ç³»æ•°: {len(edges)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªå…³ç³»
            for i, (src, tgt) in enumerate(edges[:3], 1):
                edge_data = await graph.get_edge(src, tgt)
                if edge_data:
                    keywords = edge_data.get("keywords", "æ— å…³é”®è¯")
                    # ç¡®å®šå…³ç³»æ–¹å‘
                    if src == label:
                        print(f"     å…³ç³»{i}: è¯¥è®ºæ–‡ â†’ {tgt[:30]}... (å…³é”®è¯: {keywords})")
                    else:
                        print(f"     å…³ç³»{i}: {src[:30]}... â†’ è¯¥è®ºæ–‡ (å…³é”®è¯: {keywords})")
    
    except Exception as e:
        print(f"âŒ LightRAGæŸ¥è¯¢å¤±è´¥: {e}")
    
    # ===================
    # æ–¹æ³•3: ç‰¹å®šèŠ‚ç‚¹æŸ¥è¯¢å’Œåˆ†æ (æœ€çµæ´»)
    # ===================
    print(f"\nğŸ¯ æ–¹æ³•3: ç‰¹å®šèŠ‚ç‚¹æŸ¥è¯¢å’Œåˆ†æ (æœ€çµæ´»)")
    print("-" * 50)
    
    try:
        # æ¼”ç¤ºå¤æ‚æŸ¥è¯¢ï¼šæ‰¾åˆ°åŒ…å«"domain adaptation"çš„è®ºæ–‡åŠå…¶å®Œæ•´å­å›¾
        print("ğŸ” æœç´¢'domain adaptation'ç›¸å…³è®ºæ–‡...")
        
        # æ–¹æ³•3a: é€šè¿‡JSONè¿›è¡Œå¤æ‚åˆ†æ
        papers, rqs, solutions = [], [], []
        
        # è¯»å–æ‰€æœ‰æ•°æ®è¿›è¡Œåˆ†æ
        import json
        # ä½¿ç”¨APIçš„è·¯å¾„æ£€æµ‹åŠŸèƒ½
        from node_attributes_api import get_kg_file_path
        kg_file_path = get_kg_file_path()
        with open(kg_file_path, 'r', encoding='utf-8') as f:
            kg_data = json.load(f)
        
        entities = kg_data["entities"]
        
        # æŸ¥æ‰¾ç›¸å…³è®ºæ–‡
        domain_papers = []
        for entity in entities:
            if entity["entity_type"] == "paper":
                title = entity.get("title", "").lower()
                abstract = entity.get("abstract", "").lower()
                if "domain" in title or "adaptation" in title or "domain" in abstract:
                    domain_papers.append(entity)
        
        print(f"âœ… æ‰¾åˆ° {len(domain_papers)} ç¯‡åŸŸé€‚åº”ç›¸å…³è®ºæ–‡")
        
        # å¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œå­å›¾åˆ†æ
        for i, paper in enumerate(domain_papers[:2], 1):  # åˆ†æå‰2ç¯‡
            print(f"\n   ğŸ“„ è®ºæ–‡{i}: {paper['title']}")
            
            # æ‰¾åˆ°è¯¥è®ºæ–‡çš„ç ”ç©¶é—®é¢˜
            paper_rqs = get_paper_research_questions(paper['entity_name'])
            print(f"      ç ”ç©¶é—®é¢˜æ•°: {len(paper_rqs)}")
            
            for j, rq in enumerate(paper_rqs, 1):
                print(f"        RQ{j}: {rq.get('simplified_research_question', 'N/A')[:50]}...")
                
                # æ‰¾åˆ°å¯¹åº”çš„è§£å†³æ–¹æ¡ˆ
                from node_attributes_api import get_research_question_solutions
                solutions = get_research_question_solutions(rq['entity_name'])
                for k, sol in enumerate(solutions, 1):
                    print(f"          SOL{k}: {sol.get('simplified_solution', 'N/A')[:50]}...")
        
        # æ–¹æ³•3b: åœ¨LightRAGä¸­è¿›è¡Œå…³ç³»åˆ†æ
        print(f"\nğŸ”— åœ¨LightRAGä¸­åˆ†æèŠ‚ç‚¹å…³ç³»...")
        graph = rag.chunk_entity_relation_graph
        
        # ç»Ÿè®¡ä¸åŒç±»å‹èŠ‚ç‚¹çš„è¿æ¥åº¦
        node_connections = {"paper": [], "research_question": [], "solution": []}
        
        for label in all_labels[:20]:  # åˆ†æå‰20ä¸ªèŠ‚ç‚¹
            node_data = await graph.get_node(label)
            if node_data:
                entity_type = node_data.get("entity_type", "unknown")
                if entity_type in node_connections:
                    edges = await graph.get_node_edges(label)
                    node_connections[entity_type].append(len(edges))
        
        for node_type, connections in node_connections.items():
            if connections:
                avg_connections = sum(connections) / len(connections)
                print(f"   {node_type}èŠ‚ç‚¹å¹³å‡è¿æ¥æ•°: {avg_connections:.1f}")
    
    except Exception as e:
        print(f"âŒ ç‰¹å®šæŸ¥è¯¢å¤±è´¥: {e}")
    
    # ===================
    # æ€»ç»“ä¸‰ç§æ–¹æ³•çš„ä¼˜åŠ£
    # ===================
    print(f"\nğŸ“Š ä¸‰ç§æ–¹æ³•å¯¹æ¯”æ€»ç»“:")
    print("-" * 50)
    print("ğŸ“ æ–¹æ³•1 (JSONæŸ¥è¯¢):   é€Ÿåº¦å¿« | ç®€å• | åŠŸèƒ½åŸºç¡€")
    print("ğŸ”„ æ–¹æ³•2 (LightRAG):   åŠŸèƒ½å…¨ | å®æ—¶ | éœ€åˆå§‹åŒ–")  
    print("ğŸ¯ æ–¹æ³•3 (ç‰¹å®šæŸ¥è¯¢):   çµæ´» | å¤æ‚ | æ€§èƒ½ä¸­ç­‰")
    print()
    print("ğŸ’¡ å»ºè®®ä½¿ç”¨åœºæ™¯:")
    print("   - å¿«é€ŸæŸ¥çœ‹å±æ€§ â†’ ä½¿ç”¨æ–¹æ³•1")
    print("   - åˆ†æå›¾ç»“æ„å…³ç³» â†’ ä½¿ç”¨æ–¹æ³•2") 
    print("   - å¤æ‚æœç´¢åˆ†æ â†’ ä½¿ç”¨æ–¹æ³•3")


async def main():
    rag = None  # åˆå§‹åŒ–å˜é‡ä»¥é¿å…finallyå—ä¸­çš„å¼•ç”¨é”™è¯¯
    try:
        #Clear old data files
        files_to_delete = [
            "graph_chunk_entity_relation.graphml",
            "kv_store_doc_status.json",
            "kv_store_full_docs.json",
            "kv_store_text_chunks.json",
            "vdb_chunks.json",
            "vdb_entities.json",
            "vdb_relationships.json",
        ]

        for file in files_to_delete:
            file_path = os.path.join(WORKING_DIR, file)
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"Deleting old file:: {file_path}")

        # Initialize RAG instance
        rag = await initialize()

        custom_kg = load_custom_kg()

        await rag.ainsert_custom_kg(custom_kg)


        # ğŸ“Š éªŒè¯çŸ¥è¯†å›¾è°±æ’å…¥ç»“æœ
        print("\n=====================")
        print("ğŸ“Š éªŒè¯è®ºæ–‡çŸ¥è¯†å›¾è°±æ’å…¥ç»“æœ")
        print("=====================")
        
        # æ£€æŸ¥æ’å…¥çš„å®ä½“æ•°é‡
        graph = rag.chunk_entity_relation_graph
        all_labels = await graph.get_all_labels()
        print(f"âœ… æ€»å®ä½“æ•°: {len(all_labels)}")
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        entity_types = {}
        for label in all_labels:
            node_data = await graph.get_node(label)
            if node_data:
                entity_type = node_data.get("entity_type", "unknown")
                entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        
        print("ğŸ“‹ å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in entity_types.items():
            print(f"   {entity_type}: {count}ä¸ª")

        # ğŸš€ æ¼”ç¤ºä¸‰ç§èŠ‚ç‚¹å±æ€§æŸ¥è¯¢æ–¹æ³•
        #await demonstrate_node_query_methods(rag)

        # ğŸ”— è·¨è®ºæ–‡å…³è”åˆ†æ
        #await perform_cross_paper_analysis(rag)


        print("\nğŸ‰ è®ºæ–‡çŸ¥è¯†å›¾è°±æŸ¥è¯¢æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if rag:
            await rag.llm_response_cache.index_done_callback()
            await rag.finalize_storages()

            # ä½¿ç”¨ä¼ ç»ŸGraphMLå¯è§†åŒ–
            try:
                from visualize import generate_html
                generate_html(add_timestamp=True)
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")



if __name__ == "__main__":
    # Configure logging before running the main function
    configure_logging()
    asyncio.run(main())
    print("\nDone!")