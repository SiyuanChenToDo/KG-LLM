#!/usr/bin/env python3
"""
è·¨è®ºæ–‡å…³è”å¼•æ“ - é›†æˆç‰ˆ
========================

åŸºäºè®ºæ–‡çŸ¥è¯†å›¾è°±çš„å®é™…èŠ‚ç‚¹ç±»å‹ï¼ˆpaper, research_question, solutionï¼‰
è®¾è®¡æ›´ç§‘å­¦çš„è·¨è®ºæ–‡å…³è”ç®—æ³•ï¼Œç›´æ¥é›†æˆåˆ°LightRAGç³»ç»Ÿä¸­

æ ¸å¿ƒæ”¹è¿›ï¼š
1. åŸºäºçœŸå®èŠ‚ç‚¹å±æ€§çš„å¤šç»´ç›¸ä¼¼åº¦è®¡ç®—
2. è¯­ä¹‰å‘é‡ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
3. ç›´æ¥é›†æˆåˆ°LightRAGï¼Œæ”¯æŒå‘é‡å­˜å‚¨å’Œå›¾æ›´æ–°
4. é’ˆå¯¹ä¸‰ç§èŠ‚ç‚¹ç±»å‹çš„ä¸“é—¨å…³è”ç­–ç•¥
"""

import os
import json
import pickle
import hashlib
import asyncio
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import logging
from datetime import datetime
import random
import re

# å¯¼å…¥LightRAGç›¸å…³
from lightrag.llm.ollama import ollama_embed
from lightrag.utils import logger, compute_mdhash_id

# æ·»åŠ å½“å‰ç›®å½•åˆ°sys.pathï¼Œä»¥ä¾¿ç»å¯¹å¯¼å…¥åŒç›®å½•æ¨¡å—
import sys
from pathlib import Path
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

# å¯¼å…¥æŠ€æœ¯è¯æ®æå–å™¨
from technical_evidence_extractor import (
    TechnicalEvidenceExtractor, 
    TransferOpportunity,
    TechnicalComponent,
    TechnicalChallenge
)

class CrossPaperLinkingEngine:
    """è·¨è®ºæ–‡å…³è”å¼•æ“"""
    
    def __init__(self, 
                 rag_instance,
                 embed_model: str = "bge-m3:latest",
                 embed_host: str = "http://localhost:11434",
                 cache_dir: str = "./cache",
                 similarity_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è·¨è®ºæ–‡å…³è”å¼•æ“
        
        Args:
            rag_instance: LightRAGå®ä¾‹
            embed_model: åµŒå…¥æ¨¡å‹åç§°
            embed_host: OllamaæœåŠ¡åœ°å€
            cache_dir: ç¼“å­˜ç›®å½•
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.rag = rag_instance
        self.embed_model = embed_model
        self.embed_host = embed_host
        self.cache_dir = cache_dir
        self.similarity_threshold = similarity_threshold
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # å‘é‡ç¼“å­˜
        self.vector_cache_file = os.path.join(cache_dir, "node_vectors.pkl")
        self.vector_cache = self._load_vector_cache()
        
        # å…³è”æƒé‡é…ç½®
        self._setup_linking_weights()
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = True
        
        # ğŸ”§ åˆå§‹åŒ–æŠ€æœ¯è¯æ®æå–å™¨ï¼ˆå»¶è¿ŸåŠ è½½é¿å…é˜»å¡ï¼‰
        self.technical_extractor = None
        
        # ğŸ”§ åˆå§‹åŒ–å¤šæ ·åŒ–è¯­ä¹‰æ¨¡æ¿
        self._init_diverse_templates()
        
        logger.info(f"è·¨è®ºæ–‡å…³è”å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {cache_dir}")
    
    def _init_diverse_templates(self):
        """åˆå§‹åŒ–å¤šæ ·åŒ–è¯­ä¹‰æ¨¡æ¿"""
        self.semantic_templates = [
            # æŠ€æœ¯è¿ç§»ç±»
            "ã€æŠ€æœ¯è¿ç§»å¯å‘ã€‘ä»{source_domain}é¢†åŸŸçš„{source_method}ä¸­æå–æ ¸å¿ƒç®—æ³•{core_algorithm}ï¼Œé€šè¿‡{adaptation_strategy}æ”¹é€ ååº”ç”¨äº{target_domain}çš„{target_challenge}",
            
            # æ–¹æ³•è®ºå€Ÿé‰´ç±»  
            "ã€æ–¹æ³•è®ºå€Ÿé‰´ã€‘{source_method}åœ¨è§£å†³{source_challenge}æ—¶é‡‡ç”¨çš„{key_insight}ç­–ç•¥ï¼Œä¸º{target_domain}é¢†åŸŸçš„{target_challenge}æä¾›äº†{innovation_angle}æ€è·¯",
            
            # æ¶æ„è®¾è®¡ç±»
            "ã€æ¶æ„è®¾è®¡çµæ„Ÿã€‘{source_method}çš„{architecture_component}è®¾è®¡ç†å¿µï¼Œç»è¿‡{modification_approach}è°ƒæ•´ï¼Œå¯ä¸º{target_problem}æä¾›{expected_benefit}",
            
            # ç†è®ºèåˆç±»
            "ã€ç†è®ºèåˆæ¢ç´¢ã€‘å°†{source_theory}ç†è®ºæ¡†æ¶ä¸{target_context}ç›¸ç»“åˆï¼Œé€šè¿‡{integration_mechanism}å®ç°{theoretical_contribution}",
            
            # å®éªŒéªŒè¯ç±»
            "ã€å®éªŒéªŒè¯å¯å‘ã€‘{source_method}åœ¨{source_dataset}ä¸Šçš„{performance_metric}è¡¨ç°ï¼Œå¯å‘æˆ‘ä»¬åœ¨{target_dataset}ä¸Šè®¾è®¡{validation_strategy}éªŒè¯æ–¹æ¡ˆ"
        ]
        
        self.template_variables = {
            'source_domains': [
                'æ—¶é—´åºåˆ—åˆ†æ', 'è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'å›¾ç¥ç»ç½‘ç»œ', 
                'å¼ºåŒ–å­¦ä¹ ', 'æ— ç›‘ç£å­¦ä¹ ', 'å¯¹æ¯”å­¦ä¹ ', 'å…ƒå­¦ä¹ '
            ],
            'target_domains': [
                'åŸŸé€‚åº”', 'è¿ç§»å­¦ä¹ ', 'å°‘æ ·æœ¬å­¦ä¹ ', 'è¿ç»­å­¦ä¹ ',
                'å¤šæ¨¡æ€å­¦ä¹ ', 'é²æ£’æ€§å­¦ä¹ ', 'å¯è§£é‡ŠAI', 'è”é‚¦å­¦ä¹ '
            ],
            'core_algorithms': [
                'æ³¨æ„åŠ›æœºåˆ¶', 'å¯¹æŠ—è®­ç»ƒ', 'æ­£åˆ™åŒ–ç­–ç•¥', 'ç‰¹å¾å¯¹é½',
                'åˆ†å¸ƒåŒ¹é…', 'æ¢¯åº¦ä¼˜åŒ–', 'æŸå¤±å‡½æ•°è®¾è®¡', 'ç½‘ç»œæ¶æ„'
            ],
            'adaptation_strategies': [
                'å‚æ•°å¾®è°ƒ', 'ç»“æ„é‡ç»„', 'æŸå¤±å‡½æ•°ä¿®æ”¹', 'ç‰¹å¾ç©ºé—´å˜æ¢',
                'æ•°æ®å¢å¼º', 'æ¨¡å‹è’¸é¦', 'é›†æˆå­¦ä¹ ', 'å±‚çº§é€‚é…'
            ],
            'innovation_angles': [
                'æ–°é¢–çš„ç†è®ºè§†è§’', 'å®ç”¨çš„å·¥ç¨‹æ–¹æ¡ˆ', 'é«˜æ•ˆçš„è®¡ç®—ç­–ç•¥',
                'é²æ£’çš„è¯„ä¼°æ–¹æ³•', 'å¯æ‰©å±•çš„æ¡†æ¶è®¾è®¡', 'å¯è§£é‡Šçš„æœºåˆ¶'
            ]
        }
        
        # ç»Ÿè®¡æ¨¡æ¿ä½¿ç”¨æƒ…å†µ
        self.template_usage_stats = defaultdict(int)
    
    def _load_vector_cache(self) -> Dict[str, np.ndarray]:
        """åŠ è½½å‘é‡ç¼“å­˜"""
        if os.path.exists(self.vector_cache_file):
            try:
                with open(self.vector_cache_file, 'rb') as f:
                    cache = pickle.load(f)
                logger.info(f"æˆåŠŸåŠ è½½å‘é‡ç¼“å­˜ï¼ŒåŒ…å« {len(cache)} ä¸ªå‘é‡")
                return cache
            except Exception as e:
                logger.warning(f"åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _save_vector_cache(self):
        """ä¿å­˜å‘é‡ç¼“å­˜"""
        try:
            with open(self.vector_cache_file, 'wb') as f:
                pickle.dump(self.vector_cache, f)
            logger.info(f"å‘é‡ç¼“å­˜å·²ä¿å­˜ï¼ŒåŒ…å« {len(self.vector_cache)} ä¸ªå‘é‡")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡ç¼“å­˜å¤±è´¥: {e}")
    
    def _setup_linking_weights(self):
        """è®¾ç½®å…³è”æƒé‡é…ç½® - ä¸“æ³¨Solutionåˆ°Paperçš„è¿æ¥"""
        
        # ğŸ¯ åªå…è®¸solutionåˆ°paperçš„è·¨è®ºæ–‡è¿æ¥
        self.allowed_cross_paper_types = {("solution", "paper")}
        
        # å¤šç»´ç›¸ä¼¼åº¦æƒé‡ï¼ˆè¯­ä¹‰ä¸»å¯¼ï¼‰
        self.similarity_weights = {
            "semantic": 0.90,
            "attribute": 0.10,
            "structural": 0.0,
            "type_compatibility": 0.0,
        }

        # ğŸ”¥ Solution -> Paper ä¸“ç”¨å±æ€§æƒé‡
        self.solution_to_paper_weights = {
            # Solutionä¾§æƒé‡
            "solution_attrs": {
                "solution": 0.70,              # ä¸»è¦è§£å†³æ–¹æ³•
                "simplified_solution": 0.30    # ç®€åŒ–è§£å†³æ–¹æ³•
            },
            # Paperä¾§æƒé‡  
            "paper_attrs": {
                "abstract": 0.50,              # ğŸ¯ ä¸»è¦ï¼šæ‘˜è¦
                "core_problem": 0.25,          # æ ¸å¿ƒé—®é¢˜
                "basic_problem": 0.15,         # åŸºç¡€é—®é¢˜
                "preliminary_innovation_analysis": 0.07,  # åˆ›æ–°åˆ†æ
                "title": 0.03,                 # æ ‡é¢˜
            }
        }
        
        # é€šç”¨å±æ€§æƒé‡ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.attribute_weights = {
            "paper": {
                "abstract": 0.50,
                "core_problem": 0.25,
                "basic_problem": 0.15,
                "preliminary_innovation_analysis": 0.07,
                "title": 0.03,
            },
            "solution": {
                "solution": 0.70,
                "simplified_solution": 0.30
            }
        }
    
    def _get_node_hash(self, node_data: Dict) -> str:
        """ç”ŸæˆèŠ‚ç‚¹æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜é”®"""
        # ä½¿ç”¨å…³é”®å±æ€§ç”Ÿæˆå“ˆå¸Œ
        key_attrs = ['entity_name', 'entity_type']
        for attr_name, weight in self.attribute_weights.get(node_data.get('entity_type', ''), {}).items():
            if weight > 0.1:  # åªè€ƒè™‘é‡è¦å±æ€§
                key_attrs.append(attr_name)
        
        hash_input = ""
        for attr in key_attrs:
            value = str(node_data.get(attr, ""))
            hash_input += f"{attr}:{value}|"
        
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    async def get_node_vector(self, node_data: Dict, force_refresh: bool = False) -> Optional[np.ndarray]:
        """
        è·å–èŠ‚ç‚¹çš„è¯­ä¹‰å‘é‡ï¼Œæ”¯æŒç¼“å­˜
        
        Args:
            node_data: èŠ‚ç‚¹æ•°æ®
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            è¯­ä¹‰å‘é‡
        """
        node_hash = self._get_node_hash(node_data)
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and node_hash in self.vector_cache:
            return self.vector_cache[node_hash]
        
        # æ„å»ºç”¨äºåµŒå…¥çš„æ–‡æœ¬
        embed_text = self._build_embedding_text(node_data)
        
        if not embed_text.strip():
            logger.warning(f"èŠ‚ç‚¹ {node_data.get('entity_name', 'Unknown')} çš„åµŒå…¥æ–‡æœ¬ä¸ºç©º")
            return None
        
        try:
            # è°ƒç”¨Ollamaè·å–åµŒå…¥å‘é‡
            embedding_result = await ollama_embed(
                [embed_text],
                embed_model=self.embed_model,
                host=self.embed_host,
                timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            if embedding_result is not None and len(embedding_result) > 0:
                vector = np.array(embedding_result[0], dtype=np.float32)
                # å½’ä¸€åŒ–
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm
                else:
                    logger.warning(f"èŠ‚ç‚¹å‘é‡èŒƒæ•°ä¸º0: {node_data.get('entity_name', 'Unknown')}")
                    return None
                
                # ä¿å­˜åˆ°ç¼“å­˜
                self.vector_cache[node_hash] = vector
                return vector
            else:
                logger.error(f"è·å–èŠ‚ç‚¹å‘é‡å¤±è´¥: {node_data.get('entity_name', 'Unknown')}")
                return None
                
        except Exception as e:
            logger.error(f"è®¡ç®—èŠ‚ç‚¹å‘é‡æ—¶å‡ºé”™: {e}")
            # ğŸ”§ å¤‡é€‰æ–¹æ¡ˆï¼šç”ŸæˆåŸºäºæ–‡æœ¬é•¿åº¦çš„ç®€å•å‘é‡
            try:
                # åˆ›å»ºä¸€ä¸ªåŸºäºæ–‡æœ¬ç‰¹å¾çš„ç®€å•å‘é‡
                text_words = embed_text.lower().split()
                if len(text_words) > 0:
                    # ç®€å•çš„æ–‡æœ¬ç‰¹å¾å‘é‡ï¼ˆ1024ç»´ï¼Œä¸bge-m3ä¸€è‡´ï¼‰
                    simple_vector = np.zeros(1024, dtype=np.float32)
                    for i, word in enumerate(text_words[:100]):  # åªå–å‰100ä¸ªè¯
                        word_hash = hash(word) % 1024
                        simple_vector[word_hash] += 1.0 / (i + 1)  # ä½ç½®æƒé‡é€’å‡
                    
                    # å½’ä¸€åŒ–
                    norm = np.linalg.norm(simple_vector)
                    if norm > 0:
                        simple_vector = simple_vector / norm
                        self.vector_cache[node_hash] = simple_vector
                        logger.info(f"ä½¿ç”¨ç®€å•å‘é‡ä½œä¸ºå¤‡é€‰: {node_data.get('entity_name', 'Unknown')}")
                        return simple_vector
            except Exception as backup_e:
                logger.error(f"å¤‡é€‰å‘é‡è®¡ç®—ä¹Ÿå¤±è´¥: {backup_e}")
            
            return None
    
    def _build_embedding_text(self, node_data: Dict) -> str:
        """
        æ ¹æ®èŠ‚ç‚¹ç±»å‹å’Œå±æ€§æƒé‡æ„å»ºç”¨äºåµŒå…¥çš„æ–‡æœ¬
        
        Args:
            node_data: èŠ‚ç‚¹æ•°æ®
            
        Returns:
            ç”¨äºåµŒå…¥çš„æ–‡æœ¬
        """
        entity_type = node_data.get('entity_type', 'unknown')
        texts = []
        
        if entity_type in self.attribute_weights:
            # æŒ‰æƒé‡æ’åºå±æ€§
            sorted_attrs = sorted(
                self.attribute_weights[entity_type].items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            for attr_name, weight in sorted_attrs:
                value = node_data.get(attr_name, "")
                if value and str(value).strip():
                    # æ ¹æ®æƒé‡é‡å¤æ–‡æœ¬ï¼Œå¢å¼ºé‡è¦å±æ€§çš„å½±å“
                    repeat_count = max(1, int(weight * 5))
                    texts.extend([str(value).strip()] * repeat_count)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å±æ€§ï¼Œä½¿ç”¨é»˜è®¤çš„descriptionæˆ–title
        if not texts:
            for fallback_attr in ['description', 'title', 'entity_name']:
                value = node_data.get(fallback_attr, "")
                if value and str(value).strip():
                    texts.append(str(value).strip())
                    break
        
        return " ".join(texts)
    
    def calculate_attribute_similarity(self, node1: Dict, node2: Dict) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªèŠ‚ç‚¹çš„å±æ€§ç›¸ä¼¼åº¦
        
        Args:
            node1, node2: èŠ‚ç‚¹æ•°æ®
            
        Returns:
            å±æ€§ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        type1 = node1.get('entity_type', 'unknown')
        type2 = node2.get('entity_type', 'unknown')
        
        # åŒç±»å‹èŠ‚ç‚¹çš„å±æ€§ç›¸ä¼¼åº¦
        if type1 == type2 and type1 in self.attribute_weights:
            similarities = []
            weights = []
            
            for attr_name, weight in self.attribute_weights[type1].items():
                val1 = str(node1.get(attr_name, "")).strip().lower()
                val2 = str(node2.get(attr_name, "")).strip().lower()
                
                if val1 and val2:
                    # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„ç®—æ³•ï¼‰
                    similarity = self._text_similarity(val1, val2)
                    similarities.append(similarity)
                    weights.append(weight)
            
            if similarities:
                # åŠ æƒå¹³å‡
                total_weight = sum(weights)
                weighted_sum = sum(sim * weight for sim, weight in zip(similarities, weights))
                return weighted_sum / total_weight if total_weight > 0 else 0.0
        
        # è·¨ç±»å‹èŠ‚ç‚¹çš„ç‰¹æ®Šå¤„ç†
        elif (type1, type2) in [("paper", "research_question"), ("research_question", "paper")]:
            # è®ºæ–‡çš„æ ¸å¿ƒé—®é¢˜ä¸ç ”ç©¶é—®é¢˜çš„ç›¸ä¼¼åº¦
            paper_problem = ""
            rq_question = ""
            
            if type1 == "paper":
                paper_problem = str(node1.get("core_problem", "")).strip().lower()
                rq_question = str(node2.get("research_question", "")).strip().lower()
            else:
                paper_problem = str(node2.get("core_problem", "")).strip().lower()
                rq_question = str(node1.get("research_question", "")).strip().lower()
            
            if paper_problem and rq_question:
                return self._text_similarity(paper_problem, rq_question)
        
        elif (type1, type2) in [("research_question", "solution"), ("solution", "research_question")]:
            # ç ”ç©¶é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆçš„ç›¸å…³æ€§
            rq_text = ""
            sol_text = ""
            
            if type1 == "research_question":
                rq_text = str(node1.get("research_question", "")).strip().lower()
                sol_text = str(node2.get("solution", "")).strip().lower()
            else:
                rq_text = str(node2.get("research_question", "")).strip().lower()
                sol_text = str(node1.get("solution", "")).strip().lower()
            
            if rq_text and sol_text:
                return self._text_similarity(rq_text, sol_text)
        
        return 0.0
    
    def calculate_solution_paper_similarity(self, solution_node: Dict, paper_node: Dict) -> float:
        """
        ä¸“é—¨è®¡ç®—Solutionåˆ°Paperçš„ç›¸ä¼¼åº¦
        
        Args:
            solution_node: SolutionèŠ‚ç‚¹æ•°æ®
            paper_node: PaperèŠ‚ç‚¹æ•°æ®
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        solution_weights = self.solution_to_paper_weights["solution_attrs"]
        paper_weights = self.solution_to_paper_weights["paper_attrs"]
        
        similarities = []
        weights = []
        
        # Solutionä¾§çš„æ–‡æœ¬
        solution_texts = []
        for attr_name, weight in solution_weights.items():
            value = str(solution_node.get(attr_name, "")).strip()
            if value:
                solution_texts.append((value, weight))
        
        # Paperä¾§çš„æ–‡æœ¬
        paper_texts = []
        for attr_name, weight in paper_weights.items():
            value = str(paper_node.get(attr_name, "")).strip()
            if value:
                paper_texts.append((value, weight))
        
        # è®¡ç®—æ‰€æœ‰ç»„åˆçš„ç›¸ä¼¼åº¦
        for sol_text, sol_weight in solution_texts:
            for paper_text, paper_weight in paper_texts:
                text_sim = self._text_similarity(sol_text, paper_text)
                if text_sim > 0:
                    # ç»„åˆæƒé‡
                    combined_weight = sol_weight * paper_weight
                    similarities.append(text_sim)
                    weights.append(combined_weight)
        
        if similarities:
            # åŠ æƒå¹³å‡
            total_weight = sum(weights)
            weighted_sum = sum(sim * weight for sim, weight in zip(similarities, weights))
            return weighted_sum / total_weight if total_weight > 0 else 0.0
        
        return 0.0

    def calculate_technical_evidence_similarity(self, solution_node: Dict, paper_node: Dict) -> Tuple[float, Dict]:
        """
        ğŸ”§ æ–°å¢ï¼šåŸºäºæŠ€æœ¯è¯æ®çš„ç›¸ä¼¼åº¦è®¡ç®—
        
        Args:
            solution_node: SolutionèŠ‚ç‚¹æ•°æ®
            paper_node: PaperèŠ‚ç‚¹æ•°æ®
            
        Returns:
            (ç›¸ä¼¼åº¦åˆ†æ•°, æŠ€æœ¯è¯æ®è¯¦æƒ…)
        """
        # 1. ä»solutionä¸­æå–æŠ€æœ¯ç»„ä»¶
        technical_components = self.technical_extractor.extract_technical_components(solution_node)
        
        # 2. ä»paperä¸­æå–æŠ€æœ¯æŒ‘æˆ˜
        technical_challenges = self.technical_extractor.extract_technical_challenges(paper_node)
        
        if not technical_components or not technical_challenges:
            return 0.0, {}
        
        # 3. å‘ç°è¿ç§»æœºä¼š
        transfer_opportunities = self.technical_extractor.find_transfer_opportunities(
            technical_components, technical_challenges
        )
        
        if not transfer_opportunities:
            return 0.0, {}
        
        # 4. è®¡ç®—æœ€ä½³è¿ç§»æœºä¼šçš„å¯è¡Œæ€§ä½œä¸ºç›¸ä¼¼åº¦
        best_opportunity = transfer_opportunities[0]  # å·²æŒ‰å¯è¡Œæ€§æ’åº
        similarity_score = best_opportunity.transfer_feasibility
        
        # 5. æ„å»ºæŠ€æœ¯è¯æ®è¯¦æƒ…
        evidence_details = {
            "best_opportunity": best_opportunity,
            "total_opportunities": len(transfer_opportunities),
            "technical_components": technical_components,
            "technical_challenges": technical_challenges,
            "adaptation_evidence": best_opportunity.adaptation_evidence,
            "adaptation_mechanism": best_opportunity.adaptation_mechanism,
            "validation_suggestion": best_opportunity.validation_suggestion
        }
        
        logger.debug(f"æŠ€æœ¯è¯æ®ç›¸ä¼¼åº¦: {similarity_score:.3f}, å‘ç° {len(transfer_opportunities)} ä¸ªè¿ç§»æœºä¼š")
        
        return similarity_score, evidence_details
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆæ”¹è¿›çš„ç®—æ³•ï¼‰
        
        Args:
            text1, text2: å¾…æ¯”è¾ƒçš„æ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # é¢„å¤„ç†ï¼šè½¬å°å†™ï¼Œç§»é™¤å¸¸è§åœè¯
        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an'}
        
        def clean_text(text):
            words = text.lower().split()
            return set(word for word in words if len(word) > 2 and word not in stop_words)
        
        words1 = clean_text(text1)
        words2 = clean_text(text2)
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        jaccard_sim = intersection / union if union > 0 else 0.0
        
        # è®¡ç®—é‡å æ¯”ä¾‹ï¼ˆå¯¹è¾ƒçŸ­æ–‡æœ¬çš„å¥–åŠ±ï¼‰
        overlap_ratio = intersection / min(len(words1), len(words2)) if min(len(words1), len(words2)) > 0 else 0.0
        
        # ç»¼åˆç›¸ä¼¼åº¦ï¼šJaccard + é‡å å¥–åŠ±
        final_sim = 0.7 * jaccard_sim + 0.3 * overlap_ratio
        
        return min(final_sim, 1.0)
    
    def _calculate_semantic_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            vector1, vector2: è¯­ä¹‰å‘é‡
            
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ (0-1)
        """
        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vector1, vector2)
            norm1 = np.linalg.norm(vector1)
            norm2 = np.linalg.norm(vector2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            cosine_sim = dot_product / (norm1 * norm2)
            # å°†ä½™å¼¦ç›¸ä¼¼åº¦ä»[-1,1]æ˜ å°„åˆ°[0,1]
            return (cosine_sim + 1) / 2
        except Exception as e:
            logger.error(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def get_type_compatibility_score(self, type1: str, type2: str) -> float:
        """
        è·å–ä¸¤ä¸ªèŠ‚ç‚¹ç±»å‹çš„å…¼å®¹æ€§åˆ†æ•°
        
        Args:
            type1, type2: èŠ‚ç‚¹ç±»å‹
            
        Returns:
            å…¼å®¹æ€§åˆ†æ•° (0-1)
        """
        # æ ‡å‡†åŒ–ç±»å‹å¯¹
        type_pair = tuple(sorted([type1, type2]))
        
        return self.type_compatibility.get(type_pair, self.type_compatibility["default"])
    
    async def calculate_comprehensive_similarity(self, 
                                               node1: Dict, 
                                               node2: Dict,
                                               vector1: Optional[np.ndarray] = None,
                                               vector2: Optional[np.ndarray] = None) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªèŠ‚ç‚¹çš„ç»¼åˆç›¸ä¼¼åº¦
        
        Args:
            node1, node2: èŠ‚ç‚¹æ•°æ®
            vector1, vector2: é¢„è®¡ç®—çš„è¯­ä¹‰å‘é‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_sim = 0.0
        if vector1 is not None and vector2 is not None:
            semantic_sim = float(np.dot(vector1, vector2))
        else:
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—å‘é‡ï¼Œä¸´æ—¶è®¡ç®—
            if vector1 is None:
                vector1 = await self.get_node_vector(node1)
            if vector2 is None:
                vector2 = await self.get_node_vector(node2)
            
            if vector1 is not None and vector2 is not None:
                semantic_sim = float(np.dot(vector1, vector2))
        
        # 2. å±æ€§ç›¸ä¼¼åº¦
        attribute_sim = self.calculate_attribute_similarity(node1, node2)
        
        # 3. ç±»å‹å…¼å®¹æ€§
        type1 = node1.get('entity_type', 'unknown')
        type2 = node2.get('entity_type', 'unknown')
        type_compat = self.get_type_compatibility_score(type1, type2)
        
        # è¯­ä¹‰æœ€ä½é—¨æ§›ï¼šä¸è¾¾æ ‡ç›´æ¥ä¸å»ºè¾¹
        semantic_min = float(os.getenv("CPL_MIN_SEMANTIC", "0.60"))
        if semantic_sim < semantic_min:
            return 0.0

        # 4. ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä¸å‚ä¸ï¼‰
        structural_sim = 0.0
        
        # 5. ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—
        comprehensive_score = (
            self.similarity_weights["semantic"] * semantic_sim +
            self.similarity_weights["attribute"] * attribute_sim +
            self.similarity_weights["type_compatibility"] * type_compat +
            self.similarity_weights["structural"] * structural_sim
        )
        
        # ğŸ” è°ƒè¯•ï¼šè®°å½•é«˜åˆ†ç›¸ä¼¼åº¦è¯¦æƒ…
        final_score = min(1.0, max(0.0, comprehensive_score))
        if final_score >= 0.5:  # è®°å½•ä¸­ç­‰ä»¥ä¸Šçš„ç›¸ä¼¼åº¦
            entity1 = node1.get('entity_name', 'Unknown')[:30]
            entity2 = node2.get('entity_name', 'Unknown')[:30]
            logger.debug(f"ç›¸ä¼¼åº¦è¯¦æƒ… {entity1}...â†”{entity2}...: "
                        f"ç»¼åˆ={final_score:.3f} (è¯­ä¹‰={semantic_sim:.3f}, "
                        f"å±æ€§={attribute_sim:.3f}, ç±»å‹={type_compat:.3f}, "
                        f"ç»“æ„={structural_sim:.3f})")
        
        return final_score
    
    def _calculate_structural_similarity(self, node1: Dict, node2: Dict) -> float:
        """
        è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦ï¼ˆåŸºäºèŠ‚ç‚¹åœ¨è®ºæ–‡ä¸­çš„ç»“æ„ä½ç½®ï¼‰
        
        Args:
            node1, node2: èŠ‚ç‚¹æ•°æ®
            
        Returns:
            ç»“æ„ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # æå–è®ºæ–‡IDå’ŒèŠ‚ç‚¹åœ¨è®ºæ–‡ä¸­çš„åºå·
        name1 = node1.get('entity_name', '')
        name2 = node2.get('entity_name', '')
        
        # å‡è®¾èŠ‚ç‚¹å‘½åæ ¼å¼ä¸º: PaperName_TYPE_åºå·
        try:
            parts1 = name1.split('_')
            parts2 = name2.split('_')
            
            if len(parts1) >= 3 and len(parts2) >= 3:
                # æ¯”è¾ƒèŠ‚ç‚¹ç±»å‹å’Œåºå·
                type1, seq1 = parts1[-2], parts1[-1]
                type2, seq2 = parts2[-2], parts2[-1]
                
                # åŒç±»å‹åŒåºå·çš„èŠ‚ç‚¹å…·æœ‰æ›´é«˜çš„ç»“æ„ç›¸ä¼¼åº¦
                if type1 == type2 and seq1 == seq2:
                    return 0.8
                elif type1 == type2:
                    return 0.6
                else:
                    return 0.3
            
        except (IndexError, ValueError):
            pass
        
        return 0.4  # é»˜è®¤ç»“æ„ç›¸ä¼¼åº¦
    
    def calculate_comprehensive_node_similarity(self, node1: Dict, node2: Dict) -> float:
        """
        è®¡ç®—åŸºäºèŠ‚ç‚¹å±æ€§çš„ç»¼åˆç›¸ä¼¼åº¦ï¼ˆæ— LLMè°ƒç”¨ï¼‰
        
        Args:
            node1, node2: èŠ‚ç‚¹æ•°æ®
            
        Returns:
            ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # 1. åŸºç¡€å±æ€§ç›¸ä¼¼åº¦
        attr_sim = self._calculate_attribute_similarity(node1, node2)
        
        # 2. å®ä½“åç§°è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
        name_sim = self._calculate_name_similarity(node1, node2)
        
        # 3. ç±»å‹å…¼å®¹æ€§
        type_compat = self._calculate_type_compatibility(node1, node2)
        
        # 4. ç»“æ„ç›¸ä¼¼åº¦
        struct_sim = self._calculate_structural_similarity(node1, node2)
        
        # 5. å†…å®¹é•¿åº¦ç›¸ä¼¼åº¦
        content_sim = self._calculate_content_similarity(node1, node2)
        
        # åŠ æƒç»¼åˆ
        comprehensive_score = (
            0.3 * attr_sim +
            0.25 * name_sim +
            0.2 * type_compat +
            0.15 * struct_sim +
            0.1 * content_sim
        )
        
        return min(1.0, max(0.0, comprehensive_score))
    
    def _calculate_name_similarity(self, node1: Dict, node2: Dict) -> float:
        """è®¡ç®—å®ä½“åç§°ç›¸ä¼¼åº¦"""
        name1 = node1.get('entity_name', '').lower()
        name2 = node2.get('entity_name', '').lower()
        
        if not name1 or not name2:
            return 0.0
        
        # å…³é”®è¯æå–å’ŒåŒ¹é…
        import re
        words1 = set(re.findall(r'\b\w+\b', name1))
        words2 = set(re.findall(r'\b\w+\b', name2))
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_content_similarity(self, node1: Dict, node2: Dict) -> float:
        """è®¡ç®—å†…å®¹é•¿åº¦ç›¸ä¼¼åº¦"""
        # è·å–ä¸»è¦å†…å®¹å­—æ®µ
        content1 = str(node1.get('description', '') or node1.get('full_solution', '') or node1.get('simplified_solution', '') or '')
        content2 = str(node2.get('description', '') or node2.get('full_solution', '') or node2.get('simplified_solution', '') or '')
        
        len1, len2 = len(content1), len(content2)
        
        if len1 == 0 and len2 == 0:
            return 1.0
        if len1 == 0 or len2 == 0:
            return 0.0
        
        # é•¿åº¦ç›¸ä¼¼åº¦ï¼ˆé¿å…æç«¯å·®å¼‚ï¼‰
        ratio = min(len1, len2) / max(len1, len2)
        return ratio
    
    def build_attribute_based_evidence(self, node1: Dict, node2: Dict, similarity: float) -> Dict:
        """æ„å»ºåŸºäºå±æ€§çš„å…³è”è¯æ®"""
        evidence = {
            'method': 'attribute_based',
            'similarity_score': similarity,
            'node1_type': node1.get('entity_type', 'unknown'),
            'node2_type': node2.get('entity_type', 'unknown'),
            'node1_name': node1.get('entity_name', '')[:50],
            'node2_name': node2.get('entity_name', '')[:50]
        }
        
        # æ·»åŠ å…·ä½“çš„ç›¸ä¼¼åº¦ç»„ä»¶
        evidence['components'] = {
            'attribute_similarity': self._calculate_attribute_similarity(node1, node2),
            'name_similarity': self._calculate_name_similarity(node1, node2),
            'type_compatibility': self._calculate_type_compatibility(node1, node2),
            'structural_similarity': self._calculate_structural_similarity(node1, node2),
            'content_similarity': self._calculate_content_similarity(node1, node2)
        }
        
        return evidence
    
    def _calculate_attribute_similarity(self, node1: Dict, node2: Dict) -> float:
        """
        è®¡ç®—èŠ‚ç‚¹å±æ€§ç›¸ä¼¼åº¦ï¼ˆåŸºäºçœŸå®å±æ€§ç»“æ„ï¼‰
        
        æ ¹æ®ä¸åŒèŠ‚ç‚¹ç±»å‹çš„å®é™…å±æ€§è¿›è¡Œç§‘å­¦åŒ¹é…
        """
        type1 = node1.get('entity_type', '')
        type2 = node2.get('entity_type', '')
        
        # åŸºäºèŠ‚ç‚¹ç±»å‹çš„æ™ºèƒ½å±æ€§åŒ¹é…
        if type1 == 'solution' and type2 == 'paper':
            return self._calculate_solution_paper_similarity(node1, node2)
        elif type1 == 'paper' and type2 == 'solution':
            return self._calculate_solution_paper_similarity(node2, node1)
        elif type1 == 'solution' and type2 == 'research_question':
            return self._calculate_solution_rq_similarity(node1, node2)
        elif type1 == 'research_question' and type2 == 'solution':
            return self._calculate_solution_rq_similarity(node2, node1)
        elif type1 == 'paper' and type2 == 'research_question':
            return self._calculate_paper_rq_similarity(node1, node2)
        elif type1 == 'research_question' and type2 == 'paper':
            return self._calculate_paper_rq_similarity(node2, node1)
        else:
            # åŒç±»å‹èŠ‚ç‚¹æ¯”è¾ƒ
            return self._calculate_same_type_similarity(node1, node2)
    
    def _calculate_solution_paper_similarity(self, solution_node: Dict, paper_node: Dict) -> float:
        """
        è®¡ç®—Solutionå’ŒPaperèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
        å……åˆ†åˆ©ç”¨äººå·¥æ ‡æ³¨çš„ä¸°å¯Œå±æ€§æ•°æ®
        """
        similarities = []
        
        # è·å–Solutionå†…å®¹
        solution_full = str(solution_node.get('solution', ''))
        solution_simple = str(solution_node.get('simplified_solution', ''))
        solution_name = str(solution_node.get('entity_name', ''))
        
        # è·¨é¢†åŸŸæŠ€æœ¯è¿ç§»ä¼˜åŒ–æƒé‡åˆ†é…
        
        # 1. æŠ€æœ¯èƒŒæ™¯æ·±åº¦åŒ¹é…: Solution vs ç›¸å…³å·¥ä½œ (35%) - æœ€é‡è¦ï¼ŒåŒ…å«æŠ€æœ¯ç»†èŠ‚
        paper_related = str(paper_node.get('related_work', ''))
        related_sim = self._calculate_enhanced_text_similarity(solution_full, paper_related)
        similarities.append(('technical_background', related_sim, 0.35))
        
        # 2. æ¡†æ¶è¿ç§»æ½œåŠ›: Solution vs Paperæ¡†æ¶æ€»ç»“ (25%)
        paper_framework = str(paper_node.get('framework_summary', ''))
        framework_sim = self._calculate_enhanced_text_similarity(solution_full, paper_framework)
        similarities.append(('framework_transfer', framework_sim, 0.25))
        
        # 3. æ–¹æ³•è®ºåˆ›æ–°: Solution vs åˆæ­¥åˆ›æ–°åˆ†æ (20%)
        paper_innovation = str(paper_node.get('preliminary_innovation_analysis', ''))
        innovation_sim = self._calculate_enhanced_text_similarity(solution_simple, paper_innovation)
        similarities.append(('methodology_innovation', innovation_sim, 0.2))
        
        # 4. é—®é¢˜æŠ½è±¡åŒ–: Solution vs Paperæ ¸å¿ƒé—®é¢˜ (15%)
        paper_core = str(paper_node.get('core_problem', ''))
        core_sim = self._calculate_enhanced_text_similarity(solution_full, paper_core)
        similarities.append(('problem_abstraction', core_sim, 0.15))
        
        # 5. é«˜å±‚æ¦‚è¿°: Solutionç®€åŒ– vs æ‘˜è¦ (5%)
        paper_abstract = str(paper_node.get('abstract', ''))
        abstract_sim = self._calculate_enhanced_text_similarity(solution_simple, paper_abstract)
        similarities.append(('high_level_match', abstract_sim, 0.05))
        
        # åŠ æƒè®¡ç®—å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
        weighted_sum = sum(sim * weight for _, sim, weight in similarities)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•é«˜åˆ†ç»„ä»¶
        high_scores = [(name, sim) for name, sim, _ in similarities if sim > 0.3]
        if high_scores:
            logger.debug(f"é«˜åˆ†ç»„ä»¶: {high_scores}")
        
        return min(1.0, max(0.0, weighted_sum))
    
    def _calculate_enhanced_text_similarity(self, text1: str, text2: str) -> float:
        """
        å¢å¼ºç‰ˆæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
        ç»“åˆå…³é”®è¯åŒ¹é…ã€æŠ€æœ¯ç›¸ä¼¼åº¦å’Œè·¨é¢†åŸŸè¿ç§»æ½œåŠ›
        """
        if not text1 or not text2:
            return 0.0
        
        # 1. åŸºç¡€å…³é”®è¯åŒ¹é…
        basic_sim = self._calculate_text_similarity(text1, text2)
        
        # 2. æŠ€æœ¯å…³é”®è¯å¢å¼ºåŒ¹é…ï¼ˆæ”¯æŒè·¨é¢†åŸŸï¼‰
        tech_sim = self._calculate_technical_keyword_similarity(text1, text2)
        
        # 3. æ¦‚å¿µçº§åˆ«ç›¸ä¼¼åº¦ï¼ˆæŠ½è±¡å±‚é¢çš„åŒ¹é…ï¼‰
        concept_sim = self._calculate_concept_similarity(text1, text2)
        
        # 4. é•¿åº¦åŠ æƒï¼ˆé•¿æ–‡æœ¬æ›´å¯é ï¼‰
        len1, len2 = len(text1), len(text2)
        length_weight = min(1.0, (len1 + len2) / 2000)
        
        # 5. è·¨é¢†åŸŸè¿ç§»å¥–åŠ±
        transfer_bonus = self._calculate_transfer_potential(text1, text2)
        
        # è·¨é¢†åŸŸä¼˜åŒ–ï¼šé™ä½åŸºç¡€æ–‡æœ¬æƒé‡ï¼Œæå‡æŠ€æœ¯å’Œè¿ç§»æƒé‡
        enhanced_sim = (
            0.25 * basic_sim + 
            0.35 * tech_sim + 
            0.15 * concept_sim + 
            0.25 * transfer_bonus  # å¤§å¹…æå‡è¿ç§»æ½œåŠ›æƒé‡
        )
        
        # è·¨é¢†åŸŸåœºæ™¯ä¸‹ï¼Œé•¿æ–‡æœ¬æƒé‡æ›´é‡è¦ï¼ˆåŒ…å«æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼‰
        final_sim = enhanced_sim * (0.7 + 0.3 * length_weight)
        
        # è·¨é¢†åŸŸè¿ç§»å¥–åŠ±ï¼šå¦‚æœå‘ç°æŠ€æœ¯è¿ç§»æ½œåŠ›ï¼Œç»™äºˆé¢å¤–æå‡
        if transfer_bonus > 0.08 and tech_sim > 0.03:  # é™ä½è§¦å‘é˜ˆå€¼
            cross_domain_bonus = min(0.2, transfer_bonus * tech_sim * 1.5)  # å¢å¼ºå¥–åŠ±
            final_sim += cross_domain_bonus
        
        # é•¿æ–‡æœ¬æ·±åº¦åˆ†æå¥–åŠ±ï¼ˆäººå·¥æ ‡æ³¨æ•°æ®å……åˆ†åˆ©ç”¨ï¼‰
        if len1 > 1000 and len2 > 1000:  # ä¸¤ä¸ªæ–‡æœ¬éƒ½å¾ˆé•¿
            depth_bonus = min(0.05, (len1 + len2) / 10000)  # åŸºäºé•¿åº¦çš„æ·±åº¦å¥–åŠ±
            final_sim += depth_bonus
        
        return min(1.0, max(0.0, final_sim))
    
    def _calculate_concept_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ¦‚å¿µçº§åˆ«çš„ç›¸ä¼¼åº¦"""
        import re
        
        # æŠ½è±¡æ¦‚å¿µå…³é”®è¯
        concept_patterns = {
            'problem_solving': r'\b(problem|challenge|issue|difficulty|obstacle)\b',
            'improvement': r'\b(improve|enhance|optimize|better|superior|advance)\b',
            'accuracy': r'\b(accuracy|precision|performance|quality|effectiveness)\b',
            'efficiency': r'\b(efficient|fast|quick|speed|computational|runtime)\b',
            'robustness': r'\b(robust|stable|reliable|consistent|invariant)\b',
            'adaptation': r'\b(adapt|adjust|modify|customize|tailor|flexible)\b',
            'learning': r'\b(learn|training|knowledge|understanding|representation)\b',
            'analysis': r'\b(analysis|analyze|examine|investigate|study|research)\b'
        }
        
        concept_scores = []
        for concept, pattern in concept_patterns.items():
            matches1 = len(re.findall(pattern, text1.lower()))
            matches2 = len(re.findall(pattern, text2.lower()))
            
            if matches1 > 0 and matches2 > 0:
                # è®¡ç®—æ¦‚å¿µå¼ºåº¦ç›¸ä¼¼åº¦
                concept_sim = 1.0 - abs(matches1 - matches2) / max(matches1, matches2, 1)
                concept_scores.append(concept_sim)
        
        return sum(concept_scores) / len(concept_scores) if concept_scores else 0.0
    
    def _calculate_transfer_potential(self, text1: str, text2: str) -> float:
        """è®¡ç®—è·¨é¢†åŸŸæŠ€æœ¯è¿ç§»æ½œåŠ›"""
        import re
        
        # å¯è¿ç§»çš„æŠ€æœ¯æ¨¡å¼
        transferable_patterns = {
            'attention_mechanism': r'\b(attention|focus|weight|importance|selection|channel|spatial)\b',
            'feature_fusion': r'\b(fusion|combine|merge|integrate|concatenate|aggregate|ensemble)\b',
            'multi_level': r'\b(multi-level|hierarchical|pyramid|scale|resolution|multi-scale)\b',
            'learning_strategy': r'\b(supervised|unsupervised|self-supervised|contrastive|adversarial)\b',
            'optimization': r'\b(loss|objective|minimize|maximize|gradient|optimization|training)\b',
            'regularization': r'\b(regularization|constraint|penalty|normalization|dropout|batch)\b',
            'adaptation_techniques': r'\b(adapt|alignment|domain|transfer|cross|bridge|gap)\b',
            'feature_learning': r'\b(feature|representation|embedding|encoding|extraction)\b'
        }
        
        transfer_scores = []
        for pattern_name, pattern in transferable_patterns.items():
            matches1 = bool(re.search(pattern, text1.lower()))
            matches2 = bool(re.search(pattern, text2.lower()))
            
            if matches1 and matches2:
                transfer_scores.append(1.0)
            elif matches1 or matches2:
                transfer_scores.append(0.3)  # å•è¾¹åŒ¹é…ä¹Ÿæœ‰è¿ç§»ä»·å€¼
        
        base_score = sum(transfer_scores) / len(transferable_patterns)
        
        # è·¨é¢†åŸŸè¿ç§»å¥–åŠ±æœºåˆ¶ä¼˜åŒ–
        num_matches = len([s for s in transfer_scores if s > 0])
        if num_matches >= 3:
            base_score *= 1.5  # å¤šæ¨¡å¼åŒ¹é…å¤§å¹…å¥–åŠ±
        elif num_matches >= 2:
            base_score *= 1.3
        elif num_matches >= 1:
            base_score *= 1.1
        
        # ç‰¹æ®Šå¥–åŠ±ï¼šattention + fusion ç»„åˆï¼ˆå¸¸è§è·¨é¢†åŸŸè¿ç§»æ¨¡å¼ï¼‰
        has_attention = bool(re.search(transferable_patterns['attention_mechanism'], text1.lower()) and 
                           re.search(transferable_patterns['attention_mechanism'], text2.lower()))
        has_fusion = bool(re.search(transferable_patterns['feature_fusion'], text1.lower()) and 
                         re.search(transferable_patterns['feature_fusion'], text2.lower()))
        has_adaptation = bool(re.search(transferable_patterns['adaptation_techniques'], text1.lower()) and 
                            re.search(transferable_patterns['adaptation_techniques'], text2.lower()))
        
        if has_attention and has_fusion:
            base_score += 0.25  # æ³¨æ„åŠ›+èåˆç»„åˆå¥–åŠ±å¢å¼º
        elif has_attention and has_adaptation:
            base_score += 0.2   # æ³¨æ„åŠ›+é€‚åº”ç»„åˆ
        elif has_fusion and has_adaptation:
            base_score += 0.18  # èåˆ+é€‚åº”ç»„åˆ
        
        return min(1.0, base_score)
    
    def _calculate_technical_keyword_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æŠ€æœ¯å…³é”®è¯ç›¸ä¼¼åº¦ - æ”¯æŒè·¨é¢†åŸŸæŠ€æœ¯è¿ç§»åˆ†æ"""
        import re
        
        # å®šä¹‰æŠ€æœ¯é¢†åŸŸå’Œæ–¹æ³•çš„å±‚æ¬¡åŒ–å…³é”®è¯
        tech_categories = {
            'deep_learning': {
                'neural', 'network', 'deep', 'learning', 'training', 'model', 'architecture',
                'layer', 'feature', 'embedding', 'representation', 'attention', 'transformer',
                'convolution', 'pooling', 'activation', 'optimization', 'gradient', 'backpropagation'
            },
            'computer_vision': {
                'segmentation', 'classification', 'detection', 'retrieval', 'recognition',
                'fusion', 'multi-modal', 'cross-modal', 'visual', 'image', 'video', 'depth',
                'stereo', 'reconstruction', 'estimation', 'matching', 'correspondence'
            },
            'domain_adaptation': {
                'domain', 'adaptation', 'transfer', 'adversarial', 'alignment', 'distribution',
                'source', 'target', 'unsupervised', 'supervised', 'semi-supervised', 'cross-domain'
            },
            'methodology': {
                'loss', 'function', 'optimization', 'training', 'learning', 'algorithm',
                'model', 'architecture', 'framework', 'approach', 'method', 'technique',
                'strategy', 'mechanism', 'module', 'component', 'layer', 'feature'
            },
            'data_processing': {
                'dataset', 'benchmark', 'annotation', 'label', 'ground', 'truth', 'synthetic',
                'preprocessing', 'postprocessing', 'filtering', 'sampling', 'selection'
            }
        }
        
        words1 = set(re.findall(r'\b\w{4,}\b', text1.lower()))
        words2 = set(re.findall(r'\b\w{4,}\b', text2.lower()))
        
        # è®¡ç®—å„ç±»åˆ«çš„ç›¸ä¼¼åº¦
        category_similarities = []
        for category, keywords in tech_categories.items():
            tech_words1 = words1.intersection(keywords)
            tech_words2 = words2.intersection(keywords)
            
            if tech_words1 and tech_words2:
                intersection = len(tech_words1.intersection(tech_words2))
                union = len(tech_words1.union(tech_words2))
                if union > 0:
                    category_sim = intersection / union
                    category_similarities.append((category, category_sim, len(tech_words1), len(tech_words2)))
        
        if not category_similarities:
            # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ¹é…ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è·¨é¢†åŸŸçš„æ–¹æ³•è®ºç›¸ä¼¼æ€§
            method_words1 = words1.intersection(tech_categories['methodology'])
            method_words2 = words2.intersection(tech_categories['methodology'])
            
            if method_words1 and method_words2:
                intersection = len(method_words1.intersection(method_words2))
                union = len(method_words1.union(method_words2))
                return 0.4 * (intersection / union) if union > 0 else 0.0  # æå‡æ–¹æ³•è®ºæƒé‡
            
            # æ£€æŸ¥æ·±åº¦å­¦ä¹ åŸºç¡€æŠ€æœ¯ç›¸ä¼¼æ€§
            dl_words1 = words1.intersection(tech_categories['deep_learning'])
            dl_words2 = words2.intersection(tech_categories['deep_learning'])
            
            if dl_words1 and dl_words2:
                intersection = len(dl_words1.intersection(dl_words2))
                union = len(dl_words1.union(dl_words2))
                return 0.25 * (intersection / union) if union > 0 else 0.0
            
            return 0.0
        
        # åŠ æƒè®¡ç®—ï¼šæ·±åº¦å­¦ä¹ å’Œæ–¹æ³•è®ºæƒé‡æ›´é«˜ï¼ˆæ”¯æŒè·¨é¢†åŸŸè¿ç§»ï¼‰
        weights = {
            'deep_learning': 0.3,
            'computer_vision': 0.25,
            'domain_adaptation': 0.25,
            'methodology': 0.15,
            'data_processing': 0.05
        }
        
        weighted_sim = sum(
            weights.get(category, 0.1) * sim 
            for category, sim, _, _ in category_similarities
        )
        
        # å¦‚æœæœ‰å¤šä¸ªç±»åˆ«åŒ¹é…ï¼Œç»™äºˆå¥–åŠ±
        diversity_bonus = min(0.2, 0.05 * len(category_similarities))
        
        return min(1.0, weighted_sim + diversity_bonus)
    
    def _calculate_solution_rq_similarity(self, solution_node: Dict, rq_node: Dict) -> float:
        """è®¡ç®—Solutionå’ŒResearch QuestionèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦"""
        # Solutionå†…å®¹ vs ç ”ç©¶é—®é¢˜
        solution_content = str(solution_node.get('solution', '') or solution_node.get('simplified_solution', ''))
        rq_content = str(rq_node.get('research_question', '') or rq_node.get('simplified_research_question', ''))
        
        # ç›´æ¥æ–‡æœ¬ç›¸ä¼¼åº¦ä½œä¸ºä¸»è¦æŒ‡æ ‡
        return self._calculate_text_similarity(solution_content, rq_content)
    
    def _calculate_paper_rq_similarity(self, paper_node: Dict, rq_node: Dict) -> float:
        """è®¡ç®—Paperå’ŒResearch QuestionèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦"""
        # Paperæ ¸å¿ƒé—®é¢˜ vs ç ”ç©¶é—®é¢˜
        paper_problem = str(paper_node.get('core_problem', '') or paper_node.get('abstract', ''))
        rq_content = str(rq_node.get('research_question', '') or rq_node.get('simplified_research_question', ''))
        
        return self._calculate_text_similarity(paper_problem, rq_content)
    
    def _calculate_same_type_similarity(self, node1: Dict, node2: Dict) -> float:
        """è®¡ç®—åŒç±»å‹èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦"""
        node_type = node1.get('entity_type', '')
        
        if node_type == 'solution':
            content1 = str(node1.get('solution', '') or node1.get('simplified_solution', ''))
            content2 = str(node2.get('solution', '') or node2.get('simplified_solution', ''))
        elif node_type == 'paper':
            content1 = str(node1.get('abstract', '') or node1.get('title', ''))
            content2 = str(node2.get('abstract', '') or node2.get('title', ''))
        elif node_type == 'research_question':
            content1 = str(node1.get('research_question', '') or node1.get('simplified_research_question', ''))
            content2 = str(node2.get('research_question', '') or node2.get('simplified_research_question', ''))
        else:
            return 0.1
        
        return self._calculate_text_similarity(content1, content2)
    
    def _calculate_domain_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—æŠ€æœ¯é¢†åŸŸç›¸ä¼¼åº¦"""
        # æå–æŠ€æœ¯é¢†åŸŸå…³é”®è¯
        domain_keywords = [
            'domain', 'adaptation', 'transfer', 'learning', 'network', 'neural',
            'attention', 'fusion', 'alignment', 'adversarial', 'contrastive',
            'segmentation', 'classification', 'detection', 'retrieval', 'recognition'
        ]
        
        import re
        words1 = set(re.findall(r'\b\w+\b', name1.lower()))
        words2 = set(re.findall(r'\b\w+\b', name2.lower()))
        
        # è®¡ç®—é¢†åŸŸå…³é”®è¯äº¤é›†
        domain_words1 = words1.intersection(set(domain_keywords))
        domain_words2 = words2.intersection(set(domain_keywords))
        
        if not domain_words1 or not domain_words2:
            return 0.0
        
        intersection = len(domain_words1.intersection(domain_words2))
        union = len(domain_words1.union(domain_words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_method_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–¹æ³•è®ºç›¸ä¼¼åº¦"""
        # æ–¹æ³•è®ºå…³é”®è¯
        method_keywords = [
            'loss', 'function', 'optimization', 'training', 'learning', 'algorithm',
            'model', 'architecture', 'framework', 'approach', 'method', 'technique',
            'strategy', 'mechanism', 'module', 'component', 'layer', 'feature'
        ]
        
        import re
        words1 = set(re.findall(r'\b\w+\b', text1.lower()))
        words2 = set(re.findall(r'\b\w+\b', text2.lower()))
        
        method_words1 = words1.intersection(set(method_keywords))
        method_words2 = words2.intersection(set(method_keywords))
        
        if not method_words1 or not method_words2:
            return 0.0
        
        intersection = len(method_words1.intersection(method_words2))
        union = len(method_words1.union(method_words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        import re
        # æå–å…³é”®è¯
        words1 = set(re.findall(r'\b\w{3,}\b', text1.lower()))  # åªè€ƒè™‘é•¿åº¦>=3çš„å•è¯
        words2 = set(re.findall(r'\b\w{3,}\b', text2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_type_compatibility(self, node1: Dict, node2: Dict) -> float:
        """è®¡ç®—èŠ‚ç‚¹ç±»å‹å…¼å®¹æ€§"""
        type1 = node1.get('entity_type', '')
        type2 = node2.get('entity_type', '')
        
        # å®šä¹‰ç±»å‹å…¼å®¹æ€§çŸ©é˜µ
        compatibility_matrix = {
            ('solution', 'paper'): 0.9,  # solutionå’Œpaperé«˜åº¦å…¼å®¹
            ('paper', 'solution'): 0.9,
            ('solution', 'research_question'): 0.7,
            ('research_question', 'solution'): 0.7,
            ('paper', 'research_question'): 0.6,
            ('research_question', 'paper'): 0.6,
            ('solution', 'solution'): 0.5,  # åŒç±»å‹ä¸­ç­‰å…¼å®¹
            ('paper', 'paper'): 0.3,
            ('research_question', 'research_question'): 0.4
        }
        
        return compatibility_matrix.get((type1, type2), 0.1)  # é»˜è®¤ä½å…¼å®¹æ€§
    
    async def find_cross_paper_links(self, progress_callback=None) -> List[Dict]:
        """
        æŸ¥æ‰¾è·¨è®ºæ–‡å…³è”å¹¶ç”Ÿæˆæ–°çš„è¾¹
        
        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ–°ç”Ÿæˆçš„è¾¹åˆ—è¡¨
        """
        logger.info("å¼€å§‹æŸ¥æ‰¾è·¨è®ºæ–‡å…³è”...")
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        graph = self.rag.chunk_entity_relation_graph
        all_labels = await graph.get_all_labels()
        
        # æŒ‰è®ºæ–‡åˆ†ç»„èŠ‚ç‚¹
        paper_nodes = defaultdict(list)
        all_nodes = []
        
        # ğŸ” è°ƒè¯•ï¼šè®°å½•åˆ†æè¿‡ç¨‹
        debug_samples = []
        empty_entity_names = 0
        
        for i, label in enumerate(all_labels):
            node_data = await graph.get_node(label)
            if node_data:
                # æå–è®ºæ–‡ID
                entity_name = node_data.get('entity_name', '')
                entity_type = node_data.get('entity_type', 'unknown')
                
                # ğŸ” è°ƒè¯•ï¼šè®°å½•å‰20ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
                if i < 20:
                    debug_samples.append({
                        'index': i,
                        'label': label,
                        'entity_name': entity_name,
                        'entity_type': entity_type,
                        'all_keys': list(node_data.keys())
                    })
                
                # ğŸ” è°ƒè¯•ï¼šç»Ÿè®¡ç©ºentity_name
                if not entity_name.strip():
                    empty_entity_names += 1
                    if empty_entity_names <= 5:  # åªè®°å½•å‰5ä¸ªç©ºçš„
                        logger.warning(f"å‘ç°ç©ºentity_name: label={label}, keys={list(node_data.keys())}")
                
                # æå–è®ºæ–‡IDï¼šç§»é™¤_RQ_Xå’Œ_SOL_Xåç¼€
                if '_RQ_' in entity_name:
                    paper_id = entity_name.split('_RQ_')[0]
                elif '_SOL_' in entity_name:
                    paper_id = entity_name.split('_SOL_')[0]
                else:
                    # å¯¹äºpaperèŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨entity_nameä½œä¸ºpaper_id
                    paper_id = entity_name
                
                paper_nodes[paper_id].append({
                    'label': label,
                    'data': node_data,
                    'paper_id': paper_id
                })
                all_nodes.append({
                    'label': label,
                    'data': node_data,
                    'paper_id': paper_id
                })
        
        # ğŸ” è°ƒè¯•ï¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯
        logger.info(f"æ‰¾åˆ° {len(all_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œåˆ†å¸ƒåœ¨ {len(paper_nodes)} ç¯‡è®ºæ–‡ä¸­")
        logger.info(f"ç©ºentity_nameæ•°é‡: {empty_entity_names}")
        
        # ğŸ” è°ƒè¯•ï¼šè¾“å‡ºå‰20ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
        logger.info("å‰20ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯:")
        for sample in debug_samples:
            logger.info(f"  {sample['index']:2d}. {sample['entity_type']:15s} | entity_name: {sample['entity_name'][:50]}... | keys: {sample['all_keys']}")
        
        # ğŸ” è°ƒè¯•ï¼šè¾“å‡ºå‰10ä¸ªè®ºæ–‡ç»„çš„ä¿¡æ¯
        logger.info(f"å‰10ä¸ªè®ºæ–‡ç»„çš„ä¿¡æ¯:")
        for i, (paper_id, nodes) in enumerate(list(paper_nodes.items())[:10], 1):
            entity_types = [n['data'].get('entity_type', 'unknown') for n in nodes]
            type_counts = {t: entity_types.count(t) for t in set(entity_types)}
            logger.info(f"  {i:2d}. {paper_id[:50]}... ({len(nodes)} ä¸ªèŠ‚ç‚¹: {type_counts})")
        
        # é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„è¯­ä¹‰å‘é‡
        logger.info("é¢„è®¡ç®—è¯­ä¹‰å‘é‡...")
        node_vectors = {}
        vector_failures = 0
        
        for i, node_info in enumerate(all_nodes):
            if progress_callback:
                progress_callback(f"è®¡ç®—å‘é‡", i, len(all_nodes))
            
            vector = await self.get_node_vector(node_info['data'])
            if vector is not None:
                node_vectors[node_info['label']] = vector
            else:
                vector_failures += 1
                # ğŸ” è°ƒè¯•ï¼šè®°å½•å‰10ä¸ªå‘é‡è®¡ç®—å¤±è´¥çš„èŠ‚ç‚¹
                if vector_failures <= 10:
                    entity_type = node_info['data'].get('entity_type', 'unknown')
                    entity_name = node_info['data'].get('entity_name', '')[:50]
                    logger.warning(f"å‘é‡è®¡ç®—å¤±è´¥ {vector_failures}: {entity_type} - {entity_name}...")
        
        logger.info(f"å‘é‡è®¡ç®—å®Œæˆ: æˆåŠŸ {len(node_vectors)} ä¸ªï¼Œå¤±è´¥ {vector_failures} ä¸ª")
        
        # ä¿å­˜å‘é‡ç¼“å­˜
        self._save_vector_cache()
        
        # ğŸš€ ä¼˜åŒ–ï¼šé¢„å…ˆåˆ†ç¦»Solutionå’ŒPaperèŠ‚ç‚¹ï¼Œé¿å…æ— æ•ˆæ¯”è¾ƒ
        logger.info("é¢„å¤„ç†èŠ‚ç‚¹ç±»å‹...")
        solution_nodes = []  # æ‰€æœ‰solutionèŠ‚ç‚¹
        paper_nodes_list = []  # æ‰€æœ‰paperèŠ‚ç‚¹
        
        for paper_id, nodes in paper_nodes.items():
            for node_info in nodes:
                entity_type = node_info['data'].get('entity_type', 'unknown')
                if entity_type == 'solution':
                    solution_nodes.append(node_info)
                elif entity_type == 'paper':
                    paper_nodes_list.append(node_info)
        
        logger.info(f"æ‰¾åˆ° {len(solution_nodes)} ä¸ªsolutionèŠ‚ç‚¹ï¼Œ{len(paper_nodes_list)} ä¸ªpaperèŠ‚ç‚¹")
        
        # ğŸš¨ ç´§æ€¥æ£€æŸ¥ï¼šå¦‚æœæ¯”è¾ƒæ¬¡æ•°è¿˜æ˜¯å¤ªå¤§ï¼Œé™åˆ¶æ•°é‡
        if len(solution_nodes) * len(paper_nodes_list) > 50000:
            logger.warning(f"âš ï¸ æ¯”è¾ƒæ¬¡æ•°è¿‡å¤§ ({len(solution_nodes)} Ã— {len(paper_nodes_list)} = {len(solution_nodes) * len(paper_nodes_list)})ï¼Œé™åˆ¶ä¸ºå‰50ä¸ªsolutionèŠ‚ç‚¹")
            solution_nodes = solution_nodes[:50]
        
        # ğŸ¯ åªè®¡ç®—Solution -> Paperçš„è·¨è®ºæ–‡ç›¸ä¼¼åº¦
        logger.info("è®¡ç®—è·¨è®ºæ–‡ç›¸ä¼¼åº¦...")
        new_edges = []
        
        # æ€»æ¯”è¾ƒæ¬¡æ•°å¤§å¹…å‡å°‘
        total_comparisons = len(solution_nodes) * len(paper_nodes_list)
        logger.info(f"æ€»è®¡ç®—é‡ï¼š{total_comparisons} æ¬¡æ¯”è¾ƒï¼ˆä¼˜åŒ–åï¼‰")
        
        current_comparison = 0
        
        for sol_info in solution_nodes:
            for paper_info in paper_nodes_list:
                current_comparison += 1
                
                if progress_callback and current_comparison % 100 == 0:  # å‡å°‘è¿›åº¦æ›´æ–°é¢‘ç‡
                    progress_callback(f"è®¡ç®—ç›¸ä¼¼åº¦", current_comparison, total_comparisons)
                
                # ç¡®ä¿æ˜¯è·¨è®ºæ–‡è¿æ¥ï¼ˆä¸åŒpaper_idï¼‰
                if sol_info['paper_id'] == paper_info['paper_id']:
                    continue
                
                label1, data1 = sol_info['label'], sol_info['data']
                label2, data2 = paper_info['label'], paper_info['data']
                
                vector1 = node_vectors.get(label1)
                vector2 = node_vectors.get(label2)
                
                if vector1 is not None and vector2 is not None:
                    # ğŸ”§ åŸºäºèŠ‚ç‚¹å±æ€§çš„ç§‘å­¦ç›¸ä¼¼åº¦è®¡ç®—
                    try:
                        # ä½¿ç”¨çº¯ç²¹çš„èŠ‚ç‚¹å±æ€§æŒ‡æ ‡ï¼Œé¿å…LLMè°ƒç”¨
                        similarity = self.calculate_comprehensive_node_similarity(data1, data2)
                        
                        # æ„å»ºåŸºäºå±æ€§çš„è¯æ®
                        evidence_for_edge = self.build_attribute_based_evidence(data1, data2, similarity)
                    except Exception as e:
                        logger.warning(f"å±æ€§ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦")
                        similarity = self._calculate_semantic_similarity(vector1, vector2)
                        evidence_for_edge = {"error_fallback": str(e), "method": "semantic_only"}
                    
                    # ğŸ” è°ƒè¯•ï¼šè®°å½•å‰10ä¸ªç›¸ä¼¼åº¦è®¡ç®—
                    if current_comparison <= 10:
                        name1 = data1.get('entity_name', '')[:40]
                        name2 = data2.get('entity_name', '')[:40]
                        logger.info(f"Solution->Paperç›¸ä¼¼åº¦ {current_comparison}: {similarity:.4f}")
                        logger.info(f"  Solution: {name1}...")
                        logger.info(f"  Paper: {name2}...")
                    
                    # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„é˜ˆå€¼é€»è¾‘
                    effective_threshold = self.similarity_threshold
                    
                    if similarity >= effective_threshold:
                        # ğŸ”§ æ ¹æ®æ˜¯å¦æœ‰æŠ€æœ¯è¯æ®ç”Ÿæˆä¸åŒçš„æè¿°
                        if "best_opportunity" in evidence_for_edge:
                            # åŸºäºæŠ€æœ¯è¯æ®çš„æè¿° - ä½¿ç”¨LLMç”ŸæˆçœŸå®åˆ†æ
                            best_opportunity = evidence_for_edge["best_opportunity"]
                            try:
                                # ğŸ†• ä½¿ç”¨LLMç”ŸæˆåŸºäºçœŸå®èŠ‚ç‚¹å±æ€§çš„åˆ†æ
                                edge_description = await self.technical_extractor.generate_llm_based_evidence(best_opportunity)
                            except Exception as e:
                                logger.warning(f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æè¿°: {e}")
                                edge_description = self.technical_extractor.generate_enhanced_edge_description(best_opportunity)
                            
                            new_edge = {
                                "source": label1,
                                "target": label2,
                                "relationship": "cross_paper_similarity",
                                "similarity_score": similarity,
                                "description": edge_description,
                                "edge_type": "cross_paper",
                                "source_type": "solution",
                                "target_type": "paper",
                                "created_at": datetime.now().isoformat(),
                                # ğŸ”§ å…³é”®ï¼šä¿å­˜è¯¦ç»†çš„æŠ€æœ¯è¯æ®
                                "technical_evidence": {
                                    "adaptation_evidence": evidence_for_edge["adaptation_evidence"],
                                    "adaptation_mechanism": evidence_for_edge["adaptation_mechanism"],
                                    "validation_suggestion": evidence_for_edge["validation_suggestion"],
                                    "source_component_name": best_opportunity.source_component.name,
                                    "source_component_type": best_opportunity.source_component.type,
                                    "target_challenge_name": best_opportunity.target_challenge.name,
                                    "target_challenge_domain": best_opportunity.target_challenge.domain,
                                    "transfer_feasibility": best_opportunity.transfer_feasibility
                                }
                            }
                        else:
                            # ä¼ ç»Ÿæè¿° - åŸºäºçœŸå®èŠ‚ç‚¹å±æ€§ç”Ÿæˆ
                            try:
                                edge_description = await self.generate_diverse_semantic_hint(data1, data2, similarity)
                            except Exception as e:
                                logger.warning(f"ç”Ÿæˆè¾¹æè¿°å¤±è´¥: {e}")
                                edge_description = f"è·¨è®ºæ–‡å…³è” (ç›¸ä¼¼åº¦: {similarity:.3f})"
                            
                            new_edge = {
                                "source": label1,
                                "target": label2,
                                "relationship": "cross_paper_similarity",
                                "similarity_score": similarity,
                                "description": edge_description,
                                "edge_type": "cross_paper",
                                "source_type": "solution",
                                "target_type": "paper",
                                "created_at": datetime.now().isoformat(),
                                "technical_evidence": None
                            }
                        
                        new_edges.append(new_edge)
                        
                        # æ¯å‘ç°5ä¸ªå…³è”è®°å½•ä¸€æ¬¡è¿›åº¦
                        if len(new_edges) % 5 == 0 or len(new_edges) <= 10:
                            logger.info(f"âœ… å‘ç°Solution->Paperå…³è” #{len(new_edges)}: ç›¸ä¼¼åº¦ {similarity:.3f}")
                else:
                    # ğŸ” è°ƒè¯•ï¼šè®°å½•å‘é‡ç¼ºå¤±æƒ…å†µ
                    if current_comparison <= 5:
                        logger.warning(f"å‘é‡ç¼ºå¤± {current_comparison}: solution={vector1 is not None}, paper={vector2 is not None}")
        
        logger.info(f"ğŸ‰ è·¨è®ºæ–‡å…³è”å‘ç°å®Œæˆï¼æ€»å…±å‘ç° {len(new_edges)} ä¸ªè·¨è®ºæ–‡å…³è”")
        
        # æ·»åŠ æ€§èƒ½ç»Ÿè®¡
        if new_edges:
            similarities = [edge['similarity_score'] for edge in new_edges]
            logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡: æœ€é«˜ {max(similarities):.3f}, æœ€ä½ {min(similarities):.3f}, å¹³å‡ {sum(similarities)/len(similarities):.3f}")
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘å†—ä½™æ—¥å¿—è¾“å‡º
        
        return new_edges
    
    async def add_cross_paper_edges_to_graph(self, new_edges: List[Dict]) -> int:
        """
        å°†æ–°çš„è·¨è®ºæ–‡è¾¹æ·»åŠ åˆ°LightRAGå›¾ä¸­
        
        Args:
            new_edges: æ–°è¾¹åˆ—è¡¨
            
        Returns:
            æˆåŠŸæ·»åŠ çš„è¾¹æ•°é‡
        """
        if not new_edges:
            return 0
        
        logger.info(f"å¼€å§‹å°† {len(new_edges)} æ¡è·¨è®ºæ–‡è¾¹æ·»åŠ åˆ°å›¾ä¸­...")
        
        graph = self.rag.chunk_entity_relation_graph
        added_count = 0

        # å¤§å¹…é™ä½å¹¶å‘æ•°ï¼Œé¿å…ç³»ç»Ÿå¡é¡¿
        sem = asyncio.Semaphore(10)  # ä»200é™åˆ°10

        async def _upsert_one(edge_data: Dict) -> int:
            try:
                async with sem:
                    existing_edge = await graph.get_edge(edge_data["source"], edge_data["target"])
                    if existing_edge is None:
                        # è¾¹å±æ€§ä¿ç•™å…¨éƒ¨ç»†ç²’åº¦å­—æ®µ
                        edge_props = {
                            "relationship": edge_data.get("relationship"),
                            "similarity_score": edge_data.get("similarity_score"),
                            "description": edge_data.get("description"),
                            "edge_type": edge_data.get("edge_type"),
                            "created_at": edge_data.get("created_at"),
                            "keywords": f"è·¨è®ºæ–‡å…³è”,ç›¸ä¼¼åº¦:{edge_data.get('similarity_score', 0):.2f}",
                        }
                        # é€ä¼ é¢å¤–å±æ€§ï¼ˆè‹¥åç»­æ‰©å±•ï¼‰
                        for k, v in edge_data.items():
                            if k not in edge_props and k not in {"source", "target"} and v is not None:
                                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœå€¼ä¸ºå­—å…¸ï¼Œåˆ™åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²
                                if isinstance(v, dict):
                                    edge_props[k] = json.dumps(v, ensure_ascii=False)
                                else:
                                    edge_props[k] = v
                        await graph.upsert_edge(
                            source_node_id=edge_data["source"],
                            target_node_id=edge_data["target"],
                            edge_data=edge_props,
                        )
                        return 1
                    return 0
            except Exception as ex:
                source = edge_data.get('source', 'unknown') if isinstance(edge_data, dict) else 'unknown'
                target = edge_data.get('target', 'unknown') if isinstance(edge_data, dict) else 'unknown'
                logger.error(f"æ·»åŠ è¾¹å¤±è´¥: {source} -> {target}, é”™è¯¯: {ex}")
                return 0

        # åˆ†æ‰¹ä¸²è¡Œå†™å…¥ï¼Œé¿å…å¹¶å‘è¿‡è½½å¯¼è‡´å¡é¡¿
        added_count = 0
        batch_size = 5  # æ¯æ‰¹å¤„ç†5æ¡è¾¹
        
        for i in range(0, len(new_edges), batch_size):
            batch = new_edges[i:i+batch_size]
            logger.info(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œå…± {len(batch)} æ¡è¾¹...")
            
            try:
                batch_results = await asyncio.gather(*[_upsert_one(e) for e in batch])
                batch_added = sum(batch_results)
                added_count += batch_added
                
                logger.info(f"ç¬¬ {i//batch_size + 1} æ‰¹å®Œæˆï¼Œæ·»åŠ  {batch_added} æ¡è¾¹")
                
                # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
                if i + batch_size < len(new_edges):
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"ç¬¬ {i//batch_size + 1} æ‰¹å¤„ç†å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸæ·»åŠ  {added_count} æ¡è·¨è®ºæ–‡è¾¹åˆ°å›¾ä¸­")
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¼‚æ­¥æŒä¹…åŒ–ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
        try:
            # ä½¿ç”¨éé˜»å¡æ–¹å¼æŒä¹…åŒ–
            asyncio.create_task(graph.index_done_callback())
            logger.info("è·¨è®ºæ–‡è¾¹æŒä¹…åŒ–ä»»åŠ¡å·²å¯åŠ¨ï¼ˆå¼‚æ­¥ï¼‰")
        except Exception as e:
            logger.warning(f"å¼‚æ­¥æŒä¹…åŒ–å¯åŠ¨å¤±è´¥ï¼Œå°†åœ¨ç¨‹åºç»“æŸæ—¶è‡ªåŠ¨ä¿å­˜: {e}")
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šè·³è¿‡å‘é‡åº“å†™å…¥ï¼Œé¿å…LightRAGå¼‚æ­¥é™æµå™¨é˜»å¡
        logger.info("æ€§èƒ½ä¼˜åŒ–ï¼šè·³è¿‡å‘é‡åº“å†™å…¥æ­¥éª¤ï¼Œé¿å…ç³»ç»Ÿé˜»å¡")
        # æ³¨é‡Šæ‰å‘é‡åº“å†™å…¥é€»è¾‘ï¼Œä¸“æ³¨äºå›¾ç»“æ„æ›´æ–°
        # try:
        #     # å‘é‡åº“å†™å…¥é€»è¾‘å·²ä¼˜åŒ–è·³è¿‡
        # except Exception as ex:
        #     logger.error(f"å†™å…¥å…³ç³»å‘é‡åº“å¤±è´¥: {ex}")

        return added_count
    
    def _generate_simple_description(self, source_node: Dict, target_node: Dict, similarity_score: float) -> str:
        """
        ç”Ÿæˆç®€åŒ–æè¿°ï¼Œé¿å…LLMè°ƒç”¨é˜»å¡
        """
        source_title = source_node.get('entity_name', '').replace('_', ' ')
        target_title = target_node.get('title', target_node.get('entity_name', '')).replace('_', ' ')
        
        # åŸºäºç›¸ä¼¼åº¦ç”Ÿæˆæè¿°
        if similarity_score >= 0.4:
            strength = "å¼ºç›¸å…³"
            desc = "å…·æœ‰æ˜¾è‘—çš„æŠ€æœ¯è¿ç§»æ½œåŠ›"
        elif similarity_score >= 0.3:
            strength = "ä¸­ç­‰ç›¸å…³"
            desc = "å­˜åœ¨ä¸€å®šçš„æŠ€æœ¯å€Ÿé‰´ä»·å€¼"
        else:
            strength = "å¼±ç›¸å…³"
            desc = "å¯èƒ½å­˜åœ¨è·¨é¢†åŸŸæŠ€æœ¯å¯å‘"
        
        return f"{source_title} ä¸ {target_title} ä¹‹é—´å­˜åœ¨{strength}æ€§ï¼ˆç›¸ä¼¼åº¦: {similarity_score:.3f}ï¼‰ï¼Œ{desc}ã€‚"
    
    async def generate_diverse_semantic_hint(self, source_node: Dict, target_node: Dict, 
                                     similarity_score: float, technical_evidence: Dict = None) -> str:
        """
        ç”Ÿæˆå¤šæ ·åŒ–çš„è¯­ä¹‰æç¤º - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        """
        # ç›´æ¥è¿”å›ç®€åŒ–æè¿°ï¼Œé¿å…LLMè°ƒç”¨é˜»å¡
        return self._generate_simple_description(source_node, target_node, similarity_score)

    def _generate_llm_prompt(self, source_title: str, target_title: str) -> str:
        """ç”ŸæˆLLMæç¤ºæ¨¡æ¿"""
        return f"""è§£å†³æ–¹æ¡ˆ: {source_title}
è®ºæ–‡: {target_title}

è¯·ç”¨ä¸€å¥è¯ç®€æ´æè¿°å®ƒä»¬ä¹‹é—´çš„æŠ€æœ¯å…³è”æ€§å’Œå¯å‘ä»·å€¼ã€‚æ ¼å¼ï¼šã€å…³è”ç±»å‹ã€‘å…·ä½“æè¿°"""

    async def _call_llm_for_description(self, source_node: Dict, target_node: Dict) -> str:
        """è°ƒç”¨LLMç”Ÿæˆæè¿°"""
        try:
            source_title = source_node.get('entity_name', 'Unknown')
            target_title = target_node.get('entity_name', 'Unknown')
            prompt = self._generate_llm_prompt(source_title, target_title)
            
            # ä½¿ç”¨LightRAGçš„LLMæ¥å£
            if hasattr(self.rag, 'llm_model_func') and self.rag.llm_model_func:
                response = await self.rag.llm_model_func(prompt)
                if response and len(response.strip()) > 10:
                    return response.strip()
            
            # é™çº§åˆ°åŸºäºå±æ€§çš„æè¿°
            return self._generate_simple_description(source_node, target_node, similarity_score)
            
        except Exception as e:
            logger.warning(f"LLMè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€åŒ–ç‰ˆæœ¬
            source_method = self._extract_method_name(source_node.get('solution', ''))
            target_title = target_node.get('title', 'æœªçŸ¥è®ºæ–‡')
            return f"ã€è·¨é¢†åŸŸå¯å‘ã€‘{source_method} â†’ {target_title}çš„åˆ›æ–°åº”ç”¨"
    
    def _select_template(self, similarity_score: float, technical_evidence: Dict = None) -> int:
        """é€‰æ‹©åˆé€‚çš„æ¨¡æ¿"""
        if technical_evidence and technical_evidence.get('transfer_feasibility', 0) > 0.7:
            return 0  # æŠ€æœ¯è¿ç§»
        elif similarity_score > 0.8:
            return 1  # æ–¹æ³•è®ºå€Ÿé‰´
        elif similarity_score > 0.6:
            return 2  # æ¶æ„è®¾è®¡
        elif similarity_score > 0.4:
            return 3  # ç†è®ºèåˆ
        else:
            return 4  # å®éªŒéªŒè¯
    
    def _extract_semantic_variables(self, source_node: Dict, target_node: Dict, technical_evidence: Dict = None) -> Dict[str, str]:
        """ä»èŠ‚ç‚¹ä¸­æå–è¯­ä¹‰å˜é‡"""
        
        # æå–æºæ–¹æ³•ä¿¡æ¯
        source_solution = source_node.get('solution', '') or source_node.get('simplified_solution', '')
        source_method = self._extract_method_name(source_solution)
        source_domain = self._infer_domain(source_node)
        
        # æå–ç›®æ ‡ä¿¡æ¯
        target_problem = target_node.get('core_problem', '') or target_node.get('basic_problem', '')
        target_domain = self._infer_domain(target_node)
        target_challenge = self._extract_main_challenge(target_problem)
        
        # åŸºäºæŠ€æœ¯è¯æ®æå–æ ¸å¿ƒæœºåˆ¶
        core_algorithm = "æœªçŸ¥ç®—æ³•"
        if technical_evidence:
            adaptation_mechanism = technical_evidence.get('adaptation_mechanism', '')
            if adaptation_mechanism:
                core_algorithm = adaptation_mechanism[:30]
        
        # ä»é¢„å®šä¹‰å˜é‡ä¸­éšæœºé€‰æ‹©ï¼Œä½†ä¿æŒä¸€å®šçš„ç¨³å®šæ€§
        seed = hash(source_method + target_domain) % 1000  # åŸºäºå†…å®¹çš„ä¼ªéšæœºç§å­
        random.seed(seed)
        
        return {
            'source_method': source_method,
            'source_domain': source_domain,
            'source_challenge': self._extract_main_challenge(source_node.get('research_question', '')),
            'target_domain': target_domain,
            'target_problem': target_node.get('title', 'æœªçŸ¥è®ºæ–‡'),
            'target_challenge': target_challenge,
            'core_algorithm': core_algorithm,
            'adaptation_strategy': random.choice(self.template_variables['adaptation_strategies']),
            'key_insight': self._extract_key_insight(source_solution),
            'innovation_angle': random.choice(self.template_variables['innovation_angles']),
            'architecture_component': 'æ¶æ„ç»„ä»¶',
            'modification_approach': 'æŠ€æœ¯æ”¹é€ ',
            'expected_benefit': 'æ€§èƒ½æå‡',
            'source_theory': f"{source_method}ç†è®º",
            'target_context': f"{target_domain}åœºæ™¯",
            'integration_mechanism': 'èåˆæœºåˆ¶',
            'theoretical_contribution': 'ç†è®ºè´¡çŒ®',
            'source_dataset': 'æºæ•°æ®é›†',
            'performance_metric': 'æ€§èƒ½æŒ‡æ ‡',
            'target_dataset': 'ç›®æ ‡æ•°æ®é›†',
            'validation_strategy': 'éªŒè¯ç­–ç•¥'
        }
    
    def _extract_method_name(self, solution_text: str) -> str:
        """ä»è§£å†³æ–¹æ¡ˆæ–‡æœ¬ä¸­æå–æ–¹æ³•å"""
        if not solution_text:
            return 'åˆ›æ–°æ–¹æ³•'
        
        # æŸ¥æ‰¾å¸¸è§çš„æ–¹æ³•åæ¨¡å¼
        patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:algorithm|method|approach|framework)',
            r'([A-Z]{2,}(?:\s+[A-Z]{2,})*)',  # ç¼©å†™æ–¹æ³•å
            r'(\w+(?:\s+\w+){0,2})\s+(?:mechanism|module|network)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, solution_text)
            if match:
                return match.group(1)
        
        # å›é€€ï¼šè¿”å›å‰å‡ ä¸ªå…³é”®è¯
        words = solution_text.split()[:3]
        return ' '.join(words) if words else 'åˆ›æ–°æ–¹æ³•'
    
    def _infer_domain(self, node: Dict) -> str:
        """æ¨æ–­èŠ‚ç‚¹æ‰€å±é¢†åŸŸ"""
        
        text = f"{node.get('title', '')} {node.get('abstract', '')} {node.get('core_problem', '')}"
        
        domain_keywords = {
            'è®¡ç®—æœºè§†è§‰': ['image', 'visual', 'detection', 'segmentation', 'face', 'object'],
            'è‡ªç„¶è¯­è¨€å¤„ç†': ['text', 'language', 'nlp', 'semantic', 'translation', 'embedding'],
            'æ—¶é—´åºåˆ—åˆ†æ': ['time series', 'temporal', 'sequence', 'forecasting', 'time'],
            'åŸŸé€‚åº”': ['domain adaptation', 'transfer', 'cross-domain', 'adaptation'],
            'æ·±åº¦å­¦ä¹ ': ['neural network', 'deep learning', 'cnn', 'rnn', 'transformer'],
            'æœºå™¨å­¦ä¹ ': ['learning', 'classification', 'regression', 'clustering']
        }
        
        text_lower = text.lower()
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return domain
        
        return 'é€šç”¨æœºå™¨å­¦ä¹ '
    
    def _extract_main_challenge(self, problem_text: str) -> str:
        """æå–ä¸»è¦æŒ‘æˆ˜"""
        if not problem_text:
            return 'æŠ€æœ¯éš¾é¢˜'
        
        # æŸ¥æ‰¾æŒ‘æˆ˜æ€§è¯æ±‡
        challenge_patterns = [
            r'challenge.*?(?:is|lies in|involves)\s+([^.]+)',
            r'difficulty.*?(?:is|lies in|involves)\s+([^.]+)', 
            r'problem.*?(?:is|lies in|involves)\s+([^.]+)'
        ]
        
        for pattern in challenge_patterns:
            match = re.search(pattern, problem_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()[:50]
        
        # å›é€€ï¼šè¿”å›å‰50ä¸ªå­—ç¬¦
        return problem_text[:50] + "..." if len(problem_text) > 50 else problem_text
    
    def _extract_key_insight(self, solution_text: str) -> str:
        """æå–å…³é”®æ´å¯Ÿ"""
        if not solution_text:
            return 'åˆ›æ–°æ€è·¯'
        
        # æŸ¥æ‰¾æ´å¯Ÿæ€§æè¿°
        words = solution_text.split()
        key_phrases = []
        
        for i, word in enumerate(words):
            if any(keyword in word.lower() for keyword in ['novel', 'new', 'innovative', 'unique']):
                # æå–è¯¥è¯å‘¨å›´çš„çŸ­è¯­
                start = max(0, i-2)
                end = min(len(words), i+3)
                key_phrases.append(' '.join(words[start:end]))
        
        if key_phrases:
            return key_phrases[0][:50]
        
        # å›é€€ï¼šè¿”å›å‰å‡ ä¸ªè¯
        return ' '.join(solution_text.split()[:5]) + 'ç­–ç•¥'


async def integrate_cross_paper_linking(rag_instance, 
                                      similarity_threshold: float = 0.50,
                                      progress_callback=None):
    """
    é›†æˆè·¨è®ºæ–‡å…³è”åŠŸèƒ½åˆ°LightRAGæµç¨‹ä¸­
    
    Args:
        rag_instance: LightRAGå®ä¾‹
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        æ·»åŠ çš„è¾¹æ•°é‡
    """
    
    # åˆ›å»ºè·¨è®ºæ–‡å…³è”å¼•æ“
    linking_engine = CrossPaperLinkingEngine(
        rag_instance=rag_instance,
        similarity_threshold=similarity_threshold
    )
    
    # æŸ¥æ‰¾è·¨è®ºæ–‡å…³è”
    new_edges = await linking_engine.find_cross_paper_links(progress_callback)
    
    # æ·»åŠ åˆ°å›¾ä¸­
    added_count = await linking_engine.add_cross_paper_edges_to_graph(new_edges)
    
    return added_count, new_edges
