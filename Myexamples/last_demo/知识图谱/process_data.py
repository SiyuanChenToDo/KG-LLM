#!/usr/bin/env python3
"""
è®ºæ–‡æ•°æ®é›†å¤„ç†ç¨‹åº
================

å¤„ç†äººå·¥æ ‡æ³¨çš„è®ºæ–‡Excelæ•°æ®ï¼Œæ„å»ºåŒ…å«ä¸‰ç§èŠ‚ç‚¹ç±»å‹çš„çŸ¥è¯†å›¾è°±ï¼š
1. paperèŠ‚ç‚¹ï¼šè®ºæ–‡ä¸»èŠ‚ç‚¹
2. research_questionèŠ‚ç‚¹ï¼šç ”ç©¶é—®é¢˜èŠ‚ç‚¹
3. solutionèŠ‚ç‚¹ï¼šè§£å†³æ–¹æ¡ˆèŠ‚ç‚¹

æ¯ç¯‡è®ºæ–‡å½¢æˆä¸€ä¸ªå­å›¾ï¼ˆæ ‘ç»“æ„ï¼‰ï¼špaper -> research_questions -> solutions

ä½œè€…ï¼šLightRAGå›¢é˜Ÿ
ç‰ˆæœ¬ï¼š2.0
"""

import pandas as pd
import json
import re
import os
from datetime import datetime

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if pd.isna(text) or text == "":
        return ""
    return str(text).strip()

def create_entity_name(base_name):
    """åˆ›å»ºå®ä½“åç§°ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿æŒå¯è¯»æ€§ï¼‰"""
    # ä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å’ŒåŸºæœ¬ç¬¦å·
    cleaned = re.sub(r'[^\w\u4e00-\u9fff\s\-_]', '', str(base_name))
    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ï¼Œä½†ä¿æŒå¯è¯»æ€§
    cleaned = re.sub(r'\s+', '_', cleaned)
    # ç§»é™¤å¼€å¤´ç»“å°¾çš„ä¸‹åˆ’çº¿
    cleaned = cleaned.strip('_')
    return cleaned

def get_safe_attribute(row, column_name, default=""):
    """å®‰å…¨è·å–Excelåˆ—å€¼"""
    try:
        value = row.get(column_name, default)
        return clean_text(value)
    except:
        return default

def process_papers_to_kg():
    """å¤„ç†è®ºæ–‡æ•°æ®ä¸ºçŸ¥è¯†å›¾è°±æ ¼å¼"""
    
    print("ğŸš€ å¼€å§‹å¤„ç†è®ºæ–‡æ•°æ®é›†...")
    
    # è¯»å–Excelæ–‡ä»¶
    #excel_path = "data/final_data.xlsx"
    #excel_path = "/root/autodl-tmp/LightRAG/data/test_data.xlsx"
    excel_path = "/root/autodl-tmp/LightRAG/data/final_data.xlsx"
    
    if not os.path.exists(excel_path):
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        return
    
    try:
        df = pd.read_excel(excel_path)
        print(f"âœ… æˆåŠŸè¯»å–Excelæ–‡ä»¶: {len(df)} ç¯‡è®ºæ–‡")
    except Exception as e:
        print(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç»“æ„
    knowledge_graph = {
        "entities": [],
        "relationships": [],
        "chunks": [],
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "total_papers": len(df),
            "node_types": ["paper", "research_question", "solution"],
            "description": "è®ºæ–‡ç ”ç©¶é—®é¢˜è§£å†³æ–¹æ¡ˆçŸ¥è¯†å›¾è°±"
        }
    }
    
    print("\nğŸ“Š å¼€å§‹å¤„ç†æ¯ç¯‡è®ºæ–‡...")
    
    # éå†æ¯ç¯‡è®ºæ–‡
    for idx, row in df.iterrows():
        paper_id = get_safe_attribute(row, "file_id", f"paper_{idx}")
        title = get_safe_attribute(row, "title")
        
        if not title:
            print(f"âš ï¸ è·³è¿‡ç¬¬{idx+1}è¡Œï¼šæ ‡é¢˜ä¸ºç©º")
            continue
            
        print(f"ğŸ“„ å¤„ç†è®ºæ–‡ {idx+1}/{len(df)}: {title[:50]}...")
        
        # åˆ›å»ºè®ºæ–‡å®ä½“åç§°
        paper_entity_name = create_entity_name(title)
        
        # 1. åˆ›å»ºè®ºæ–‡èŠ‚ç‚¹
        paper_entity = {
            "entity_name": paper_entity_name,
            "entity_type": "paper",
            "abstract": get_safe_attribute(row, "abstract"),
            "title": title,
            "authors": get_safe_attribute(row, "authors"),
            "year": get_safe_attribute(row, "year"),
            "conference": get_safe_attribute(row, "conference"),
            "venue": get_safe_attribute(row, "venue"),
            "citationCount": get_safe_attribute(row, "citationCount"),
            "core_problem": get_safe_attribute(row, "core_problem"),
            "related_work": get_safe_attribute(row, "related_work"),
            "preliminary_innovation_analysis": get_safe_attribute(row, "preliminary_innovation_analysis"),
            "basic_problem": get_safe_attribute(row, "basic_problem"),
            "datasets": get_safe_attribute(row, "datasets"),
            "experimental_results": get_safe_attribute(row, "experimental_results"),
            "evaluation_metrics": get_safe_attribute(row, "evaluation_metrics"),
            "framework_summary": get_safe_attribute(row, "framework_summary"),
            "source_id": paper_id,
            "file_path": f"papers/{paper_id}.pdf"
        }
        
        knowledge_graph["entities"].append(paper_entity)
        
        # 2. åˆ›å»ºæ–‡æ¡£å—
        chunk_content = f"""è®ºæ–‡æ ‡é¢˜: {title}
        ä½œè€…: {get_safe_attribute(row, 'authors')}
        å¹´ä»½: {get_safe_attribute(row, 'year')}
        ä¼šè®®: {get_safe_attribute(row, 'conference')}
        æ‘˜è¦: {get_safe_attribute(row, 'abstract')[:200]}..."""
        
        knowledge_graph["chunks"].append({
            "content": chunk_content,
            "source_id": paper_id,
            "file_path": f"papers/{paper_id}.pdf"
        })
        
        # 3. å¤„ç†ç ”ç©¶é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼ˆæœ€å¤š4ç»„ï¼‰
        research_questions_created = 0
        solutions_created = 0
        
        for i in range(1, 5):  # å¤„ç†research_question_1åˆ°research_question_4
            rq_text = get_safe_attribute(row, f"research_question_{i}")
            simplified_rq = get_safe_attribute(row, f"simplified_research_question_{i}")
            solution_text = get_safe_attribute(row, f"solution_{i}")
            simplified_solution = get_safe_attribute(row, f"simplified_solution_{i}")
            
            # åªæœ‰å½“ç ”ç©¶é—®é¢˜å­˜åœ¨æ—¶æ‰åˆ›å»ºèŠ‚ç‚¹
            if rq_text:
                research_questions_created += 1
                
                # åˆ›å»ºç ”ç©¶é—®é¢˜å®ä½“åç§°
                rq_entity_name = f"{paper_entity_name}_RQ_{i}"
                
                # åˆ›å»ºç ”ç©¶é—®é¢˜èŠ‚ç‚¹
                rq_entity = {
                    "entity_name": rq_entity_name,
                    "entity_type": "research_question",
                    "research_question": rq_text,
                    "simplified_research_question": simplified_rq,
                    "source_id": paper_id,
                    "file_path": f"papers/{paper_id}.pdf"
                }
                
                knowledge_graph["entities"].append(rq_entity)
                
                # åˆ›å»ºè®ºæ–‡->ç ”ç©¶é—®é¢˜çš„å…³ç³»
                paper_to_rq_relation = {
                    "src_id": paper_entity_name,
                    "tgt_id": rq_entity_name,
                    "description": f"è®ºæ–‡æå‡ºäº†ç ”ç©¶é—®é¢˜{i}",
                    "keywords": f"has_research_question_{i}",
                    "weight": 1.0,
                    "source_id": paper_id,
                    "file_path": f"papers/{paper_id}.pdf"
                }
                
                knowledge_graph["relationships"].append(paper_to_rq_relation)
                
                # 4. å¤„ç†å¯¹åº”çš„è§£å†³æ–¹æ¡ˆ
                if solution_text:
                    solutions_created += 1
                    
                    # åˆ›å»ºè§£å†³æ–¹æ¡ˆå®ä½“åç§°
                    sol_entity_name = f"{paper_entity_name}_SOL_{i}"
                    
                    # åˆ›å»ºè§£å†³æ–¹æ¡ˆèŠ‚ç‚¹
                    solution_entity = {
                        "entity_name": sol_entity_name,
                        "entity_type": "solution",
                        "solution": solution_text,
                        "simplified_solution": simplified_solution,
                        "source_id": paper_id,
                        "file_path": f"papers/{paper_id}.pdf"
                    }
                    
                    knowledge_graph["entities"].append(solution_entity)
                    
                    # åˆ›å»ºç ”ç©¶é—®é¢˜->è§£å†³æ–¹æ¡ˆçš„å…³ç³»
                    rq_to_sol_relation = {
                        "src_id": rq_entity_name,
                        "tgt_id": sol_entity_name,
                        "description": f"é’ˆå¯¹ç ”ç©¶é—®é¢˜{i}çš„è§£å†³æ–¹æ¡ˆ",
                        "keywords": f"solved_by_solution_{i}",
                        "weight": 1.0,
                        "source_id": paper_id,
                        "file_path": f"papers/{paper_id}.pdf"
                    }
                    
                    knowledge_graph["relationships"].append(rq_to_sol_relation)
        
        print(f"   âœ… åˆ›å»ºäº† {research_questions_created} ä¸ªç ”ç©¶é—®é¢˜å’Œ {solutions_created} ä¸ªè§£å†³æ–¹æ¡ˆ")
    
    # æ›´æ–°å…ƒæ•°æ®ç»Ÿè®¡
    knowledge_graph["metadata"].update({
        "total_entities": len(knowledge_graph["entities"]),
        "total_relationships": len(knowledge_graph["relationships"]),
        "total_chunks": len(knowledge_graph["chunks"]),
        "entity_types_count": {
            "paper": len([e for e in knowledge_graph["entities"] if e["entity_type"] == "paper"]),
            "research_question": len([e for e in knowledge_graph["entities"] if e["entity_type"] == "research_question"]),
            "solution": len([e for e in knowledge_graph["entities"] if e["entity_type"] == "solution"])
        }
    })
    
    return knowledge_graph

def save_knowledge_graph(kg_data, output_path="data/custom_kg_papers.json"):
    """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°JSONæ–‡ä»¶"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(kg_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {output_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False

def print_statistics(kg_data):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯:")
    print("=" * 50)
    
    metadata = kg_data.get("metadata", {})
    
    print(f"ğŸ“„ æ€»è®ºæ–‡æ•°: {metadata.get('total_papers', 0)}")
    print(f"ğŸ·ï¸ æ€»å®ä½“æ•°: {metadata.get('total_entities', 0)}")
    print(f"ğŸ”— æ€»å…³ç³»æ•°: {metadata.get('total_relationships', 0)}")
    print(f"ğŸ“ æ€»æ–‡æ¡£å—: {metadata.get('total_chunks', 0)}")
    
    print(f"\nğŸ“‹ å®ä½“ç±»å‹åˆ†å¸ƒ:")
    entity_counts = metadata.get('entity_types_count', {})
    for entity_type, count in entity_counts.items():
        print(f"   {entity_type}: {count} ä¸ª")
    
    print(f"\nğŸŒ³ å›¾ç»“æ„:")
    papers = entity_counts.get('paper', 0)
    rqs = entity_counts.get('research_question', 0)
    sols = entity_counts.get('solution', 0)
    
    print(f"   æ¯ç¯‡è®ºæ–‡å¹³å‡ç ”ç©¶é—®é¢˜æ•°: {rqs/papers:.1f}" if papers > 0 else "   æ— æ³•è®¡ç®—å¹³å‡ç ”ç©¶é—®é¢˜æ•°")
    print(f"   æ¯ç¯‡è®ºæ–‡å¹³å‡è§£å†³æ–¹æ¡ˆæ•°: {sols/papers:.1f}" if papers > 0 else "   æ— æ³•è®¡ç®—å¹³å‡è§£å†³æ–¹æ¡ˆæ•°")

def validate_knowledge_graph(kg_data):
    """éªŒè¯çŸ¥è¯†å›¾è°±æ•°æ®"""
    print("\nğŸ” éªŒè¯çŸ¥è¯†å›¾è°±æ•°æ®...")
    
    entities = kg_data.get("entities", [])
    relationships = kg_data.get("relationships", [])
    
    # éªŒè¯å®ä½“
    entity_names = set()
    for entity in entities:
        if "entity_name" not in entity or "entity_type" not in entity:
            print(f"âŒ å‘ç°æ— æ•ˆå®ä½“: {entity}")
            return False
        entity_names.add(entity["entity_name"])
    
    # éªŒè¯å…³ç³»
    for rel in relationships:
        if "src_id" not in rel or "tgt_id" not in rel:
            print(f"âŒ å‘ç°æ— æ•ˆå…³ç³»: {rel}")
            return False
        
        if rel["src_id"] not in entity_names:
            print(f"âŒ å…³ç³»æºå®ä½“ä¸å­˜åœ¨: {rel['src_id']}")
            return False
            
        if rel["tgt_id"] not in entity_names:
            print(f"âŒ å…³ç³»ç›®æ ‡å®ä½“ä¸å­˜åœ¨: {rel['tgt_id']}")
            return False
    
    print("âœ… çŸ¥è¯†å›¾è°±æ•°æ®éªŒè¯é€šè¿‡!")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ è®ºæ–‡æ•°æ®é›†å¤„ç†ç¨‹åº")
    print("=" * 60)
    
    # å¤„ç†æ•°æ®
    kg_data = process_papers_to_kg()
    
    if not kg_data:
        print("âŒ æ•°æ®å¤„ç†å¤±è´¥")
        return
    
    # éªŒè¯æ•°æ®
    if not validate_knowledge_graph(kg_data):
        print("âŒ æ•°æ®éªŒè¯å¤±è´¥")
        return
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_statistics(kg_data)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_path = "data/custom_kg_papers.json"
    if save_knowledge_graph(kg_data, output_path):
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨lightrag_ollama_demo.pyä¸­ä½¿ç”¨è¿™ä¸ªæ–‡ä»¶æ„å»ºçŸ¥è¯†å›¾è°±")
    else:
        print("âŒ ä¿å­˜å¤±è´¥")

if __name__ == "__main__":
    main()