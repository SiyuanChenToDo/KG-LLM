file_id,title,authors,year,conference,venue,citationCount,core_problem,related_work,preliminary_innovation_analysis,research_question_1,simplified_research_question_1,solution_1,simplified_solution_1,research_question_2,simplified_research_question_2,solution_2,simplified_solution_2,research_question_3,simplified_research_question_3,solution_3,simplified_solution_3,research_question_4,simplified_research_question_4,solution_4,simplified_solution_4,basic_problem,datasets,experimental_results,evaluation_metrics,framework_summary,abstract
15992,Attention-based Multi-Level Fusion Network for Light Field Depth Estimation,Jiaxin Chen; Shuo Zhang; Youfang Lin,2021,AAAI,AAAI Conference on Artificial Intelligence,60,"How can features from multiple light field views be effectively fused to accurately estimate depth, especially in challenging regions with occlusions, noise, and texture-less areas?","Depth estimation from Light Field (LF) images is a critical task for various LF applications. Traditional approaches for depth estimation from LFs often rely on photo-consistency of views or analysis of specific linear structures in Epipolar Plane Images (EPIs). Some methods construct cost volumes based on traditional stereo matching, such as those by Jeon et al. (2015) and Williem, Park, and Lee (2017). However, due to the narrow baseline of LFs, other approaches analyze the slope of lines in EPIs, like Wanner, Straehle, and Goldluecke (2014) and Zhang et al. (2016), or focusness in focal stacks (Lin et al. 2015; Tao et al. 2013). A common limitation of these local depth estimation methods is their sensitivity to occlusion, noise, and texture-less regions, often necessitating complex optimization methods (Wanner, Straehle, and Goldluecke 2014; Wang, Efros, and Ramamoorthi 2015) to achieve smooth depth maps, which also incur high computational costs due to depth space discretization.
Recently, learning-based methods have emerged for LF depth estimation. Some learning-based approaches, similar to traditional methods, are designed to learn the slope of lines in EPIs (Luo et al. 2017; Feng et al. 2018). These EPI-based methods, such as those by Luo et al. (2017), Feng et al. (2018), Leistner et al. (2019), and Li et al. (2020), primarily consider horizontal and vertical EPI characteristics, which can lead to insufficient information and reduced reliability due to a lack of global information constraints, often requiring subsequent optimization. Other learning-based methods directly explore correspondences among all views in LFs. For instance, Shi (2019) estimated disparity for all LF views using a fine-tuned FlowNet 2.0. Shin et al. (2018) introduced a multi-stream input structure that concatenates views from four directions to explore EPI information. Guo et al. (2020) proposed an occlusion-aware network by leveraging explicitly learned occlusion maps. While LFs provide abundant viewing angle information, many existing methods, such as Shin et al. (2018), tend to directly fuse all features together or apply a simple attention mechanism to the entire image (Yu-Ju et al. 2020). This often results in matching errors in complex areas with occlusions and less textures because they do not fully exploit the rich angular information to select suitable views for matching in different regions. Specifically, methods like LFattNet (Yu-Ju et al. 2020) assign the same weight to all pixels in one view, making it difficult to extract specific features for different occluded regions. These limitations highlight the challenge of effectively utilizing the wealth of viewing angle information provided by LFs, especially in the presence of occlusions and weak texture areas.","Module 1: Intra-Branch Feature Fusion - Features of views within a single angular branch are fused based on channel attention to select views with fewer occlusions.
Module 2: Inter-Branch Feature Fusion - Features from different angular branches are further fused using branch attention to select branches with less occlusions and richer textures.
Framework-level Innovation: Attention-based Multi-Level Fusion Network - An overall architecture that hierarchically combines intra-branch and inter-branch attention mechanisms to effectively fuse features from different views and angular directions for light field depth estimation.",How can features within a single light field branch be effectively fused to mitigate occlusion effects by prioritizing less occluded views?,Intra-branch light field feature fusion occlusion mitigation.,"The paper addresses this by proposing the Intra-Branch Fusion module, which uses a channel attention mechanism. When features of each view are extracted and shifted to form a cost volume (Fspp), the Intra-Branch Fusion module (HAttc) processes this cost volume. HAttc consists of a 3D global average pooling layer, three 1x1 convolutional layers, and a sigmoid layer. This module calculates a channel attention map (Attc) of size R3xHxW, representing the importance of the feature maps on the central view and two sides of the central view. Instead of estimating a weight for each of the M views in a branch, the M views are divided into two groups (sides relative to the central view), and two weights are estimated for these two groups. This constraint reduces learnable parameters and simplifies training. The calculated attention (Attc) is then multiplied element-wise with the cost volume (Fspp) to produce FAttc, effectively weighting the features within the branch to prioritize views or sides that are less likely to have occluded regions.","Intra-Branch Fusion module calculates channel attention (Attc) from cost volume (Fspp) using 3D global average pooling, 1x1 convolutions, sigmoid; Attc element-wise multiplied with Fspp to weight views, reducing occlusion impact.","How can features from multiple light field branches, representing different angular directions, be adaptively fused to prioritize information from branches with less occlusions and clearer correspondences?",Inter-branch light field feature fusion adaptive weighting.,"To adaptively fuse features from different angular branches, the paper introduces the Inter-Branch Fusion module, which employs a branch attention strategy. After the intra-branch fusion, four cost volumes (Fi Attc, where i represents the 0°, 90°, 45°, 135° branches) are obtained. To calculate the attention for each of these four branches, each Fi Attc first passes through three 3D convolutional layers (HAttb1) to produce Fi Attb1. These four Fi Attb1 features are then fused through corresponding point multiplication, allowing interaction between information of the same pixel across different branches. Subsequently, the attention maps (Attb) for each branch are generated by passing the fused features through several 2D convolutional layers and a sigmoid layer (HAttb2). Finally, the four intra-branch fused cost volumes (Fi Attc) are multiplied element-wise by their respective branch attentions (Attb i) to produce the final fused cost volume (FAttb). This process selectively merges information, ensuring that branches with more clear matching information contribute more significantly to the aggregated cost volume.","Inter-Branch Fusion module processes intra-branch cost volumes (Fi Attc) with 3D convolutions (HAttb1), point multiplication, then 2D convolutions and sigmoid (HAttb2) to generate branch attentions (Attb); Attb element-wise multiplied with Fi Attc for adaptive fusion.",How can a multi-level attention-based network architecture hierarchically integrate intra-branch and inter-branch fusion strategies to effectively process light field views for accurate depth estimation?,Multi-level attention light field depth network architecture.,"The paper proposes an Attention-based Multi-Level Fusion Network that hierarchically integrates intra-branch and inter-branch fusion strategies. The network begins with a four-branch structure, where M views in four different angular directions (0°, 45°, 90°, 135°) are grouped and fed into separate branches. First, a feature extraction module, utilizing 2D convolutions, residual blocks, and a Spatial Pyramid Pooling (SPP) module, extracts features from input views and constructs initial cost volumes (Fspp). These features are then subjected to a view shifting strategy to enlarge the receptive field and reduce large disparities. Next, the Intra-Branch Fusion module (as described in Solution 1) applies channel attention within each branch to fuse features, selecting views with fewer occlusions. Following this, the Inter-Branch Fusion module (as described in Solution 2) further fuses the cost volumes from the four branches using branch attention, prioritizing branches with less occlusions and richer textures. The resulting fused cost volume (FAttb) then undergoes Cost Volume Aggregation, which includes a widely used spatial attention strategy (HAtts) and eight 3D CNN layers (H3D) to further refine the information. Finally, a disparity regression step estimates the continuous disparity maps from the aggregated cost volume. This multi-level, hierarchical approach ensures that effective features are selected and fused both within and between angular branches, leading to more accurate depth estimation.","Multi-level network uses four branches; features extracted (SPP), shifted; Intra-Branch Fusion (channel attention) then Inter-Branch Fusion (branch attention) applied; cost volume aggregated (spatial attention, 3D CNNs); disparity regressed.",,,,,"Effectively fusing features from multiple light field views for accurate depth estimation presents significant challenges, particularly in selecting effective angular information from abundant data. This involves addressing how to fuse features *within* a single angular branch to mitigate the impact of occlusions by prioritizing less occluded views. Furthermore, it requires determining how to adaptively fuse features *between* different angular branches to leverage information from those with clearer correspondences and richer textures. Ultimately, the challenge lies in designing a multi-level attention-based network architecture that can hierarchically integrate these intra-branch and inter-branch fusion strategies to achieve state-of-the-art performance in light field depth estimation.","4D synthetic LF Dataset (Honauer et al. 2016):
Data type and domain: Synthetic Light Field images.
Composition or source: Contains images with 9x9 views and 512x512 spatial resolution.
Size or scale: 16 images in """"Additional"""" used for training, 8 images in """"Stratiﬁed"""" and """"Training"""" for validating, and 4 images in """"Test"""" for testing.
General application or purpose: Used for training, validating, and testing the model's performance on synthetic light field data, especially for quantitative evaluation with ground truth depth.
Real-world LFs (Wang, Efros, and Ramamoorthi 2015) captured by Lytro Illum cameras (Ng 2018):
Data type and domain: Real-world Light Field images.
Composition or source: Captured by Lytro Illum cameras, containing noise typical of real-world data.
Size or scale: Not specified beyond being """"real-world LFs"""".
General application or purpose: Used for qualitative evaluation and visual comparison, as ground truth depth is unavailable for these images.","The proposed method, named """"AttMLFNet"""", achieves state-of-the-art performance in light field depth estimation, demonstrated through both quantitative and qualitative evaluations.
Quantitative Evaluation (HCI 4D Light Field Benchmark):
- Overall Ranking: AttMLFNet ranks first on the commonly used HCI 4D Light Field Benchmark, achieving the highest accuracy on average across five different metrics (BadPix007, BadPix003, BadPix001, MSEx100, and Discontinuities). A screenshot from September 2020 confirms its top position.
- Comparison with State-of-the-Art Methods:
    - AttMLFNet consistently achieves the lowest errors (BadPix and MSE) in most scenes compared to SPO, Epinet-fcn-m, EPI-Shift, EPN+OS+GC, PS RF, and LFattNet.
    - Specific Scene Performance:
        - For images with many occlusions like """"Dino"""" and """"Sideboard"""", AttMLFNet shows significantly lower errors.
        - For """"Cotton"""" (unclear texture), """"Dots"""" (lots of noise), and """"Pyramids"""" (slanted surface), AttMLFNet clearly outperforms other approaches.
    - Discontinuity Regions: The method also outperforms others in discontinuity regions, as defined in the benchmark.
Ablation Study:
- BaseNet: A baseline network without intra-branch and inter-branch fusion (direct concatenation of cost volumes).
    - Avg BadPix(0.07): 4.010
    - Avg MSE: 1.926
- BattNet: Network with only inter-branch fusion based on branch attention.
    - Avg BadPix(0.07): 3.364 (much better than BaseNet)
    - Avg MSE: 1.561 (much better than BaseNet)
- Proposed (AttMLFNet): Full network with both intra-branch and inter-branch fusion.
    - Avg BadPix(0.07): 2.801 (further improvement over BattNet)
    - Avg MSE: 1.257 (further improvement over BattNet)
- Conclusion from Ablation: The inter-branch fusion effectively integrates features from different directions, and the intra-branch fusion further improves results by selecting more correct features within branches.
Qualitative Evaluation:
- Synthetic Scenes: Visual comparisons show that AttMLFNet produces sharper object boundaries and smoother surfaces compared to Epinet and LFattNet, especially in occluded, texture-less areas, and along object edges. This is attributed to its effective fusion of features inside and between the four branches for different regions.
- Real-World LFs (Lytro Illum): The estimated depth maps for real-world images also demonstrate better performance, particularly in edges of thin objects, and show more clear occlusion boundaries and smoother planes compared to LFattNet.
Attention Mechanism Effectiveness:
- An experiment analyzing average weight values of four branches in specific regions (R1: occlusion, R2: no occlusion) shows that in occluded regions (R1), the weight for the non-occluded direction (e.g., w45) is significantly higher than others. In non-occluded regions (R2), weights across branches are comparable.
- The variance (σ²) of weights in R1 is much larger than in R2, quantitatively demonstrating that the network successfully learns to differentiate and assign higher attention to effective information from less occluded branches.","BadPix(ε): Measures the percentage of wrongly estimated pixels whose errors exceed a threshold ε. It quantifies the proportion of pixels where the absolute difference between the estimated disparity and the ground truth disparity is greater than ε. The paper uses ε values of 0.07, 0.03, and 0.01.
Mean Square Error (MSE): Measures the average of the squares of the errors (the difference between estimated disparity and ground truth disparity). It provides a measure of the average magnitude of the errors. The paper reports MSE multiplied by 100 (MSEx100).
Discontinuities: A metric used in the HCI 4D Light Field Benchmark to evaluate performance in regions with depth discontinuities.","The proposed Attention-based Multi-Level Fusion Network (AttMLFNet) is designed to address the critical challenge of effectively fusing features from multiple light field views for accurate depth estimation, particularly in complex regions prone to occlusions, noise, and weak textures. The core motivation is to leverage the rich angular information available in Light Fields by adaptively selecting and combining features that provide the most reliable correspondence cues.
The framework employs a novel hierarchical fusion strategy built upon a four-branch structure, where views from four distinct angular directions (0°, 45°, 90°, 135°) are processed. The process begins with a feature extraction module that uses 2D convolutions, residual blocks, and a Spatial Pyramid Pooling (SPP) module to obtain multi-scale feature maps from each view, which are then transformed into initial cost volumes through a view shifting strategy. The key innovation lies in its two-level attention-based fusion mechanisms. First, an Intra-Branch Fusion module applies channel attention within each of the four branches. This module intelligently weights features from different sides of the central view within a branch, prioritizing those less affected by occlusions. Second, an Inter-Branch Fusion module further aggregates the already intra-branch fused cost volumes by applying branch attention. This mechanism learns to assign higher weights to branches that offer clearer correspondences and richer textures, effectively selecting the most reliable angular information across different directions. Finally, the hierarchically fused cost volume undergoes a Cost Volume Aggregation step, incorporating a spatial attention strategy and 3D CNN layers for further refinement, before a disparity regression step yields the final continuous depth map. This multi-level attention-driven approach allows the network to dynamically adapt to local scene characteristics, leading to more accurate depth estimations with sharper boundaries and smoother surfaces, especially in challenging areas where traditional methods or simpler fusion strategies often fail.","Depth estimation from Light Field (LF) images is a crucial basis for LF related applications. Since multiple views with abundant information are available, how to effectively fuse features of these views is a key point for accurate LF depth estimation. In this paper, we propose a novel attention-based multi-level fusion network. Combined with the four-branch structure, we design intra-branch fusion strategy and inter-branch fusion strategy to hierarchically fuse effective features from different views. By introducing the attention mechanism, features of views with less occlusions and richer textures are selected inside and between these branches to provide more effective information for depth estimation. The depth maps are finally estimated after further aggregation. Experimental results shows the proposed method achieves state-of-the-art performance in both quantitative and qualitative evaluation, which also ranks first in the commonly used HCI 4D Light Field Benchmark."
15997,Dual Distribution Alignment Network for Generalizable Person Re-Identification,Peixian Chen; Pingyang Dai; Jianzhuang Liu; Feng Zheng; Q. Tian; Rongrong Ji,2020,AAAI,AAAI Conference on Artificial Intelligence,47,"How can a generalizable Person Re-Identification (Re-ID) model be developed that performs well on unseen target domains without requiring model updates, effectively addressing both domain-wise variations and identity-wise similarities inherent in diverse Re-ID datasets?","Existing Person Re-Identification (Re-ID) methods, particularly those based on deep Convolutional Neural Networks (CNNs), have achieved significant performance in identifying individuals across different camera views. However, their success heavily relies on the independent and identically distributed (i.i.d.) assumption between training and test data, which is often prohibitively expensive to meet in real-world applications due to the need for extensive manual labeling and low-variation datasets. Consequently, models trained under supervised settings tend to overfit to their training data and exhibit poor performance on new, unseen datasets.
To mitigate this, Unsupervised Domain Adaptation (UDA) has been introduced in person Re-ID. UDA approaches typically learn a model by mixing data from a labeled source domain with an unlabeled target domain, thereby fitting the target domain's distribution without the cost of labeling. Various UDA techniques have been explored, including asymmetric multi-task dictionary learning for discriminative representation, and Generative Adversarial Networks (GANs) like CycleGAN for image translation or generating images with different camera styles to enforce invariance. More recent works have focused on improving domain alignment performance, such as transferable models for attribute-identity discriminative representation and clustering samples in the target domain for similarity measurement, sometimes extended to part-level features. A significant limitation of UDA methods, however, is their general requirement for a large amount of target data to avoid overfitting and achieve satisfactory results.
Domain Generalization (DG) offers an alternative, more practical real-world setting. DG aims to learn a generalizable model using multiple source domain datasets, expecting it to perform well in an unseen target domain without any subsequent model updating, adaptation, or retraining. An optimal DG model should learn a feature representation that is both discriminative for the task and insensitive to variations in domain distributions, thus eliminating the need for any data from the target domain during training. Unfortunately, most existing DG methods have been explicitly designed for classification tasks, which fundamentally differ from the retrieval task of Re-ID. In classification, DG typically handles the same set of labels across all source domains, whereas in retrieval, it must compare feature similarity between different identities, corresponding to different labels across source and target domains.
Attempts to apply DG in person Re-ID using existing techniques, such as meta-learning (e.g., DIMN) and normalization layers (e.g., Dual-Norm), have been made to learn discriminative features insensitive to domain variations. However, these approaches are often not specifically tailored for person Re-ID's unique challenges. The massive variation among Re-ID datasets means that directly applying classification-oriented DG methods with pairwise domain alignment can be detrimental to performance, as there is a known statistical trade-off between domain-invariance and classification accuracy. Moreover, these methods often naively mix all source data without associating similar samples from different domains, leading to scenarios where pedestrian features from the same domain might appear closer than visually similar ones from different domains, which contradicts real-world expectations.","Module 1: Domain-wise Adversarial Feature Learning (LDA) - This module introduces a novel adversarial learning scheme that aligns peripheral domains to a single, optimally chosen """"central domain"""" based on the minimization of Wasserstein distance. This approach selectively reduces domain discrepancy, aiming to minimize overall distributional shifts and avoid the negative impacts associated with pairwise alignment of highly divergent domains.
Module 2: Identity-wise Similarity Enhancement (LSE) - This module enhances inter-domain similarity of visually-similar samples. It achieves this by accumulating knowledge in an """"ID pool"""" using running mean representations for each identity and then minimizing the symmetric KL-divergence between new instance features and their top-k similar IDs from other domains (or the same domain if central). This mechanism explicitly forces visually similar individuals from different domains to be closer in the feature space.
Framework-level Innovation: Dual-level Constraints - The proposed Dual Distribution Alignment Network (DDAN) is innovative in its comprehensive approach to Person Re-ID Domain Generalization by simultaneously applying two distinct levels of constraints: domain-wise and identity-wise. This dual-level alignment strategy directly addresses the two fundamental challenges identified in DG for Person Re-ID (domain-wise variations and identity-wise similarities) in a coordinated manner, which is a novel integration compared to prior methods.","How can domain-wise variations be effectively reduced in Person Re-ID without compromising feature discriminability, especially when dealing with massive variations among datasets?",Domain-wise variation reduction Re-ID; Central-peripheral domain alignment,"The paper addresses the challenge of reducing domain-wise variations through a novel Domain-wise Adversarial Feature Learning (LDA) scheme. Instead of performing pairwise alignment between all source domains, which can lead to excessive distributional shifts and reduced feature discriminability, the method identifies a """"central domain"""" and aligns all """"peripheral domains"""" towards it. The central domain is defined as the one that minimizes the total Wasserstein distance (dWS) to all other source domains, quantifying the minimal distributional shift required for alignment. Wasserstein distance, approximated using Sinkhorn's method, is chosen for its symmetry and ability to handle non-overlapping distributions. Once the central domain is determined, a domain discriminator D is trained to distinguish whether an input feature belongs to the central or a peripheral domain using a cross-entropy loss (LDA-D). Concurrently, the feature mapping network M, acting as a generator, is trained to """"fool"""" the discriminator by minimizing the negative entropy of the predicted domain distributions (LDA-T). This adversarial process forces the mapping network to transform features from peripheral domains into a uniform distribution that is similar to the central domain, thereby learning domain-invariant features while minimizing the negative impact on feature discriminability by avoiding unnecessary shifts towards outlying domains.",Domain-wise adversarial learning; Central domain selection via Wasserstein distance; Peripheral-to-central alignment; Discriminator (LDA-D) and generator (LDA-T) training.,"How can identity-wise similarities be leveraged across different domains to learn more robust and generalizable features for Person Re-ID, ensuring visually similar individuals are correctly associated regardless of their source domain?",Identity-wise similarity enhancement; Cross-domain feature association,"To leverage identity-wise similarities and ensure visually similar individuals are correctly associated across domains, the paper proposes an Identity-wise Similarity Enhancement (LSE) module. This module maintains an """"ID pool"""" that accumulates the learned knowledge of all identities. Specifically, a running mean representation is computed for each identity (ID) in an iterative fashion, and these mean representations are further accumulated over training epochs to form a final effective representation for each ID. For each incoming image instance, the system searches for its top-k visually similar IDs within the ID pool. If the instance is from a peripheral domain, the search is conducted among IDs from *other* domains. If the instance is from the central domain, the search is conducted among IDs from the *same* domain to stabilize its distribution. To enforce similarity, the features of the incoming instance and the features of its top-k similar IDs from the ID pool are normalized using a softmax function with a temperature parameter (τ). The symmetric KL-divergence between these normalized feature distributions is then minimized (LSE loss). This process effectively reduces identity-wise domain-shifts by making visually similar samples, even from different domains, closer in the feature space than less similar ones, thereby forcing the network to learn more discriminative and domain-invariant features that align with real-world scenarios.",Identity-wise similarity enhancement; ID pool; Running mean representation; Top-k similar ID search; Softmax normalization; Symmetric KL-divergence minimization (LSE).,,,,,,,,,"Developing a generalizable Person Re-Identification (Re-ID) model for real-world applications presents significant challenges, as existing methods often overfit to training data and struggle with unseen target domains. This necessitates addressing how domain-wise variations can be effectively reduced without compromising feature discriminability, especially given the massive discrepancies among Re-ID datasets. Furthermore, it is crucial to determine how identity-wise similarities can be leveraged across different domains to learn more robust and generalizable features, ensuring visually similar individuals are correctly associated regardless of their source domain.","CUHK02 (Li and Wang 2013): Person Re-ID dataset. Used as a source dataset for training.
CUHK03 (Li et al. 2014): Person Re-ID dataset. Used as a source dataset for training.
Market-1501 (Zheng et al. 2015): Person Re-ID dataset. Used as a source dataset for training.
DukeMTMC-ReID (Zheng, Zheng, and Yang 2017): Person Re-ID dataset. Used as a source dataset for training.
CUHK-SYSU PersonSearch (Xiao et al. 2016): Person Re-ID dataset. Used as a source dataset for training.
*Composition/Source (for source datasets):* All images from these five datasets, regardless of their original train/test splits, are combined for training. This totals 121,765 images belonging to 18,530 unique identities.
VIPeR (Gray and Tao 2008): Person Re-ID dataset. Used as a target dataset for testing. It consists of 316 probe images and 316 gallery images in a single-shot setting.
PRID (Hirzer et al. 2011): Person Re-ID dataset. Used as a target dataset for testing. It consists of 100 probe images and 649 gallery images in a single-shot setting.
GRID (Loy, Xiang, and Gong 2010): Person Re-ID dataset. Used as a target dataset for testing. It consists of 125 probe images and 900 gallery images in a single-shot setting.
i-LIDS (Zheng, Gong, and Xiang 2009): Person Re-ID dataset. Used as a target dataset for testing. It consists of 60 probe images and 60 gallery images in a single-shot setting.
*General Application/Purpose (for all datasets):* All datasets are used for Person Re-Identification tasks, involving matching images of the same person captured by different cameras. The source datasets are used for training the generalizable model, while the target datasets are used to evaluate its performance on unseen domains.","The proposed Dual Distribution Alignment Network (DDAN) was evaluated on a large-scale Domain Generalization (DG) Re-ID benchmark, comparing its performance against various state-of-the-art supervised (S), unsupervised domain adaptation (U), and domain generalization (DG) methods.
Main Performance (Rank-1 Accuracy and mAP):
DDAN achieved state-of-the-art performance on the target datasets:
- VIPeR: Rank-1 accuracy of 56.5%, mAP of 60.8%.
- PRID: Rank-1 accuracy of 62.9%, mAP of 67.5%.
- GRID: Rank-1 accuracy of 50.6%, mAP of 50.9%.
- i-LIDS: Rank-1 accuracy of 78.5%, mAP of 81.2%.
Comparison with Baselines:
- Supervised Methods: While many supervised methods achieve high performance on large-scale datasets, they struggle with small-scale target datasets due to overfitting. DDAN, despite not using target data, achieves better performance than supervised methods like SSM and JLML on the small-scale target datasets.
- Unsupervised Domain Adaptation (UDA) Methods: DDAN outperforms all UDA methods (e.g., TJAIDL, MMFAN, Synthesis) listed in the comparison table. This highlights DDAN's advantage as it does not require unlabeled target data during training, unlike UDA approaches.
- Domain Generalization (DG) Methods:
    - DDAN consistently outperforms DIMN (Song et al. 2019) across all test datasets.
    - DDAN outperforms DualNorm (Jia et al. 2019) on GRID and i-LIDS. When DDAN is re-evaluated with the same stronger backbone (MobileNetV2) and normalization layers as DualNorm (referred to as DDAN+DualNorm), it outperforms DualNorm on all test datasets (e.g., 56.5% vs 53.9% Rank-1 on VIPeR, 62.9% vs 60.4% on PRID, 50.6% vs 46.2% on GRID, 78.5% vs 78.0% on i-LIDS).
Ablation Study:
An ablation study demonstrated the effectiveness of each component of DDAN:
- LIDE + LTriplet: The combination of Identity-Discriminative Embedding (LIDE) and Triplet loss significantly improved the effectiveness of learned representations compared to LIDE alone.
- LDA (Domain-wise Adversarial Feature Learning): Adding LDA to LIDE + LTriplet further improved performance, indicating its role in learning a more generalizable, domain-invariant feature space. For example, Rank-1 on VIPeR increased from 46.4% to 50.0%.
- LSE (Identity-wise Similarity Enhancement): Adding LSE to LIDE + LTriplet also improved performance, showing its effectiveness in enforcing a real-world distribution and reducing local domain shifts. For example, Rank-1 on VIPeR increased from 46.4% to 48.8%.
- Full DDAN (LIDE + LTriplet + LDA + LSE): The combination of all components yielded the best performance, confirming that all modules jointly contribute to learning discriminative, domain-invariant features that are insensitive to both domain-wise and identity-wise variations.
Analysis of Components:
- Domain-wise Adversarial Feature Learning: The analysis of Wasserstein distances between source domains confirmed that PersonSearch was the most """"generalizable"""" central domain, leading to the best performance when chosen as such. Using Duke as the central domain or employing pairwise alignment (All domains) resulted in significantly worse performance, validating the selective alignment strategy.
- Identity-wise Similarity Enhancement: Cosine similarity comparisons showed that LSE successfully made visually similar images from different domains closer in the feature space than non-similar images from the same domain, which was not the case without LSE. This confirms LSE's ability to correctly reflect real-world visual relationships.
- Visualization (t-SNE): t-SNE plots illustrated that the full DDAN (LDA + LSE) achieved the best alignment of feature distributions across different source domains, significantly reducing domain shift compared to baseline models.
Parameter Sensitivity:
- Temperature (τ) for Softmax in LSE: A temperature value of τ < 1 generally yielded better results, with τ = 2e-3 achieving the best performance.
- Number k of Similar IDs for LSE: Setting k=8 for the top-k similar IDs generally achieved the best Rank-1 accuracy and mAP across most datasets, indicating an optimal balance for capturing relevant similarities without introducing noise from too many non-similar examples.","Rank-1 Accuracy (R-1): Measures the percentage of queries for which the correct match is found at the top position (rank 1) in the gallery.
Rank-5 Accuracy (R-5): Measures the percentage of queries for which the correct match is found within the top 5 positions in the gallery.
Rank-10 Accuracy (R-10): Measures the percentage of queries for which the correct match is found within the top 10 positions in the gallery.
mean Average Precision (mAP): A comprehensive metric that considers both precision and recall, providing a single-number summary of the ranking quality across all queries. It is the mean of the average precision (AP) scores for each query, where AP is the area under the precision-recall curve.","The Dual Distribution Alignment Network (DDAN) is an end-to-end framework designed to address the fundamental challenges of domain-wise variations and identity-wise similarities in Generalizable Person Re-Identification (Re-ID). The overall design motivation is to learn domain-invariant and discriminative features from multiple source domains that can generalize effectively to unseen target domains without any further adaptation.
The framework comprises an encoder (E) and a feature mapping network (M) that together form the feature extractor (F). Beyond standard Re-ID constraints like Identity-Discriminative Embedding (IDE) and Triplet losses, DDAN introduces two core innovative modules operating at dual levels: Domain-wise Adversarial Feature Learning (LDA) and Identity-wise Similarity Enhancement (LSE).
The LDA module tackles domain-wise variations by adopting a novel adversarial alignment strategy. Instead of the common pairwise domain alignment, DDAN identifies a """"central domain"""" among the source datasets—the one that minimizes the total Wasserstein distance to all other domains. This central domain acts as a target distribution. A domain discriminator (D) is trained to distinguish between features from the central domain and those from """"peripheral domains."""" Simultaneously, the mapping network (M) is trained as a generator to fool this discriminator, effectively transforming features from peripheral domains to align with the central domain's distribution. This selective alignment minimizes unnecessary distributional shifts, preserving feature discriminability, especially when dealing with highly divergent Re-ID datasets.
The LSE module addresses identity-wise similarities by explicitly enhancing the resemblance between visually similar individuals across different domains. It maintains an """"ID pool"""" that stores running mean representations of all identities encountered during training. For each incoming image, the system identifies its top-k visually similar IDs from the ID pool, searching across domains for peripheral inputs and within the same domain for central inputs. The features of the incoming instance and its similar IDs are then normalized using softmax, and their symmetric KL-divergence is minimized. This mechanism ensures that visually similar pedestrians, regardless of their source domain, are mapped to closer points in the feature space, thereby reducing local domain shifts and forcing the network to learn more robust and identity-discriminative features.
These two modules interact synergistically: LDA provides a global domain alignment, while LSE refines this alignment at a finer, identity-specific level. The overall objective function combines the baseline Re-ID losses (IDE, Triplet) with the adversarial loss for domain alignment (LDA-T) and the loss for identity-wise similarity enhancement (LSE), along with the discriminator's loss (LDA-D). This unique combination of dual-level constraints allows DDAN to learn features that are both globally domain-invariant and locally identity-consistent, representing a significant advancement over prior approaches that often address these challenges in isolation or with less tailored strategies.","Domain generalization (DG) offers a preferable real-world setting for Person Re-Identification (Re-ID), which trains a model using multiple source domain datasets and expects it to perform well in an unseen target domain without any model updating. Unfortunately, most DG approaches are designed explicitly for classification tasks, which fundamentally differs from the retrieval task Re-ID. Moreover, existing applications of DG in Re-ID cannot correctly handle the massive variation among Re-ID datasets. In this paper, we identify two fundamental challenges in DG for Person Re-ID: domain-wise variations and identity-wise similarities. To this end, we propose an end-to-end Dual Distribution Alignment Network (DDAN) to learn domain-invariant features with dual-level constraints: the domain-wise adversarial feature learning and the identity-wise similarity enhancement. These constraints effectively reduce the domain-shift among multiple source domains further while agreeing to real-world scenarios. We evaluate our method in a large-scale DG Re-ID benchmark and compare it with various cutting-edge DG approaches. Quantitative results show that DDAN achieves state-of-the-art performance."
15999,Mind-the-Gap! Unsupervised Domain Adaptation for Text-Video Retrieval,Qingchao Chen; Yang Liu; Samuel Albanie,2021,AAAI,AAAI Conference on Artificial Intelligence,22,"How can a text-video retrieval system effectively generalize to new datasets that exhibit domain shift at query-time, without requiring corresponding text annotations in the target domain?","Text-video retrieval aims to rank videos based on their relevance to natural language queries. Significant advancements in this field have been observed on popular benchmarks, largely driven by the effective use of multi-modal cues and powerful pre-trained models. However, much of this progress has relied on the assumption that training and test data originate from the same domain. This dependency necessitates extensive data annotation and model retraining or fine-tuning when applying these methods to novel domains, posing a considerable practical challenge.
One approach to address this challenge is zero-shot text-video retrieval, which assumes no access to any training content from the target domain. While recent methods employing large-scale pretraining have shown early promise in this area, the zero-shot assumption is often overly restrictive for many real-world problems. Consequently, a less constrained formulation, Unsupervised Domain Adaptation (UDA), has gained attention. In UDA, a model has access to labeled data from a source domain but only unlabeled data from the target domain of interest. While video-based UDA scenarios have been explored for tasks like action recognition, unsupervised domain adaptation specifically for text-video retrieval has received limited attention in existing literature.
UDA techniques have been extensively researched to measure and reduce domain discrepancy. These include variants of Maximum Mean Discrepancy (MMD), adversarial learning, and transportation plan modeling. More recently, UDA methods have been extended to object detection and various forms of action recognition on video data. A related area is text-image retrieval, though existing work in this domain, such as that by Cao et al. (2018), often restricts query text to single words from predefined categories, unlike the open-set free-form sentence queries typical in text-video retrieval. Pseudo-labeling techniques have also been investigated in prior UDA work, particularly for image classification. However, these approaches typically rely on discrete class labels, which are not directly applicable to the video-text retrieval setting where discrete class boundaries are unavailable. A key challenge in adapting retrieval models using empirical risk minimization is the presence of two types of domain shift: video content/style shift and description distribution shift. The latter, a shift in description distribution, is a notable difference from other UDA applications, often driven by variations in annotation styles across different datasets.","Module: Concept Selector (C) - Maps generic video descriptors to a concept distribution, providing a common signal for both source and target domains.
Module: Hallucinator (H) - Generates predictions from joint text/video embeddings to match the concept selector's output, ensuring embeddings retain knowledge of external concepts.
Module: Concept Preservation Loss (LP) - A loss function that uses the Concept Selector and Hallucinator to enforce that joint space embeddings preserve knowledge from pre-trained models, enhancing transferability and making features domain-agnostic.
Module: Mutually-Exclusive Pseudo-Text Selection Mechanism - A novel pseudo-labeling algorithm specifically designed for cross-modal retrieval that selects """"best"""" pseudo-texts for target videos while mitigating the hubness problem and avoiding over-exploitation of candidates.
Framework-level Innovation: Concept-Aware-Pseudo-Query (CAPQ) Framework - An overall architectural design that cooperatively integrates concept preservation for transferability and a novel pseudo-labeling algorithm for discriminability, bridging cross-domain discrepancies in text-video retrieval without target domain text supervision.",How can the transferability of learned text and video embeddings be enhanced to bridge cross-domain discrepancies and make them domain-agnostic in text-video retrieval?,Enhance text video embedding transferability domain adaptation.,"The paper addresses this by introducing a Concept Preservation Loss (LP) within the Concept-Aware-Pseudo-Query (CAPQ) framework. This loss is designed to preserve previously acquired knowledge from pre-trained models, making both video and text features in the joint embedding space transferable and domain-agnostic. The process involves a Concept Selector (C) and a Hallucinator (H). The Concept Selector (C) first maps a generic video descriptor, obtained from a frozen Feature Extractor (F), to a concept distribution (y) associated with the original pretraining task (e.g., ImageNet's 1000 concepts). This provides a common signal across domains. For source domain video-text pairs, the predicted concept distribution for the source video (yS) is propagated to its paired text embedding, requiring them to map to the same concept distribution if the text describes the video. Next, a Hallucinator (H), implemented as a two-layer Multi-Layer Perceptron (MLP), takes the joint video embeddings (g_vid) and text embeddings (g_text) as input. Its goal is to generate predictions (y_hat) that match the concept selector's output (y). To accurately generate these predictions, the embeddings must retain the ability to distinguish between external concepts known to the pre-trained models. The same Hallucinator is used for both text and video embeddings, implicitly encouraging feature alignment between g_vid(v) and g_text(t). The Concept Preservation Loss (LP) is then computed as the Kullback-Leibler divergence between the concept distribution (y) and the hallucinator's predictions (y_hat). In the source domain, LP minimizes (yS, y_hat_S_v) + (yS, y_hat_S_t), preserving concepts in both video and text embeddings. In the target domain, where text queries are unavailable, LP minimizes (yT, y_hat_T_v), preserving concepts only in the target video embedding. This mechanism encourages feature alignment between source and target video embeddings and broadens the semantic coverage of the text encoder's output.","Concept Preservation Loss (LP) uses Concept Selector (C) and Hallucinator (H) to align video and text embeddings with pre-trained concept distributions, ensuring transferability and domain-agnostic features.","In what way can discriminative features be learned for target domain retrieval without direct access to target text annotations, while mitigating issues like hubness?",Learn discriminative features target domain pseudo-text hubness mitigation.,"To learn discriminative features for target domain retrieval without direct target text annotations, the paper proposes a novel mutually-exclusive pseudo-text selection mechanism. This mechanism refines the joint video-text embedding space, which is initially trained using a Source Ranking Loss (LS) on source domain paired data. The core idea is to assign """"pseudo text"""" (pT) to unlabeled target videos (vT) from a pool of unbiased source text embeddings (g_text(tS)). A naive selection (picking the text embedding with the highest similarity score) is problematic because a single pseudo-text might become the nearest neighbor for many target videos, leading to the """"hubness problem."""" To mitigate this, the mutually-exclusive selection algorithm operates on a collection of source text embeddings (NQ) and a minibatch of target video features (NB). It first computes a similarity matrix (S) between target video features and source text embeddings. To enhance discriminability and enforce mutual exclusivity, it applies the softmax function along the text dimension of S to get S_text (amplifying text discriminative capability) and along the video dimension of S to get S_video (amplifying video discriminability). These two directional softmax outputs are then combined (S' = S_text * S_video) to refine the similarity matrix. The selection assignment for each target video (g_vid(vT_j)) then follows from this refined similarity matrix S' by choosing the unbiased source text embedding that yields the highest similarity score. This process ensures that the selected pseudo-text for one target video is less likely to be the """"best"""" match for other different target videos, thereby diversifying pseudo-label assignments and robustly picking distinct source text descriptions for target visual queries. Finally, the joint space is refined by minimizing a second ranking loss (LT) between the target video embedding (g_vid(F(vT))) and its selected pseudo query embedding (pT), similar in form to the source ranking loss.","Mutually-exclusive pseudo-text selection assigns unbiased source text embeddings to target videos using a refined similarity matrix (S_text * S_video) to learn discriminative features and mitigate hubness, followed by a Target Ranking Loss (LT).",How can a unified framework effectively combine strategies for feature transferability and discriminability to achieve robust unsupervised domain adaptation for text-video retrieval?,Unified framework transferability discriminability unsupervised domain adaptation text-video retrieval.,"The Concept-Aware-Pseudo-Query (CAPQ) framework unifies the strategies for enhancing feature transferability and discriminability through a cooperative training objective. The overall training objective is a weighted sum of three loss components: the Source Ranking Loss (LS), the Concept Preservation Loss (LP), and the Target Ranking Loss (LT). The LS component ensures that paired text and video samples from the source domain are embedded closely together, while non-matching pairs are pushed apart, establishing initial discriminative capabilities. The LP component, as detailed in Solution 1, focuses on making the learned features transferable and domain-agnostic by preserving knowledge from pre-trained models and aligning concept distributions across domains. This is achieved by encouraging the joint text-video embeddings to retain the ability to distinguish between external concepts, implicitly reducing video and description distribution shifts. The LT component, as detailed in Solution 2, leverages the novel mutually-exclusive pseudo-text selection mechanism to learn discriminative features for the target domain. By generating pseudo-text queries for unlabeled target videos and applying a ranking loss, it progressively aligns cross-domain visual features at the level of text descriptions, even without ground-truth target text. The framework's design ensures that the concept preservation mechanism (LP) helps select reliable pseudo-texts for the LT, restricting the accumulation of false labeling risk. This cooperative minimization of all three losses allows CAPQ to learn features that are both robustly transferable across domains and highly discriminative for retrieval tasks in the target domain, effectively bridging the cross-domain discrepancy.","CAPQ framework combines Source Ranking Loss (LS), Concept Preservation Loss (LP), and Target Ranking Loss (LT) to cooperatively optimize feature discriminability and transferability for robust unsupervised text-video retrieval.",,,,,"The core challenge in text-video retrieval under unsupervised domain adaptation lies in effectively generalizing models to new data sources without target domain annotations. This necessitates enhancing the transferability of learned embeddings to bridge visual and description distribution shifts across domains. Furthermore, it requires learning discriminative features for target domain retrieval in the absence of explicit target text, while robustly assigning pseudo-labels and mitigating issues like hubness. The overarching problem is to integrate these solutions into a cohesive framework that optimizes both transferability and discriminability for robust cross-domain performance.","AudioVisualEvents (MSR-VTT):
Data type and domain: Videos with audio tracks and text descriptions. Sourced from YouTube using popular user queries.
Composition or source: 10,000 videos, 200,000 sentences. Average video length 20s. Annotations by AMT workers.
General application or purpose: Used to study text-video retrieval, particularly for audio-visual events.
VisualEvents (MSVD):
Data type and domain: Videos with text descriptions, but audio tracks removed. Sourced from YouTube.
Composition or source: 1,970 videos, 70,028 sentences. Average video length 10s. Curated for single unambiguous events, no overlaid text/subtitles. Annotations by AMT workers.
General application or purpose: Used for text-video retrieval focusing on visual events.
Activities (ActivityNet-Captions):
Data type and domain: Web videos with text descriptions, focusing on complex human activities.
Composition or source: 14,926 videos, 54,926 sentences. Average video length 180s. Each video contains multiple sequential events. Annotations by AMT workers.
General application or purpose: Used for text-video retrieval, particularly for complex human activities.
MovieClips (LSMDC):
Data type and domain: Short video clips from movies with text descriptions.
Composition or source: 102,046 videos, 102,046 sentences. Average video length 4s. Annotations from Descriptive Video Service (DVS) and scripts.
General application or purpose: Used for text-video retrieval, specifically for movie content.","The proposed CAPQ (Concept-Aware-Pseudo-Query) framework was evaluated on the UDAVR benchmark across four adaptation directions: splitVideo, splitText, splitAnnoF, and splitHard. CAPQ consistently outperformed alternative domain adaptation strategies, including Maximum Mean Discrepancy (MMD), adversarial feature alignment (DANN), Deep Coral (D-CORAL), and optimal transport (OT), as well as source-only (SO) baselines.
Specifically:
- Overall Performance: CAPQ achieved significant relative gains in geometric mean (R1, R10) over the second-best methods: approximately 52% on splitVideo, 27% on splitText, 29% on splitAnnoF, and 23% on splitHard.
- Comparison with Baselines:
    - On `splitVideo` (VisualEvents -> MovieClips), CAPQ achieved R1 of 2.3 (v2t) and 1.7 (t2v), and R10 of 10.5 (v2t) and 9.7 (t2v), with MR of 164 (v2t) and 162 (t2v). This significantly surpassed SO (R1: 0.8/0.9, R10: 6.9/3.6) and other UDA methods.
    - On `splitHard` (VisualEvents -> Activities), CAPQ achieved R1 of 3.7 (v2t) and 3.0 (t2v), and R10 of 19.1 (v2t) and 16.3 (t2v), with MR of 64.3 (v2t) and 70 (t2v). This was a substantial improvement over SO (R1: 2.0/2.3, R10: 11.3/11.8).
    - On `splitText` (AudioVisualEvents -> MovieClips), CAPQ achieved R1 of 3.1 (v2t) and 2.7 (t2v), and R10 of 11.9 (v2t) and 12.4 (t2v), with MR of 137 (v2t) and 150 (t2v).
    - On `splitAnnoF` (MovieClips -> Activities), CAPQ achieved R1 of 1.7 (v2t) and 1.9 (t2v), and R10 of 9.2 (v2t) and 10.7 (t2v), with MR of 182 (v2t) and 142 (t2v).
- Conventional UDA Limitations: The results showed that conventional domain adaptation techniques are effective for video content and text shifts but can under-perform the source-only model when encountering different annotation styles (e.g., on splitAnnoF). CAPQ, however, maintained strong performance.
- Ablation Study (on splitText):
    - Each proposed component contributed to performance improvement over the SO baseline.
    - The pseudo-text selection module yielded the most significant individual performance gain, highlighting the value of refining the joint text-video embedding space.
    - CAPQ, with both concept preservation and pseudo-text selection, achieved the best performance, demonstrating a relative gain of 64.2% over the SO baseline.
    - CAPQ-preserve (concept preservation only on source data, no target videos during training) still outperformed the SO baseline by a small margin, indicating the utility of generic feature embedding, though it's insufficient alone for cross-domain video retrieval.
- Computational Overhead: CAPQ introduces 14M more parameters during training due to the hallucinator, but at test-time, the hallucinator is discarded, resulting in no computational overhead and identical inference speed to the baseline model.
- Qualitative Results: Successful applications of CAPQ showed that the selected pseudo-text, while not always precise, captured relevant semantic concepts. Source domain videos linked with selected pseudo-text often shared significant commonalities with target videos. In cases where CAPQ was less effective, it still provided some benefit over the SO model.","R@K (Recall at K): Represents the percentage of test queries for which at least one relevant item is found among the top-K retrieved results. Used with K=1 and K=10.
MR (Median Rank): The median rank of the first relevant item in the retrieved results. A lower MR indicates better performance.","The Concept-Aware-Pseudo-Query (CAPQ) framework is designed to address the critical challenge of unsupervised domain adaptation for text-video retrieval, enabling effective generalization to new data sources despite domain shifts and without requiring target domain text annotations. The framework's overall design is motivated by the need to learn features that are both highly transferable across domains and discriminative for retrieval tasks. It achieves this by cooperatively optimizing three key aspects: initial discriminative learning on source data, concept preservation for transferability, and pseudo-text-guided discriminative learning on target data.
The framework comprises several main components. A frozen Feature Extractor (F) first extracts generic, multi-modal features from both source and target domain videos, serving as a robust, pre-trained foundation. These generic features are then fed into a Video Encoder (g_vid) and a Text Encoder (g_text), which project them into a joint text-video embedding space. The core innovation lies in how these encoders are trained to achieve transferability and discriminability. Transferability is enforced through a Concept Preservation Loss (LP), which leverages a Concept Selector (C) and a Hallucinator (H). The Concept Selector maps video features to a concept distribution derived from pre-trained models, while the Hallucinator learns to predict these concept distributions from the joint embeddings. By minimizing the Kullback-Leibler divergence between these, the LP loss ensures that the embeddings retain knowledge of external concepts, implicitly reducing domain shifts and making features domain-agnostic. Discriminability for the target domain, in the absence of ground-truth text, is achieved through a novel mutually-exclusive pseudo-text selection mechanism. This mechanism selects the """"best"""" pseudo-text queries for unlabeled target videos from a pool of unbiased source text embeddings, carefully mitigating the hubness problem by ensuring diverse assignments. A Target Ranking Loss (LT) then refines the joint embedding space using these pseudo-pairs. The overall training objective combines a Source Ranking Loss (LS) for initial discriminative learning, the LP for transferability, and the LT for target domain discriminability, with weighting coefficients to balance their contributions. This unique integration of concept preservation and robust pseudo-labeling allows CAPQ to effectively bridge cross-domain discrepancies, making it a powerful solution for unsupervised text-video retrieval.","When can we expect a text-video retrieval system to work effectively on datasets that differ from its training domain? In this work, we investigate this question through the lens of unsupervised domain adaptation in which the objective is to match natural language queries and video content in the presence of domain shift at query-time. Such systems have significant practical applications since they are capable generalising to new data sources without requiring corresponding text annotations. We make the following contributions:(1) We propose the UDAVR (Unsupervised Domain Adaptation for Video Retrieval) benchmark and employ it to study the performance of text-video retrieval in the presence of domain shift.(2) We propose Concept-Aware-Pseudo-Query (CAPQ), a method for learning discriminative and transferable features that bridge these cross-domain discrepancies to enable effective target domain retrieval using source domain supervision.(3) We show that CAPQ outperforms alternative domain adaptation strategies on UDAVR."
16052,Error-Aware Density Isomorphism Reconstruction for Unsupervised Cross-Domain Crowd Counting,Yuhang He; Zhiheng Ma; Xing Wei; Xiaopeng Hong; Wei Ke; Yihong Gong,2021,AAAI,AAAI Conference on Artificial Intelligence,32,"How can pre-trained crowd counting models be effectively adapted to new, unlabeled video domains despite significant data distribution differences and the absence of target domain annotations?","Crowd counting research has seen significant advancements, particularly with density map estimation-based methods. These approaches typically involve estimating a density map for an input image and then summing over it to obtain the count. Early methods, such as those by Lempitsky and Zisserman (2010), converted target annotations into ground-truth density maps using Gaussian kernels and trained estimators with loss functions like Maximum Excess over SubArrays (MESA). More recent work, like Ma et al. (2019), introduced Bayesian Loss (BL) for constructing density distributions and applying count expectation supervision. Attention mechanisms have also been integrated into crowd counting models (Sindagi and Patel 2019; Chen, Su, and Wang 2020; Zhang et al. 2019a; Liu et al. 2019; Zhang et al. 2019b; Guo et al. 2019) to reduce background noise by re-weighting target densities. Addressing scale variation, a common challenge in crowd scenarios, has led to methods generating multipolar normalized density maps (Xu et al. 2019), employing scale-aware pyramid networks (Chen et al. 2020), or using scale aggregation networks (Cao et al. 2018). While image-based counting has matured, video-based crowd counting is an emerging area.
For video sequences, approaches have attempted to leverage temporal information. Xiong et al. (2017) utilized convolutional LSTMs and bidirectional LSTMs to capture spatial and temporal dependencies, including long-term information in crowd flows. Fang et al. (2019) proposed a locality-constrained spatial transformer network to exploit spatial-temporal consistency. Other methods, like Zhou et al. (2019), introduced deep trainable networks for temporal dependencies, and Liu et al. (2019b) exploited the consistency of people flows (PFlow) between adjacent images using optical flow to model temporal correlations. However, annotation in video remains expensive, and transferring pre-trained models to unlabeled video sequences is an open problem.
Domain adaptation (DA) methods have been developed to address performance deterioration when pre-trained models are applied to target domains with different data distributions. These methods are broadly categorized into semi-supervised domain adaptation (SDA) and unsupervised domain adaptation (UDA). SDA methods, such as those by Change Loy, Gong, and Xiang (2013) with their Semi-Supervised Regression (SSR) framework and Reddy et al. (2020) with Few-Shot Scene Adaptation (FSSA) using meta-learning, require a small amount of labeled data from the target domain. In contrast, UDA methods aim to transfer models using only unlabeled target domain data. For crowd counting, UDA approaches include Zhang et al.'s (2015) Crowd-Scene Crowd Counting (CSCC) method, which retrieves similar source domain images for fine-tuning, and adversarial learning-based methods like Wang et al.'s (2019) Count Objects via scale-aware adversarial Density Adaptation (CODA) and Han et al.'s (2020) Semantic Consistency Predictor (SCP). A common limitation of these unsupervised methods is their focus on image-level information, such as image similarity or domain distinctiveness, often neglecting the consistency information inherent in video sequences.","Module 1: Isomorphism Reconstruction Module - Reconstructs density maps of a target frame using density maps from adjacent frames, leveraging the assumption of density isomorphism in consecutive video frames.
Module 2: Reconstruction Erroneousness Modeling Module - Monitors and models the erroneousness of density reconstructions by maximizing estimation-reconstruction consistency, suppressing unreliable reconstructions during training.
Framework-level Innovation: Error-aware Density Isomorphism REConstruction Network (EDIREC-Net) - Integrates density isomorphism reconstruction with an error-aware mechanism to enable unsupervised domain adaptation for crowd counting in videos, using self-supervised signals from video consistency.",How can the inherent consistency of crowd flows in video sequences be leveraged to reconstruct density maps for unsupervised domain adaptation?,Video crowd density isomorphism reconstruction for unsupervised domain adaptation.,"The paper addresses the challenge of leveraging crowd flow consistency by proposing a density isomorphism reconstruction objective. This objective is based on the assumption that density
maps in adjacent frames are isomorphic, meaning they are mutually transformable via bijective mapping. Instead of directly calculating mapping correspondences using potentially inaccurate density maps, the method first computes an image mapping correspondence (Mj_i) between consecutive image frames (Ij to Ii) by minimizing an image reconstruction error using a warping function. This image mapping is efficiently computed using the Gunner-Farneback algorithm, which approximates pixel transformations with quadratic polynomials. Once the image mapping matrices (Mi-d_i and Mi+d_i) are obtained, they are transformed into density mapping matrices (Gi-d_i and Gi+d_i) by linear sampling and scaling, accounting for the different dimensions of image frames and density maps. Finally, the density map of the central image Ii is reconstructed (Di^0) from its preceding (Di-d) and succeeding (Di+d) density maps using these derived density mapping matrices. This reconstructed density map then serves as a self-supervised signal for training.","Density isomorphism reconstruction leverages image mapping matrices from Gunner-Farneback algorithm to transform adjacent frame density maps into a reconstructed target density map, serving as a self-supervised signal.",How can the reliability of self-supervised density map reconstructions be monitored and improved in the absence of ground-truth annotations?,Unsupervised density reconstruction erroneousness modeling and suppression.,"The paper addresses the challenge of monitoring and improving reconstruction reliability by developing a reconstruction erroneousness modeling mechanism based on estimation-reconstruction consistency. This mechanism posits that if both the estimated density map (Di) and the reconstructed density map (Di^0) for a given frame Ii are accurate, they should be consistent (i.e., their difference should be negligible). Conversely, significant differences indicate erroneousness. To quantify this, a reconstruction erroneousness matrix (Ei) is introduced, where larger values of Ei(u,v) correspond to greater inconsistency between Di(u,v) and Di^0(u,v). This erroneousness is then integrated into the error-aware density isomorphism reconstruction objective (Liso). Liso minimizes the L2-norm difference between Di and Di^0, but crucially, it divides this difference by the erroneousness matrices (Ei-d and Ei+d), effectively suppressing the impact of unreliable density reconstructions during training. To prevent trivial solutions where erroneousness matrices become extraordinarily large, a regularization term (Lmod) is introduced. Lmod is defined as the logarithmic sum of the erroneousness matrices, which constrains their values. The overall objective function (L) for the EDIREC-Net is then formulated as the sum of Liso and Lmod, jointly encouraging accurate reconstruction while modeling and mitigating the influence of unreliable regions.","Reconstruction erroneousness is modeled by estimation-reconstruction consistency, using the difference between estimated and reconstructed density maps to create an erroneousness matrix. This matrix is used in an error-aware objective and regularized to suppress unreliable reconstructions.",,,,,,,,,"Adapting pre-trained crowd counting models to new, unlabeled video domains presents a significant challenge due to the inherent domain gap and lack of target annotations. This necessitates exploring how the consistency of crowd flows in video sequences can be leveraged to reconstruct density maps for self-supervision. Furthermore, it is critical to determine how the reliability of these self-supervised density map reconstructions can be monitored and improved in the absence of ground-truth annotations, ensuring robust knowledge transfer.","UCF-QNRF (UCF-QNRF): A large-scale dataset for crowd counting, containing 1535 images with 1,251,642 point annotations. Each image averages about 800 targets. It is used as the source domain dataset, with 1,201 images for training and 334 for testing.
UCSD (UCSD): A video dataset captured from a surveillance camera, containing 2,000 frames. Image frames have a resolution of 238 x 158, and the frame rate is 10 fps. Frames 601 to 1400 are used for training, and the remaining 1200 for testing.
MALL (MALL): A video dataset with 2,000 frames captured from a mall, with a fixed resolution of 640 x 480. The video frame rate is about 2 fps, and each frame contains approximately 30 targets. The first 800 frames are used for training, and the remaining 1,200 for testing.
VENICE (VENICE): A video dataset containing 167 annotated frames from 4 different scenarios. The image resolution is 1280 x 720, with about 250 targets per frame. 80 images from a single scenario are used for training, and the rest from the other 3 scenarios for testing.
FDST (FDST): A video dataset capturing 100 video sequences from 13 different scenes, comprising 150,000 image frames and 394,081 annotated head points. 60 video sequences are used for training, and the rest for testing. Videos are captured at 30 fps with a resolution of 1920 x 1080.
ShanghaiTech-A (ShanghaiTech-A): A crowd counting dataset used as an additional source domain dataset for robustness studies. It includes 300 training images and 182 testing images, with approximately 500 targets per image.","The proposed method, EDIREC-Net, demonstrates superior performance in unsupervised cross-domain crowd counting across four benchmark datasets (Venice, UCSD, MALL, FDST).
- Overall Performance: Compared to the Baseline (pre-trained model without adaptation), EDIREC-Net significantly improves performance, reducing MAE error by 67% on Venice, 77% on UCSD, 42% on MALL, and 28% on FDST.
- Comparison with Other Methods: EDIREC-Net outperforms all other unsupervised cross-domain counting methods (CSCC, CODA, SCP) across all four target datasets in terms of both MAE and MSE. It also achieves highly competitive results with state-of-the-art semi-supervised methods (SSR, FSSA) and comparable results with representative fully-supervised methods (PFlow, BL), which represent the upper bounds for domain adaptation.
- Ablation Study on 'd' parameter: Experiments on the MALL validation set show that the time interval parameter 'd' (for image tuple Id_i) influences accuracy. The best performance is achieved at d=3, and accuracy remains steady when d is smaller than 5. Performance decreases when d exceeds 5, indicating that large 'd' values can collapse density isomorphism due to varying target numbers.
- Impact of Reconstruction Erroneousness Modeling: The 'Ours' method (with erroneousness modeling) consistently improves counting performance compared to 'Ours-w/o mod' (without erroneousness modeling), showing around 20% improvement across the four benchmark datasets. This indicates the efficiency of the erroneousness modeling mechanism in suppressing unreliable density reconstructions.
- Robustness to Different Pre-trained Models: The method's robustness was tested using different baseline models (Bayesian Loss (BL) and MESA) and source datasets (UCF-QNRF and ShanghaiTech-A). While UCF-QNRF pre-trained models are more accurate due to larger dataset size, EDIREC-Net consistently and significantly improves performance (over 30% MAE improvement) even with less accurate pre-trained models, achieving favorable results on all four target datasets.
- Reconstruction Erroneousness Analysis: During training, the reconstruction erroneousness (jEij) decreases significantly, and concurrently, the MAE error improves. This correlation demonstrates that the erroneousness modeling mechanism effectively suppresses reconstruction erroneousness and enhances counting performance. Visual examples show that the Baseline model suffers from missing and redundant counts, 'Ours-w/o mod' improves, and 'Ours' achieves more accurate results by leveraging erroneousness modeling.","Mean Absolute Error (MAE): Measures the average absolute difference between the ground-truth count and the estimated count. It indicates the average magnitude of the errors.
Mean Squared Error (MSE): Measures the average of the squares of the errors. It gives a relatively high weight to large errors, making it useful when large errors are particularly undesirable.","The Error-aware Density Isomorphism REConstruction Network (EDIREC-Net) is proposed as a novel end-to-end framework for unsupervised domain adaptation in video-based crowd counting. The primary motivation is to effectively transfer knowledge from a labeled source domain to an unlabeled target domain, overcoming the significant domain gap and the absence of target annotations. The framework is built upon the core insight that crowd flows in videos are consecutive, implying that density maps in adjacent frames exhibit isomorphism.
EDIREC-Net comprises three major modules: the density and erroneousness inference module, the isomorphism reconstruction module, and the reconstruction erroneousness modeling module. Initially, the inference module takes an image tuple (Ii-d, Ii, Ii+d) as input and estimates density maps (Dj) and reconstruction erroneousness matrices (Ej) for each image using dedicated headers. The isomorphism reconstruction module then leverages the estimated density maps from adjacent frames (Di-d and Di+d) to reconstruct the density map of the central frame (Ii). This reconstruction process is based on computing image mapping correspondences between frames using the Gunner-Farneback algorithm, which are then converted into density mapping matrices. The reconstruction error between the estimated and reconstructed density maps serves as a self-supervised signal for knowledge transfer. Crucially, the reconstruction erroneousness modeling module addresses the inherent inaccuracy of self-supervised signals. It monitors the reliability of density reconstructions by enforcing an estimation-reconstruction consistency, where consistency between estimated and reconstructed maps implies accuracy. This module formulates an error-aware density isomorphism reconstruction objective (Liso) that minimizes reconstruction errors while suppressing unreliable regions by weighting them with the learned erroneousness. A regularization term (Lmod) is also introduced to constrain the erroneousness values and prevent trivial solutions. By jointly optimizing Liso and Lmod, EDIREC-Net effectively transfers the pre-trained counting model to target domains, leveraging video consistency and self-monitoring the quality of its self-supervised signals, thereby achieving robust and accurate crowd counting in unlabeled video sequences.","This paper focuses on the unsupervised domain adaptation problem for video-based crowd counting, in which we use labeled data as source domain and unlabelled video data as target domain. It is challenging as there is a huge gap between the source and the target domain and no annotations of samples are available in the target domain. The key issue is how to utilize unlabelled videos in the target domain for knowledge learning and transferring from the source domain. To tackle this problem, we propose a novel Error-aware Density Isomorphism REConstruction Network (EDIREC-Net) for cross-domain crowd counting. EDIREC-Net jointly transfers a pre-trained counting model to target domains using a density isomorphism reconstruction objective and models the reconstruction erroneousness by error reasoning. Specifically, as crowd flows in videos are consecutive, the density maps in adjacent frames turn out to be isomorphic. On this basis, we regard the density isomorphism reconstruction error as a self-supervised signal to transfer the pre-trained counting models to different target domains. Moreover, we leverage an estimation-reconstruction consistency to monitor the density reconstruction erroneousness and suppress unreliable density reconstructions during training. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method and ablation studies investigate the efficiency and robustness. The source code is available at https://github. com/GehenHe/EDIREC-Net."
16081,Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation,Minsu Kim; Sunghun Joung; Seungryong Kim; Jungin Park; Ig-Jae Kim; K. Sohn,2020,AAAI,AAAI Conference on Artificial Intelligence,15,"How can semantic segmentation models be effectively adapted from a labeled source domain to an unlabeled target domain, especially when the target domain exhibits multi-modal data distributions, significant inter-class variations, and class imbalance?","Semantic segmentation, which involves assigning pixel-level semantic labels to images, has seen remarkable progress due to deep neural networks, particularly Fully Convolutional Networks (FCNs) and their advancements like dilated convolutions and pyramid pooling. However, these methods typically necessitate extensive, densely labeled datasets, which are costly and labor-intensive to acquire. While synthetic datasets such as GTA5 and SYNTHIA offer an alternative with abundant labels, models trained on them often experience a substantial performance decline when applied to real-world scenes due to inherent domain discrepancies.
To mitigate this domain mismatch, unsupervised domain adaptation (UDA) techniques have emerged. Initial UDA efforts, exemplified by domain adversarial networks, focused on transferring feature distributions. Subsequent research has broadened to align data distributions at various granularities, including image-level, feature-level, and output-level. Image-level adaptation aims to transform source images to match the visual characteristics of target images while preserving structural information. Feature-level methods align intermediate feature representations, frequently employing adversarial frameworks. Output-level adaptation, particularly relevant for structured outputs like semantic segmentation, seeks to align the probability space of predictions.
Despite these advancements, global domain adaptation methods, which attempt to reduce overall domain differences, often overlook inter-class variations within the target domain, limiting their efficacy with multi-modal data distributions. To address this, category-level domain adaptation methods have been proposed. These approaches strive to minimize class-specific domain discrepancy by leveraging a category classifier trained on the source domain to generate pseudo class labels for the target domain. However, this reliance on potentially inaccurate pseudo labels can lead to misaligned domain adaptation and error accumulation. Furthermore, these methods frequently encounter a class imbalance problem, where performance is robust for majority categories but deficient for minority categories due to their sparse representation. Some studies have also explored patch-level adaptation, utilizing multiple modes of patch-wise output distribution. Iterative self-training approaches have also been adopted, where unlabeled target samples with high class probability are iteratively selected and used as pseudo ground-truth. Deep clustering algorithms also contribute to this field by simultaneously discovering groups within training data and learning representations, with techniques ranging from pairwise classification to pixel clustering and Deep Feature Factorization (DFF).","Module 1: Cross-Domain Grouping Network (C) - A learnable module that clusters the output probability distribution into K groups across source and target domains, replacing conventional category classifiers. It uses 1x1 convolutions and element-wise multiplication to produce group-specific features.
Module 2: Semantic Consistency Loss (Lco) - A loss function that encourages the class distribution of each learned group to be consistent between the source and target domains, providing supervisory signals for group-level feature alignment.
Module 3: Orthogonality Loss (Lorth) - A loss function that enforces distinctness among the class distributions of different groups within the same domain, ensuring that the multi-modal complex distribution is effectively divided into simpler, orthogonal sub-distributions.
Module 4: Group-level Class Equivalence Loss (Lcl) - A loss function designed to address the class imbalance problem by aligning the maximum classification scores for each category within a group between source and target domains, using source maximum scores as pseudo-labels for target.
Framework-level Innovation: Cross-Domain Grouping and Alignment Framework - An overall architecture that integrates a learnable grouping module with group-level domain adaptation, allowing for joint and boosting training of the grouping and segmentation networks. This approach aligns samples at a group level rather than globally or strictly by category, enabling adaptation for multi-modal data distributions and mitigating issues of error-prone category classifiers and class imbalance.",How can a learnable grouping mechanism effectively cluster cross-domain data distributions to facilitate domain adaptive semantic segmentation?,Learnable grouping mechanism cross-domain clustering semantic segmentation.,"The paper introduces a Cross-Domain Grouping Network (C), which is a learnable module designed to cluster the output probability distribution into K sub-spaces. This network consists of two 1x1 convolutional layers. The first convolution generates a 64-channel feature map, followed by a Rectified Linear Unit (ReLU) activation and batch normalization. The second convolution then produces K grouping scores, which are passed through a softmax function to yield group probabilities, denoted as Hk_l, for each pixel. These group probabilities Hk_l are then element-wise multiplied with the original pixel-wise class probability distribution Pl (output of the segmentation network G before softmax) to obtain group-specific features, Fk_l. This process allows the framework to decompose complex data distributions into K simpler ones, enabling group-level domain alignment without relying on a potentially inaccurate category classifier.","Cross-Domain Grouping Network C, 1x1 convolutions, softmax, element-wise multiplication, group-specific features.",How can semantic consistency and orthogonality constraints be enforced among learned groups to ensure meaningful and distinct sub-distributions for domain alignment?,Semantic consistency orthogonality constraints learned groups domain alignment.,"To ensure meaningful and distinct sub-distributions, the paper proposes two loss functions for the Cross-Domain Grouping Network (C): Semantic Consistency Loss (Lco) and Orthogonality Loss (Lorth).
The Semantic Consistency Loss (Lco) encourages the category distribution of each group to be consistent between the source and target domains. For each group k, the class distribution Qk_l is estimated by applying an average pooling layer on the group-specific feature Fk_l. The loss minimizes the L2-norm difference between the source group's class distribution (Qk_S) and the target group's class distribution (Qk_T) across all K groups. This provides supervisory signals for aligning group-level features.
The Orthogonality Loss (Lorth) ensures that the class distributions of different groups within the same domain are distinct and non-overlapping. It achieves this by minimizing the cosine similarity between any two distinct group class distributions (Qj1_l and Qj2_l) within the same domain l. Since Qk_l values are non-negative, minimizing cosine similarity effectively pushes them towards orthogonality, allowing the grouping module to divide a multi-modal complex distribution into K simple, differentiated class distributions.","Semantic Consistency Loss Lco, Orthogonality Loss Lorth, average pooling, L2-norm, cosine similarity.",How can the class imbalance problem be addressed within a group-level domain adaptation framework to improve performance on minority categories?,Class imbalance group-level domain adaptation minority categories.,"The paper addresses the class imbalance problem through the Group-level Class Equivalence Loss (Lcl). This loss aims to align the maximum classification scores for each category within a group between the source and target domains. First, for each group k and each category cls, a maximum pooling layer is applied to the group-specific feature Fk_l to obtain Mk_l, which represents the maximum score for that category within group k. The maximum classification score from the source domain (Mk_S) is then used as a pseudo-label. The loss function is a multi-class binary cross-entropy applied for each category u, comparing Mk_S,u with Mk_T,u. A threshold parameter (delta) is introduced to exclude very low probability values, preventing noisy or unreliable pseudo-labels from influencing the training. By encouraging the target domain's maximum scores to be similar to the source's for each category within each group, this loss helps ensure that minority categories, which might otherwise be overlooked, are properly considered during adaptation.","Group-level Class Equivalence Loss Lcl, max pooling, source pseudo-label, multi-class binary cross-entropy, threshold.",,,,,"The core challenge in domain adaptive semantic segmentation lies in effectively transferring knowledge from a labeled source domain to an unlabeled target domain, particularly when facing complex multi-modal data distributions. This necessitates developing a learnable grouping mechanism that can robustly cluster cross-domain data, ensuring that the resulting sub-distributions are both semantically consistent across domains and orthogonal within domains. Furthermore, a critical difficulty is mitigating the class imbalance problem inherent in real-world datasets, which often leads to poor performance on minority categories during adaptation.","GTA5 (Grand Theft Auto V): Synthetic images of urban scenes generated from the video game Grand Theft Auto V. It contains 24,966 images with 1914x1052 resolution, used as a source dataset for domain adaptation to real-world urban scenes.
SYNTHIA (SYNTHetic collection of Imagery and Annotations): Synthetic images of urban scenes, specifically the SYNTHIA-RAND-CITYSCAPES variant. It comprises 9,400 images with 1280x760 resolution, serving as a source dataset for domain adaptation to real-world urban scenes.
Cityscapes: Real-world images of urban street scenes collected from 50 different cities. It consists of 2,975 training images, 500 validation images, and 1,525 test images. Images are resized to 1024x512 for experiments and used as a target dataset for semantic segmentation.","The proposed method, """"Ours,"""" demonstrates superior adaptation performance in semantic segmentation across both GTA5 -> Cityscapes and SYNTHIA -> Cityscapes domain adaptation settings, consistently outperforming state-of-the-art approaches.
On the GTA5 -> Cityscapes benchmark, """"Ours"""" achieves a mean Intersection over Union (mIoU) of 51.5. This significantly surpasses the """"Without Adaptation"""" baseline (36.0 mIoU) and various contemporary methods, including Tsai et al. (2018) (43.2 mIoU), Li et al. (2019) (48.6 mIoU), and Wang et al. (2020) (48.8 mIoU). The method shows notable performance gains on categories such as """"car, truck, bus, motor, and bike,"""" which share similar visual characteristics, and particularly on """"pole and traffic sign,"""" indicating the effectiveness of the group-level class equivalence in addressing class imbalance.
For the SYNTHIA -> Cityscapes benchmark (evaluated on 13 common classes), """"Ours"""" achieves an mIoU of 51.5, again outperforming existing methods like Tsai et al. (2018) (42.2 mIoU), Li et al. (2019) (36.0 mIoU), and Wang et al. (2020) (45.9 mIoU). Similar to the GTA5 scenario, the model exhibits substantial improvements on """"traffic sign and traffic light"""" categories compared to the baseline.
An ablation study on GTA5 -> Cityscapes validates the contribution of each proposed loss function. The full model, utilizing all proposed losses (Lseg, Lcadv, Lco, Lorth, Lcl), yields the best mIoU of 51.5. Removing the Orthogonality Loss (Lorth) leads to a performance drop to 49.1 mIoU, underscoring its role in distinguishing group distributions. The Semantic Consistency Loss (Lco) and Group-level Class Equivalence Loss (Lcl) also contribute significantly, with their removal resulting in 48.8 mIoU and 50.8 mIoU, respectively. The Group-level Adversarial Loss (Lcadv) is critical, as its absence reduces performance to 36.6 mIoU. The study also determined that K=8 clusters provide optimal performance, with K=1 (global domain adaptation) yielding 43.2 mIoU, and performance generally improving with increasing K up to a point, then plateauing or slightly declining due to over-clustering. Qualitative results and t-SNE visualizations further confirm the model's ability to align domain distributions and effectively group minority categories, surpassing other models and non-learnable clustering methods.","Intersection over Union (IoU): Measures the overlap between the predicted segmentation mask and the ground truth mask for a specific class.
Mean Intersection over Union (mIoU): The average IoU calculated across all semantic classes, providing an overall measure of segmentation performance.","The proposed """"Cross-Domain Grouping and Alignment"""" framework is designed to overcome the limitations of traditional domain adaptation methods for semantic segmentation, specifically their challenges with multi-modal data distributions, inter-class variations, and class imbalance. Its core motivation is to implement a more granular and learnable grouping mechanism, moving beyond global or simplistic category-level alignment.
The framework integrates three primary components: a Semantic Segmentation Network (G), a novel Cross-Domain Grouping Network (C), and a Discriminator (D). The segmentation network G, built upon DeepLab-V2, processes both source and target images to generate pixel-wise class probability distributions. A key innovation is the Cross-Domain Grouping Network C, which takes these probability distributions and, through a series of 1x1 convolutions and a softmax function, learns to cluster the data into K distinct sub-spaces. This network produces group probabilities (Hk_l) that are element-wise multiplied with the segmentation output to derive group-specific features (Fk_l). This learnable grouping module innovatively replaces the reliance on potentially inaccurate","Existing techniques to adapt semantic segmentation networks across source and target domains within deep convolutional neural networks (CNNs) deal with all the samples from the two domains in a global or category-aware manner. They do not consider an inter-class variation within the target domain itself or estimated category, providing the limitation to encode the domains having a multi-modal data distribution. To overcome this limitation, we introduce a learnable clustering module, and a novel domain adaptation framework, called cross-domain grouping and alignment. To cluster the samples across domains with an aim to maximize the domain alignment without forgetting precise segmentation ability on the source domain, we present two loss functions, in particular, for encouraging semantic consistency and orthogonality among the clusters. We also present a loss so as to solve a class imbalance problem, which is the other limitation of the previous methods. Our experiments show that our method consistently boosts the adaptation performance in semantic segmentation, outperforming the state-of-the-arts on various domain adaptation settings."
16177,Gradient Regularized Contrastive Learning for Continual Domain Adaptation,Peng Su; Shixiang Tang; Peng Gao; Di Qiu; Ni Zhao; Xiaogang Wang,2020,AAAI,AAAI Conference on Artificial Intelligence,67,"How can deep neural networks incrementally adapt to new target domains in a sequential manner without losing generalization ability on previously encountered domains, given the challenges of domain shift and catastrophic forgetting?","Unsupervised Domain Adaptation (UDA) is a field focused on transferring knowledge from a labeled source domain to an unlabeled target domain. Various approaches have been developed, including discrepancy-based methods that aim to minimize the distance between feature distributions of source and target domains, adversary-based approaches that use adversarial training to learn domain-invariant features, and reconstruction-based methods that reconstruct data to bridge domain gaps. More recently, contrastive learning-based approaches have emerged, leveraging self-supervised learning to map similar images closer in an embedding space. A common strategy across many UDA methods is the use of multitask learning, where a task loss (e.g., classification loss) on the source domain is combined with an adaptation loss. However, a significant limitation of these multitask approaches is that minimizing the overall objective function does not guarantee the preservation of the model's discriminative ability on the source domain, often leading to an increase in the source domain's task loss. This can weaken the model's foundational knowledge.
Continual learning, a related area, addresses catastrophic forgetting in a sequence of supervised learning tasks. Methods in this domain typically fall into regularization-based approaches, which add penalties to the loss function to protect important parameters, or memory-based approaches, which store and replay samples from old tasks. A notable memory-based method, Gradient Episodic Memory (GEM), uses episodic memory and constrained optimization to mitigate forgetting. However, GEM is primarily designed for continuous learning of different classes, not for continual domain adaptation where the label space is common but data distributions shift. Existing continual domain adaptation methods often assume a specific pattern of gradual domain shift, such as slowly changing weather conditions, and may suffer from catastrophic forgetting if this assumption is violated. Some approaches attempt to bridge domains by generating intermediate states or focus on generalizing to a transitioning target domain, but many do not explicitly address the catastrophic forgetting problem. Those that do, like some multitask learning methods with simple replay mechanisms, still face challenges in maintaining discriminative features and effectively balancing multiple objectives without explicit constraints on knowledge preservation. The reliance on manually set trade-off parameters in multitask learning further complicates their application, as these parameters may not adapt optimally across different iterations or domain shifts.","Module 1: Source Discriminative Constraint (SDC) - A novel constraint formulated to ensure that the gradient of the parameters is positively correlated to the gradient of the classification loss for the source domain, preventing the degradation of discriminative ability on source features.
Module 2: Target Memorization Constraint (TMC) - A novel constraint designed to ensure that the gradient of the parameters is positively correlated to the gradient of the classification loss for every old target domain, explicitly combating catastrophic forgetting on previously learned target domains.
Framework-level Innovation: Gradient Regularization Mechanism - The overall approach of formulating the parameter update as a quadratic programming (QP) problem that minimizes the distance to the contrastive loss gradient while simultaneously satisfying the Source Discriminative Constraint and Target Memorization Constraint. This adaptively updates the gradient ratio, differing from conventional multitask learning's fixed trade-off parameters.",How can the discriminative ability of source features be preserved while adapting a model to new target domains in a continual learning setting?,Preserve source discriminative ability continual domain adaptation?,"The discriminative ability of source features is preserved by introducing a Source Discriminative Constraint (SDC). This constraint ensures that the classification loss on the source domain, denoted as Lce(θt, Ds), does not increase during the adaptation process. Mathematically, this is expressed as Lce(θt, Ds) <= Lce(θt-1, Ds), where θt are the current model parameters and θt-1 are the parameters from the previous step. This condition is rephrased as an inner product constraint: hw, gsi >= 0, where w is the vector used to update the model parameters and gs is the gradient of the classification loss on the source domain (∂Lce(θt, Ds)/∂θt). This means the update direction w must have a non-negative projection onto the source classification loss gradient, preventing updates that would increase this loss. This constraint is integrated into a quadratic programming (QP) problem that determines the optimal parameter update vector.",The discriminative ability of source features is preserved by introducing a Source Discriminative Constraint (SDC).,"How can catastrophic forgetting on previously adapted target domains be prevented when the model is subsequently adapted to new, incoming target domains?",Prevent catastrophic forgetting old target domains?,"Catastrophic forgetting on old target domains is addressed through the Target Memorization Constraint (TMC). This constraint ensures that the classification loss for each domain-episodic memory (Mi) does not increase when adapting to a new target domain Dt. Formally, Lce(θt, Mi) <= Lce(θt-1, Mi) for all i < t. To make this computationally efficient, especially as the number of target domains increases, an approximation is used: Lce(θt, M1:t-1) <= Lce(θt-1, M1:t-1), where M1:t-1 represents the aggregated domain-episodic memories from all previous target domains. Similar to the SDC, this is translated into an inner product constraint: hw, gdmi >= 0, where gdm is the gradient of the classification loss on the aggregated old target domain memories (∂Lce(θt, M1:t-1)/∂θt). The pseudo-labels required for computing the classification loss on old target domains are generated by clustering, and only high-confidence samples are retained. This constraint is then incorporated into the overall quadratic programming problem that calculates the model's parameter update vector.",Catastrophic forgetting on old target domains is addressed through the Target Memorization Constraint (TMC).,"How can multiple, potentially conflicting, objectives (contrastive learning, source discriminative ability, and old target domain memorization) be adaptively balanced during model training without manually setting trade-off parameters?",Adaptive objective balancing gradient regularization?,"The paper adaptively balances multiple objectives by formulating the parameter update as a quadratic programming (QP) problem. Instead of using manually set hyper-parameters to weight different loss terms (as in conventional multitask learning), the method seeks an optimal update vector `w` that is as close as possible to the gradient of the unified contrastive loss (`gt`) while satisfying the Source Discriminative Constraint (hw, gsi >= 0) and the Target Memorization Constraint (hw, gdmi >= 0). The QP problem is defined as: min w ||w - gt||^2 subject to hw, gsi >= 0 and hw, gdmi >= 0. This problem is solved in the dual space, which reduces the complexity to a much smaller QP with only two variables (u1, u2). Once the optimal dual variables (u*1, u*2) are found, the optimal update vector `w` is computed as w = gt - u*1gs - u*2gdm. This adaptive calculation of `w` ensures that the model updates prioritize minimizing the contrastive loss while strictly adhering to the non-increasing constraints on source and old target domain classification losses, effectively balancing the objectives without fixed trade-off parameters.",The paper adaptively balances multiple objectives by formulating the parameter update as a quadratic programming (QP) problem.,,,,,"Adapting deep neural networks to dynamic environments presents a significant challenge, as models must continually learn from sequential, unlabeled target domains while overcoming domain shift. A key difficulty lies in preserving the discriminative ability of source features, which are crucial for guiding adaptation to new target domains. Furthermore, a major obstacle is preventing catastrophic forgetting on previously learned target domains when the model is exposed to subsequent new domains. The core challenge then becomes how to effectively balance these potentially conflicting objectives—learning new domain knowledge, maintaining source discriminative power, and retaining old domain knowledge—without relying on manually tuned trade-off parameters.","Digits:
Data Type/Domain: Image classification of digits.
Composition/Source: Composed of five distinct digit datasets: MNIST, MNIST-M, USPS, SynNum, and SVHN.
Size/Scale: Each domain contains 7,500 images for training and 1,500 images for testing.
Purpose: Used for a continual domain adaptation task in the sequence SynNum -> MNIST -> MNIST-M -> USPS -> SVHN.
DomainNet:
Data Type/Domain: Large-scale image classification across various domains.
Composition/Source: One of the largest domain adaptation datasets, comprising approximately 0.6 million images distributed among 345 categories.
Size/Scale: For the experiments, 40,000 images are randomly selected for training and 8,000 images for testing from each domain.
Purpose: Used to build a continual domain adaptation task in the sequence Clipart -> Real -> Infograph -> Sketch -> Painting.
Office-Caltech:
Data Type/Domain: Object recognition.
Composition/Source: Includes 10 categories shared by Office-31 and Caltech-256 datasets. Office-31 contains three domains: DSLR, Amazon, and WebCam.
Size/Scale: Not explicitly stated, but implies a smaller scale than DomainNet.
Purpose: Used for a continual domain adaptation task in the sequence DSLR -> Amazon -> WebCam -> Caltech.","The proposed Gradient Regularized Contrastive Learning (GRCL) method was evaluated against five alternative methods (DANN, MCD, DADA, CUA, GRA) on three continual domain adaptation benchmarks: Digits, DomainNet, and Office-Caltech. The evaluation metrics used were Average Accuracy (ACC) and Average Backward Transfer (BWT).
On the Digits benchmark, GRCL achieved an ACC of 87.23 +/- 0.06 and a BWT of -0.67 +/- 0.12. This significantly outperformed all baselines, with the closest competitor GRA achieving 86.53 +/- 0.11 ACC and -1.15 +/- 0.16 BWT. Memory-based methods (CUA, GRA, GRCL) showed better performance than memory-free methods (DANN, MCD, DADA), particularly in BWT, highlighting the importance of memory for combating catastrophic forgetting.
For the DomainNet benchmark, GRCL obtained an ACC of 37.74 +/- 0.13 and a BWT of 0.05 +/- 0.02. This result was superior to all other methods, with GRA being the next best at 35.84 +/- 0.19 ACC and -0.03 +/- 0.03 BWT. Notably, GRCL achieved a positive BWT, indicating minimal or even beneficial impact on previously learned domains.
On the Office-Caltech benchmark, GRCL achieved an ACC of 85.34 +/- 0.10 and a BWT of -1.0 +/- 0.03. This again surpassed all baselines, with GRA at 84.10 +/- 0.15 ACC and -0.93 +/- 0.10 BWT.
Overall, GRCL consistently achieved the best ACC across all three benchmarks, demonstrating superior generalization capability. It also exhibited significantly better BWT compared to most methods, indicating its effectiveness in combating catastrophic forgetting. The methods using memory (CUA, GRA, GRCL) generally outperformed those without memory by 2-5% in ACC and 3-5% in BWT. GRCL specifically showed minimal forgetting and even positive backward transfer on the DomainNet and Office-Caltech benchmarks.
Ablation studies confirmed the effectiveness of individual components:
*   Importance of Contrastive Learning: """"Crt.+Src."""" (Contrastive learning with source domain) outperformed """"Src."""" (Source only) by about 2.5% in ACC, showing contrastive learning's ability to bridge domain gaps.
*   Effectiveness of Source Discriminative Constraint (SDC): """"Crt.+SDC"""" (Contrastive learning with SDC) improved ACC by approximately 2.2% and BWT by 0.8% compared to """"Crt.+Src."""", indicating that SDC maintains source discriminative ability and benefits adaptation to new target domains.
*   Importance of Target Memorization Constraint (TMC): """"Crt.+SDC+TMC"""" (GRCL) improved BWT by around 6% and ACC by about 3.5% compared to """"Crt.+SDC"""", confirming that TMC effectively overcomes catastrophic forgetting.
*   Comparison with Multitask Learning: GRCL (""""Crt.+SDC+TMC"""") outperformed """"Crt.+Src.+Mem."""" (multitask learning with memory) by over 2% in ACC and about 4% in BWT, verifying that gradient constraints are more effective than multitask learning for restoring old knowledge.
*   Memory Size and Training Epochs: GRCL's performance consistently improved with larger domain-episodic memory sizes and longer training schedules, aligning with general benefits of contrastive learning.","Average Accuracy (ACC): Measures the average performance of the model over all domains (source and all seen target domains) after the model has finished all sequential adaptation tasks. A higher ACC indicates better overall generalization.
Average Backward Transfer (BWT): Quantifies the influence on previously observed domains when adapting to a new domain. It is calculated as the average difference between the final accuracy on a domain and the accuracy on that domain immediately after it was adapted to. A negative BWT indicates performance degradation on previous domains (catastrophic forgetting), while a positive or near-zero BWT indicates effective knowledge retention. A larger BWT (closer to zero or positive) is desirable.","The Gradient Regularized Contrastive Learning (GRCL) framework is designed to address the challenges of continual unsupervised domain adaptation, specifically tackling domain shift and catastrophic forgetting. Its overall design motivation is to enable deep neural networks to incrementally adapt to new, unlabeled target domains while preserving knowledge from both the labeled source domain and previously encountered target domains. The framework integrates contrastive learning with a novel gradient regularization mechanism to achieve this.
GRCL operates by maintaining a feature bank (Bt) that stores representative features from the source domain, domain-episodic memories (M1:t-1) of old target domains, and features from the new incoming target domain (Dt). A unified contrastive loss (Lq) is employed to learn domain-invariant representations by pulling similar instances closer and pushing dissimilar ones apart within this feature space. The core innovation of GRCL lies in its two gradient-based constraints: the Source Discriminative Constraint (SDC) and the Target Memorization Constraint (TMC). The SDC ensures that the model's updates do not harm the discriminative ability of source features by requiring the parameter update vector to have a non-negative inner product with the source classification loss gradient. Simultaneously, the TMC prevents catastrophic forgetting on old target domains by imposing a similar non-negative inner product constraint with the gradient of the classification loss on the aggregated domain-episodic memories. These constraints are not enforced through fixed trade-off parameters, but rather by formulating the parameter update as a quadratic programming (QP) problem. This QP problem seeks an update vector that is as close as possible to the gradient of the contrastive loss while strictly satisfying both the SDC and TMC. This adaptive gradient regularization mechanism allows GRCL to dynamically balance the objectives of learning new domain knowledge, maintaining source discriminative power, and retaining old domain knowledge, providing a robust solution for continual domain adaptation.","Human beings can quickly adapt to environmental changes by leveraging learning experience. However, adapting deep neural networks to dynamic environments by machine learning algorithms remains a challenge. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labelled source domain and a sequence of unlabelled target domains. The obstacles in this problem are both domain shift and catastrophic forgetting. We propose Gradient Regularized Contrastive Learning (GRCL) to solve the obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforcing the gradient not to harm the discriminative ability of source features which can, in turn, benefit the adaptation ability of the model to target domains; (2) constraining the gradient not to increase the classification loss on old target domains, which enables the model to preserve the performance on old target domains when adapting to an in-coming target domain. Experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art."
16186,Self-Domain Adaptation for Face Anti-Spoofing,Jingjing Wang; Jingyi Zhang; Ying Bian; Youyi Cai; Chunmao Wang; Shiliang Pu,2021,AAAI,AAAI Conference on Artificial Intelligence,99,"How can face anti-spoofing methods achieve robust generalization to unseen attacks and domains, especially when target domain data is not available during the training phase?","Traditional face anti-spoofing (FAS) methods are broadly categorized into texture-based and temporal-based approaches. Texture-based methods differentiate real and fake faces by analyzing various appearance cues, such as color, distortion, or deep features. Early works in this category often relied on handcrafted features like Local Binary Patterns (LBP), Histogram of Oriented Gradients (HoG), Scale-Invariant Feature Transform (SIFT), and Speeded-Up Robust Features (SURF, sometimes referred to as Speeded-Up Robust Features), combined with binary classifiers like Support Vector Machines (SVM) or Linear Discriminant Analysis (LDA). More recently, deep learning-based Convolutional Neural Networks (CNNs) have been employed to extract more discriminative features. Temporal-based methods, on the other hand, leverage dynamic cues from consecutive frames, such as specific liveness facial motions (e.g., mouth motion, eye-blinking) or more general temporal cues learned through architectures like CNN-LSTM. Some researchers have also focused on capturing discriminative rPPG signals as robust temporal features. While these traditional methods achieve promising results in intra-dataset testing, their performance significantly degrades in cross-dataset scenarios. This degradation is primarily attributed to their failure to account for relationships among different domains, leading to the extraction of dataset-biased features.
To address the generalization issue, recent FAS research has increasingly adopted domain adaptation (DA) or domain generalization (DG) techniques. DA-based approaches aim to minimize the distribution discrepancy between source and target domains by utilizing labeled source data and unlabeled target data, often through methods like Maximum Mean Discrepancy (MMD) minimization or adversarial training. However, a significant limitation of DA methods in real-world FAS applications is the assumption that target domain data is accessible during training, which is often not the case. Conversely, DG-based methods tackle the problem under a more realistic assumption of no access to target domain information during training. These methods typically exploit multiple source domains to learn a shared, domain-agnostic feature space that can generalize to unseen target domains. Despite their ability to operate without target data, DG methods often lose valuable test domain-specific information when mapping extracted features to the shared feature space at test time. Some DG works, like those by Shao et al., utilize meta-learning strategies to learn generalized feature spaces. Furthermore, a nascent area of self-domain adaptation methods focuses on adapting a deployed model to various target domains during inference without requiring access to source data. Existing works in this area include methods that require living faces for adaptation, generate labeled target-style data using generative adversarial networks, modulate representations with affine transformations to minimize entropy, or transform input images and features via adaptors to reduce domain shift.","Module 1: Adaptor (A) - A novel 1x1 convolution module connected residually to the classifier, designed to learn domain-specific information and be updated independently at inference. Its design allows for easy removal to revert to the original model.
Module 2: Meta-learning based Adaptor Learning Algorithm - A two-step (meta-train, meta-test) training strategy that uses multiple source domains to learn a well-initialized adaptor, optimizing it for efficient unsupervised test-time adaptation. This is distinct from prior meta-learning uses in domain generalization.
Module 3: Unsupervised Adaptor Loss (LAdap) - A composite loss function specifically designed for unsupervised adaptation of the adaptor at both meta-test time and inference. It combines reconstruction error, entropy minimization, and orthogonality regularization to ensure target domain features are similar to source, confident, and prevent mode collapse.
Framework-level Innovation: Self-Domain Adaptation Framework - The overall architecture and strategy of learning an adaptable component (the adaptor) during training using meta-learning and then *only* adapting that component at inference using unlabeled target data, without requiring source data access at test time. This addresses the limitations of both traditional domain adaptation (needs target data at training) and domain generalization (loses target-specific information).",How can a domain-specific adaptor be effectively learned and initialized during training to facilitate efficient adaptation to unseen target domains at inference?,Domain adaptor learning initialization meta-learning,"The paper proposes a meta-learning based adaptor learning algorithm to effectively learn and initialize a domain-specific adaptor during training. This algorithm operates in two main steps: meta-train and meta-test, simulating the real-world scenario where a model trained on source domains needs to adapt to an unseen target domain. First, a feature extractor (F), a classification header (C), a depth estimator (D), an adaptor (A), and an autoencoder (R) are defined. The adaptor (A) is designed as a 1x1 convolution connected to the first convolution of the classifier (C) through a residual architecture, allowing it to be easily removed to revert to the original model. During the meta-train step, a meta-train domain (Dtrn) is randomly selected from the available source domains. The model parameters (F, C, D, R) are updated using supervised losses on Dtrn, including a cross-entropy classification loss (LCls), a depth estimation loss (LDep) for auxiliary supervision, and an autoencoder reconstruction loss (LAE) to train the autoencoder (R) on source features. After this supervised update, in the meta-test step, a different meta-test domain (Dval) is randomly selected from the source domains. The adaptor (A) is then added to the classifier (C) to form a combined classifier with adaptor (Ca). The adaptor (A) is updated unsupervisedly on Dval using a proposed unsupervised adaptor loss (LAdap). Finally, a meta-optimization step updates the overall model parameters (F, C, D, A, R) based on the classification performance on the meta-test domain (Dval) after the adaptor's unsupervised update. This meta-learning process trains the adaptor to be well-initialized, enabling efficient adaptation to new, unseen domains at inference time.","Meta-learning adaptor initialization, two-step meta-train meta-test, residual 1x1 convolution adaptor, supervised source training, unsupervised adaptor update on meta-test domain.",What unsupervised loss functions are effective for optimizing a domain adaptor using only unlabeled target domain data at inference to improve face anti-spoofing performance?,Unsupervised adaptor loss functions face anti-spoofing,"The paper proposes a composite unsupervised adaptor loss (LAdap) to optimize the domain adaptor using only unlabeled target domain data. This loss function consists of three main components: a reconstruction loss, an entropy minimization loss, and an orthogonality regularization loss. Firstly, the reconstruction loss, denoted as LAE(target), minimizes the reconstruction error of the target domain features using the autoencoder (R) that was trained on source domain data. This encourages the feature distribution of the target domain, after adaptation by the adaptor, to be similar to that of the source domain, ensuring the discriminative classifier (trained on source data) remains effective. Secondly, an entropy minimization loss, LEnt(target), is applied to the predicted probabilities on the target domain. This component encourages the model to produce more confident predictions on the unlabeled target data, thereby leveraging the discriminative information present in the target domain itself. Thirdly, an orthogonality regularization loss, LOrth(target), is imposed on the adaptor's parameters using the Spectral Restricted Isometry Property Regularization. This term prevents feature mode collapse, ensuring that the adaptor learns diverse and meaningful transformations without collapsing the feature space. The final unsupervised adaptor loss (LAdap) is a weighted sum of these three components, balancing their contributions to guide the unsupervised optimization of the adaptor.","Composite unsupervised adaptor loss, reconstruction error, entropy minimization, orthogonality regularization.",How can a self-domain adaptation framework be structured to leverage target domain information at inference while overcoming the limitations of traditional domain adaptation and domain generalization methods in face anti-spoofing?,Self-domain adaptation framework structure inference target data,"The self-domain adaptation framework is structured in three main procedures to leverage target domain information at inference while addressing the limitations of traditional domain adaptation (DA) and domain generalization (DG). First, during the training step, the framework learns a discriminative feature extractor (F) and classifier (C) using available labeled source domains, and simultaneously learns a domain-specific adaptor (A) through a meta-learning based adaptor learning algorithm. This pre-training ensures the adaptor is well-initialized for efficient adaptation. Unlike traditional DA, this training phase does not require access to the target domain data. Second, at inference time, the framework uniquely leverages the unlabeled test domain data. All parameters of the feature extractor, classifier, depth estimator, and autoencoder are fixed. Only the pre-learned adaptor (A) is updated using the proposed unsupervised adaptor loss (LAdap) applied to the unlabeled test domain data. This step allows the model to fine-tune itself to the specific characteristics of the unseen target domain, overcoming the DG limitation of losing target-specific information. Finally, after the adaptor optimization is complete, the entire model (with the adapted adaptor) is fixed, and predictions are made on the test domain data. This two-stage approach (training the adaptor with meta-learning, then adapting it at inference) allows the framework to benefit from target domain information without requiring it during initial training, providing a practical solution for face anti-spoofing in real-world scenarios.","Self-domain adaptation framework, meta-learning adaptor training, inference-time adaptor update, fixed other parameters, leverages unlabeled target data.",,,,,"The challenge in face anti-spoofing is to develop methods that generalize robustly to unseen attacks and domains, particularly when target domain data is unavailable during training. This necessitates addressing how a domain-specific adaptor can be effectively learned and initialized during training to facilitate efficient adaptation to unseen target domains at inference. Furthermore, it requires identifying what unsupervised loss functions are effective for optimizing such a domain adaptor using only unlabeled target domain data at inference to improve face anti-spoofing performance. Ultimately, the core problem is how to structure a self-domain adaptation framework that can leverage target domain information at inference while overcoming the limitations of traditional domain adaptation, which requires target data during training, and domain generalization, which often loses target-specific information.","OULU-NPU (O): Face anti-spoofing dataset. Data type and domain: Mobile face presentation attack database with real-world variations. Composition or source: Contains videos captured with mobile phones. Size or scale: Not explicitly stated, but implied to be a public dataset. General application or purpose: Used for evaluating face anti-spoofing methods.
CASIA-MFSD (C): Face anti-spoofing dataset. Data type and domain: Multi-spectral face spoofing database. Composition or source: Contains videos of various spoofing attacks (e.g., photo, video replay). Size or scale: Not explicitly stated, but implied to be a public dataset. General application or purpose: Used for evaluating face anti-spoofing methods.
Idiap Replay-Attack (I): Face anti-spoofing dataset. Data type and domain: Face presentation attack database. Composition or source: Contains videos of replay attacks. Size or scale: Not explicitly stated, but implied to be a public dataset. General application or purpose: Used for evaluating face anti-spoofing methods.
MSU-MFSD (M): Face anti-spoofing dataset. Data type and domain: Multi-spectral face spoofing database. Composition or source: Contains videos of various spoofing attacks. Size or scale: Not explicitly stated, but implied to be a public dataset. General application or purpose: Used for evaluating face anti-spoofing methods.","The proposed method, """"Ours,"""" demonstrates superior performance compared to most state-of-the-art face anti-spoofing methods across four cross-dataset testing tasks.
When compared to traditional FAS methods (MS LBP, Binary CNN, IDA, CT, LBPTOP, Auxiliary) and DG methods (MMD-AAE, MADDG, RFM):
*   O&C&I to M: Ours achieves HTER of 17.3% and AUC of 88.1%. This is significantly better than most methods, with RFM being the closest (15.6% HTER, 84.4% AUC).
*   O&M&I to C: Ours achieves HTER of 16.4% and AUC of 93.9%. This is the best performance, outperforming RFM (23.1% HTER, 91.8% AUC).
*   O&C&M to I: Ours achieves HTER of 24.5% and AUC of 90.4%. This is the best performance, outperforming RFM (20.2% HTER, 90.1% AUC).
*   I&C&M to O: Ours achieves HTER of 15.4% and AUC of 84.3%. This is the best performance, outperforming RFM (13.8% HTER, 91.1% AUC).
The paper notes that while RFM also uses meta-learning, it focuses on domain-invariant features, whereas """"Ours"""" focuses on an adaptable component. The authors suggest their approach and RFM's are complementary.
When compared to related self-domain adaptation methods (AdapBN, FTTA, SDAN):
*   O&C&I to M: Ours (17.3% HTER, 88.1% AUC) significantly outperforms AdapBN (20.5% HTER, 72.0% AUC), FTTA (20.1% HTER, 71.2% AUC), and SDAN (17.7% HTER, 81.3% AUC).
*   O&M&I to C: Ours (16.4% HTER, 93.9% AUC) significantly outperforms AdapBN (34.5% HTER, 80.3% AUC), FTTA (35.0% HTER, 79.6% AUC), and SDAN (25.9% HTER, 84.2% AUC).
*   O&C&M to I: Ours (24.5% HTER, 90.4% AUC) significantly outperforms AdapBN (27.7% HTER, 88.0% AUC), FTTA (27.2% HTER, 88.0% AUC), and SDAN (28.2% HTER, 90.0% AUC).
*   I&C&M to O: Ours (15.4% HTER, 84.3% AUC) significantly outperforms AdapBN (28.2% HTER, 80.8% AUC), FTTA (28.3% HTER, 80.7% AUC), and SDAN (32.9% HTER, 75.0% AUC).
The results indicate that simply adjusting Batch Normalization parameters (AdapBN, FTTA) is insufficient, and even more complex adaptors (SDAN) are less effective without a well-initialized adaptor.
Ablation studies confirm the importance of each component:
*   Influence of unsupervised adaptor loss parts: Removing any part of the unsupervised adaptor loss (LAE, LOrth, LEnt) leads to degraded performance, validating the effectiveness of each component. For example, """"Ours wo/LAE"""" (without reconstruction) shows higher HTERs across all tasks compared to """"Ours.""""
*   Influence of each step of the method:
    *   """"Ours wo/meta"""" (randomly initialized adaptor, then optimized at inference) performs worse than """"Ours,"""" demonstrating the benefit of meta-learning for adaptor initialization.
    *   """"Ours wo/adapt"""" (using pre-learned adaptor without further optimization at inference) performs worse than """"Ours,"""" showing the importance of adapting the adaptor to the test domain.
    *   """"Baseline"""" (no adaptor, just source training) performs significantly worse than all variants of """"Ours,"""" highlighting the overall effectiveness of the proposed framework.
The paper also notes that the degree of improvement from adaptor optimization at inference varies depending on the domain shift; for small shifts, the pre-learned adaptor is already effective, while for significant shifts, further optimization is crucial. Visualizations using t-SNE show that after adaptation, features of fake and real faces in the target domain become more compact and separable.","Half Total Error Rate (HTER): Measures the average of the False Acceptance Rate (FAR) and False Rejection Rate (FRR). A lower HTER indicates better performance, representing a balance between incorrectly accepting spoof attacks and incorrectly rejecting legitimate users.
Area Under Curve (AUC): Measures the area under the Receiver Operating Characteristic (ROC) curve. A higher AUC indicates better overall discriminative performance of the model across all possible classification thresholds.","The proposed self-domain adaptation framework is designed to address the critical challenge of poor generalization in face anti-spoofing (FAS) to unseen domains, particularly when target domain data is unavailable during initial training. The core innovation lies in its ability to leverage unlabeled target domain information at inference time through a dynamically adaptable component. The framework comprises three main procedures: initial adaptor learning during training, unsupervised adaptor optimization at inference, and final prediction.
During the training phase, the framework learns a feature extractor, a classifier, and a depth estimator using labeled data from multiple source domains. Crucially, a novel adaptor, implemented as a 1x1 convolution with a residual connection to the classifier, is learned simultaneously. This adaptor is designed to capture domain-specific information and can be easily removed to revert to the base model. To ensure the adaptor is well-initialized for efficient adaptation, a meta-learning based algorithm is employed. This algorithm simulates the adaptation process by iteratively training on a """"meta-train"""" source domain and then optimizing the adaptor on a different """"meta-test"""" source domain using an unsupervised loss. This meta-learning step teaches the adaptor how to adapt effectively to new domains.
At inference time, the framework uniquely leverages the unlabeled test domain data. All parameters of the feature extractor, classifier, depth estimator, and autoencoder are fixed. Only the pre-learned adaptor is optimized using a specialized unsupervised adaptor loss. This composite loss function combines three objectives: minimizing reconstruction error (using an autoencoder trained on source data) to align target features with source distributions, minimizing entropy of predictions to encourage confident classifications, and applying orthogonality regularization to prevent feature mode collapse. This unsupervised optimization allows the model to fine-tune itself to the specific characteristics of the unseen target domain without requiring any labeled target data or access to source data. Finally, once the adaptor is optimized, the entire model is fixed, and predictions are made. This architecture overcomes the limitations of traditional domain adaptation (which requires target data during training) and domain generalization (which often loses target-specific information), providing a practical and effective solution for robust face anti-spoofing in real-world, unseen scenarios.","Although current face anti-spoofing methods achieve promising results under intra-dataset testing, they suffer from poor generalization to unseen attacks. Most existing works adopt domain adaptation (DA) or domain generalization (DG) techniques to address this problem. However, the target domain is often unknown during training which limits the utilization of DA methods. DG methods can conquer this by learning domain invariant features without seeing any target data. However, they fail in utilizing the information of target data. In this paper, we propose a self-domain adaptation framework to leverage the unlabeled test domain data at inference. Specifically, a domain adaptor is designed to adapt the model for test domain. In order to learn a better adaptor, a meta-learning based adaptor learning algorithm is proposed using the data of multiple source domains at the training step. At test time, the adaptor is updated using only the test domain data according to the proposed unsupervised adaptor loss to further improve the performance. Extensive experiments on four public datasets validate the effectiveness of the proposed method."
16652,Exploiting Diverse Characteristics and Adversarial Ambivalence for Domain Adaptive Segmentation,Bowen Cai; Huan Fu; Rongfei Jia; Binqiang Zhao; Hua Li; Yinghui Xu,2020,AAAI,AAAI Conference on Artificial Intelligence,4,"How can semantic segmentation models be robustly adapted to new target domains that comprise heterogeneous sub-domains with diverse characteristics, such as varying weather conditions?","The field of unsupervised domain adaptive segmentation primarily focuses on two main research directions: feature alignment and self-training. Early advancements in feature alignment, exemplified by methods like FCN in the wild, introduced adversarial learning to achieve global feature alignment and match category statistics between source and target domains. AdaptSegNet further refined this by modeling structured spatial similarities within the semantic output space. Subsequent research in this area has explored various strategies to learn domain-invariant features, including the use of conditional generators, task-specific decision boundaries, co-training approaches, and patch distribution matching techniques. Additionally, style transfer methods have been widely investigated to mitigate the domain gap, incorporating concepts such as semantic consistency, channel-wise feature alignment, symmetric adaptation consistency, and controllable translation degrees to enhance both style translation and the overall adaptation process.
The second major stream, self-training, has gained considerable attention for its potential in unsupervised domain adaptation. Initial works, such as CBST, proposed iterative class-balanced self-placed learning to generate pseudo-labels and subsequently retrain networks. More recent self-training methods have built upon this foundation by integrating confidence regularization, employing bidirectional adversarial translation alongside self-supervised learning, and utilizing category-wise centroids to improve pseudo-label quality and model performance. A common limitation observed across many existing self-training and feature alignment approaches is their tendency to assume uniform conditions within the target domain. This oversight can lead to models that are sensitive to the diverse characteristics inherently present in real-world images, such as variations in weather or lighting.
Beyond these two main streams, the paper also acknowledges related adaptation settings like multi-target domain adaptation (MTDA) and compound domain adaptation (CDA). MTDA aims to develop general domain-agnostic models for broadly unconstrained target domains, as seen in datasets like PACS which encompass diverse artistic styles. CDA algorithms, on the other hand, are designed to discover unknown sub-domains within a target domain. However, the current work distinguishes itself by focusing on scenarios where sub-domains within a single target domain are strongly correlated in content and their labels are readily available (e.g., BDD100K). The emphasis here is on explicitly modeling the correlations and complementarities among these multi-modalities, a setting where obtaining image-level sub-domain labels is considered inexpensive and can even be achieved through a condition classification network.","Module 1: Condition-Guided Style Transfer (CGST) - Generates realistic, condition-specific samples from synthetic images to bridge the domain gap in a condition-aware manner.
Module 2: Condition Attention Module (CAM) - Exploits the complementarities of multi-modalities by learning attention weights from condition-specific features to produce an enhanced feature representation.
Module 3: Condition-Specific Adversarial Training (CSAT) - Progressively aligns semantic feature distributions under each specific condition and mitigates global discrepancy using multiple discriminators.
Module 4: Adversarial Ambivalence - Utilizes dense discriminator outputs to identify and regulate easy and hard adaptation regions, revising pseudo-labels and applying targeted supervision.
Framework-level Innovation: The integration of condition-guided adaptation throughout both the attentive progressive adversarial training (APAT) and the self-training policy, enabling robust adaptation to heterogeneous target domains by explicitly reasoning about diverse characteristics and leveraging adversarial confidence for pseudo-label refinement.","How can synthetic source images be effectively transformed into realistic, condition-specific target-like images to facilitate domain adaptation in heterogeneous target environments?",Synthetic image condition-specific style transfer domain adaptation,"The paper addresses the challenge of transforming synthetic source images into realistic, condition-specific target-like images using a Condition Guided Style Transfer (CGST) module. This module learns a conditional generator, denoted as GST(XS;c), which maps source images (XS) to target-like images (XT) corresponding to a specific latent variable 'c' (representing conditions like T1 or T2). The CGST follows standard conditional Generative Adversarial Networks (GANs) principles, borrowing concepts from StarGAN and BicycleGAN for improved translation. A multi-level concatenation strategy injects the condition 'c' into GST as a translation guidance. Adversarial training enforces GST to produce realistic images with preferred styles, using a discriminator DT that distinguishes real target images from generated ones. An auxiliary condition classification loss (Lcls) further constrains the solution space by ensuring the generated images are correctly classified for their intended condition. To prevent semantic distortion or inconsistency during translation (e.g., trees becoming building materials), a semantic consistency constraint (Lsc) is incorporated, which uses a segmentation network (Fseg) to ensure that the translated image retains the semantic information of the original source image's ground truth label. The full objective for CGST combines these losses: LcGAN + Lcls + λscLsc.","CGST: Conditional GAN, multi-level concatenation, semantic consistency, condition classification loss.","In what way can the correlations and complementarities among multiple modalities (e.g., weather conditions) within a heterogeneous target domain be exploited to enhance semantic feature representation?",Multi-modality correlation exploitation semantic segmentation,"To exploit correlations and complementarities among multiple modalities within a heterogeneous target domain, the paper proposes a Condition Attention Module (CAM). After obtaining the stylized images from the Condition-Guided Style Transfer (CGST) module, denoted as ^XS = GST(XS;c), and using real target images XT, both are fed into a shared encoder (Enc). The encoder output is then processed by two individual decoders, DecC1 and DecC2, which capture condition-specific features fC1(x) and fC2(x) and probability predictions pC1(x) and pC2(x). Instead of simply using separate heads for each condition, CAM learns attention weights W from the concatenated condition-specific feature maps (fC1(x) and fC2(x)) using a convolutional block. These attention weights are then used to combine fC1(x) and fC2(x) into an enhanced feature 'f' (f = WC1 ⊙ fC1(x) + WC2 ⊙ fC2(x)). Finally, fC1(x), fC2(x), and 'f' are concatenated and fed into an additional parsing head (CA) to produce the final enhanced semantic feature map fCA(x) and probability predictions pCA(x). This mechanism allows the model to dynamically weigh and combine information from different condition-specific pathways, effectively leveraging their complementarities.","CAM: Shared encoder, condition-specific decoders, attention weights, enhanced feature parsing head.","How can adversarial training be structured to progressively align feature distributions by considering specific conditions within the target domain, rather than just global domain discrepancy?",Condition-specific progressive adversarial training domain adaptation,"The paper structures adversarial training to progressively align feature distributions by considering specific conditions within the target domain through Condition-Specific Adversarial Training (CSAT). Instead of a single domain classifier, CSAT employs three distinct discriminators: DC1, DC2, and DCA. DC1 and DC2 are condition-specific discriminators, each addressing a particular condition (e.g., T1 or T2). DCi operates on the stylized source images corresponding to condition 'i' (^XSi) and the real target images of condition 'i' (XTi), aiming to match their respective feature distributions (pCi(^XSi) and pCi(XTi)). DCA, on the other hand, targets transferring knowledge from the overall stylized source domain (^XS) to the overall target domain (XT), acting as a standard domain classifier. This multi-discriminator setup, where some discriminators focus on condition-specific alignment and one on global alignment, allows for a progressive reduction of the domain shift. The discriminators follow a PatchGAN fashion, and the adaptation is performed in a target-to-source direction to facilitate the later extraction of adversarial ambivalence. The encoder and decoders (DecC1, DecC2, CA) are optimized to fool these discriminators, while the discriminators are optimized to correctly classify the source of the features.","CSAT: Multiple discriminators, condition-specific alignment, global alignment, PatchGAN, target-to-source adaptation.",How can the reliability of pseudo-labels in self-training be improved by leveraging adversarial confidence to distinguish and adaptively handle easy versus hard adaptation regions?,Adversarial ambivalence pseudo-label refinement self-training,"The reliability of pseudo-labels in self-training is improved by leveraging adversarial confidence to distinguish and adaptively handle easy versus hard adaptation regions, a concept termed Adversarial Ambivalence. The method first obtains an initial adaptation model M0. For a given target image xt, M0 produces predictions pC1(xt), pC2(xt), and pCA(xt), along with attention weights W(xt). Attentive Pseudo-Label Assignment (APLA) generates initial pseudo-labels yt by combining these predictions, weighted by WCi(xt), and applying a confidence threshold (ηp) to filter out low-confidence pixels. The key innovation is using the dense output from the discriminator DCA (which follows PatchGAN) to obtain an """"adaptation confidence map"""" d(xt). This map indicates regions where the discriminator is uncertain, implying hard adaptation regions. The pseudo-labels are then refined by incorporating these pixel-wise confidence weights: the standard cross-entropy loss (LCE(xt)) is weighted by 1/d_hw(xt), effectively giving more weight to regions where the discriminator is less confident (i.e., harder regions). Additionally, for regions identified as """"hard adaptation regions"""" (where yt_hw(xt) = 0, meaning no pseudo-label was assigned due to low confidence), a separate adversarial loss (LT_adv(xt)) is applied. This loss encourages the network to push these hard adaptation regions into the shared feature space, preventing them from being ignored. This dual approach of weighting easy regions and applying adversarial training to hard regions effectively exploits the adversarial ambivalence to improve self-training.","Adversarial Ambivalence: Discriminator confidence map, pixel-wise weighting, hard region adversarial loss, pseudo-label refinement.","Adapting semantic segmentation models to new domains is challenging, particularly when the target domain exhibits heterogeneous sub-domains with diverse characteristics, such as varying weather conditions, which can lead to unsatisfactory performance. To address this, the paper investigates how synthetic source images can be effectively transformed into realistic, condition-specific target-like images to bridge the domain gap in a condition-aware manner. Furthermore, it explores in what way the correlations and complementarities among multiple modalities within such heterogeneous target domains can be exploited to enhance semantic feature representation. A key challenge is structuring adversarial training to progressively align feature distributions by considering specific conditions, rather than just global domain discrepancy. Finally, the paper tackles how the reliability of pseudo-labels in self-training can be improved by leveraging adversarial confidence to distinguish and adaptively handle easy versus hard adaptation regions, thereby mitigating the impact of noisy labels.","GTA5 (Grand Theft Auto V): Synthetic dataset used as a source domain. It contains 24,966 images with a resolution of 1914x1052 and 19 semantic categories consistent with Cityscapes.
SYNTHIA (SYNTHetic collection of Imagery and Annotations): Synthetic dataset used as a source domain. It offers 9,400 images of size 1914x1052.
Cityscapes: Real-world dataset used as a target domain, comprising several sub-domains:
Cityscapes-Cloudy: The standard Cityscapes dataset, providing 3,475 images (2,975 training, 500 validation) with a resolution of 2048x1024.
Cityscapes-Foggy: Created by simulating different levels of fog on Cityscapes-Cloudy images.
Cityscapes-Rainy: Constructed by attaching 36 degrees of rain to 262 training and 33 validation images from Cityscapes-Cloudy, resulting in 9,432 and 1,118 realistic rainy scenes respectively. The final dataset used for experiments includes 3,144 training images and 396 evaluation images.
Cityscapes(3): A combined target benchmark consisting of 9,094 training images (2975 from Cloudy + 2975 from Foggy + 3144 from Rainy) and 1,396
training images and 1,396 evaluation images.
BDD100K: Real-world dataset used as a target domain for heterogeneous multitask learning, with images under diverse weather conditions. The paper specifically uses BDD100K: {Rainy; Snowy; Cloudy} as target sub-domains during training, and evaluates on these plus an unseen BDD100K-Overcast domain.
Rainy: 4,855 training / 215 test images.
Snowy: 5,307 training / 242 test images.
Cloudy: 4,535 training / 346 test images.
Overcast: 627 images (unseen during training).","The proposed method, DCAA, demonstrates superior performance across various challenging adaptation scenarios.
General Performance:
*   GTA5 → Cityscapes(3): DCAA achieves a mean IoU (mIoU) of 51.4%, significantly outperforming the baseline """"Source Only"""" (30.6%) and yielding a 6.0% improvement over the state-of-the-art (SOTA) method CAG-UDA (45.4%). Other methods like OCDA and MDTA-ITA show limited improvements over AdaptSegNet, suggesting their focus on disentangling domain-style codes is less effective for pixel-level semantic segmentation. The paper notes that mIoU scores reported in previous publications for Cityscapes-Cloudy are consistently higher (about 5%) than the reproduced versions for Cityscapes(3), indicating that explicitly exploiting diverse weather conditions is crucial.
*   SYNTHIA → Cityscapes(3): DCAA achieves a remarkable mIoU of 47.4%, demonstrating appealing semantic parsing for target images.
*   GTA5 → BDD100K: DCAA shows consistently promising improvements (approximately 9.0%) across all target sub-domains (Rainy, Snowy, Cloudy) and even for the unseen """"Overcast"""" images. For instance, DCAA achieves an average mIoU of 38.3% across Rainy, Snowy, and Cloudy, compared to the """"Source Only"""" baseline of 20.2%. It also performs well on the unseen """"Overcast"""" domain with 34.6% mIoU.
Qualitative Results:
*   Qualitative comparisons in Figure 4 show that DCAA preserves more details in boundary areas and corrects incorrectly labeled regions compared to CAG-UDA, especially in images with adverse weather conditions.
Ablation Studies (GTA5 → Cityscapes(3)):
*   Condition Attention Module (CAM): CAM (47.2% mIoU) effectively exploits special characteristics and correlations of multi-modalities, outperforming other options like Mix-Head (46.0%), Sep-Head (45.3%), and SM-Head (45.6%), which sometimes even decrease performance.
*   Condition-Specific Adversarial Training (CSAT): CSAT-S2 (51.4% mIoU) significantly outperforms standard domain adversarial training (DAT-S2, 47.8% mIoU) and even the first stage of CSAT (CSAT-S1, 47.2% mIoU). This indicates that directly aligning source and target feature distributions is challenging for heterogeneous target domains, and condition-specific alignment is more effective.
*   Self-Training (Attentive Pseudo-Label Assignment (APLA) + Adversarial Ambivalence):
    *   Trivial pseudo-label generation strategies like MaxV (48.1% mIoU) and MeanV (48.7% mIoU) show only marginal improvements (0.7% to 1.3%) over the baseline (47.2% mIoU).
    *   APLA+ (49.8% mIoU) yields a significant improvement (+2.7%) over the baseline, demonstrating its effectiveness in exploiting multi-modal semantics.
    *   The revised LCE (50.6% mIoU) outperforms the baseline LCE (48.3% mIoU) by a convincing margin (+2.3%) when pseudo-labels are noisy.
    *   The proposed adversarial training for hard adaptation regions (LT_adv), combined with the revised LCE, further boosts the mIoU to 51.4%. This confirms that adversarial ambivalence effectively suppresses the adverse impact of noisy pseudo-labels and drives the network to focus on hard adaptation regions.","Mean Intersection over Union (mIoU): This is the primary metric used for semantic segmentation. It measures the average of the Intersection over Union (IoU) for all semantic categories. IoU for a given class is calculated as the area of overlap between the predicted segmentation and the ground truth, divided by the area of union between the predicted segmentation and the ground truth. A higher mIoU indicates better segmentation performance.","The DCAA (Diverse Characteristics and Adversarial Ambivalence) framework is designed to address the challenging problem of adapting semantic segmentation models to target domains that exhibit diverse characteristics, such as varying weather conditions. The core motivation stems from the observation that existing unsupervised domain adaptation (UDA) methods often assume homogeneous target domains, leading to unsatisfactory performance on real-world datasets with heterogeneous sub-domains. DCAA proposes a novel condition-guided adaptation framework that explicitly reasons about multiple modalities within the target domain to improve robustness.
The framework is empowered by two main components: an Attentive Progressive Adversarial Training (APAT) mechanism and a novel self-training policy. APAT progressively mitigates source-target discrepancies by incorporating condition-specific guidance. This begins with a Condition Guided Style Transfer (CGST) module, which generates realistic, condition-specific samples from synthetic images, ensuring that the translated images maintain semantic consistency. Following this, a Condition Attention Module (CAM) is introduced to effectively exploit the complementarities of multi-modalities by learning attention weights that combine features from different condition-specific decoders, leading to an enhanced feature representation. Finally, Condition-Specific Adversarial Training (CSAT) employs multiple discriminators—some focusing on condition-specific alignment and one on global alignment—to progressively align semantic feature distributions, which is more effective than direct global alignment for heterogeneous domains.
The self-training policy in DCAA focuses on generating robust pseudo-labels and leveraging adversarial ambivalence. It first uses Attentive Pseudo-Label Assignment (APLA) to produce initial pseudo-labels for target images, benefiting from the condition-aware features generated by APAT. Crucially, DCAA recognizes that pseudo-labels can be noisy, especially in hard adaptation regions. To address this, it exploits the """"adversarial ambivalence"""" derived from the dense outputs of the discriminators. This adversarial confidence map is used to revise the pseudo-labels by weighting the supervision for easy adaptation regions and applying a specific adversarial loss to push hard adaptation regions into the shared feature space. This unique integration of condition-guided adversarial training with a confidence-aware self-training scheme allows DCAA to effectively handle diverse characteristics in the target domain, leading to superior adaptation performance compared to existing methods.","Adapting semantic segmentation models to new domains is an important but challenging problem. Recently enlightening progress has been made, but the performance of existing methods is unsatisfactory on real datasets where the new target domain comprises of heterogeneous sub-domains (e.g. diverse weather characteristics). We point out that carefully reasoning about the multiple modalities in the target domain can improve the robustness of adaptation models. To this end, we propose a condition-guided adaptation framework that is empowered by a special attentive progressive adversarial training (APAT) mechanism and a novel self-training policy. The APAT strategy progressively performs condition-specific alignment and attentive global feature matching. The new self-training scheme exploits the adversarial ambivalences of easy and hard adaptation regions and the correlations among target sub-domains effectively. We evaluate our method (DCAA) on various adaptation scenarios where the target images vary in weather conditions. The comparisons against baselines and the state-of-the-art approaches demonstrate the superiority of DCAA over the competitors."
16653,Time Series Domain Adaptation via Sparse Associative Structure Alignment,Ruichu Cai; Jiawei Chen; Zijian Li; Wei Chen; Keli Zhang; Junjian Ye; Zhuozhang Li; Xiaoyan Yang; Zhenjie Zhang,2020,AAAI,AAAI Conference on Artificial Intelligence,98,"How can domain-invariant representations be extracted from time series data for domain adaptation, given the complex dependencies among timestamps, including varying time lags and offsets, which hinder the discovery of stable underlying structures?","Unsupervised domain adaptation methods primarily aim to extract domain-invariant representations between source and target domains. A prominent technique involves using Maximum Mean Discrepancy (MMD) to reduce domain discrepancy in kernel-reproducing Hilbert spaces. Researchers have applied MMD-based approaches, sometimes incorporating second-order statistics or optimal multi-kernel selection, to align distributions across domains. Another significant approach leverages generative adversarial networks (GANs) to learn domain-invariant features by training a feature extractor to """"fool"""" a domain classifier. This adversarial training paradigm has been extended to unified frameworks for domain adaptation, with some recent efforts focusing on fine-grained alignment, such as aligning class centroids using pseudo-labels to prevent false alignment.
In the context of causality, domain adaptation scenarios can be determined by the causal mechanism, with discussions around target shift, condition shift, and generalized target shift. Some works explore disentangled semantic representation in recovered latent spaces based on causal models of data generation.
However, applying these general domain adaptation techniques directly to time series data presents unique challenges. Existing methods for time series domain adaptation are limited and often involve straightforward extensions of non-time series approaches, typically employing Recurrent Neural Network (RNN)-based feature extractors. These methods implicitly assume that conditional distributions are equal across domains, an assumption that is difficult to satisfy in time series data due to the complex temporal dependencies. Small discrepancies like varying time lags and offsets can significantly impede the extraction of domain-invariant representations. Furthermore, for multivariate time series, variables are not always independently and identically distributed (i.i.d.), and existing methods often ignore the associative structure among variables, which can lead to overfitting. The challenge of constructing associative structures in time series data, particularly in the presence of time lags and offsets, remains a critical open problem.","Module 1: Adaptive Segment Summarization - Generates multiple candidate segments for each variable and uses variable-specific LSTMs to obtain segment representations, addressing the obstacle of offsets and varying segment durations.
Module 2: Sparse Associative Structure Discovery - Devises intra-variables and inter-variables attention mechanisms to extract the sparse associative structure among variables, specifically considering time lags in the inter-variables attention.
Module 3: Sparse Associative Structure Alignment - Aligns the extracted sparse associative structures between source and target domains using Maximum Mean Discrepancy (MMD), guiding knowledge transfer and extracting domain-invariant information.",How can time series data be effectively segmented and summarized to mitigate the impact of varying offsets across different domains?,Time series segmentation offsets mitigation,"To address the challenge of varying offsets in time series data, the paper proposes an Adaptive Segment Summarization module. This module first constructs multiple candidate segments of different lengths for each univariate time series `xi`. Specifically, for a given time series `xi`, a set of segments `fxi = {xi_t-N+1:t;...;xi_t-delta+1:t;...;xi_t:t;xi_t-N+1:t}` is generated, where `delta` represents the segment length. An independent Long Short-Term Memory (LSTM) network, parameterized by `theta_i`, is allocated for each variable `xi`. Each segment `xi_t-delta+1:t` is fed into its corresponding variable-specific LSTM to obtain a segment representation `h_i_delta`. This process results in a set of segment representations `h_i = {h_i_1;...;h_i_delta;...;h_i_N}` for each variable.
Subsequently, a Segments Representation Selection via Intra-Variables Attention Mechanism is employed to identify the most suitable segment representations. A self-attention mechanism is used to calculate weights `alpha_i_delta` for each segment representation `h_i_delta`. These weights are computed using trainable projection parameters `W_Q` (query) and `W_K` (key) and a scaling factor `dh`, and then normalized using `sparsemax` to ensure sparsity. The weighted segment representation `Z_i` for variable `xi` is then obtained by summing the product of `alpha_i_delta` and `h_i_delta` over all possible segment lengths. To further mitigate the obstacle of offsets and ensure that the duration of segments from different domains is similar, the Maximum Mean Discrepancy (MMD) between the `alpha` weights from the source and target domains (`L_alpha = MMD(alpha_S, alpha_T)`) is minimized. This loss function restricts the segment duration distributions to be similar across domains, aiding in the extraction of transferable structures.","Adaptive Segment Summarization, variable-specific LSTM, intra-variables attention, sparsemax, MMD segment length restriction.","In what manner can sparse associative structures among variables in time series data be discovered, while explicitly accounting for time lags?",Sparse associative structure discovery time lags,"The paper addresses the discovery of sparse associative structures among variables, while considering time lags, through a Sparse Associative Structure Reconstruction via Inter-variables Attention Mechanism. After obtaining the weighted segment representations `Z_i` for each variable `i` (as described in Solution 1), the goal is to model the association between variable `i` and variable `j`. Unlike a simple dot product that ignores time lags, the method calculates the degree of association between variable `i` and variable `j` by considering the segment representations `h_j_delta` of variable `j` at different time lags `delta`.
Specifically, the degree of association `e_ij_delta` between variable `i` and variable `j` at a specific time lag `delta` is computed as the normalized dot product between the weighted segment representation of variable `i` (`Z_i`) and the segment representation of variable `j` at that lag (`h_j_delta`). This is formulated as `e_ij_delta = (Z_i * h_j_delta) / (||Z_i|| * ||h_j_delta||)`. This calculation is performed for all relevant time lags `delta`. These degrees of association `e_ij_delta` are then normalized across all variables `j` and time lags `delta` using `sparsemax` to obtain the associative strength `beta_ij_delta`. The `beta_ij_delta` represents the associative strength between variable `i` and variable `j` with regard to a specific segment duration `delta`. This mechanism allows the model to capture the influence of one variable on another at different temporal offsets, thereby explicitly accounting for time lags in the discovered associative structure.","Inter-variables attention, weighted segment representation, segment representation dot product, sparsemax normalization, associative strength.",How can the learned sparse associative structures be aligned across source and target domains to facilitate knowledge transfer and achieve domain-invariant representation?,Sparse associative structure alignment MMD,"To facilitate knowledge transfer and extract domain-invariant information, the paper proposes a Sparse Associative Structure Alignment mechanism. This mechanism focuses on aligning the sparse associative structures learned from the source and target domains. Since the associative strength `beta_ij_delta` (representing the association between variable `i` and variable `j` at a specific segment duration `delta`) can be viewed as a distribution, the problem of measuring structure distance is transformed into measuring distribution distance.
The method employs Maximum Mean Discrepancy (MMD) to align these associative structure distributions. The objective is to minimize the MMD between the `beta` distributions from the source domain (`beta_S`) and the target domain (`beta_T`). This is formulated as the loss function `L_beta = MMD(beta_S, beta_T)`. By minimizing this `L_beta` loss, the model restricts the associative structures to be similar across domains, effectively guiding the transfer of knowledge. This approach differs from traditional domain adaptation methods that align features directly; instead, it aligns the underlying sparse associative structures, which are assumed to be more stable and domain-invariant due to their inspiration from causal mechanisms. The overall objective function for the model combines this `L_beta` with the label prediction loss (`L_y`) and the segment length restriction loss (`L_alpha`) as `L = L_y + lambda * (L_alpha + L_beta)`, where `lambda` is a hyper-parameter. This joint optimization ensures that the model learns domain-invariant representations by aligning the fundamental associative structures.","MMD, sparse associative strength distribution, source-target alignment, joint loss optimization.",,,,,"Domain adaptation for time series data is challenging due to the complex dependencies among timestamps, where even minor changes in time lags or offsets can impede the extraction of domain-invariant information. To address this, a key challenge is how to effectively segment and summarize time series data to mitigate the impact of varying offsets across different domains. Furthermore, it is critical to determine in what manner sparse associative structures among variables can be discovered, while explicitly accounting for these inherent time lags. Finally, a crucial aspect involves how these learned sparse associative structures can be aligned across source and target domains to facilitate robust knowledge transfer and achieve truly domain-invariant representation.","Boiler Fault Detection Dataset:
Data type and domain: Sensor data from industrial boilers.
Composition or source: Collected from three boilers between March 24, 2014, and November 30, 2016. Each boiler is treated as a distinct domain.
Size or scale: Not explicitly stated in terms of number of samples, but total parameters for the """"Ours"""" method are 82924.
General application or purpose: To predict the faulty blowdown valve of each boiler, addressing the rarity of fault data by utilizing labeled source data and unlabeled target data.
Air Quality Forecast Dataset:
Data type and domain: Environmental data, specifically air quality and meteorological data.
Composition or source: Collected from the Urban Air project from May 1, 2014, to April 30, 2015. It covers four major Chinese cities: Beijing (B), Tianjin (T), Guangzhou (G), and Shenzhen (S). Each city is considered a domain.
Size or scale: Not explicitly stated in terms of number of samples, but total parameters for the """"Ours"""" method are 45636.
General application or purpose: To predict PM2.5 levels using air quality and meteorological data, chosen due to commonality of air quality data, complex causality among sensors in smart city systems, and sparse associations among sensors.
In-hospital Mortality Prediction Dataset (MIMIC-III):
Data type and domain: De-identified health-related data from critical care units.
Composition or source: Contains data associated with over forty thousand patients who stayed in Beth Israel Deaconess Medical Center between 2001 and 2012. The paper uses 12 time series variables (e.g., Heart Rate, Temperature, Systolic blood pressure) from 35637 records. Patients are split into four age groups (20-45, 46-65, 66-85, >85), with each group serving as a domain.
Size or scale: Total parameters for the """"Ours"""" method are 71402.
General application or purpose: To predict in-hospital mortality, serving as a benchmark for time series domain adaptation.","The proposed Sparse Associative Structure Alignment (SASA) model consistently outperforms state-of-the-art baselines across all three real-world datasets.
On the Boiler Fault Detection Dataset, SASA achieves significantly higher AUC scores compared to other methods. For instance, in the 1 -> 2 task, SASA scores 71.54%, an improvement of 3.95 points over VRADA (67.59%). Similarly, for the 3 -> 2 task, SASA reaches 96.39%, a 2.56 point increase from VRADA (93.83%). Even on tasks where baselines perform well (e.g., 1 -> 3, 2 -> 3, 3 -> 1), SASA maintains comparable or superior results. A Wilcoxon signed-rank test yields a p-value of 0.027, indicating statistically significant outperformance over baselines.
For the Air Quality Forecast Dataset, SASA demonstrates superior performance in terms of RMSE. For example, in the B -> T task, SASA achieves an RMSE of 13.38, outperforming VRADA's 13.68. The model shows better performance between geographically closer cities (e.g., Beijing to Tianjin) due to shared associative structures, but still achieves the best results even for distant city pairs (e.g., Beijing to Shenzhen). The performance for Shenzhen as a target domain is noted to be less notable, attributed to lower label values in that domain. A Wilcoxon signed-rank test results in a p-value of 0.002, confirming statistical significance.
On the In-hospital Mortality Prediction Dataset (MIMIC-III), SASA again surpasses all comparison methods in AUC score. For tasks like 2 -> 1, SASA achieves 85.03%, an improvement of 2.91 points over VRADA (82.12%). In the 3 -> 2 task, SASA scores 85.20%, a 2.11 point increase from VRADA (83.09%). Performance is observed to be better between similar age groups (domains). A Wilcoxon signed-rank test yields a p-value of 0.0022, indicating statistically significant superiority.
Ablation Studies:
*   SASA-(alpha): Removing the segment length restriction loss (`L_alpha`) leads to a performance drop. This confirms that aligning the probability of segment lengths across domains is crucial for excluding the influence of domain-specific segment durations.
*   SASA-(beta): Removing the sparse associative structure alignment loss (`L_beta`) also results in a performance decrease compared to the full SASA model. This highlights the importance of aligning the associative structures themselves. Despite the drop, SASA-(beta) still outperforms other baselines, suggesting that the `L_alpha` component (aligning offsets) provides a foundational benefit for extracting associative structures.
Visualization:
Visualization of the correlation structure adjacent matrix for the Air Quality dataset (Beijing -> Shenzhen) reveals that the associative structures are sparse and that source and target domains share many common associative relationships, validating the model's ability to extract and align stable, sparse structures.","Root Mean Squared Error (RMSE): Used for regression problems (e.g., Air Quality Forecast). It measures the average magnitude of the errors between predicted and actual values.
Area Under the Receiver Operating Characteristic Curve (AUC): Used for classification problems (e.g., Boiler Fault Detection, In-hospital Mortality Prediction). It measures the overall performance of a binary classifier across all possible classification thresholds.","The Sparse Associative Structure Alignment (SASA) framework is designed to tackle the challenging problem of unsupervised domain adaptation for time series data, particularly addressing the complexities introduced by varying time lags and offsets across different domains. The core motivation is to leverage the inherent stability of causal mechanisms by relaxing the problem to discovering and aligning sparse associative structures, which are assumed to be domain-invariant.
The framework comprises three main functional modules. First, the Adaptive Segment Summarization module addresses the obstacle of offsets. It generates multiple candidate segments of varying lengths for each univariate time series and processes them using variable-specific Long Short-Term Memory (LSTM) networks to obtain segment representations. To select the most relevant segments and mitigate offset noise, an intra-variables attention mechanism is applied, which assigns sparse weights to these segment representations. Crucially, a Maximum Mean Discrepancy (MMD) loss (`L_alpha`) is introduced to align the distributions of segment lengths between source and target domains, ensuring that the model focuses on comparable temporal patterns.
Second, the Sparse Associative Structure Discovery module focuses on reconstructing the associative relationships among different variables while explicitly accounting for time lags. This is achieved through an inter-variables attention mechanism. Unlike simple correlation, this mechanism calculates the degree of association between variables by considering the segment representations of one variable at different time lags relative to another. The resulting degrees of association are then normalized using sparsemax to yield sparse associative strengths (`beta`).
Finally, the Sparse Associative Structure Alignment module guides the transfer of knowledge. Instead of aligning raw feature representations, SASA aligns these learned sparse associative structures between the source and target domains. This alignment is performed by minimizing the MMD between the `beta` distributions of the source and target domains (`L_beta`). This unique strategy ensures that the model learns domain-invariant representations by focusing on the underlying structural relationships, which are more robust to domain shifts. The overall model is optimized by minimizing a combined loss function that includes a task-specific label prediction loss (`L_y`), the `L_alpha` for segment length alignment, and the `L_beta` for associative structure alignment. This comprehensive approach allows SASA to effectively distill domain-invariant information from complex time series data, leading to improved generalization across domains.","Domain adaptation on time series data is an important but challenging task. Most of the existing works in this area are based on the learning of the domain-invariant representation of the data with the help of restrictions like MMD. However, such extraction of the domain-invariant representation is a non-trivial task for time series data, due to the complex dependence among the timestamps. In detail, in the fully dependent time series, a small change of the time lags or the offsets may lead to difficulty in the domain invariant extraction. Fortunately, the stability of the causality inspired us to explore the domain invariant structure of the data. To reduce the difficulty in the discovery of causal structure, we relax it to the sparse associative structure and propose a novel sparse associative structure alignment model for domain adaptation. First, we generate the segment set to exclude the obstacle of offsets. Second, the intra-variables and inter-variables sparse attention mechanisms are devised to extract associative structure time-series data with considering time lags. Finally, the associative structure alignment is used to guide the transfer of knowledge from the source domain to the target one. Experimental studies not only verify the good performance of our methods on three real-world datasets but also provide some insightful discoveries on the transferred knowledge."
